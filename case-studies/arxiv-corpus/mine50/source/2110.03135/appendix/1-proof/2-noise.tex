% ------------------------------------------
% \subsection{Proofs and remarks for the existence of label noise}
\subsection{Proofs in Section~\ref{sect:reason-true}}
\label{sect:label-noise-more-proof}


\smallsection{Proof of Lemma~\ref{theorem:distribution-mismatch-true-model}}

\begin{proof}
For simplicity, we consider the adversarial perturbation generated by FGSM. Other adversarial perturbation can be viewed as a Taylor series of such perturbation.

    \begin{equation}
        \delta = -\varepsilon  \frac{\nabla~f(x)_y}{\|\nabla~ f(x)_y\|},  
    \end{equation}

First, we bound the distribution mismatch by gradient norm.
$$
    \begin{aligned}
    \|P(Y|x) - P(Y'|x')\|_{\text{TV}}
    & = \frac{1}{2} \sum_j \left|P(Y=j|x) - P(Y'=j|x')\right|\quad \boxed{\text{TV distance}}\\
    & \ge \frac{1}{2} \left|P(Y=y|x) - P(Y'=y|x')\right|\\
    & = \frac{1}{2} \left|f(x)_{y} - f(x')_{y}\right| \\
    & = \frac{1}{2} \left[ -\nabla f(x)_{y} \cdot \delta - \frac{1}{2}\delta^T \nabla^2 f(z)_{y} \delta \right]\\
    & \ge \frac{1}{2} \left[-\nabla f(x)_{y} \cdot \delta - \frac{\sigma_M}{2}\|\delta\|_2^2\right]  \quad \boxed{\text{Bounded Hessian}} \\ % \boxed{\text{Local convexity}} \\
    & \ge \frac{1}{2} \left[\varepsilon \frac{\|\nabla f(x)_{y}\|^2_2}{\|\nabla f(x)_{y}\|} - \frac{\sigma_M}{2} \varepsilon^2 \frac{\|\nabla f(x)_{y}\|^2_2}{\|\nabla f(x)_{y}\|^2}  \right].\\
    \end{aligned}
$$
Now if $\|\cdot\| = \|\cdot\|_2$, we have
\begin{equation}
  \label{eq:mismatch-bound-l2}
  \|P(Y|x) - P(Y'|x')\|_{\text{TV}} \ge \frac{1}{2} \left[\varepsilon \|\nabla f(x)_{y}\|_2 - \frac{\sigma_M}{2}\varepsilon^2\right].    
\end{equation}
If $\|\cdot\| = \|\cdot\|_\infty$, we can utilize the fact that $\|\cdot\|_\infty \le \|\cdot\|_2 \le \sqrt{d} \|\cdot\|_\infty$,
thus 
\begin{equation}
    \label{eq:mismatch-bound-linf}
    \|P(Y|x) - P(Y'|x')\|_{\text{TV}} \ge \frac{1}{2} \left[\varepsilon \|\nabla f(x)_{y}\|_\infty - \frac{\sigma_M}{2} \varepsilon^2 \sqrt{d}\right].
\end{equation}
    
Second, we bound the gradient norm by the $L$-local Lipschitzness assumption.
    % \chengyu{Implicit assumption, there is a $x^*$ within the $\varepsilon$ ball of $x$ where $f$ is Lipschitz.}     
    Let $x^*$ be a closest input that achieves the local maximum on the predicted probability at $y$, namely $x^* = \argmin_{z \in X, f(z)_y = 1} \|x - z\|$. Because $x^*$ is the local maximum and $f$ is continuously differentiable, $\nabla f(x^*)_y = 0$, thus
    $$
    \nabla f(x)_y 
    = \nabla f(x^*)_y  + \nabla^2 f(z)_y  (x - x^*) = \nabla^2 f(z)_y  (x - x^*).
    $$
    Therefore we have
    $$
    \begin{aligned}
    \|\nabla f(x)_y \| 
    & =  \|\nabla^2 f(z)_y  (x - x^*) \| \\
    & \ge \sigma_{m} \|x - x^*\| \\
    & \ge \sigma_{m}  \frac{|f(x^*)_y  - f(x)_y |}{L} \\
    & = \frac{\sigma_{m}}{L} (1 - f(x)_y). \\
    % & \ge \frac{\sigma_{m} }{L(f)} |1 - q(x)| \\
    \end{aligned}
    $$
Plug this into Equation~(\ref{eq:mismatch-bound-l2}) or Equation~(\ref{eq:mismatch-bound-linf}) we then obtain the desired result.
\end{proof}




\smallsection{Proof of Lemma~\ref{theorem:implicit-label-noise}}
    \begin{proof}
    
    First, we show that the expectation of the label error is lower bounded by the mismatch between the true label distribution and the assigned label distribution.
    \begin{equation}
    % \small
    \begin{aligned}
    \|P(\tilde{Y}|x) - P(Y|x)\|_{TV} 
    & = \frac{1}{2} \sum_j |P(\tilde{Y}=j|x) - P(Y=j|x)|  \\
    & = \frac{1}{2} \sum_j |P(\tilde{Y}=j, Y=j|x) + P(\tilde{Y}=j, Y\ne j|x) \\
    & \quad - P(Y=j, \tilde{Y}=j|x)- P(Y=j, \tilde{Y}\ne j|x)| \\
    & =  \frac{1}{2} \sum_j | P(\tilde{Y}=j, Y\ne j|x) - P(Y=j, \tilde{Y}\ne j|x)| \\
    & \le \frac{1}{2} \sum_j  P(\tilde{Y}=j, Y\ne j|x) + P(Y=j, \tilde{Y}\ne j|x) \\
    & = P(Y’\ne Y | x) \\
    & = P(E=1|x)\\
    \end{aligned}
    \end{equation}
    
    Second, given a sampled training set $\mathcal{D}=\{(x_i, \tilde{y}_i)\}_{i\in [N]}$, the empirical measure of label error $E$ should converge to its expectation almost surely, namely
    $$
    \lim_{N\to \infty} p_e(\mathcal{D}) = \lim_{N\to \infty} \frac{1}{N} \sum_{i\in [N]} e_i = \mathbb{E} [E] = P(E = 1).
    $$
    Using standard concentration inequality such as Hoeffding's inequality we have, 
    % $$
    % P(|p_e(\mathcal{D}) - P(E = 1) \le \epsilon|) \ge 1 - 2 \exp\left(-2N\epsilon^2\right),
    % $$
    % which means 
    with probability $ 1 - \delta$,
    $$
    |p_e(\mathcal{D}) - P(E = 1)| \le \sqrt{\frac{1}{2N}\log\frac{2}{\delta}}.
    $$
    This implies
    $$
    p_e(\mathcal{D}) \ge P(E=1) -  \sqrt{\frac{1}{2N}\log\frac{2}{\delta}}.    
    $$
    
    Since $P(E = 1) = \mathbb{E}_x P(E=1|x)$, we have, with probability $1 - \delta$,
    $$
    p_e(\mathcal{D}) \ge \mathbb{E}_x \|P(\tilde{Y}| x) - P(Y | x)\|_{\text{TV}} -\sqrt{\frac{1}{2N}\log\frac{2}{\delta}} .
    $$
    which means $p_e(\mathcal{D}) > 0$ as long as $N$ is large.
    
    \end{proof}
    
    
    
    
    
    
\smallsection{Proof of Theorem~\ref{theo:main}}
    \begin{proof}
    First, by the fact that $P(\tilde{Y}'| x') = P(\tilde{Y} | x)$ and $P(\tilde{Y} | x) = P(Y | x)$ we have $P(\tilde{Y}'| x') =  P(Y | x)$.
    
    Therefore, apply Lemma~\ref{theorem:implicit-label-noise} to an adversarially augmented training set we have with probability $1 - \delta$,
    $$
    \begin{aligned}
    p_e(\mathcal{D'}) 
    & \ge \mathbb{E}_x \|P(\tilde{Y}'| x') - P(Y' | x')\|_{\text{TV}} -\sqrt{\frac{1}{2N}\log\frac{2}{\delta}} \\
    & \ge \mathbb{E}_x \|P(Y | x) - P(Y' | x')\|_{\text{TV}} -\sqrt{\frac{1}{2N}\log\frac{2}{\delta}}. \\
    \end{aligned}
    $$
    Further, apply Lemma~\ref{theorem:distribution-mismatch-true-model} and the definition of data quality, we have with probability $1 - \delta$,
    $$
    \begin{aligned}
    p_e(\mathcal{D'}) 
    & \ge \frac{\varepsilon}{2} (1 - \mathbb{E}_x f(x)_y) \frac{\sigma_m}{L}  - \frac{\varepsilon^2}{4} \sigma_M - \sqrt{\frac{1}{2N}\log\frac{2}{\delta}} \\
    & \ge \frac{\varepsilon}{2} (1 - q(\mathcal{D})) \frac{\sigma_m}{L}  - \frac{\varepsilon^2}{4} \sigma_M - \sqrt{\frac{1}{2N}\log\frac{2}{\delta}}. \\    
    \end{aligned}
    $$
    % $$
    % \begin{aligned}
    % \mathbb{E}_{y'} P(\tilde{Y’}\ne Y' | Y'=y', x')
    % & \ge
    % \|P(\tilde{Y’}|x') - P(Y’|x')\|_{TV} \\
    % & = \|P(\tilde{Y’}|x') - P(\tilde{Y}|x) + P(\tilde{Y}|x) - P(Y|x) + P(Y|x) - P(Y’|x')\|_{TV} \\
    % & = \|P(Y|x) - P(Y’|x')\|_{TV} \\
    % \end{aligned}
    % $$
    
    \end{proof}
    

    