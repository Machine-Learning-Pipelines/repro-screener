% ------------------------------------------------------
% \subsection{Proofs for mitigating double descent}
\subsection{Proofs in Section~\ref{sect:mitigate-double-descent}}
% \smallsection{Proof of Theorem~\ref{theorem: loss-error}}
% \begin{proof}
%       $$
%       \begin{aligned}
%         L & = \text{CE}(\tilde{p} \|  f(x; \theta) ) \\
%           & = - \sum_j \tilde{p}_j \log f(x; \theta)_j\\
%           & = - \sum_j p_j \log f(x; \theta)_j + \sum_j (p_j - \tilde{p}_j) \log f(x; \theta)_j\\
%           & = \text{CE}(p \|f(x; \theta) ) + \left|\sum_j (p_j - \tilde{p}_j) \log f(x; \theta)_j\right|\\
%           & \le \text{CE}(p \|f(x; \theta) ) + \left\|p - \tilde{p}\right\|_2 \left\|\log f(x; \theta)\right\|_2\\
%           & \le \text{CE}(p \|f(x; \theta) ) + \left\|p - \tilde{p}\right\|_1 \left\|\log f(x; \theta)\right\|_1\\
%           & = \text{CE}(p \|f(x; \theta) ) + 2 \left\|p - \tilde{p}\right\|_{TV} \left\|\log f(x; \theta)\right\|_1\\
%       \end{aligned}
%       $$
% \end{proof}


\smallsection{Proof of Theorem~\ref{theorem: model-probability}}
\begin{proof}


Let $j^* = \text{argmax}~P(Y'=j|x')$ and thus $P(Y'=j^*|x') \in [1/c,1]$. Let $g(T) := f(x'; \theta, T)_{j^*}$, which is a continuous function defined on $[0, \infty]$. The condition $j^* = \argmax_j f(x'; \theta, T)_j$ ensures that $g(T) \in [1/c, 1]$, where $c$ is the number of classes. By the intermediate value theorem, there exists $T^*$, such that $g(T^*) = P(Y'=j^*|x')$.

Let $T = T^*$, we have
$$
\begin{aligned}
\|f(x'; \theta, T) - P(Y' | x')\|_{TV} 
& = \frac{1}{2} \sum_j \left|f(x'; \theta, T)_j - P(Y' =j | x')\right|\\
& = \frac{1}{2} \sum_{j, j\ne j^*} \left|f(x'; \theta, T)_j - P(Y' = j| x')\right|\\
& \le \frac{1}{2} \left[ \sum_{j, j\ne j^*} f(x'; \theta, T)_j + \sum_{j, j\ne j^*} P(Y' = j| x') \right]\\
& = 1 - P(Y'=j^*|x'),\\
\end{aligned}
$$
where the inequality holds by the triangle inequality.

% \note{First let $P(Y'|x') = P(y|x) \approx \mathbbm{1}(y)$. This is our assumption. But the approx here would be a problem, we need exactly one-hot. Since this theorem targets correctly classified examples, it is very likely the examples are one-hot.}

Meanwhile, we have
$$
\begin{aligned}
\|P(\tilde{Y}' | x') - P(Y' | x')\|_{TV} 
& = \|P(Y | x) - P(Y' | x')\|_{TV}\\ 
& = \|\mathbbm{1}(y) - P(Y' | x')\|_{TV} \\
& = \frac{1}{2} \left[1 - P(Y' = y| x') + \sum_{j,j\ne \hat{y}} P(Y' = y| x')\right]\\
& = 1 - P(Y' = y| x')\\
& \ge 1 - P(Y' = j^* | x').\\
\end{aligned}
$$
Therefore, it can seen that for $T=T^*$,
$$
\| f(x'; \theta, T) - P(Y' | x') \|_{TV} \le \|  P(\tilde{Y}' | x') - P(Y' | x')\|_{TV}.
$$
\end{proof}





\smallsection{Proof of Theorem~\ref{theorem: model-probability-coefficient}}

\begin{lemma}
\label{lemma: model-probability-coefficient}
Let $x'$ be an example incorrectly classified by a classifier $f$ in terms of the true label distribution $P(Y'=j|x')$, namely
$$
\argmax_j~f(x'; \theta, T)_j \ne j^*,
$$
where $j^* = \argmax_j ~P(Y'=j|x')$. Assume $P(Y' = j^* | x') \ge 1/2$,
then 
$$ f(x'; \theta, T)_{j^*}  \le P(Y' = j^* | x').$$
\end{lemma}

\begin{proof}
We prove it by contradiction.
Assume $f(x'; \theta, T)_{j^*}  > P(Y' = j^* | x')$, we have $f(x'; \theta, T)_{j^*} > P(Y' = j^* | x') \ge 1/2$. Therefore,
$$
f(x'; \theta, T)_j \le \sum_{j, j\ne j^*} f(x'; \theta, T)_j = 1 - f(x'; \theta, T)_{j^*} < 1/2, ~ \forall j \ne j^*,
$$
which means $f(x'; \theta, T)_j < f(x'; \theta, T)_{j^*}, ~\forall j\ne j^*$. This leads to $j^* = \argmax_j f(x'; \theta, T)_j$, which contradicts our condition.
\end{proof}


Now we prove Theorem~\ref{theorem: model-probability-coefficient}
\begin{proof}

First let $P(Y'|x') = P(y|x) \approx \mathbbm{1}(y)$. % This is our assumption. But the approx here would be a problem, we need exactly one-hot. 
Let $j^* = \argmax_j P(Y'=j|x')$. By Lemma \ref{lemma: model-probability-coefficient} we have $f(x'; \theta, T)_{j^*} \le P(Y'^*=j^*|x') \le 1$. Then there exists $\lambda^* > 0$, such that $\lambda^* \cdot f(x'; \theta, T)_{j^*} + (1 - \lambda^*) = P(Y'=j^* | x')$ by the intermediate value theorem.

Let $\lambda = \lambda^*$, we have
$$
\begin{aligned}
& 2 \left[ \|\lambda \cdot f(x'; \theta, T) + (1 - \lambda) \cdot P(\tilde{Y}' | x')- P(Y' | x')\|_{TV} - \| f(x'; \theta, T) - P(Y' | x')\|_{TV} \right]\\
= & 2 \left[ \|\lambda \cdot f(x'; \theta, T) + (1 - \lambda) \cdot \mathbbm{1}(y)- P(Y' | x')\|_{TV} - \| f(x'; \theta, T) - P(Y' | x')\|_{TV} \right]\\
= & \sum_j |\lambda \cdot f(x'; \theta, T)_j + (1 - \lambda) \cdot 1(j=y) - P(Y' =j | x')| - \sum_j |f(x'; \theta, T)_j - P(Y' =j | x')|\\
= & \sum_j |\lambda \cdot f(x'; \theta, T)_j + (1 - \lambda) \cdot 1(j=Y) - P(Y' =j | x')| - \sum_j |f(x'; \theta, T)_j - P(Y' =j | x')|\\
= & \sum_{j, j\ne j^*} |\lambda \cdot f(x'; \theta, T)_j - P(Y' = j | x')| - \sum_{j, j\ne j^*} |f(x'; \theta, T)_j - P(Y' = j | x')| - |f(x'; \theta, T)_{j^*} - P(Y' = j^* | x')|\\
\le & \sum_{j, j\ne j^*} |\lambda \cdot f(x'; \theta, T)_j - f(x'; \theta, T)_j| - |f(x'; \theta, T)_{j^*} - P(Y' = j^* | x')| \\
= & \sum_{j, j\ne j^*} [f(x'; \theta, T)_j - \lambda \cdot f(x'; \theta, T)_j ] - [P(Y' = j^* | x') - f(x'; \theta, T)_{j^*}] \\
= & \sum_{j, j\ne j^*} [f(x'; \theta, T)_j - \lambda \cdot f(x'; \theta, T)_j ] - [\lambda \cdot f(x'; \theta, T)_{j^*} + (1 - \lambda) - f(x'; \theta, T)_{j^*}] \\
= & \sum_j f(x'; \theta, T)_j - \lambda \sum_j f(x'; \theta, T)_j - (1-\lambda)\\
= & ~ 0.
\end{aligned}
$$
\end{proof}
