\subsection{Proofs in Section~\ref{sect:reason-realistic}}
\label{sect:label-noise-proof-realistic}

\smallsection{Proof of Lemma~\ref{lemma:Learn-true-distribution}}    
\begin{proof}

\input{appendix/1-proof/2.5.1-approximation}

% \chengyu{Or from manifold learning perspective? the perturbation must overlap a little? Recall the Gaussian noise proof, we just need to prove that the perturbation is not orthogonal to the true gradient if a model is robust?
% Since the model doesn't change prediction in a ball, if this ball contains another example, means the model gradient will be aligned with the true gradient?
% }

% \chengyu{Or robust viewed as a extension of clean learning? While clean learning can already learn the distribution (unbiased) at all everywhere as long as the training examples are densely sampled. We just need robustness to further enlarge the radius of learned distribution. This appears to still conflict with distribution mismatch as if training examples are denser, then the distribution mismatch also becomes smaller in the first place.}
\end{proof}


% \smallsection{Proof of Lemma~\ref{lemma:Learn-true-distribution}}    
\smallsection{Proof of Theorem~\ref{theo:realistic-classifier}}
\begin{proof}
    First, we show that adversarial perturbation generated by a realistic classifier can change its predictive distribution. Considering adversarial perturbation based on FGSM and cross-entropy loss, namely $x' = x -\varepsilon \|\nabla~f_\theta(x)_y\|^{-1} \nabla~f_\theta(x)_y$, we can obtain a result similar to Lemma~\ref{theorem:distribution-mismatch-true-model}.
    \begin{lemma}
    \label{theorem:distribution-mismatch-real-model}
    Assume $f_\theta(x)_y$ is $L_\theta$-locally Lipschitz around $x$ with bounded Hessian. Let $\sigma_m = \inf_{z \in \mathcal{B}_\varepsilon(x)} \sigma_{\min} (\nabla^2 f_\theta(z)_y) > 0$ and $\sigma_M = \sup_{z \in \mathcal{B}_\varepsilon(x)} \sigma_{\max} (\nabla^2 f_\theta(z)_y) > 0$.
    Here $\sigma_{\min}$ and $\sigma_{\max}$ denote the minimum and maximum eigenvalues of the Hessian, respectively.
    We then have
    \begin{equation}
        \| f_\theta(x) -  f_\theta(x')\|_{\text{TV}} \ge
        \frac{\varepsilon}{2} (1 - f_\theta(x)_y) \frac{\sigma_m}{L_\theta}  - \frac{\varepsilon^2}{4} \sigma_M,
    \end{equation}
    \end{lemma}


Second, We prove that the true label distribution will be distorted by the adversarial perturbation generated by a realistic classifier. This is guaranteed if the predictive distribution of a realistic classifier can approximate the true label distribution. Specifically, by utilizing Lemma~\ref{theorem:distribution-mismatch-real-model} and Lemma~\ref{lemma:Learn-true-distribution}, we have with probability $1 - 2\delta$,
\begin{equation}
% \chengyu{looks like the late two terms should be doubled here}
\begin{aligned}
& \|P(Y|x)  - P(Y'|x') \|_{\text{TV}} \\ 
\ge & \|f_\theta(x)  - f_\theta(x')\|_{\text{TV}} - (\|f_\theta(x) - P(Y|x)\|_{\text{TV}} + \|f_\theta(x') - P(Y'|x')\|_{\text{TV}})\\
\ge & \frac{\varepsilon}{2} (1 - f_\theta(x)_y) \frac{\sigma_m}{L_\theta}  - \frac{\varepsilon^2}{4} \sigma_M - \sqrt{\frac{2\kappa N_{\rho\varepsilon} K}{N}\log\frac{2}{\delta}}  - \left(\left(\frac{3}{2} - \frac{1}{K}\right) L_\theta + L\right)2\rho\varepsilon \\
= & \varepsilon \left[(1 - f_\theta(x)_y) \frac{\sigma_m}{2L_\theta} - 2\rho\left(\left(\frac{3}{2} - \frac{1}{K}\right) L_\theta + L\right)\right] - \varepsilon^2\frac{\sigma_M}{4}  - \sqrt{\frac{2\kappa N_{\rho\varepsilon} K}{N}\log\frac{2}{\delta}}. \\
\end{aligned}    
\end{equation}

Finally, we show that such distribution mismatch induces label noise in the adversarially augmented training set. Similar to the proof for the true classifier, by the common labeling practice of adversarial examples we have $P(\tilde{Y}'|x') = P(\tilde{Y}|x) = P(Y|x)$. 
By utilizing Lemma~\ref{theorem:implicit-label-noise}~\footnote{Note this is a result only associated with the training set, thus is not dependent on the specific classifier. } we then have with probability $1-3\delta$, 

\begin{equation}
    p_e(\mathcal{D}') \ge
    \varepsilon \left[(1 - \mathbb{E}_x f_\theta(x)_y) \frac{\sigma_m}{2L_\theta}- 2\rho\left(\left(\frac{3}{2} - \frac{1}{K}\right) L_\theta + L\right)\right]  - \varepsilon^2\frac{\sigma_M}{4}  
    %  \|f_\theta(x)  - f_\theta(x')\|_{\text{TV}}
    - \xi \sqrt{\frac{1}{2N} \log\frac{2}{\delta}},
\end{equation}
where $\xi = 1 + \sqrt{4\kappa N_{\rho\varepsilon} K}$.
    
% \chengyu{If the loss is minimized for maximum in a ball, the classifier must be able to predict the same true label of $x$ for all $x' \in \mathcal{B}_\varepsilon(x)$. }
% \chengyu{Make the radius of the cover $\varepsilon$. It won't be a problem even considering Yang because inputs in the same class can still have distance smaller than $\varepsilon$? Or if radius is larger, we just prove with high probability all inputs with share at least one point given the radius is not too large.}
% \chengyu{Intuition: The maximum in a ball has minimized loss to its label should imply any shared point between balls of neighboring inputs has minimized loss to corresponding labels, which forces the predictive distribution of the shared point to be close to the sample mean. And if the radius of the cover is also $\varepsilon$, then all inputs within the cover must share at least one point (i.e. the center).}
% $1 - 2\delta$ comes from the Bool's inequality used for intersection of events. 
% See https://math.stackexchange.com/questions/2356190/inequality-on-probability-of-intersection-of-n-events

% $$
% 2\|f_\theta(x) - f(x)\|_{\text{TV}} = \|f_\theta(x) - f(x)\|_1 \le \sqrt{\frac{2\kappa N_s K}{N}\log\frac{2}{\delta}}  + L_\theta r ( 1 + K\|\mathbf{1}_{(\circ)} - K^{-1}\mathbf{1}\|_1) + 2 L r
% % |f_\theta(x) - f(x)| \le o(\delta)
% $$
\end{proof}

