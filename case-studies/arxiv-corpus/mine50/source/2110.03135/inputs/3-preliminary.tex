\section{Preliminaries}
% \subsection{Notation and Background} 
\label{sect:notation}

    % \smallsection{Multi-class classification} 
    % Let $\mathcal{X} \subset \mathbbm{R}^d$ define the input space equipped with a norm $\|\cdot\|: \mathcal{X} \rightarrow \mathbbm{R}^+$ 
    % % \jingbo{Why do we exclude 0?}\chengyu{math convention} 
    % and $\mathcal{Y} = [K] := \{1, 2,\ldots, K\}$ define the label space. 
    % Let $X \in \mathcal{X}$ be a random variable representing the input, and $Y \in \mathcal{Y}$ be a random variable representing the true label of the input.
    % For a classification problem a set of training examples are sampled identically and independently as $\mathcal{D} = \{(x_i, y_i)\}_{i\in[N]}$, where $(x_i, y_i) \sim P_{X,Y}$.
    % % We consider a classification problem which maps an input $x \in \mathcal{X}$ to its label. 
    % % Let $Y \in \mathcal{Y}$ be a random variable representing the true label of $x$, which follows a probability distribution $P(Y | x)$.
    % % Let $P(Y | x)$ be the probability distribution for $Y$ and $y = \argmax_j P(Y=j|x)$ be the argmax label.
    % % For supervised learning a training set is constructed as $\mathcal{D} = \{(x, y)\}$, where $y$ is sampled from $P(Y|x)$.
    % Note that $y_i$ is not accessible in practice as the true label distribution is unknown.

    \smallsection{A statistic model of label noise~\citep{Frnay2014ClassificationIT}}
    Let $\mathcal{X} \subset \mathbbm{R}^d$ define the input space equipped with a norm $\|\cdot\|: \mathcal{X} \rightarrow \mathbbm{R}^+$ 
    % \jingbo{Why do we exclude 0?}\chengyu{math convention} 
    and $\mathcal{Y} = [K] := \{1, 2,\ldots, K\}$ define the label space. 
    We introduce four random variables to describe noisy labeling process.
    Let $X \in \mathcal{X}$ denote the input, $Y \in \mathcal{Y}$ denote the true label of the input, $\tilde{Y} \in \mathcal{Y}$ denote the assigned label of an input provided by an annotator, and finally $E$ denote the occurrence of a label error by this annotator. $E$ is a binary random variable with value $1$ indicating that the assigned label is different from the true label for a given input, i.e., $E = \mathbf{1}(\tilde{Y} \ne Y)$. We study the case where the label error depends on both the input $X$ and the true label $Y$. 
    % \chengyu{The statistical model of label noise is illustrated in Figure~\ref{}.}
    % \smallsection{Multi-class classification}
    % \smallsection{Training set}
    For a classification problem, a training set consists of a set of examples that are sampled as $\mathcal{D} =\{(x_i, \tilde{y}_i)\}_{i\in[N]}$. % , which we refer as the clean training set. 
    % where $x_i \sim P(X)$ and $\tilde{y}_i \sim P(\tilde{Y})$
    % a labeled training set consists of a set of training inputs and their corresponding assigned labels, i.e., $\tilde{\mathcal{D}} =\{(x_i, \tilde{y}_i)\}_{i\in[N]}$. % which is then used for training.
    % \lucas{maybe in the generation process, we dont define E (i.e., how E is generated). We only define the generation model for $Y$, $X$, and $\tilde{Y}$; and simply define $E$ as $1(\tilde{Y} \ne Y)$?} 
    
    \begin{definition}[Label noise]
    \label{definition:implicit-label-noise}
     We define label noise $p_e$ in a training set $\mathcal{D}$ as the empirical measure of the label error, namely $p_e(\mathcal{D}) = 1/N \sum_{i\in [N]} \mathbf{1}(\tilde{y}_i \ne y_i)$. % = 1/N \sum_{i\in[N]} E_i
    %  \jingbo{shall we use $\mathbbm{1}$ here?}
    \end{definition}
    
    
    % \smallsection{Data annotation}    
    % \lucas{ can we simply saying that $\tilde{Y} = {Y} + Z(Y)$?, where $Z(X, Y)$ is the annotation noise. Or alternatively, can we directly plot a graph (plate notation) of the generative model of these random variables (e.g., 
    % [x]->Y->$\tilde{Y}$->[y] (plus one edge from [x] to $\tilde{Y}$))}
    % \chengyu{You mean figure 2c in \citep{Frnay2014ClassificationIT}?} \lucas{similar figure, but different}. 
    % Let $\tilde{Y} \in \mathcal{Y}$ be a random variable representing the assigned label of $x$ given by an annotator, which follows a probability distribution $P(\tilde{Y}|x)$.
    % % Let $P(\tilde{Y}|x)$ be the probability distribution for $\tilde{Y}$ and $\tilde{y} = \argmax_j P(\tilde{Y}=j|x)$ be the argmax label. 
    % Note that the assigned label $\tilde{Y}$ is always dependent on the true label $Y$ through an (unknown) annotation process, i.e. $P(\tilde{Y} = \tilde{j} | x) = \sum_j P(\tilde{Y}=\tilde{j} | Y=j, x) P(Y=j | x)$. Otherwise, the assigned label is not meaningful. By giving each $x$ in the training set an assigned label, we can construct a labeled training set as $\mathcal{D} = \{(x, \tilde{y})\}$, where $\tilde{y}$ is sampled from $P(\tilde{Y}|x)$.

    
    % \smallsection{Label noise} $P(\tilde{Y} \ne Y | x)$
    
    % % Specifically, we define the implicit label noise as the probability that the assigned and true labels of an example are different. 
    % \begin{definition}[Label noise]
    % \label{definition:implicit-label-noise}
    % % Given an input $x\in\mathcal{X}$, we define the label noise as the probability that its assigned label is different from its true label, namely $P(\tilde{Y} \ne Y | x)$.
    % Given an input $x\in\mathcal{X}$, we define the label noise as the probability that a labeling error occurs, namely $p_e(j,x) = P(\tilde{Y}\ne j|Y=j, x)$~\citep{Frnay2014ClassificationIT}.
    % % its assigned label is different from its true label, namely $P(\tilde{Y} \ne Y | Y=y, x)$.
    % Note here we discuss a general sense of label noise that depends on both the input and the true label.
    % \end{definition}
    
%   \begin{remark}
%   \label{remark:label-noise}
%   Implicit label noise is equivalent to instance-dependent and class-dependent label noise, since 
%   $ p(Y\ne Y^* | x) = 1 - \sum_j p(Y= j | Y^* = j, x) p(Y^* = j|x)$, and $p(Y\ne j | Y^* = j, x) = 1 - p(Y= j | Y^* = j, x)$ is a typical definition of label noise.
%   It can easily seen that if $p(Y\ne Y^* | x) > 0$, $p(Y\ne j | Y^* = j, x) > 0$ for some $j$.
% %   \jingbo{this definition seems a bit complicated to me. Why not define it as $ p(y\ne y^* | x) = \sum_j (1 - p(y = j | y^* = j, x)) p(y^* = j|x). $? this one seems more straightforward to me.} \chengyu{Because $p(y\ne j | y^* = j| x)$ is the label noise while $p(y = j | y^* = j| x)$ is a transition matrix. Use two formulas.}
%   \end{remark}
   
    \begin{assumption}% [Clean dataset]
        \label{assumption:clean-dataset}
        We assume the annotation of a clean dataset involves no label error, namely $P(E=1|Y=y, x) = 0$. This directly implies $P(\tilde{Y}|x) = P(Y|x)$ (see proof in the Appendix).
        % We assume there is no label noise in the clean dataset $\mathcal{D}$, % namely $P(Y|x) = P(Y^* | x)$ and $y = y^*$.
        % namely $P(\tilde{Y}\ne Y| Y=y, x) = 0, ~\forall~y, x$. %  and $\tilde{y} = y$.
        % namely $p_e(j, x) = 0, ~\forall~j, x$.
        % namely $\mathbb{E}_y P_e(y, x) = 0, ~\forall~x$. %  and
        % This directly implies that $P(\tilde{Y}|x) = P(Y|x)$. \lucas{it should be: $P(\tilde{Y} = j |Y=j, x) = 1$?}
        % \chengyu{\sout{equivalent}}\chengyu{sufficient}
        % \lucas{maybe not? i think it is possible to have $\forall j, P(\tilde{Y}=j|x) = P(Y=j|x)$, but $P(\tilde{Y} = j |Y=j, x) \neq 1$. For example, if all of $P(\tilde{Y}=j|x)$, $P(Y|x)$, and $P(\tilde{Y} |Y=j, x)$ are uniform distribution. } 
    \end{assumption}
    
    \begin{definition}[Data quality]
    \label{definition:data-quality}
    % \chengyu{Remove this data quality definition if moving the analysis of dependence to the realistic case? But this will cause some problem as Lemma 4.1 requires the assumption that most data in the training set are of high quality.}
    % Given an input $x$, we define its data quality $q(x) = \max_j P(Y = j | x)$, namely the probability mass of the true label distribution at the argmax label $y$.
    % Given an input $x$, we define its data quality as $q(x) :=  P(Y = y | x)$, namely the probability mass of the true label distribution at the sampled true label $y$.
    Given a training set $\mathcal{D}$, we define its data quality as $q(\mathcal{D}) = \mathbb{E}_{(x, y) \in \mathcal{D}} P(Y=y|x)$
    \end{definition}
    
    % Only need for the proof lemma 4.1. Goes to appendix
    % \begin{assumption}
    %     \label{assumption:one-hot-label}
    %     We assume the clean dataset $\mathcal{D}$ contains mostly high-quality data, namely $q(x) \approx 1$.
    % \end{assumption}
    
    % \jingbo{is it better to define these when we talk about them? I feel it's a bit hard to follow here as we don't know why we want to have these notations.}
    % \smallsection{Other notations}
    % We denote the one-hot vector with one appearing at label $y$ as $\mathbf{1}_y$. 
    % % All one vector will be denoted by $\mathbf{1}$. 
    % We use $\mathcal{B}_\varepsilon (x)$ to denote the norm ball centered at $x$ with radius $\varepsilon$, i.e., $\mathcal{B}_\varepsilon (x) = \{z\in \mathcal{X}: \|z - x\| \le \varepsilon\}$. We use $\mathcal{N}_\varepsilon(\mathcal{S})$ to denote the neighborhood of a set $\mathcal{S}$, i.e. $\mathcal{N}_\varepsilon(\mathcal{S}) = \bigcup_{x\in\mathcal{S}} \mathcal{B}_\varepsilon (x)$.  We use $\sigma(H)$ to denote the eigenvalues of a square matrix $H$.
    
    \smallsection{Adversarially augmented training set}
    Let $f: \mathcal{X} \rightarrow \mathcal{Y}$ be a probabilistic classifier and $f(\cdot)_j$ be its predictive probability at class $j$.
    The adversarial example of $x$ generated by $f$ is obtained by solving the maximization problem $x' = \argmax_{z \in \mathcal{B}_\varepsilon(x)}~\ell(f(z), y)$.
    % \begin{equation}
    %     \label{eq:adversarial-example}
    %     % Note there should not be expectation here, \delta is calculated for every single x.
    %     % \delta = \argmax_{\delta, \|\delta\|\le \epsilon}~\ell(f(x + \delta), y),
    %     x' = \argmax_{z \in \mathcal{B}_\varepsilon(x)}~\ell(f(z), y),
    % \end{equation}  
    Here $\ell$ can be a typical loss function such as cross-entropy. And $\mathcal{B}_\varepsilon (x)$ denotes the norm ball centered at $x$ with radius $\varepsilon$, i.e., $\mathcal{B}_\varepsilon (x) = \{z\in \mathcal{X}: \|z - x\| \le \varepsilon\}$.
    
    Following previous notations, we denote $Y'$ as a random variable representing the true label of $x'$ and $\tilde{Y'}$ as a random variable representing the assigned label of $x'$.
    % Let $Y'$ be a random variable representing the true label of $x'$.
    % , which follows a probability distribution $P(Y' | x')$.
    % and $y' = \argmax_j P(Y'=j | x')$ be the argmax true label.
    % Let $\tilde{Y'}$ be a random variable representing the assigned label of $x'$.
    % , which follows a probability distribution $P(\tilde{Y}' | x')$. 
    % and $\tilde{y}' = \argmax_j P(Y' = j| x')$ be the argmax label. 
    We refer $\mathcal{D}' = \{(x', \tilde{y}')\}$ as the adversarially augmented training set.
    % Finally, denote $y_\delta^* = \argmax_j P(Y^*_\delta | x_\delta)$ as the argmax class of the true label distribution of $x_\delta$.
    
    \smallsection{Adversarial training}
    % \chengyu{also mentioned in `ROBUSTNESS MAY BE AT ODDS WITH ACCURACY`. see `Adversarial Training as a Form of Data Augmentation` section}
    Adversarial training can be viewed as a data augmentation technique that trains the parametric classifier $f_\theta$ on the adversarially augmented training set~\citep{Tsipras2019RobustnessMB}, namely
    \begin{equation}
        \label{eq:outer-minimization}
        \theta^* = \argmin_\theta \frac{1}{|\mathcal{D'}|}\sum_{(x', \tilde{y}') \in D'}~ \ell(f_\theta(x'), \tilde{y}').
    \end{equation}
    % Here, $x_\delta \equiv x + \delta$ is the adversarial example of $x$ produced by the inner maximization, where
    % \begin{equation}
    %     % Note there should not be expectation here, \delta is calculated for every single x.
    %     \delta = \argmax_{\delta, \|\delta\|\le \epsilon}~\ell(f(x + \delta; \theta), y),
    % \end{equation}    
    % Here $y' = \argmax_j P(Y_\delta = j| x_\delta)$.
    % and $y_\delta \in \mathcal{Y}$ is the assigned label of $x_\delta$. 



    
    
    % \begin{remark}
    % \end{remark}
    

    
    % Let $\mathcal{X} \subset \mathbbm{R}^d$ define the input space equipped with a norm $\|\cdot\|: \mathcal{X} \rightarrow \mathbbm{R}^+$. 
    % Let $\mathcal{Y} = \{1, \ldots, c\}$ define the label space.
    % Let the ground-truth label distribution of an input $x \in \mathcal{X}$ be $p(y|x)$, 
    % The input $x \in \mathcal{X}$ and label $y \in \mathcal{Y}$ follow a ground-truth joint distribution $p(x, y) = p(y|x)p(x)$. Let $f^*(\cdot): \mathcal{X} \rightarrow [0,1]^c$ denote the optimal probabilistic classifier such that $f^*(x) = p(y|x)$. $f^*(\cdot)_j$ will refer to the predictive probability of the $j$-th class.
    % Let $\mathcal{D} = \{x_i, \hat{y}_i\}_{i=1}^N$ be a clean training set, where $\hat{y}$ is the \emph{assigned label} of $x$. 
    % We assume there is no label noise in $\mathcal{D}$, namely $p(\hat{y} | x) = p(y | x)$.
    
    % \todo{Avoid joint distribution construction here. It naturally violates label noise view. See Rebuttal.}
    
    % Let $x + \delta$ be an adversarial example of $x$, where $\delta \in \mathbbm{R}^d$ is the adversarial perturbation and $\|\delta\| \le \varepsilon$. 
    % Let $\mathcal{D}_\delta$ denote the training set $\mathcal{D}$ perturbed by $\delta$, namely $\mathcal{D}_\delta = \{x_i + \delta_i, \hat{y}_{\delta, i}\}_{i=1}^N$. 
    % We denote the ground-truth label of $x + \delta$ as $y_{\delta}$.
    % \jingbo{I wish to distinguish $y$ and $y_\delta$, so it's easier to understand the later formulas. }
    

