% \subsection{Equivalence between implicit label noise and label flipping noise}
% \subsection{Influence of implicit label noise in adversarial training}
% \subsection{Influence of implicit label noise}
% \subsection{Implicit label noise is a specific type of label noise}
% \smallsection{Connection and difference between implicit label noise and conventional label noise}
\smallsection{Intuitive interpretation of label noise in adversarial training}
% \chengyu{If no section 4.1, can also remove this section}
% In this section, we connect the implicit label noise to a more familiar definition of label noise and show it can have a significant impact.
%     \begin{proposition}[Implicit label noise is equivalent to instance-dependent and class-dependent label noise]
%     \label{proposition:label-noise}
%     Let $p_e(j, x) = P(\tilde{Y}\ne j | Y = j, x)$ be a typical definition of label noise which depends on both the class $j$ and input $x$. Then implicit label noise is equivalent to
%     \begin{equation}
%       % p(Y\ne Y^* | x) = 1 - \sum_j p(Y= j | Y^* = j, x) p(Y^* = j|x),
%       P(\tilde{Y}\ne Y | x) = 1 - \sum_j (1 - p_e(j, x)) P(Y = j|x),
%     \end{equation}
%   % It can easily seen that if $p(Y\ne Y^* | x) > 0$, $p(Y\ne j | Y^* = j, x) > 0$ for some $j$.
%   \end{proposition}
%   \begin{proof}
%   See Appendix~\ref{sect:label-noise-more-proof}.
%   \end{proof}
We introduce a simple example to help understand the emergence of label noise in adversarial training.
% Towards an intuitive understanding of implicit label noise, we introduce a simple example to discuss the differences and connections between implicit label noise and convention label noise.
% We now try to quantify the implicit label noise given its connection with typical label noise.
% \chengyu{We now discuss a simplified example of implicit label noise, and discuss its connection and difference with conventional label noise.}
\begin{example}
% [Quantify the implicit label noise]
[Label noise due to a symmetric distribution shift]
\label{example:label-noise-influence}
% We would like to note that the adversarial training setting would amplify the impact of the implicit label noise, since it adds perturbations to every training sample. 
% In fact, Theorem 3.1 in our main paper, which lower-bounds the implicit label noise by the distance between the assigned label distribution and the true label distribution, provides a way to intuitively quantify the implicit label noise. 
Let $\mathcal{D}=\{(x_i, y_i)\}_{i\in[N]}$ be a clean labeled training subset where all inputs $x_i=x$ are identical and have a one-hot true label distribution, i.e., $P(Y|x) = \mathbf{1}_y$. %, and there is no label noise in $\mathcal{D}$, i.e. $y = \tilde{y}$. 

We now construct an adversarially augmented training subset $\mathcal{D'} = \{(x'_i, \tilde{y}'_i)\}_{i\in[N]}$, where $\tilde{y}' = y$ and $x'$ is generated based on adversarial perturbation that distorts the true label distribution symmetrically. Specifically,
$$
P(Y'= j' | x') =
\begin{cases} 
1 - \eta, & \text{if}~~j = y, \\
\eta/(K-1), & \text{otherwise}. \\
\end{cases}
$$
Then by Lemma~\ref{theorem:implicit-label-noise} 
% we have $P(\tilde{Y}' \ne Y' | x') \ge \eta$.
% we have $E_{j'} P_e(j', x') \ge \eta$.
we have $p_e (\mathcal{D}')\gtrsim \eta$.
% \ge % \left\| P(Y^*|x) -  P(Y^*_\delta|x_\delta).\right\|_{\text{TV}}
% which is equivalent to $p_e(j,x) = \sigma$ by Proposition~\ref{proposition:label-noise}. % , meaning $10\%$ label noise is injected in $\mathcal{S}$.
% which means the label noise injected in $\mathcal{S}_\delta$ is at least $10\%$  by Proposition~\ref{proposition:label-noise}.
% the total variation distance between these distributions is 0.1, which means the implicit label noise is at least 0.1. This is already equivalent to 10\% label noise based on the connection between implicit label noise and the (instance-wise) probabilistic definition of label flipping noise (see Remark 3.2 in our paper).
% \jingbo{what is this 10? I didn't quite get this.}
\end{example}
   
% \chengyu{Talk about observation distribution, noisy process and true distribution?}

One can find that there is indeed $\eta$ faction of noisy labels in $D'$. This is because if we sample the labels of $x'$ based on its true label distribution, we expect $1 - \eta$ faction of $x'$ are labeled as $y$, while $\eta$ fraction of $x'$ are labeled to be other classes. However, in $D'$, all $x'$ are assigned with label $y$ , which means $\eta$ fraction of $x'$ are labeled incorrectly. In realistic datasets we can consider inputs with similar features for such reasoning.

% \chengyu{Add a interpretation from population view?? I remember there is a case about dogs and cats in previous revision.}
% Conventionally speaking, label noise is perceived as the fraction of the noisy labels in the training set, i.e. the assigned labels that are different from their corresponding true labels. However, in the adversarially augmented training set no assigned label is noisy since $\tilde{y}' = y'$ 
% ~\footnote{Recall $\tilde{y}' = \argmax_j P(\tilde{Y'}=j|x')$ and $y' = \argmax_j P(Y'=j|x')$}
% for every augmented input $x'$. Yet, rather counter-intuitively, at least $\eta$ label noise exists in $\mathcal{D}'$, which is due to the fact that every input is now more likely to be mislabeled after adversarial perturbation.


% \chengyu{Difference from a process view. There is no noisy process defined, only the final assigned distribution after noisy process is known. But as long as the final distribution is different, suggests the annnotation must go through some unknown noisy process.}
% \chengyu{Why same argmax doesn't mean there is no label noise? Because label noise is always associated with a noisy random process. Because the change of the underlying true distribution should be reflected in the sampled labels, otherwise there must be some noisy process an annotator goes through.}

The above example also shows that label noise in adversarial training may be stronger than one's impression. Even a slight distortion of the true label distribution, e.g. $\eta=0.1$, will be equivalent to at least $10\%$ noisy label in the training set. This is because the true label distribution of every training input is distorted, resulting in significant noise in the population. 
% \sout{Such example also implies that even static adversarial perturbation~\footnote{namely the adversarial perturbation is added to the training set only once and the standard training is performed subsequently} can produce clear double descent as shown in Appendix~\ref{sect:exp-static}.} Therefore we believe implicit label noise can be an important source of label noise that makes double descent more evident in adversarial training.



    % An informal proof can be sketched from a frequentist's view and help the understanding of implicit label noise.
    % % One can interpret the implicit label noise in a frequentist's view.
    % Say there are $M$ identical copies of $x_\delta$ in the training set $\mathcal{D}_\delta$, with their true labels and traditional adversarial labels distributing according to $p(y^*_\delta | x_\delta)$ and $p(\tilde{y}_\delta| x_\delta)$, respectively.
    % % by Remark~\ref{remark:common-practice} and Assumption~\ref{assumption:clean-dataset}, respectively.
    % % The true label of $x_\delta$ is sampled based on $p(y_\delta |x + \delta)$, the assigned label is sampled based on $p(y|x)$. 
    % The number of copies that have the same true label and assigned label is $ M \sum_j \min \{p(\tilde{Y}_\delta=j|x_\delta), p(Y^*_\delta=j |x_\delta)\}$.
    % The fraction of label noise exists in $\mathcal{D}_\delta$ is thus $1 - \sum_j \min \{p(\tilde{Y}_\delta=j|x_\delta), p(Y^*_\delta=j|x_\delta)\} = \|p(\tilde{y}_\delta|x_\delta) - p(y^*_\delta|x_\delta) \|_{\text{TV}}$  by the definition of the total variation distance.
    
