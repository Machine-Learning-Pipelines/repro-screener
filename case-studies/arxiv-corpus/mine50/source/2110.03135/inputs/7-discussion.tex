\section{Conclusion and Discussions}
\label{sect:conclusion}

% \chengyu{if no size, remove all equation number}

In this paper, we show that label noise exists implicitly in adversarial training due to the mismatch between the true label distribution and the assigned label distribution of adversarial examples. Such label noise can explain the dominant overfitting phenomenon. Based on a label noise perspective, we also extend the understanding of robust overfitting and show that it is the early part of an epoch-wise double descent in adversarial training. Finally, we propose an alternative labeling of adversarial examples by rectifying model probability, which can effectively mitigate robust overfitting without any manual hyper-parameter tuning. 
% Our further analyses show that the double descent may originate from the implicit label noise introduced by the improper labeling of the adversarial examples. Based on our understanding, we propose an alternative labeling of adversarial examples by rectifying model probability, which can effectively mitigate robust overfitting without any manual hyper-parameter tuning. 
% \jingbo{this without hyper-parameter tuning reads odd to me. Maybe say without manual tuning?}
% In the future, we also plan to explore if it is possible to rectify the model probability on the fly during the adversarial training for a better efficiency.

The label noise implicitly exists in adversarial training may have other important effects on adversarially robust learning. This can potentially consolidate the theoretical grounding of robust learning. For instance, since label noise induces model variance, from a model-wise view, one may need to increase model capacity to reduce the variance. This may partially explain why robust generalization requires significantly larger model than standard generalization. 
% achieved by adversarial training is still significantly lower than the accuracy. One often has to greatly increase model capacity 

% our definition of implicit label noise from a distribution mismatch perspective may also exist in a variety of scenarios, especially in real-world tasks where labeling is often complicated and ambiguous. Our method to rectify such label noise should thus be applicable as well. This would be an interesting future work. 


% By showing robust overfitting and epoch-wise double descent are in fact the same phenomena, our work eases the difficulty in understanding overfitting in modern generalization theory. Existing theories on double descent can be readily applied to adversarially robust learning, and potentially consolidate the theoretical grounding of robust learning.



% \note{Since unify the two phenomena, we can deal with one. Theoretically we don't have to invent theory for robust overfitting, But just have to explore double descent in adversarial training.  And more specifically we can focus on dealing with the label noise induced by distribution mismatch.}

% \note{Remove this if no space left.}
% Beyond the double descent in adversarial training, our definition of implicit label noise from a distribution mismatch perspective may also exist in a variety of scenarios, especially in real-world tasks where labeling is often complicated and ambiguous. Our method to rectify such label noise should thus be applicable as well. This would be an interesting future work. 

% Can broadly benefit similar overfitting case
% - Label noise \#TODO
% - Ambiguity in text?
% - Image classification with multiple objects (classification without detection?)

% from a hard-label mismatch to a distribution mismatch, thus enabling a wide variety of approaches to observe double descent in modern learning practice including the adversarial augmentation and mixup augmentation mentioned in the paper other than adversarial training. On the other hand, Gaussian noise and other common corruptions~\citep{Hendrycks2019BenchmarkingNN}, which does not cause distribution mismatch, have been shown to fail to produce double descent~\citep{Yang2020RethinkingBT}. These empirical observations suggest there is a deeper connection between implicit label noise and double descent in classification problems, where the theoretical progress still lags behind.

% \note{thus offer a theoretical playground for analyzing the label noise in classification problems. Currently still rely on label flipping noise and some feature noise}

% -- Finally, on the practical side, our understanding of robust overfitting opens new opportunities to advance adversarial training practice. Better labeling of the adversarial examples may be still in demand to further reduce robust overfitting.

% Techniques that have been shown to improve the uncertainty quality of the model probability such as ensembles~\citep{Lakshminarayanan2017SimpleAS} can further mitigate the double descent and improve adversarial robustness. 
% Meanwhile, we note that an alternative labeling not necessarily has to be obtained from a surrogate model, but can also be based on the model itself during training, which can minimize the computation overhead and is more suitable for practical deployment of adversarial training methods. Strategies in this vein have already demonstrated their effectiveness on mitigating robust overfitting~\citep{Huang2020SelfAdaptiveTB}. % Following this idea, a group of strategies succeeding in conventional noisy learning is known as \emph{bootstrapping}~\citep{Reed2015TrainingDN, Sanchez2019UnsupervisedLN, Huang2020SelfAdaptiveTB}, where the last work already demonstrated the effectiveness of their method on adversarial training.




% * Potential explanation to knowledge distillation

% * kd should also be useful in label noise, interesting no work use this, currently people use a small clean data
% \citep{Li2017LearningFN, Zhang2020DistillingES}.

% \clearpage
% \newpage

% \section*{Reproduciblity Statement}

% We conduct experiments on public benchmark. 
% We will release implementations for all methods and scripts for all experiments on GitHub, under the Apache-2.0 license.

% \section*{Ethic Statement}

% In this paper, we explore the double descent phenomenon in adversarial robust learning from the implicit label noise perspective.
% We experiment on three benchmark datasets that are publicly available.
% Our analyses offer more in-depth understandings about adversarial training and can better improve the robustness of neural network models.
% Therefore, we believe our work is ethically on the right side of spectrum and has no potential for misuse, and cannot harm any vulnerable population.
