% \section{Mitigate Double Descent in Adversarial Training from the Implicit Label Noise Perspective}
\section{Mitigate Label Noise in Adversarial Training}
\label{sect:mitigate-double-descent}

% To mitigate the label noise in adversarial training, we focus on suppressing the implicit label noise from both theoretical and practical perspectives. 
Since the label noise is incurred by the mismatch between the true label distribution and assigned label distribution of adversarial examples in the training set, we wish to find an alternative label (distribution) for the adversarial example to reduce such distribution mismatch.
% \chengyu{But why this is better than the assigned distribution? Because the distribution mismatch is lower bounded by a positive, while here the mismatch can converge to $0$ as long as Lipschitz is sufficiently small.}
% Our goal is to construct an adversarially augmented dataset $\mathcal{D'}$ which contains little or no label noise.
% We've already shown that the predictive label distribution of a classifier trained on the conventional adversarially augmented dataset $\mathcal{D'} = \{(x', \tilde{y}'=\tilde{y})\}$, which we denote as \emph{model probability} in the following discussion, can in fact approximate the true label distribution. 
We've already shown that the predictive label distribution of a classifier trained by conventional adversarial training, which we denote as \emph{model probability} in the following discussion, can in fact approximate the true label distribution. 
Here we show that it is possible to further improve the predictive label distribution and reduce the label noise by calibration.
    
% Since the double descent is mainly caused by the mismatch between the assigned label and true label distributions and also the true label distribution is missing in most of real-world datasets, we propose to approximate the true label distribution.

% \subsection{Approximate the true label distribution}
% --- This previous title feels like we need true label distribution
\subsection{Rectify model probability to reduce distribution mismatch}
% \subsection{Calibrated model probability as adversarial labels}
% approximate the true label distribution}
\label{sect:approximate-true-distribution}





% \sout{We have shown that the double descent in adversarial training is due to the mismatch between the true label distribution and the assigned label distribution, which implicitly introduces label noise and promotes variance.} 
% \sout{A straightforward solution to double descent is thus to employ the true label distribution into training, based on which the label of the adversarial example can be sampled.
% However, since the perturbation radius allowed in adversarial training is typically small, the distribution mismatch might not be reflected by hard-label sampling if the training set is not sufficiently large. 
% To overcome this problem without significantly augmenting the training set, one can directly employ the true label distribution into the training objective.} 
% \sout{For cross-entropy loss it is easy to prove that the training objective with soft-label supervision is equivalent to that with hard labels sufficiently sampled based on the corresponding label distribution, namely}~\footnote{It is worth mentioning that $x$ in this section could either be a clean example or adversarial example. We therefore do not distinguish for simplicity.}
% \begin{equation}
%     \label{eq:loss-soft-label}
%     \mathbbm{E}_{(x, y) \sim p(x, y)} \ell(f(x; \theta), y) = \mathbbm{E}_{x \sim p(x)} \ell(f(x; \theta), p(y|x)).
% \end{equation}

    % \begin{theorem}[Error induced by an approximate label distribution]
    % \label{theorem: loss-error}
    % Let $\tilde{p}(y|x)$ be an approximation of $p(y|x)$, we have
    % \begin{equation}
    %     \ell(f(x;\theta), \tilde{p}) \le \ell(f(x;\theta), p) + 2 \left\|\tilde{p} - p\right\|_{TV} \left\|\log f(x; \theta)\right\|_1,
    % \end{equation}
    % where we neglect the expectation for simplicity. \jingbo{we may want to directly define the loss based on the label distribution as a weighted sum.}
    % \end{theorem}
    % It is worth mentioning that $x$ and $y$ here can be either a clean example or a adversarial example. 
    % One can simply replace them by $x + \delta$ and $y_\delta$ and the conclusion will still hold.
    % We therefore do not distinguish for simplicity.
    
    
% \note{We believe model probability can approach the true label distribution better than the assigned label} \chengyu{need this sentence as the next Section hasn't experiment on temperature and ratio yet. Or we have to emphasize in next section the optimal temperature is already 1.0!}




% \chengyu{We now wish to find such an assigned label of adversarial example such it is closer to the true label distribution $p_\theta(y_\delta | x)$}

%% -- Do Not Delete! -- \note{Problem of using calibration: validation set is not always available in practice. If a validation set is readily available, we can just use early stopping. Solution: We can also use other calibration methods without the need of a validation set, like learning well-calibrated confidence during training.}
%% --------------------

We show that it is possible to reduce the distribution mismatch by \emph{temperature scaling}~\citep{Hinton2015DistillingTK, Guo2017OnCO} enabled in the softmax function.
% We show that it is possible to reduce the distribution mismatch by utilizing the predictive probability of a classifier trained on traditional adversarial labels, which we refer as the \emph{model probability} for simplicity.
% % We denote the predictive probability of a classifier trained based on traditional adversarial labels as \textbf{model probability}. 
% We provide a theoretical guarantee to show that, with \emph{temperature scaling}~\citep{Hinton2015DistillingTK, Guo2017OnCO} enabled in the softmax function, model probability induces a distribution mismatch provably smaller than the traditional adversarial label.
% \note{Need another assumption here such the original label distribution is almost one-hot.} Given in notation section
% \begin{theorem}[Model probability induces smaller distribution mismatch than the traditional adversarial label]
\begin{theorem}[Temperature scaling can reduce the distribution mismatch]
\label{theorem: model-probability}
% \todo{Before we already show that robust model can learn the true label distribution. It is thus natural to use model probability as surrogate assigned label. Here we just need to show with temperature $T$ it can be better.}
    % Let $\mathbbm{1}(\hat{y})$ denote the one-hot vector of a label $\hat{y}$.
    Let $f_\theta(x; T)$ denote the predictive probability of a probabilistic classifier scaled by temperature $T$, namely $f_\theta(x; T)_j = \exp(z_j/T) / (\sum_j \exp(z_j / T)), $
    % $$
    %     f_\theta(x; T)_j = \frac{\exp(z_j/T)}{\sum_j \exp(z_j / T)},
    % $$
    where $z$ is the logits of the classifier from $x$. 
    Let $x'$ be an adversarial example correctly classified by a classifier $f_\theta$, i.e. $\argmax_j f_\theta(x')_j = y'$,
    % Assume the classifier can correctly classify an adversarial example $x_\delta$, i.e. $\argmax_j f(x_\delta; \theta, T)_j = p^*_\delta$,
    then there exists $T$, such that
    $$
    % \| f(x_\delta; \theta, T) - p(y^*_\delta|x) \|_{TV} \le \| \mathbbm{1}(\hat{y}) - p\|_{TV}.
    % \| f_\theta(x_\delta; T) - P(Y^*_\delta|x_\delta) \|_{TV} \le \| P(\tilde{Y}_\delta | x_\delta) - P(Y^*_\delta | x_\delta)\|_{TV}.
    \| f_\theta(x'; T) - P(Y'|x') \|_{TV} \le \| f_\theta(x') - P(Y' | x')\|_{TV}.
    $$

    % where $\mathbbm{1}(\hat{y})$ is the one-hot vector of $\hat{y}$.
\end{theorem}

% \todo{Replace the definition of traditional adversarial label  by the distribution of clean input directly.}


    Another way to further reduce the distribution mismatch is to interpolate between the model probability and the one-hot assigned label.
    % the traditional adversarial label. 
    We show that the interpolation works specifically for incorrectly classified examples and thus can be viewed as a complement to temperature scaling.
    \begin{theorem}[Interpolation can further reduce the distribution mismatch]
    \label{theorem: model-probability-coefficient}
    % \chengyu{Will it be a problem if $y$ is now no longer the argmax?}
        % Assume the classifier $f$ incorrectly classify an adversarial example $x_\delta$, i.e. $\argmax_j f(x_\delta; \theta)_j \ne p^*_\delta$.
        Let $x'$ be an adversarial example incorrectly classified by a classifier $f_\theta$, i.e. $\argmax_j f_\theta(x'; T)_j \ne y'$. % \argmax_j p(y=j|x)$. 
        % Assume $\hat{y} = \argmax_j~p(y=j|x)$ and $\max_j p(y=j|x) \ge 1/2$, then there exists $\lambda$, such that
        Assume $\max_j P(Y'=j|x') \ge 1/2$, then there exists an interpolation ratio $\lambda$, such that
        \begin{equation*}
        \small
        % \|  P_{\theta}^{T, \lambda}(Y'|x') - P(Y' | x') \|_{TV} \le \| f_\theta(x_\delta; T) - P(Y_\delta^* | x_\delta)\|_{TV},  
         \| f_\theta(x'; T, \lambda) - P(Y' | x') \|_{TV} \le \| f_\theta(x'; T) - P(Y' | x')\|_{\text{TV}},
        \normalsize
        \end{equation*}
        where $f_\theta(x'; T, \lambda) = \lambda \cdot f_\theta(x'; T) + (1 - \lambda)\cdot P(\tilde{Y}' | x')$.
        % where $P_{\theta}^{T, \lambda}(Y_\delta|x_\delta) = \lambda \cdot f_\theta(x_\delta; T) + (1 - \lambda)\cdot P(\tilde{Y}_\delta | x_\delta) $.
        % \jingbo{Also, did you forget the include $T$ in $f$?}\chengyu{This theorem should work for any $T$, so i neglect it.}
    \end{theorem}
    % Note the above theorem focus on incorrectly classified examples and thus can be regarded as a complement to Theorem~\ref{theorem: model-probability}.
    % We show that a proper interpolation approximates provably better for incorrectly classified examples, therefore this theorem can be regarded as a complement to the above one.
    
    
    As a summarization, 
    % an potentially better approximation of the true label distribution based on a model can be formulated as
    to reduce the distribution mismatch, we propose to use 
    $f_\theta(x'; T, \lambda)$
    % $P_{\theta}^{T, \lambda}(Y_\delta|x_\delta)$ 
    % following distribution 
    as the assigned label of the adversarial example in adversarial training, which we refer as the \emph{rectified model probability}.
    % \begin{equation}
    %     \label{eq:approximate-label-distribution}
    %     P_{\theta}^{T, \lambda}(Y_\delta|x_\delta) = \lambda \cdot f_\theta(x_\delta; T) + (1- \lambda) \cdot P(\tilde{Y}_\delta | x_\delta),
    % \end{equation}
    % We refer this label distribution as the \textbf{rectified model probability}.
    
    In Appendix~\ref{sect:optimal-temperature-mixup}, we show that the optimal hyper-parameters (i.e. $T$ and $\lambda$) of almost all training examples concentrate on the same set of values by studying on a synthetic dataset with known true label distribution. 
    % \jingbo{can we name the values here?}
    Therefore it is possible to find an universal set of hyper-parameters that reduce the distribution mismatch for all adversarial examples. 
    % the rectified model probability can often approximate the true distribution sufficiently well with


