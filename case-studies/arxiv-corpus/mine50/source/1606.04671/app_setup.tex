\section{Setup Details}
\label{sec:appendix_jobs_detail}

In our grid we sample hyper-parameters from categorical distributions:
\begin{itemize}
  \item Learning rate was sampled from $\{10^{-3}, 5\cdot 10^{-4}, 10^{-4}\}$.
  \item Strength of the entropy regularization from $\{10^{-2}, 10^{-3}, 10^{-4}\}$
  \item Gradient clipping cut-off from $\{20, 40\}$
  \item scalar multiplier on the lateral feature is initialized randomly to one from $\{1, 10^{-1}, 10^{-2}\}$
\end{itemize}

For the Atari experiments we used a model with 3 convolutional layers followed by a fully connected
layer and from which we predict the policy and value function. The convolutional layers are as
follows. All have 12 feature maps. The first convolutional layer has a kernel of size 8x8 and a stride
of 4x4. The second layer has a kernel of size 4 and a stride of 2. The last convolutional layer has
size 3x4 with a stride of 1.  The fully connected layer has 256 hidden units.

Learning follows closely the paradigm described in \citep{mnih2016a3c}. We use 16 workers and the
same RMSProp algorithm without momentum or centring of the variance. The score for each point of
a training curve is the average over all the episodes the model gets to finish in $25e4$ environment steps.

The whole experiments are run for a maximum of $1.6e8$ environment step. The agent has an action repeat of 4 as
in \cite{mnih2016a3c}, which means that for 4 consecutive steps the agent will use the same action picked at the
beginning of the series. For this reason through out the paper we actually report results in terms of agent
perceived steps rather than environment steps. That is, the maximal number of agent perceived step that we do
for any particular run is $4e7$.
