
\section{Introduction}

Machine learning fundamentally relies on the availability of data, which can
be sensitive or confidential.
It is now well-known that preventing learned models from leaking information
about individual training points requires particular attention
\citep{shokri2017Membership}.
A standard approach for training models while provably controlling the amount of
leakage is to solve an empirical risk minimization (ERM) problem
under a differential privacy (DP) constraint \citep{chaudhuri2011Differentially}.
In this work, we aim to design a differentially private algorithm which
approximates the solution to a composite ERM problem of the form:
\begin{align}
  \label{eq:dp-erm}
  w^* \in
  \argmin_{w \in \mathbb{R}^p}
  \left\{
  \frac{1}{n} \sum_{i=1}^n \ell(w; d_i) + \psi(w)
  \right\}
  \enspace,
\end{align}
where $D = (d_1, \dots, d_n)$
is a dataset of $n$ samples drawn from a universe $\cX$,
$\ell: \RR^p \times \cX \rightarrow \RR$ is a loss function which is convex
and smooth in $w$, and
$\psi: \RR^p \rightarrow \RR$ is a convex regularizer which is separable (\ie
$\psi(w) = \sum_{j=1}^p \psi_j(w_j)$) and typically nonsmooth (\eg
$\ell_1$-norm).

Differential privacy constraints induce a trade-off between the privacy and
the utility (i.e., optimization error) of the solution of~\eqref{eq:dp-erm}.
This trade-off was made explicit by \citet{bassily2014Private}, who derived
lower bounds on the achievable error given a fixed privacy budget.
To solve the DP-ERM problem in practice, the most popular approaches are based
on Differentially Private variants of Stochastic Gradient Descent (DP-SGD)
\citep{bassily2014Private,abadi2016Deep,wang2017Differentially}, in which
random perturbations are added to the (stochastic) gradients.
\citet{bassily2014Private} analyzed DP-SGD in the non-smooth DP-ERM setting,
and \citet{wang2017Differentially} then proposed an efficient DP-SVRG
algorithm for composite DP-ERM.
Both algorithms match known lower bounds.
SGD-style algorithms perform well in a wide variety of settings, but
also have some flaws: they either require small (or decreasing) step
sizes or variance reduction schemes to guarantee convergence, and they
can be slow when gradients' coordinates are imbalanced.
These flaws propagate to the private counterparts of these
algorithms.
Despite a few attempts at designing other differentially private solvers for
ERM under different setups
\citep{talwar2015Nearly,damaskinos2021Differentially}, the differentially
private optimization toolbox remains limited, which undoubtedly restricts the
resolution of practical problems.



In this paper, we propose and analyze a Differentially Private proximal
Coordinate
Descent algorithm (DP-CD), which performs updates based on perturbed
coordinate-wise gradients (\ie partial derivatives).  Coordinate
Descent (CD) methods have encountered a large success in non-private
machine learning due to their simplicity and effectiveness
\citep{liu2009Blockwise,friedman2010Regularization,chang2008Coordinate,sardy2000Block},
and have seen a surge of practical and theoretical interest in the
last decade \citep{Nesterov12,wright2015Coordinate,shi2017Primer,
  richtarik2014Iteration,fercoq2014Accelerated,tappenden2016Inexact,
  hanzely2020Variance,nutini2015Coordinate,karimireddy2019Efficient}.
In contrast to SGD, they converge with constant step sizes that adapt to
the coordinate-wise smoothness of
the objective. Additionally, CD updates naturally tend to
have a lower sensitivity. Operating with partial gradients thus enables
our private algorithm to reduce the perturbation required to
guarantee privacy without resorting to
amplification by
subsampling \citep{Balle_subsampling,mironov2019Enyi}.


We propose a novel analysis of proximal CD with perturbed gradients to
derive optimal upper bounds on the privacy-utility trade-off achieved
by DP-CD.
We prove a
recursion on distances of CD iterates to an optimal point that keeps track of
coordinate-wise regularity
constants in a tight manner and allows to use
large, constant step sizes that
yield high utility. Our results highlight the fact that DP-CD
can exploit imbalanced gradient coordinates to outperform DP-SGD.
They also improve upon known convergence rates for inexact CD in the
non-private setting
\citep{tappenden2016Inexact}.
We assess the optimality of DP-CD by deriving lower bounds
that capture coordinate-wise Lipschitz regularity measures, and show that
DP-CD matches those bounds up to logarithmic factors.
Our lower bounds also suggest interesting perspectives for future work on
DP-CD algorithms.

Our theoretical results
have important consequences for practical
implementations, which heavily rely on gradient clipping to achieve good
utility.
In contrast to DP-SGD, DP-CD requires to set \emph{coordinate-wise} clipping
thresholds, which can lead to impractical coordinate-wise hyperparameter tuning.
We instead propose a simple rule for adapting these thresholds from a
single hyperparameter. We also show how the coordinate-wise smoothness
constants used by DP-CD can be
estimated privately. We validate our theory with numerical
experiments on real and synthetic datasets. These experiments further
show that even in balanced problems, DP-CD can still improve over
DP-SGD, confirming the relevance of DP-CD for DP-ERM.

Our main contributions can be summarized as follows:
\begin{enumerate}
  \item We propose the first proximal CD algorithm for composite DP-ERM,
        formally prove its utility, and highlight regimes where it outperforms DP-SGD.
  \item We show matching lower bounds under coordinate-wise regularity
        assumptions.
      \item We give practical guidelines to use DP-CD, and show its
        relevance through numerical experiments.
\end{enumerate}


The rest of this paper is organized as follows.
We first describe some mathematical background in
\Cref{sec:preliminaries}.
In \Cref{sec:diff-priv-coord}, we present our DP-CD algorithm,
show that it satisfies DP, establish utility guarantees, and
compare these guarantees with those of DP-SGD.
In \Cref{sec:utility-lower-bounds}, we derive lower bounds under
coordinate-wise regularity assumptions, and
show that DP-CD can match them. \Cref{sec:dp-cd-practice} discusses practical
questions related to gradient clipping and the private estimation of
smoothness constants.
\Cref{sec:numerical-experiments} presents our numerical experiments,
comparing DP-CD and DP-SGD on LASSO and $\ell_2$-regularized
logistic regression problems. %
Finally, we review existing work in
\Cref{sec:related-works}, and conclude with promising lines of future work in
\Cref{sec:conclusion-and-discussion}.

