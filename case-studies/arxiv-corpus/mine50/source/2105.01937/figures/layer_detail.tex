\begin{table*}[bht]
\setlength{\abovecaptionskip}{5pt plus 3pt minus 2pt}
\setlength{\belowcaptionskip}{0pt plus 3pt minus 2pt}
\caption{FLEX structure. $\bJ$ denotes the number of joints, $\bK$ the number of views, and $\bL$ denotes the number of limbs. $k$ and $s$ denote kernel width and the stride, respectively. $\rightarrow$ denotes parallel convolutions while $\downarrow$ denotes sequential ones.}
\resizebox{\textwidth}{!}{
\begin{tabular}{| l | c | l | c | c | c |}
\hline
\toprule
Name & & Layers & $k$ & $s$ & in\; /\; out \\
\hline
\midrule %motion encoder 
$F_Q$  & &1D-Conv + BatchNorm + LReLU + Dropout  & $1$ & 1 & $3J\; /\; 1024$\\
\cline{2-5}
& $\rightarrow$ &2D-Conv + BatchNorm + LReLU + Dropout + Adap AP & $5$ & 3 & $1024\; /\; 1024$\\
& $\rightarrow$ &2D-Conv + BatchNorm + LReLU + Dropout + Adap AP & $3$ & 1 & $1024\; /\; 1024$\\
& $\rightarrow$ &2D-Conv + BatchNorm + LReLU + Dropout + Adap AP & $1$ & 1 & $1024\; /\; 1024$\\
\cline{2-5}
% & &Transformer Encoder (2 attention heads) & $1$ & 1 & $1024\; /\; 1024$\\
% & &Transformer Encoder (2 attention heads) & $1$ & 1 & $1024\; /\; 1024$\\
& & Multi-head Attention layer (64 heads) & $-$ & $-$ & $1024\; /\; 1024$\\

\hline
$E_Q$ & $\rightarrow$ &1D-Conv + BatchNorm + LReLU + Dropout + Adap AP & $5$ & 3 & $1024\; /\; 1024$\\
& $\rightarrow$ &1D-Conv + BatchNorm + LReLU + Dropout + Adap AP & $3$ & 1 & $1024\; /\; 1024$\\
& $\rightarrow$ &1D-Conv + BatchNorm + LReLU + Dropout + Adap AP & $1$ & 1 & $1024\; /\; 1024$\\
\cline{2-5}
& & 1D-Conv & $1$ & 1 & $1024 / 4(J\!-\!1)\!+\!4K$\\
\hline
\midrule %body encoder
$D$ & & 1D-Conv + ReLU  & $1$ & 1 & $4J \; /\; 1024$\\
& $\downarrow$&1D-Conv + ReLU + Adap AP & $1$ & 1 & $1024\; /\; 24J$\\
& &Linear & $-$ & $-$ & $24J \; /\; J$\\
\hline
\midrule %body encoder
$F_S$ & & 1D-Conv + BatchNorm + LReLU + Dropout  & $1$ & 1 & $J3\; /\; 1024$\\
\cline{2-5}
& &1D-Conv + BatchNorm + LReLU + Dropout & $5$ & 1 & $1024\; /\; 1024$\\
& $\downarrow$ &1D-Conv + BatchNorm + LReLU + Dropout & $3$ & 1 & $1024\; /\; 1024$\\
& &1D-Conv + BatchNorm + LReLU + Dropout & $1$ & 1 & $1024\; /\; 1024$\\
\cline{2-5}
& &1D-Conv + BatchNorm + LReLU + Dropout & $5$ & 1 & $1024\; /\; 1024$\\
& $\downarrow$ &1D-Conv + BatchNorm + LReLU + Dropout & $3$ & 1 & $1024\; /\; 1024$\\
& &1D-Conv + BatchNorm + LReLU + Dropout & $1$ & 1 & $1024\; /\; 1024$\\
\cline{2-5}
% & &Transformer Encoder (2 attention heads) & $1$ & 1 & $1024\; /\; 1024$\\
% & &Transformer Encoder (2 attention heads) & $1$ & 1 & $1024\; /\; 1024$\\
& & Multi-head Attention layer (64 heads) & $-$ & $-$ & $1024\; /\; 1024$\\

\hline
$E_S$ & &Adaptive MP & $-$ & $-$ & $-$\\
& &1D-Conv & $1$ & 1 & $1024\; /\; L$\\
\hline
\bottomrule
\end{tabular}
} % end resizebox
\setlength{\abovecaptionskip}{0pt plus 3pt minus 2pt}
\setlength{\belowcaptionskip}{-20pt plus 3pt minus 2pt}
\caption*{}
\label{tab:layer_detail}
\end{table*}