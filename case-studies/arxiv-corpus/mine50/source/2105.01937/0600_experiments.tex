\sectiontinyvert{Experiments and evaluation} \label{sec:experiments}

%We evaluate FLEX quantitatively on the Human3.6M human %pose~\cite{h36m_pami,IonescuSminchisescu11} dataset, and qualitatively on the Human3.M %and the KTH Multi-view  Football II~\cite{footballDS} datasets,
%and on synthetic 
%multi-person 
%videos captured by dynamic cameras. 

We present quantitative results on the Human3.6M~\cite{h36m_pami,IonescuSminchisescu11} and Ski-Pose PTZ-Camera~\cite{ski_ptz} datasets. We present qualitative results on the Human3.6M,  KTH Multi-view Football II~\cite{footballDS} and Ski-Pose PTZ-Camera~\cite{ski_ptz} datasets,
and on synthetic 
%multi-person 
videos captured by dynamic cameras. Detailed description of these datasets can be found in the supplementary material.

%%%%%%%%%%%%%%%%%%%%%%
\paragraphtinyvert{Quantitative results} 

\input{./figures/quantitative_table}

We show quantitative results using the Mean Per Joint Position Error (MPJPE)~\cite{h36m_pami,IonescuSminchisescu11}, 
%\sr{, which is the average joint location error.}
%\brchange{}{Deleted MPJPE equation from here}
% \begin{equation}
% E(\tilde{\bP},\bP) =\frac{1}{J}\sum_{j=1}^J \Vert\tilde{\bP}_j - \bP_j\Vert.
% \label{eq:mpjpe}
% \end{equation}
%which is the common metric to evaluate pose and motion reconstruction. % works report it, so they provide a means to the comparison. 
%Although MPJPE fits pose estimation more than motion reconstruction, we show comparable results using it.
and report standard protocol \#1 MPJPE (that is, error relative to the pelvis), in millimeters.

\Cref{tab:quant_human36} presents a quantitative comparison of the MPJPE metric on the Human3.6M~\cite{h36m_pami} dataset. 
%We start with results of monocular methods, proceed with multi-view methods that require camera parameters, and conclude with multi-view methods where camera parameters are not given. 
\sr{We present monocular methods, followed by multi-view ones that are split into ones that are acquainted with camera parameters and ones that are not.}
% are split into ones that use extrinsic camera parameters and ones that do not.
We show that in the absence of camera parameters, our model outperforms state-of-the-art methods by a large margin, and that even when camera parameters are available, 
FLEX is among the top methods. Note that these achievements are although FLEX aims at a slightly different task, which is motion reconstruction rather than pose estimation.

Being the only ep-free algorithm, we have no methods to compare to directly. However, algorithms can mitigate the lack of extrinsic camera parameters by estimating them. In the following comparisons, we show that when extrinsic parameters are not given, using estimated ones induces larger prediction errors, due to the innate inaccuracy of predicted values. On the other hand, FLEX is not affected by the lack of extrinsic parameters, since it does not use them whatsoever.
We compare FLEX with two models:
\begin{enumerate}[nosep,leftmargin=0cm,itemindent=0.5cm,labelwidth=\itemindent,labelsep=0cm,align=left]
\item[(1)]
% \paragraphtinyvert{Works that estimate extrinsic parameters}
%The first one is 
There are two methods that do not use given camera parameters~\cite{chu_and_pan_semisupervised,kocabas2019selfsupervised}. 
They are not ep-free since they use estimated camera parameters, but we can still use them in settings where camera parameters are not given.
Only one of them~\cite{chu_and_pan_semisupervised} publishes MPJPE protocol \#1 results, and we significantly outperform it
%by a large margin 
%Chu and Pan~\cite{chu_and_pan_semisupervised} publish MPJPE protocol \#1 results so we compare to them.
%Their MPJPE error is 56.9 compared to 30.2 for FLEX 
(See \Cref{tab:quant_human36}). 
%We deduce that this gap is partly due to the inaccuracy of predicted parameters, and partly due to the fact that their model is semi-supervised. 
This gap is mostly because of the inaccuracy of parameter prediction and partially because their model is semi-supervised. 
    
\item[(2)] 
% \paragraphtinyvert{Works with perturbed camera parameters}
%The second one is 
For comparing with the best available method, 
we have chosen the current state-of-the-art multi-view algorithm of Iskakov \etal~\cite{iskakov2019learnable} (TesseTrack~\cite{Reddy2021TesseTrackEL} is marginally better, but it does not provide code). Since their algorithm is \emph{not} ep-free, we
imitate parameter estimation by running a controlled perturbation of the camera parameters.
%In their work, they use data that have undergone distortion rectification. 
We re-train their method with distorted data to simulate an environment where camera distortion parameters are unknown. 
In addition, we perturb the 
%intrinsic and extrinsic 
extrinsic parameters by Gaussian noise with an extremely small standard deviation of 3\% of each parameter's 
%absolute 
value. That is, for a parameter $p$, we sample $\tilde{p}\sim\nn(p,(0.03 p)^2)$ and use $\tilde{p}$ as the input extrinsic parameter. 
%We chose to perturb the parameters rather than estimate them, in order to be more fair towards the compared methods, as estimated parameters induce large inaccuracy~\cite{chu_and_pan_semisupervised,wandt2020canonpose,chen2021deductive} \sr{(shown below)}. 
%\bg{To validate that the added noise is extremely small, we estimate the cameras extrinsics parameters with two different known frameworks: COLMAP\cite{schoenberger2016sfm} and OpenCV-SFM\cite{opencv_library}. 
% Using the COLMAP framework requires shared visual features between the cameras, a fact that is an important limitation when working with "facing cameras" as there is very small amount of shared visual features. The estimation with this framework lead to an relative error of 5.5\% between the estimated and real-value of the parameters. For the OpenCV-SFM framework we use the 2D joints locations from each view to estimate the camera extrinsincs, this experiment results in a relative error of 8.6\%.}
We show that increasing the standard deviation from 3\% to 4\% yields a significant increase in the error, reflecting the sensitivity of non ep-free methods to  inaccuracy in camera parameters.
To obtain an equivalent environment, we compare FLEX to the method of Iskakov \etal~\shortcite{iskakov2019learnable} after using their own 2D pose estimation.
The lower part of \Cref{tab:quant_human36} shows that FLEX outperforms the non ep-free state-of-the-art, even when perturbation percentage is extremely small.
%The results of Iskakov \etal~\cite{iskakov2019learnable} 
Their results
in that lower part 
    %of Table~\ref{tab:quant_human36} 
are grayed out, to emphasize that we simulate an unrealistic setting.
%(in reality the estimation error of the camera parameters is larger). 
\sr{Next, we show that a 3\% perturbation, rather than estimation, is fairer toward the compared method, as estimation induces larger inaccuracy.
%Next, we show that parameter estimation is less accurate comparing to a 3\%-4\% perturbation, hence perturbation is fairer toward the compaired method.
%We measure the inaccuracy of estimated extrinsic parameters with two leading frameworks: COLMAP~\cite{schoenberger2016sfm} and OpenCV-SFM~\cite{opencv_library}. COLMAP requires that each pair of cameras observes partially overlapping images, a limiting factor that prevents its usage in settings where the cameras face each other. OpenCV-SFM strongly depends on an initial guess. The mean error, computed by $\frac{|p-\tilde{p}|}{p}$ for all extrinsic values $p$ and their estimation $\tilde{p}$, is 5.5\% and 8.6\% for COLMAP and OpenCV-SFM, respectively.}
%
We estimate the extrinsic camera parameters with two leading frameworks, COLMAP~\cite{schoenberger2016sfm} and OpenCV-SFM~\cite{opencv_library}, and obtain errors of 5.5\% and 8.6\%, respectively. The error is the mean value of $\frac{|p-\tilde{p}|}{p}$ for all extrinsic values $p$ and their estimation $\tilde{p}$. Moreover, the estimation process involves friction: OpenCV-SFM 
strongly depends on an initial guess, and COLMAP requires that each pair of cameras observes partially overlapping images, a limiting factor that prevents its usage in settings where the cameras face each other.}
\end{enumerate}

\input{./figures/quantitative_ski}
In addition to the comprehensive comparison on the Human3.6 dataset, 
in \Cref{tab:ski_quantitative} we show a quantitative comparison on the Ski-Pose PTZ-Camera~\cite{ski_ptz} dataset, for methods that are trained when camera parameters are \emph{not} given.
These methods are comparable in settings that lack extrinsic parameters because they estimate them. 
However, since they still use (estimated) parameters, they are not ep-free.
FLEX leads the table with a large gap.
This gap is mostly because parameter estimation induces an inevitable inaccuracy, and partially because the compared models are self/semi-supervised. 

\input{./figures/double_tables_acceleration_fusion_arch_ablation}

A known strength of predicting rotation angles rather than locations, is the \emph{smoothness} of predicted motion. 
% In \Cref{tab:acc_error} we compare the smoothness of our work against others, using a metric proposed by Kanazawa \etal~\cite{kanazawa2019learning}. Smoothness is quantified by the acceleration error of each joint. Notice our work leads the table by a large margin.
In \Cref{tab:acc_error} we show that FLEX's smoothness result outperforms others by a large margin. Following Kanazawa \etal~\cite{kanazawa2019learning}, we measure smoothness using the acceleration error of each joint. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraphtinyvert{Qualitative results}


In the following figures we show rigs, that is, bone structure 
%in frames 
from reconstructed animation videos, selecting challenging %postures and 
scenes. Videos of the reconstructed motions are  
\ifanonymous{provided \iftog{as well,}\else{in the sup. mat.,}\fi}
\else{available on our project page,}
\fi
presenting the smoothness of motion and the naturalness of rotations. 
\Cref{fig:football_teaser,fig:quality_h36,fig:ski_ptz_qualitative}
 show scenes from the KTH Multi-view Football II~\cite{footballDS}, the Human3.6M~\cite{h36m_pami,IonescuSminchisescu11} and the Ski-Pose PTZ-Camera~\cite{ski_ptz} datasets, respectively. 
Each row depicts three views of one time frame. To the right of each image, we place a reconstructed rig, which is sometimes zoomed in for better visualization. 
%KTH results are computed by using the model that was trained on Human3.6M, without re-training it on this one, demonstrating that FLEX generalizes well. 
Notice the occluded and blurry scenes in the football figure (\ref{fig:football_teaser}). The KTH Football dataset is filmed using dynamic (moving) cameras, 
a setting where extrinsic parameters are rarely given, thus disqualifying methods that require camera parameters.
%hence it is challenging for other multi-view algorithms to reconstruct pose/motion over it without using the given camera parameters.
Our algorithm is agnostic to the lack of camera parameters and attains good qualitative results. 

%\brchange{}{\textbf{REMOVED FROM HERE KTH FIGURE WITH BLURRED WAVING HAND TO SUP.MAT. \ref{fig:quality_football_hand_wave}}}
% In Figure \ref{fig:quality_football_hand_wave} we show that our algorithm is able to grasp fine details. The player's left hand cannot be seen in the center view and is blurred in the left views. Yet, our model accurately reconstructs it.
In \Cref{fig:competitors} we show qualitative results of FLEX, compared to current non ep-free  multi-view state-of-the-art \cite{iskakov2019learnable}, and to our monocular baseline~\cite{shi2020motionet}. 
Note that the method in \cite{iskakov2019learnable} produces unnatural poses such as a huge leg in the first row and a backward-bent elbow in the last row. 
\input{./figures/h36m_and_ski_qualitative}
\input{./figures/competitors_and_root_pos_vertical}
\input{./figures/fight_results_with_circles}


\paragraphtinyvert{Multi-person captured by dynamic cameras} 
We evaluate our algorithm on
a setting with dynamic cameras, with multi-person scenes introducing
severe inter-person occlusions.
% multi-person scenes, where severe inter-person occlusions are present, and the cameras are dynamic.
Recall that the term \emph{dynamic} refers to moving cameras that occasionally change their location and rotation.
%
There are several multi-view datasets. Most of them are not fully dynamic: Human3.6M~\cite{h36m_pami,IonescuSminchisescu11}, CMU Panoptic~\cite{CMU:mocap} and TUM Shelf $\&$ Campus~\cite{campus_shelf} contain static scenes only, while Tagging~\cite{tagging_dataset} and Ski-Pose PTZ-Camera~\cite{ski_ptz} contain  rotating cameras whose locations are fixed. KTH~\cite{footballDS} is fully dynamic, but it is too blurry and does not provide ground-truth for all subjects.
Despite its limitations, we use the KTH dataset for qualitative analysis, but we cannot use it for thorough research.
To mitigate the lack of a dynamic dataset, we generate synthetic videos using animated characters downloaded from Mixamo~\cite{mixamo}, an online dataset of character animation. Then, we generate video sequences of two interacting characters using Blender~\cite{blender}, which is a 3D creation suite. The newly created data is available on our project page.
Our "synthetic studio" is illustrated at the \ifarxiv{appendix}\else{sup. mat.}\fi, where two interacting figures are video-filmed by multiple dynamic cameras.
Using Blender, we obtain a rendered video stream from the view angle of each synthetic camera. 
Recall that the input to our algorithm is 2D joint locations, hence it is agnostic to the video appearance, and to whether the input image is real or synthetic.

The 2D backbone we use over the rendered video sequences is Alphapose~\cite{alphapose}, a state-of-the-art multi-person 2D pose estimator.
Once obtaining the 2D joint locations, we use a na\"ive heuristic, which is not part of the suggested algorithm, to associate each detected person with its ID: for each frame, we associate the detected 2D pose with the one that is geometrically closest to it in the previous frame. 
% This heuristic is not part of the suggested algorithm. 
%It simply uses the assumption that the motion between two consecutive frames is minimal. Thus, for each frame, we associate the detected 2D pose with the one that is geometrically closest to it in the previous frame.
In \Cref{fig:fight_results_with_circles} we depict qualitative results of two boxers. We emphasize several viewpoints where the 2D estimator attains large errors. Yet, FLEX compensates for these errors by fusing multi-view information. In the \ifarxiv{appendix }\else{sup. mat. }\fi we show additional characters and the predicted 2D pose for all the viewpoints.

%%%%%%%%%%%%%%%%%%%%
\paragraphtinyvert{Global position}

In \Cref{fig:root_pos} we draw the global position of the scaled predicted root joint along time.
Ground-truth is depicted using a thin black curve, and our prediction is an overlay on top of it, changing from light to dark as time progresses. The start and the end of each trajectory are signaled by the letters S and E, respectively.
Depicted motions are evaluated on the test set of Human3.6M, on the motions of walking, talking on the phone, and eating.
Note that our predictions almost completely overlap the ground-truth curve. 
Recall we use weak perspective to bypass dependency on intrinsic parameters, resulting in up-to-scale global position accuracy. 
\sr{Quantitatively, our MPJPE on the H36M validation
set is $118mm$, outperforming Iskakov \etal~\cite{iskakov2019learnable} (perturbed by 3\%) that attain $123mm$. The other ep-free work \cite{chu_and_pan_semisupervised} does not solve global locations.}
%\bg{FLEX absolute positions MPJPE results to 118mm error, while Iskakov \etal~\cite{iskakov2019learnable} perturbed with noise (as explained in Quantitative Results section) produces an MPJPE of 123mm. The other ep-free work \cite{chu_and_pan_semisupervised} does not solve global locations whatsoever.}

% {
% \color{red}
% \input{./figures/global_mpjpe_table}
% }

%%%%%%%%%%%%%%%%%%%%
\paragraphtinyvert{Ablation study}

\ifeccv{
    \input{./figures/triple_table_ablation}
}
\else{
    \input{./figures/double_tables}
    \input{./figures/ablation}
}
\fi

We evaluate the impact of different settings on the performance of FLEX using various ablation tests. 
\Cref{tab:fusion_arch} compares different multi-view fusion architectures.  
% Notice there is a 2mm performance improvement using our multi-head attention layer. 
Note that using attention rather than convolution yields a 2mm improvement.
The performance degrades with the transformer encoder due to its large number of parameters, which require more data for training than what is available in our case.

\Cref{tab:num_views} measures MPJPE on Human3.6M in several studies.
\Cref{tab:num_views}(a) studies a varying number of views, where the 2D pose is once given and once estimated. It confirms that a larger number of views induces more accurate results. Note that the gap between the two columns decreases once the number of views increases. It shows that using several views compensates for the inaccuracy of estimated 2D poses. 
%
\Cref{tab:2D_backbone}(b) compares 2D pose estimation backbones, and justifies our use of Iskakov \etal~\cite{iskakov2019learnable}.
%. It justifies using the pose estimation used by Iskakov \etal~\cite{iskakov2019learnable} as our 2D pose backbone. 
%
Finally, in \Cref{tab:ablation}(c) we explore
% measure our network performance using 
two variations, both with ground-truth 2D inputs. 
The first variation runs FLEX as a monocular method (K=1) and averages the monocular predictions.
%, and show that it yields results that are inferior to fusing the views by our network. 
%
The second changes the fusion layers, $F_S$ and $F_Q$, to use late fusion instead of an early one. 
We conclude that the configuration used by FLEX is better than both variations.
%We find that early fusion works better in our setting. 
% All models use ground-truth 2D poses. 

\input{./figures/generalization_inset}

\setcolor{violet}
\paragraphtinyvert{Generalization}
%
% The evaluation on the KTH Football dataset and the synthetic videos
% is done with a model trained on Human3.6M, demonstrating generalization.
%
% We demonstrate  \\ generalization by training on one dataset, Human3.6M, and evaluating on a different, more challenging, dataset, KTH Football, and on the synthetic videos.
We exhibit generalization by training on one dataset and evaluating on a different, more challenging one. The train dataset is Human3.6M, and the evaluation ones are the KTH Football dataset, and the synthetic videos.
%
%In \Cref{fig:football_teaser} and in the sup. mat. we show results in which FLEX was trained on the Human3.6M dataset and evaluated on the Football dataset.
For quantitative measurement, we train our model on two of the four cameras of the Human3.6M dataset.
We test it using the other two cameras, on which the model has not been trained. 
We repeat this process for all possible camera pairs and obtain an average MPJPE of 148mm. Note that this error is not large compared to the human body size, and indeed we attain pleasing visual results as shown in the inset on the right.
\setcolor{black}

