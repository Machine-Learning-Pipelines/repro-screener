\label{sec:hessian}

\rnote{Title of this section is a bit long and don't have a real focus. We might consider break it into multiple sections? One possibility is that we talk about the decomposition, then talk about properties of $xx^T$ (both zero-mean and non-zero-mean case) and briefly talk about properties of $M$ (mostly empirical); in the next section we can then use this decomposition to justify (a) Hessian overlap; (b) low rank structure of the top eigenvectors; (c) rank C-1 Hessian}

The fact that layer-wise Hessian for fully connected layers can be decomposed into the expectation of Kronecker products as in \cref{eqn:decomp} poses a natural question: Can the Hessian be approximated using the Kronecker product of the expectations? That is, whether 
\begin{equation}
    \HessL^{(p)}(\theta) = \E\left[\mM\otimes \vx\vx^T\right] \approx \E[\mM] \otimes \E[\vx\vx^T]. 
\end{equation}

\rnote{Maybe we should call the above a conjecture with a name (something like ``the decoupling conjecture''?), then we say that empirically we show the conjecture is true, and this has a lot of benefits.}

If yes, we can analyze the layer-wise Hessian by analyzing of the two components. Note that $\E[\mM]$ is the Hessian of the layer-wise output with respect to empirical loss, and $\E[\vx\vx^T]$ is the auto-correlation matrix of the layer-wise inputs. For simplicity we call $\E[\mM]$ the output Hessian and $\E[\vx\vx^T]$ the input auto-correlation. 

Moreover, we can approximate the eigenvectors and eigenvalues of the layer-wise Hessian $\HessL^{(p)}(\theta)$ by $\E[\mM]$ and $\E[\vx\vx^T]$. Let $\vm$ and $\vv$ be an eigenvector of $\E[\mM]$ and $\E[\vx\vx^T]$ respectively, with corresponding eigenvalues $\lambda_\vm$ and $\lambda_\vv$. Since both matrices are positive semi-definite, $\vm \otimes \vv$ is an eigenvector of $\E[\mM] \otimes \E[\vx\vx^T]$ with eigenvalue $\lambda_\vm\lambda_\vv$. In this way, since $\E[\mM]$ has $n$ eigenvectors and $\E[\vx\vx^T]$ has $m$ eigenvectors, we can approximate all $mn$ eigenvectors for the layerwise Hessian.

As both $\E[\mM]$ and $\E[\vx\vx^T]$ can be computed in time comparable to back-propagation, algorithm then provides us with a fast inference of second order information. Also, explicit calculation of the layerwise Hessian requires $O(m^2n^2)$ space but our approximation only requires $O(m^2+n^2)$ space. 

Similar approximation for Fisher Information matrices have been used in K-FAC optimization algorithm \citep{martens2015optimizing}. While they gives efficient calculation for the matrix inverse, our analysis focuses more on the structure analysis. Also, unlike Fisher Information matrices, we can calculate top eigenvalues and eigenvectors of the Hessian efficiently \citep{yao2019pyhessian} so that the approximation of top eigenspace can be directly compared. \ynote{Put in for comparison, maybe unnecessary}

For convolutional layers (convs), we proposed a similar Kronecker factorization motivated by \citet{grosse2016kronecker}. We also denote it as $\E[\mM]\otimes \E[\vx\vx^T]$, with a different definition of $\mM$.(See \cref{sec:appendix_conv} for details) \ynote{Where should we put this paragraph?}.
\subsection{Hessian Approximation}
We run experiments on both CIFAR-10 and MNIST \cnote{May need to cite these datasets} using several different Fully Connected (FC) networks and LeNets, where we only analyze the FC layers\cnote{Not sure whether want to emphasize this}. All the experiments give qualitatively similar results. The results shown are for networks .... Other results are shown in the Appendix. The exact eigenvalues and eigenvectors of the layer-wise Hessians are computed using a modified Lanczos algorithm\znote{need description in appendix on the modified Lanczos.}

Empirical results show that this approximation works reasonably well, especially for the top eigenvalues and eigenspace, as shown in \cref{fig:approx_result} The eigenvectors are ranked in the order of decreasing eigenvalues. The overlap at dimension $k$ is the overlap of subspaces consist of top $k$ eigenvectors. We define the subspace overlap as the L2 average of the cosine of canonical angles: For $k$-dimensional subspaces $\mU,\mV$ in $\R^d$ ($d\geq k$) where the basis vectors $\vu_i$s and $\vv_i$s are column vectors, we define the subspace overlap of $\mU$ and $\mV$ as
\begin{equation}
    \Overlap(\mU, \mV) := \frac{1}{k}\|\mU^T\mV\|^2_F =\frac{1}{k}\|\cos\vtheta\|^2,
    \label{eqn:overlap}
\end{equation}
where $\vtheta$ is the size $k$ vector of canonical angles between $\mU$ and $\mV$. It is equivalent to the definition in \citet{gur2018gradient}.(Proof shown in appendix)

\begin{figure}[th]
    \centering
    \begin{subfigure}[b]{0.495\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[width=\textwidth]{Figures/ApproxQuality/FC2_fixlr/sample_kron_decomp_traceoverlap_d80_MNIST_Exp1_FC2_fixlr0.01R2_E-1.pdf}
        \caption{Subspace overlap between\\ approximated and exact layer-wise hessian}
        \label{fig:overlap_approx}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.495\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[width=\textwidth]{Figures/ApproxQuality/FC2_fixlr/eigenval_compare_top50_MNIST_Exp1_FC2_fixlr0.01R2_E-1_fc2.pdf}
        \caption{Top eigenvalues of approximated\\ and exact layer-wise hessian\cnote{Need to specify which layer it is.}}
        \label{fig:eigenval_approx}
    \end{subfigure}
    \caption{FC2 MNIST \znote{To be completed}}
    \label{fig:approx_result}
\end{figure}

\subsection{Eigenvector Correspondence}
\label{sec:eigen_corr}
\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.46\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[width=\textwidth]{Figures/Correspondence/LeNet5_fixlr0.01/xxT_Trueest_real_corr_expand_t200_CIFAR10_Exp1_LeNet5_fixlr0.01R2_E-1_fc1.pdf}
        \caption{$\E[\vx\vx^T]$}
        \label{fig:Corr_xxT_True_fc}
    \end{subfigure}
    \begin{subfigure}[t]{0.46\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[width=\textwidth]{Figures/Correspondence/LeNet5_fixlr0.01/UTAU_Trueest_real_corr_expand_t200_CIFAR10_Exp1_LeNet5_fixlr0.01R2_E-1_fc1.pdf}
        \caption{$\E[\mM]$}
        \label{fig:Corr_UTAU_True_fc}
    \end{subfigure}
    \begin{subfigure}[t]{0.065\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Misc/colorbar.pdf}
    \end{subfigure}
    \caption{Eigenvector Correspondence for fc1 in LeNet5 ($n$=120)}
    \label{fig:Corr_fc}
\end{figure}
\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.46\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[width=\textwidth]{Figures/Correspondence/LeNet5_fixlr0.01/Conv/xxT_Trueest_real_corr_expand_t50_CIFAR10_Exp1_LeNet5_fixlr0.01_E-1_conv2.pdf}
        \caption{$\E[\vx\vx^T]$}
        \label{fig:Corr_xxT_True_conv}
    \end{subfigure}
    \begin{subfigure}[t]{0.46\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[width=\textwidth]{Figures/Correspondence/LeNet5_fixlr0.01/Conv/UTAU_Trueest_real_corr_expand_t50_CIFAR10_Exp1_LeNet5_fixlr0.01_E-1_conv2.pdf}
        \caption{$\E[\mM]$}
        \label{fig:Corr_UTAU_True_conv}
    \end{subfigure}
    \begin{subfigure}[t]{0.04\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Misc/colorbar.pdf}
    \end{subfigure}
    \caption{Eigenvector Correspondence for conv2 in LeNet5 ($n$=16)}
    \label{fig:Corr_conv}
\end{figure}
To better visualize how eigenvectors of layer-wise Hessian are correlated with eigenvectors of $\E[\mM]$ and $\E[\vx\vx^T]$, we introduce `eigenvector correspondence matrices' as shown in \cref{fig:Corr_fc} and \cref{fig:Corr_conv}. Take $\E[\vx\vx^T]$ as an example. Let $\vv_i$ be the $i$th eigenvector of $\E[\vx\vx^T]$ and $\vh_j$ be the $j$th eigenvector of layer-wise Hessian, both ranking in decreasing order of eigenvalues. We can then define $\Krsp(\vv_i)$ as the $n$\cnote{What is $n$?} dimensional subspace of $\vv_i$'s Kronecker products\cnote{It's still confusing to me if you say ``one's Kronecker products'' because Kronecker product is a binary operator, perhaps use ``one's Kronecker products with xxx'' instead?} in $\R^{mn}$ where $\vh_j$ lives in\cnote{What's Krsp? Is it defined in the next sentence?}. Columns of $\mI_n \otimes \vv_i$ form the basis for $\Krsp(\vv_i)$. We can then define the correspondence between $\vv_i$ and $\vh_j$ as
\begin{equation}
    \Corr(\vv_i, \vh_j) = \|(\mI_n \otimes \vv_i)^T\vh_j\|^2.
\end{equation}
Its value is shown in the $(i,j)$-entry of the heatmap. The eigenvector correspondence for $\E[\mM]$ is defined in the same way, except that the subspace for $\E[\mM]$'s $i$th eigenvector $\vm_i$ is $\vm_i \otimes \mI_m$.
\cnote{I think this subsection is very hard for the readers to understand. My suggestion is to interpret our result in a reverse way, i.e., we first hypothesize that our approximation is totally accurate, then the top eigenvectors of Hessian should be the top eigenvector of $E[xx^T]$ Kronecker with the top eigenvectors of $E[M]$, and we design this method to test our hypothesis, and then show the figures. Besides, for the figures, I think we need more interpretations instead of letting the readers look at them directly. We can perhaps say ``We can see a diagonal pattern in Figure x, indicating that the $i$-th eigenvector of the layerwise Hessian indeed mainly comes from xxx''.}

\subsection{Further Approximation for the Top Eigenspace of Layerwise Hessian}
\label{sec:approx_top_eig}
If $\Corr(\vv_i, \vh_j) \approx 1$, we have $\vh_j \approx \vu \otimes \vv_i$, with some $\vu \in \R^n$. We can also say $\vh_j \in \Krsp(\vv_i)$. We define the first eigenvector of $\E[\vx\vx^T]$ as $\tx$. In both \cref{fig:Corr_fc} and \cref{fig:Corr_conv}, around $n$ top eigenvectors of layer-wise Hessian are all in $\Krsp(\tx)$.

Let the number of top eigenvectors of layerwise Hessian are approximated in $\Krsp(\tx)$ be $s$. In most cases as in \cref{fig:Corr_fc} and \cref{fig:Corr_conv}, $s \approx n$. Thus, for $i \leq s$, $i$th eigenvector $\vh_i \approx \vu_i \otimes \hE[\vx]$ for some $\vu_i \in \R^n$. Thus, for any $k \leq s$, the top $k$ eigenspace of the Hessian can be approximated as $\mU_k \otimes \hE[\vx]$, where columns of $\mU_k$ are $\vu_1, \ldots, \vu_k$.

\cnote{I fell like we are using too many formulas and a bit complicated notations, which will confuse the readers. Here perhaps we need to remind people what is Krsp and what is n. I think it's better to interpret things by words instead of formulas in the main text since this is not really a theoretical paper. For example, for the last sentence in the above paragraph, we can instead say ``Thus, the top $s$ eigenvectors of the Hessian can be well approximated by the Kronecker products between $E[x]$ and the top eigenvectors of $E[M]$.'' I would assume that people can only remember what is $M$ and $xx^T$, so perhaps we need to avoid using notations like $u_i$ if possible. Also, I think the use of ``temporary variables'' is not necessary in most of the texts. We can use English instead, and it also saves some space I guess. I mean, instead of saying ``$\forall i\leq s$'', we can say ``for the first $s$ eigenvectors of ...''}

Also, for some FC layers, the eigenvector correspondance heatmap of $\E[\mM]$ has a diagonal pattern in the first $s$ columns, as shown in \cref{fig:Corr_fc}. We can further approximate the $\vu_i$'s as the $i$th eigenvector of $\E[\mM]$. In this case, $\mU_k$ is the top $k$ eigenspace of $\E[\mM]$. There are also some cases where $s$ is considerably smaller than $n$, and larger than $n$ in rare cases. We will discuss them in the Appendix.
\subsection{Structure of Auto-correlation Matrix \texorpdfstring{$\E[\vx\vx^T]$}{ExxT}}
\label{sec:xxT}
The importance of the first eigenvector of $\E[\vx\vx^T]$, $\tx$, leads us to the analyze the structure of $\E[\vx\vx^T]$. From \cref{fig:xxT}, we can see $\E[\vx\vx^T]$ are all close to rank 1. This phenomenon is also observed along the training trajectory and in other networks.
\begin{figure}[th]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[width=\textwidth]{Figures/Eigenspectrum/xxT/xxT_sigval_d20_MNIST_Exp1_FC2_fixlr0.01R1_E-1_fc1fc2.pdf}
        \caption{Fully connected layers of FC2 (MNIST)}
        \label{fig:xxT_sig_fc2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[width=\textwidth]{Figures/Eigenspectrum/xxT/xxT_sigval_d20_CIFAR10_Exp1_LeNet5_fixlr0.01R1_E-1_fc1fc2.pdf}
        \caption{Fully connected layers of LeNet5 (CIFAR10)}
        \label{fig:xxT_sig_lenet}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[width=\textwidth]{Figures/Eigenspectrum/xxT/xxTConv_sigval_d20_CIFAR10_Exp1_LeNet5_fixlr0.01_E-1.pdf}
        \caption{Convolution layers of LeNet5 (CIFAR10)}
        \label{fig:xxT_sig_lenet}
    \end{subfigure}
    \captionsetup{justification=centering}
    \caption{Eigenspectrum of $\E[\vx\vx^T]$ for different layers in different models. All are close to rank 1.}
    \label{fig:xxT}
\end{figure}

Let the $\hE[\vx]$ be normalized $\E[\vx]$ (first left singular vector of $\E[\vx]$ for conv). We also find that $\tx$ is almost equal to $\hE[\vx]$ (or the top singular vector of $\hE[x]$ for conv), as their inner product are usually larger than 0.998, except for the input layer where the input is close to zero-mean. This suggests that $\E[\vx]\E[\vx]^T$ approximately equals to $\E[\vx\vx^T]$ and dominates the covariance $\mSigma_\vx$. It is verified as the Spectral norm of $\E[\vx]\E[\vx]^T$ is more than 5 times that of $\mSigma_\vx$ in all our experiments. This is natural because $\vx$ are output of ReLU layers and thus nonnegative, for all layers other than the input layer.
 \rnote{We should just show the empirical observation that the covariance matrix has small spectral norm, therefore by standard matrix perturbation bounds the true matrix is still close to rank 1 and the top eigenvector is close to $\E[x]$}
