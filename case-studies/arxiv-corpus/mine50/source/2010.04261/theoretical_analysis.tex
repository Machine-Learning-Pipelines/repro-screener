% In this section, we provide the proof sketch of the decoupling conjecture for the top $c-1$ eigenspace of the layer-wise parameter Hessian of a simplified setting. In particular, we consider an infinite width neural network over random data at initialization.

In this section, %we analyze the top eigenspace of the Hessian for a special network: 
we show that for a simple setting of 2-layer networks, % 2-layer infinite width neural network over random data at initialization, 
the layer-wise parameter Hessian has $c-1$ large eigenvalues and its top $c-1$ eigenspace is close to the top $c-1$ eigenspace of the Kronecker product approximation.
%\znote{not sure about the wording here} \ynote{changed wording}

\paragraph{Problem Setting and Notations}
\label{sec:theory:setting}
%In this section we will generally follow the notation defined in \sectionref{sec:prelim}. Moreover, 
Let bold non-italic letters such as $\rvv, \rmM$ denote random vectors (lowercase) and matrices (uppercase).
Consider a two layer fully connected ReLU activated neural network with input dimension $d$, hidden layer dimension $n$ and output dimension $c$. In particular, let $d=n^{1+\alpha}$ for some constant $\alpha>0$. Let the network has positive input from a rectified Gaussian $\rvx\sim \rectNormal(0, \mI_d)$ where every entry is identically distributed as $\max\{\hat{\rvx},0\}$ for $\hat{\rvx}\sim \mathcal{N}(0,1)$. %which, for a single entry, has density $f_\rectNormal(x) =\frac12\delta(x)+\frac{1}{\sqrt{2\pi}}\exp(-\frac{x^2}{2})\ind\sbr{x> 0}$ where $\delta(x)$ is the Dirac delta function. 
Let $\mW^{(1)}\in\R^{n\times d}$ and $\mW^{(2)}\in\R^{c\times n}$ be the weight matrices. In this problem we consider a random Gaussian initialization that $\mW^{(1)}\sim \cN(0,\frac1d\mI_{dn})$ and $\mW^{(2)}\sim \cN(0,\frac1n\mI_{nc})$. Both weight matrices has expected row norm of 1. Let the loss objective be cross entropy $\ell$. Training labels are irrelevant as they are independent from the Hessian at initialization. %We do not need to specify the training labels as they are independent from the Hessian at initialization.

Denote the output of the first and second layer as $\rvy$ and $\rvz$ respectively. We have $\rvy = \sigma(\mW^{(1)}\rvx)$ and $\rvz = \mW^{(2)}\rvy.$ Here $\sigma$ is the element-wise ReLU function.
Let $\rmD\triangleq\diag(\ind\sbr{\rvy\geq 0})\in\R^{n\times n}$ denote the 0/1 diagonal matrix representing the activation of $\sigma$ that $\rvy=\rmD\mW^{(1)}\rvx$.
Let $\rvp=\mbox{softmax}(\rvz)$ and let $\rmA\triangleq\diag(\rvp)-\rvp\rvp^\T$. Note $\rmA$ is rank $c-1$ with the null space of the all one vector.  We give full details about our settings in \sectionref{sec:proof-prelim}.
By simple matrix calculus (see \sectionref{sec:appendix_derivation}), the output Hessian of $\mM^{(1)}$ and the full layer-wise Hessian has closed-form
\begin{equation}
    \mM^{(1)} = \exop{\rvx\sim \rectNormal(0, \mI_d)}{\rmD\mW^{(2)\T}\rmA\mW^{(2)}\rmD}, \mH^{(1)} = \exop{\rvx\sim \rectNormal(0, \mI_d)}{\rmD\mW^{(2)\T}\rmA\mW^{(2)}\rmD\otimes \rvx\rvx^\T}.
\end{equation}
Following the decoupling conjecture, the Kronecker approximation of the layer-wise Hessian is \begin{equation}
    \hat\mH^{(1)} \triangleq \exop{\rvx\sim \rectNormal(0, \mI_d)}{\rmD\mW^{(2)\T}\rmA\mW^{(2)}\rmD}\otimes \exop{\rvx\sim \rectNormal(0, \mI_d)}{\rvx\rvx^\T}.
\end{equation}
Since we are always taking the expectation over the input $\rvx$, we will neglect the subscript and use $\E$ for expectation. Now we are ready to state our main theorem.

\begin{theorem}
\label{thm:main-full}
For an infinite width two-layer ReLU activated neural network with Gaussian initialization as defined above, let $V_1$ and $V_2$ be the top $c-1$ eigenspaces of $\mH^{(1)}$ and $\hat\mH^{(1)}$ respectively, for all $\eps>0$, 
$
    \lim_{n\to\infty}\mathop{\Pr}_{\mW^{(1)}\sim\gN(0,\frac{1}{d}\mI_{nd}), \mW^{(2)}\sim\gN(0,\frac{1}{n}\mI_{cn})}\left[\Overlap\left(V_1,V_2\right)>1-\eps\right] = 1.
$
Moreover $\mH^{(1)}$ has $c-1$ large eigenvalues that, \begin{equation}
    \lim_{n\to\infty}\mathop{\Pr}_{\mW^{(1)}\sim\gN(0,\frac{1}{d}\mI_{nd}), \mW^{(2)}\sim\gN(0,\frac{1}{n}\mI_{cn})}\left[\left(\left.\frac{\lambda_c(\mH^{(1)})}{\lambda_{c-1}(\mH^{(1)})}\right|_{\mW^{(1)}, \mW^{(2)}}\right) < \eps\right] = 1.
\end{equation}
\end{theorem}

Instead of directly working on the layer-wise Hessian, we first show a similar theorem for the output Hessian $\mM^{(1)}$. We will then show that the proof technique of the following theorem can be easily generalized to prove our main theorem.

\begin{theorem}
\label{thm:main-out}
For the same network as in \theoremref{thm:main-full}, let $\mM^*\triangleq \ex{\rmD'\mW^{(2)\T}\rmA\mW^{(2)}\rmD'}$ where $\rmD'$ is an independent copy of $\rmD$ and is independent of $\rmA$. Let $S_1$ and $S_2$ be the top $c-1$ eigenspaces of $\mM^{(1)}$ and $\mM^*$ respectively, $S_2$ is approximately $\gR\{\mW_i\}_{i=1}^c\backslash\{\textbf{1}^\T\mW\}$ where $\gR$ is the row span, and for all $\eps>0$,
$\lim_{n\to\infty}\mathop{\Pr}_{\mW^{(1)}\sim\gN(0,\frac{1}{d}\mI_{nd}), \mW^{(2)}\sim\gN(0,\frac{1}{n}\mI_{cn})}\left[\Overlap\left(S_1,S_2\right)>1-\eps\right] = 1.$
Moreover, $\mM$ has $c-1$ large eigenvalues that \begin{equation}
    \lim_{n\to\infty}\mathop{\Pr}_{\mW^{(1)}\sim\gN(0,\frac{1}{d}\mI_{nd}), \mW^{(2)}\sim\gN(0,\frac{1}{n}\mI_{cn})}\left[\left(\left.\frac{\lambda_c(\mM^{(1)})}{\lambda_{c-1}(\mM^{(1)})}\right|_{\mW^{(1)}, \mW^{(2)}}\right) < \eps\right] = 1.
\end{equation}
\end{theorem}

\textbf{Remark.}
The closed form approximating of $S_1$ in \cref{thm:main-out} can be heuristically extended to the case with multiple layers, that the top eigenspace of the output Hessian of the $k$-layer would be approximately $\gR(\mS^{(k)})\setminus\{\textbf{1}^\T\mS^{(k)}\}$
where $\mS^{(k)} = \mW^{(n)}\mW^{(n-1)}\cdots\mW^{(k+1)}$ and $\gR(\mS^{(k)})$ is the row space of $\mS^{(k)}$.
Though our result was only proven for random initialization and random data, we observe that this subspace also has high overlap with the top eigenspace of output Hessian at the minima of models trained with real datasets. The corresponding empirical results are shown in \cref{sec:app_outhessian_exp}. 

\paragraph{Proof Sketch for \theoremref{thm:main-out}}
For simplicity of notations, in this section we will use $\mW$ to denote $\mW^{(2)}$ and $\mM$ to denote $\mM^{(1)}$ unless specified otherwise.
Our proof of \theoremref{thm:main-out} mainly consists of three parts. First we analyze the structure of $\mM^*$ and show that it is approximately rank $c-1$. Then we show that $\mM^*$ and $\mM$ are roughly equivalent via an approximate independence between $\rmD$ and $\rmA$. Finally, by projecting both $\mM$ and $\mM^*$ onto a $c\times c$ matrix using $\mW$, we can apply the approximate independence and prove that the top $c-1$ eigenspace of $\mM^*$ is approximately that of $\mM$, which concludes the proof.

\paragraph{(1) Structure of $\mM^*$} When $n\to\infty$, the output of the second layer $\rvy$ converges to a multivariate Gaussian (\lemmaref{lemma:y-gaussian}), hence we can consider each diagonal entry of $\rmD$ as a $p=\frac12$ Bernoulli random variable. Since we assumed that $\rmD'$ and $\rmA$ are independent, by some simple calculation,
\begin{equation}
    \mM^*=\frac14\left(\mW^\T\E[\rmA]\mW+\text{diag}(\mW^\T\E[\rmA]\mW)\right).
\end{equation}
Here $\E[\rmA]$ is rank $c-1$ with the $(c-1)$-th eigenvalue bounded below from 0 (\lemmaref{lemma:A-rank-c-1}).
Since the two terms in the sum has the same trace while $\mW^\T\E[\rmA]\mW$ is rank $c-1$ compared to rank $n$ of $\diag(\mW^\T\E[\rmA]\mW)$, we can show that the top eigenspace is dominated by the eigenspace of $\mW^\T\E[\rmA]\mW$, which is approximately $\gR\{\mW_i\}_{i=1}^c\backslash\{\textbf{1}^\T\mW\}$. 

\paragraph{(2) Approximate Independence Between $\rmA$ and $\rmD$} Intuitively, if $\rmD$ and $\rmA$ are independent, then $\mM = \mM^*$. However, this is clearly not true - if the activations align with a row of $\mW$ then the corresponding output is going to be large, which changes $\rmA$ significantly. To address this problem, we observe that the formula for $\mM$ is only of degree 2 in $\rmD$, so one can focus on conditioning on two of the activations \--- a negligible fraction in the limit. More precisely, if one expand out the expression of each element squared in $\mM$, it is an homogeneous polynomial of the form
$p(\rmA,\rmD,\bar\rmA,\bar\rmD) = \sum_{i,j,k,l=1}^c\sum_{p,q=1}^n c_{ijklpq}\rmA_{ij}\bar\rmA_{kl}\rmD_{pp}\bar\rmD_{qq},
$
where $(\bar\rmA,\bar\rmD)$ are independent copies of $(\rmA, \rmD)$. The same element squared in $\mM^*$ is just going to be $p(\rmA,\rmD',\bar\rmA,\bar\rmD')$.
By nice properties of the Gaussian initialized weight matrix, we show that as $n\to\infty$, $\rmA$ is invariant when conditioning on two entries of $\rmD$ (\lemmaref{lemma:z-invariant}). Therefore, in the limit we have $\lim_{n\to\infty}\ex{p(\rmA, \rmD, \bar\rmA, \bar\rmD)} = \ex{p(\rmA, \rmD', \bar\rmA, \bar\rmD')}$ (detailed proof in Appendix). 
%Hence by the portmanteau theorem we have the following key lemma:
%\begin{lemma}
%\label{lemma:polynomial-maintext} (informal)
%Let $p(\rmA,\rmD,\bar\rmA,\bar\rmD)$ be a homogeneous polynomial as defined above with an upper bounded $\ell_1$ norm of the coefficients, then
%\begin{equation}
%    \lim_{n\to\infty}\ex{p(\rmA, \rmD, \bar\rmA, \bar\rmD)} = \ex{p(\rmA, \rmD', \bar\rmA, \bar\rmD')}.
%\end{equation}
%\end{lemma}

\paragraph{(3) Equivalence between $\mM^*$ and $\mM$} Since the size of $\mM$ also goes to infinity as we take the limit on $n$, it is technically difficult to directly compare their eigenspaces. In this case we utilize the fact that $\mW$ has approximately orthogonal rows, and project $\mM$ onto $\mW\mM\mW^\T$. In particular, by expanding out the Frobenious norms as polynomials and bounding the $\ell_1$ norm of the coefficients, using \lemmaref{lemma:z-invariant} we are able to show that $\fns{\mM}\approx\fns{\mW\mM\mW^\T}\approx \fns{\mW\mM^*\mW^\T}\approx \fns{\mM^*}$ (\lemmaref{lemma:M-proj-preserve-f-norm}- \lemmaref{lemma:F-norm-equal}).
This result tells us that the projection does not lose information, and hence indirectly gives us the dominating eigenspace of $\mM$. This concludes our proof for \theoremref{thm:main-out}

\paragraph{Proving \theoremref{thm:main-full} and Beyond}
To prove Theorem~\ref{thm:main-full}, we use a very similar strategy. We consider a re-scaled Hessian $\tmH\triangleq\frac1d\mH$ and show that in the independent setting $\tmH^* = \frac1d\E[\rmD'\mW\rmA\mW\rmD'\otimes \rvx''\rvx''^\T] = \mM^*\otimes\frac1d \E[\rvx''\rvx''^\T].$ We then generalize the conditioning technique to involve conditioning on two entries of $x$. 
\iffalse For simplicity of notations, we will use $\mH$ to denote $\mH^{(1)}$. Since the norm of $\mH$ goes to infinity as $n\to\infty$, we consider a re-scaled Hessian $\tmH\triangleq\frac1d\mH$. Similar to the proof for the output Hessian, we consider $\tmH^*\triangleq\frac1d\ex{\rmD'\mW\rmA\mW\rmD'\otimes \rvx''\rvx''^\T}$ that $(\rmD', \rvx'')$ are independent copies of $(\rmD, \rvx)$. Note that by the independence assumption, $\tmH^* = \frac1d\E[\rmD'\mW\rmA\mW\rmD'\otimes \rvx''\rvx''^\T] = \mM^*\otimes\frac1d \E[\rvx''\rvx''^\T].$
Since $\rvx$ is a multivariate rectified Gaussian, its mean dominates its variance for large $d$. Hence $\frac1d \E[\rvx''\rvx''^\T]$ is approximately rank-1 with the first eigenvector as $\frac{1}{\sqrt{d}}\1_d$. Combining with the result from \theoremref{thm:main-out}, the top eigenspace of $\tmH$ is just $\gR\{\frac{1}{\sqrt{d}}\mW_i\otimes\1_d^\T\}_{i=1}^c\backslash\{\frac{1}{\sqrt{d}}\mW\otimes\1_d^\T\cdot\textbf{1}\}$.
We may generalize \lemmaref{lemma:polynomial-maintext} so that we can condition on two extra entries of $\rvx$ (\corollaryref{cor:polynomial}), and finally establish the equivalence between $\tmH$ and $\tmH^*$ by projection using $\mW\otimes\1_d$ (\lemmaref{lemma:H-proj-preserve-f-norm}- \lemmaref{lemma:F-norm-equal-H}). Following similar arguments of the output Hessian conclude the proof.

\paragraph{Remark}
Using the same proof strategy, we can generalize the theorem to the second layer-wise Hessian $\mH^{(2)} = \E[\rmA\otimes \rvy\rvy^\T]$. Moreover, since \lemmaref{lemma:polynomial-maintext} can be generalized to include activations and inputs of multiple layers, this proof technique can be generalized to deal with deeper networks.
\fi