\newpage
\section{Detailed Experiment Setup}
\label{sec:appendix_exp_setup}
\subsection{Datasets}
\label{sec:appendix_exp_dataset}
We conduct experiment on CIFAR-10, CIFAR-100 (MIT) \citep{Krizhevsky09learningmultiple} (\url{https://www.cs.toronto.edu/~kriz/cifar.html}), and MNIST (CC BY-SA 3.0) \citep{lecun1998gradient} (\url{http://yann.lecun.com/exdb/mnist/}). The datasets are downloaded through torchvision \citep{NEURIPS2019_9015} (\url{https://pytorch.org/vision/stable/index.html}). We used their default splitting of training and testing set.

To compare our work on PAC-Bayes bound with the work of \citet{dziugaite2017computing}, we created a custom dataset MNIST-2 by setting the label of images 0-4 to 0 and 5-9 to 1.
We also created random-labeled datasets MNIST-R and CIFAR10-R by randomly labeling the images from the training set of MNIST and CIFAR10.
The dataset information is summarized in \tableref{tab:appendix_dataset}
\begin{table}[h]
\small
  \centering
  \caption{Datasets}
  \vskip 0.1in
    \begin{center}

    \begin{tabular}{lccccc}
    \toprule
    &   \multicolumn{2}{c}{\# Data Points}    &    & &     \\
    Dataset & Train & Test & Input Size & \# Classes & Label \\
    \midrule
    CIFAR10 & 50000 & 10000 & $3\times32\times32$ & 10 & True \\
    CIFAR10-R & 50000 & 10000 & $3\times32\times32$ & 10 & Random \\
    CIFAR100 & 50000 & 10000 & $3\times32\times32$ & 100 & True \\
    MNIST & 60000 & 10000 & $28\times28$ & 10 & True \\
    MNIST-2 & 60000 & 10000 & $28\times28$ & 2 & True \\
    MNIST-R & 60000 & 10000 & $28\times28$ & 10 & Random \\\bottomrule
    \end{tabular}
\end{center}
  \label{tab:appendix_dataset}%
\end{table}%

All the datasets (MNIST, CIFAR-10, and CIFAR-100) we used are publicly available. According to their descriptions on the contents and collection methods, they should not contain any personal information or offensive content. MNIST is a remix of datasets from the National Institute of Standards and Technology (NIST), which obtained consent for collecting the data. However, we also note that CIFAR-10 and CIFAR-100 are subsets of the dataset 80 Million Tiny Image \citep{torralba2007tiny} (\url{http://groups.csail.mit.edu/vision/TinyImages/}), which used automatic collection and includes some offensive images.

\subsection{Network Structures}
\label{appendix_exp_nn}
\paragraph{Fully Connected Network:}
We used several different fully connected networks varying in the number of hidden layers and the number of neurons for each hidden layer. The output of all layers except the last layer are passed into ReLU before feeding into the subsequent layer.  As described in \sectionref{subsec:approx}, we denote a fully connected network with $m$ hidden layers and $n$ neurons each hidden layer by F-$n^m$. For networks without uniform layer width, we denote them by a sequence of numbers (e.g. for a network with three hidden layers, where the first two layers has 200 neurons each and the third has 100 neurons, we denote it as F-$200^2$-$100$). For example, the structure of F-$200^2$ is shown in \tableref{tab:appendix_fc_struct}.

\begin{table}[h]
\small
  \centering
  \caption{Structure of F-$200^2$ on MNIST}
  \vskip 0.1in
    \begin{center}
    \begin{tabular}{rllcc}
    \toprule
    \# & Name & Module & In Shape & Out Shape\\
    \midrule
    1 & & Flatten & (28,28) & 784\\
    2 & fc1 & Linear(784, 200) & 784 & 200\\
    3 & & ReLU & 200 & 200\\
    4 & fc2 & Linear(200, 200) & 200 & 200\\
    5 & & ReLU & 200 & 200\\
    6 & fc3 & Linear(200, 10) & 200 & 10\\
    \multicolumn{5}{c}{\emph{output}}\\\bottomrule
    \end{tabular}%
\end{center}

  \label{tab:appendix_fc_struct}%
\end{table}%

\paragraph{LeNet5:} We adopted the LeNet5 structure proposed by \citet{lecun1998gradient} for MNIST, and slightly modified the input convolutional layers to adapt the input of CIFAR-10 dataset. The standard LeNet5 structure we used in the experiments is shown in \tableref{tab:appendix_lenet_struct}. We further modified the dimension of fc1 and conv2 to create several variants for the experiment in \sectionref{sec:models}. Take the model whose first fully connected layer is adjusted to have 80 neurons as an example, we denote it as LeNet5-(fc1-80).

\begin{table}[h]
\small
  \centering
  \caption{Structure of LeNet5 on CIFAR-10}
  \vskip 0.1in
    \begin{center}
    \begin{tabular}{rllcc}
    \toprule
    \# & Name & Module & In Shape & Out Shape\\\midrule
    1 & conv1 & Conv2D(3, 6, 5, 5) & (3, 32, 32) & (6, 28, 28)\\
    2 & & ReLU & (6, 28, 28) & (6, 28, 28)\\
    3 & maxpool1 & MaxPooling2D(2,2) & (6, 28, 28) & (6, 14, 14)\\
    4 & conv2 & Conv2D(6, 16, 5, 5) & (6, 14, 14) & (16, 10, 10)\\
    5 & & ReLU & (16, 10, 10) & (16, 10, 10)\\
    6 & maxpool2 & MaxPooling2D(2,2) & (16, 10, 10) & (16, 5, 5)\\
    7 & & Flatten & (16, 5, 5) & 400\\
    8 & fc1 & Linear(400, 120) & 400 & 120\\
    9 & & ReLU & 120 & 120\\
    10 & fc2 & Linear(120, 84) & 120 & 84\\
    11 & & ReLU & 84 & 84\\
    12 & fc3 & Linear(84, 10) & 84 & 10\\
    \multicolumn{5}{c}{\emph{output}} \\\bottomrule
    \end{tabular}%
\end{center}
  \label{tab:appendix_lenet_struct}%
\end{table}%

\paragraph{Networks with Batch Normalization:} In \sectionref{sec:appendix_batchnorm} we conducted several experiments regarding the effect of batch normalization on our results. For those experiments, we use the existing structures and add batch normalization layer for each intermediate output after it passes the ReLU module. In order for the Hessian to be well-defined, we fix the running statistics of batch normalization and treat it as a linear layer during inference. We also turn off the learnable parameters $\theta$ and $\beta$ \citep{ioffe2015batch} for simplicity. For network structure X, we denote the variant with batch normalization after all hidden layers X-BN.
For example, the detailed structure LeNet5-BN is shown in \tableref{tab:appendix_lenetBN_struct}.

\begin{table}[h]
\small
  \centering
  \caption{Structure of LeNet5-BN on CIFAR-10}
  \vskip 0.1in
  \begin{center}
    \begin{tabular}{rllcc}
    \toprule
    \# & Name & Module & In Shape & Out Shape\\\midrule
    1 & conv1 & Conv2D(3, 6, 5, 5) & (3, 32, 32) & (6, 28, 28)\\
    2 & & ReLU & (6, 28, 28) & (6, 28, 28)\\
    3 & & BatchNorm2D & (6, 28, 28) & (6, 28, 28)\\
    4 & maxpool1 & MaxPooling2D(2,2) & (6, 28, 28) & (6, 14, 14)\\
    5 & conv2 & Conv2D(6, 16, 5, 5) & (6, 14, 14) & (16, 10, 10)\\
    6 & & ReLU & (16, 10, 10) & (16, 10, 10)\\
    7 & & BatchNorm2D & (16, 10, 10) & (16, 10, 10)\\
    8 & maxpool2 & MaxPooling2D(2,2) & (16, 10, 10) & (16, 5, 5)\\
    9 & & Flatten & (16, 5, 5) & 400\\
    10 & fc1 & Linear(400, 120) & 400 & 120\\
    11 & & ReLU & 120 & 120\\
    12 & & BatchNorm1D & 120 & 120\\
    13 & fc2 & Linear(120, 84) & 120 & 84\\
    14 & & ReLU & 84 & 84\\
    15 & & BatchNorm1D & 84 & 84\\
    16 & fc3 & Linear(84, 10) & 84 & 10\\
    \multicolumn{5}{c}{\emph{output}} \\\bottomrule
    \end{tabular}%
\end{center}
  \label{tab:appendix_lenetBN_struct}%
\end{table}%

\paragraph{Variants of VGG11:} To verify that our results apply to larger networks, we trained a number of variant of VGG11 (originally named VGG-A in the paper, but commonly refered as VGG11) proposed by \citet{simonyan2014very}. For simplicity, we removed the dropout regularization in the original network. To adapt the structure, which is originally designed for the $3\times224\times224$ input of ImageNet, to $3\times32\times32$ input of CIFAR-10.

Since the original VGG11 network is too large for computing the top eigenspace up to hundreds of dimensions, we reduce the number of output channels of each convolution layer in the network to 32, 48, 64, 80, and  200. We denote the small size variants as VGG11-W32, VGG11-W48, VGG11-W64, VGG11-W80, and VGG11-W200 respectively. We use conv1 - conv8 and fc1 to denote the layers of VGG11 where conv1 is closest to the input feature and fc1 is the classification layer.

\paragraph{Variants of ResNet18:} We also trained a number of variant of ResNet18 proposed by \citet{kaiming2015}. As batch normalization will change the low rank structure of the auto correlation matrix and reduce the overlap, we removed all batch normalization operations.
Following the adaptation of ResNet to CIFAR dataset as in \url{https://github.com/kuangliu/pytorch-cifar}, we changed the input size to $3\times32\times32$ and added a 1x1 convolutional layer for each shortcut after the first block.

Similar to VGG11, we reduce the number of output channels of each convolution layer in the network to 48, 64, 80. We denote the small size variants as ResNet18-W48, ResNet18-W64, and ResNet18-W80 respectively.
We use conv1 - conv17 and fc1 to denote the layers of the ResNet18 backbone where conv1 is closest to the input feature and fc1 is the classification layer. For the 1x1 convolutional layers in the shortcut, we denote them by sc-conv1 - sc-conv3. where sc-conv1 is the convolutional layer on the shortcut of the second ResNet block and  sc-conv3 is the convolutional layer on the shortcut of the fourth ResNet block.

\subsection{Training Process and Hyperparameter Configuration}
\label{sec:appendix_exp_train}
For all datasets, we used the default splitting of training and testing set. All models (except explicitly stated otherwise) are trained using batched stochastic gradient descent (SGD) with batch-size 128 and fixed learning rate 0.01 for 1000 epochs. No momentum and weight decay regularization were used. The loss objective converges by the end of training, so we may assume that the final models are at local minima. For generality we also used a training scheme with fixed learning rate at 0.001, and a training scheme with fixed learning rate at 0.01 with momentum of 0.9 and weight-decay factor of 0.0005. Models trained with these settings will be explicitly stated. Otherwise we assume they were trained with the default scheme mentioned above.

Follow the default initialization scheme of PyTorch\citep{NEURIPS2019_9015}, the  weights of linear layers and convolutional layers are initialized using the Xavier method  \citep{glorot2010understanding}, and bias of each layer are initialized to be zero.