\label{sec:conclusion}
In this paper we proposed the decoupling conjecture which helps in understanding many different structures for the top eigenspace of layer-wise Hessian. Our theory only applies to the initialization for a 2-layer network. How the property can be maintained throughout training is a major open problem. %Although we can only prove the decoupling conjecture and the top eigenspace of output Hessian for a simple 2-layer model, the conjecture and 
However, the implications of the decoupling conjecture can be verified empirically. Having such a conjecture allows us to predict how the structure of the Hessian changes based on architecture/training method (such as batch normalization), and has potential applications in understanding training and generalization (as we demonstrated by the new generalization bounds in Section~\ref{sec:pac}). We hope this work would be a starting point towards formally proving the structures of neural network Hessians.

%In this paper we identified two new surprising structures in the dominating eigenspace of layerwise Hessians for neural networks. Specifically, the eigenspace overlap reveals a novel similarity between different models. We showed that under a decoupling conjecture, these structures can be explained by a Kronecker factorization. We analyze each component in the factorization, in particular we prove that the output Hessian is approximately rank $c-1$ for random two-layer neural networks. Our proof gives a simple heuristic formula to estimate the top-eigenspace of the Hessian. As a proof of concept, we showed that these structures can be used to find better explicit generalization bounds. Since the dominating eigenspace of Hessian, which is the sharpest directions on the loss landscape, plays an important role in both generalization \citep{keskar2016large, jiang2020fantastic} and optimization \citep{gur2018gradient}, we believe our new understanding can benefit both fields. We hope this work would be a starting point towards formally proving the structures of neural network Hessians. 

% \paragraph{Limitations and Open Problems} Most of our work focuses on the layerwise Hessian except for \sectionref{subsec:approx} and \sectionref{sec:appendix_full_hessian}. The eigenspace overlap phenomenon depends on properties of auto-correlation and output Hessian, which are weaker for earlier layers of larger networks. Our theoretical results need to assume the parameters are random, and only applies to fully-connected networks. The immediate open problems include why the decoupling conjecture is correct and why the output Hessian is low rank in more general networks.