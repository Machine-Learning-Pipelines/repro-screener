\documentclass[12pt]{colt2022} %% Anonymized submission
\usepackage{times}
\usepackage{mathcommands}
\begin{document}
\title{}
We would like to thank all reviewers for your constructive feedbacks and comments. We realized that %due to some ambiguities in our deliveries 
there are some common confusions regarding our work, which we will address first: %. We would like to address them in this general response.
%In particular, the infinite width network serves as an approximation to the finite width cases in which we have observed interesting phenomenons empirically (Section 5).
\vskip 1mm
\noindent\textbf{* Relationship between this paper and K-FAC\quad}
We are studying a very similar approximation as used in K-FAC, but the two works are focusing/using \textbf{two different ends of the eigenspectrum}.
In this work, we investigated the properties of the \textbf{top eigenspace} of the layer-wise Hessians. We proved theorem 4 and 5 as a theoretical step toward explaining the $c-1$ large eigenvalue, which is an important observation in the empirical studies on neural network Hessians.
We also provided a new heuristic formula for approximating the top eigenspace of output Hessian.
In K-FAC, the approximation is used for the \textbf{inverse} of layer-wise Fisher information matrices (FIMs), and the qualitative empirical results are for the \textbf{bottom eigenspace} of the layerwise FIMs.
%While the flat directions of the landscape are important for second-order optimization, the sharpest direction and the low-rank structure could be important as well for its potential implications on generalization.
\\\noindent\textbf{* Relationship between this paper and NTK\quad}
%\znote{Move to response to reviewer 3?}
%Though we are using a two-layer infinite width network for theoretical analysis, our object of interest (understanding the top eigenspace of the weight Hessians) is very different from the NTK line of work which mainly focuses on training dynamics and generalization. In particular, 
Our analysis is quite different from NTK. NTK convergence analysis relies on the smallest eigenvalue of the kernel matrix while we capture properties of the top eigenspace of Hessian.
\\\noindent\textbf{* Implication of Theorem 4 \& 5\quad}
Theorem 5 proves the output Hessian $\mM$ is approximately rank $c-1$, since the ratio between the $c$-th largest eigenvalue and the $c-1$th largest eigenvalue is negligible. Further, we can approximate the top $c-1$ eigenspace of $\mM$ using the top $c-1$ eigenspace of $\mM^*$, which can be directly computed from the weight matrices. The eigenspace overlap of this approximation converges to $1$ in probability.
Theorem 4 extends similar results to the layerwise Hessian which also has the low-rank property when the input has a nonzero mean.\\%It proves that the decoupling conjecture is valid for the top $c-1$ eigenspace of the output Hessian $\mH^{(1)}$ by proving the eigenspace overlap with the approximated output Hessian using the decoupling conjecutre $\hat{\mH}^{(1)}$. It then shows that the layerwise Hessian $\mH^{(1)}$ is approximately rank $c-1$ since the ratio between the $c$th largest eigenvalue and the $c-1$th largest eigenvalue is negligible. We can also compute the top $c-1$ eigenspace of $\mH^{(1)}$ together with Theorem 5, as stated in \textbf{Section 4.3}\\
\noindent\textbf{* Theorem 4 and post-training network\quad}
We have no theoretical results regarding the structures of Hessians of post-training networks. However, our empirical results suggest that the structures are likely to be similar to the structure of Hessians at initializations. It may be possible to extend our theoretical results to post-training networks in NTK regime, which is an interesting future direction.
\vskip 1mm
\noindent
\textbf{Response to Reviewer 2}\\
5. For theorem 5, $\mW_i, \mW, \mM$ are $\mW_i^{(2)}, \mW^{(2)}, \mM^{(1)}$. We are sorry for the missing superscripts.\\
6. What we mean by$\mW \cdot \vone$ is $\vone^\T \mW$. $\mW^{(2)}$ is a $c \times n$ matrix so $\mW^{(2)} \cdot \1$ is in $\R^n$. $\1$ in page 9 is $\1_c$ so it is different from $\1_d$. We will make it clearer by specifying the dimensions.\\
7. The Hessians in Figure 2 are after training.\\
8. We will add the dependence of $\E[\vx] \neq 0$ to the conjecture. If $\E[\vx]=0$, the phenomenon will be weakened but there is still nontrivial overlap on the top eigenspace.
\vskip 1mm
\noindent
\textbf{Response to Reviewer 3}
\vskip 1mm
\noindent
%\znote{Not sure how to respond since we have limited space}\\
We follow (and improve upon) Dziugaite and Roy (2017) to compute non-vacuous PAC-Bayes bound. The bound is not infinity because additional noise is added to the parameters of the network.%  It is applied on practical neural networks (as in the previous paper) instead of infinite width networks. The reasons why PAC-Bayes bound can be applied in this case is shown in Dziugaite and Roy (2017). 

\vskip 1mm
\noindent
\textbf{Response to Reviewer 4}
\vskip 1mm
\noindent
1. (Intro \& Related Works). Thank you for mentioning these works, we will add them. \\% to our main text.\\
\noindent
2. (K-FAC) Please see the general response. \\%Our work and KFAC are focusing on two ends of the eigenspectrum, please see the general response.\\
\noindent
3. (Rectified Gaussian). We are using rectified Gaussian for representing passage through ReLU. The covariance term $\mI_d$ in $\gN^{\emph{R}}(0,\mI_d)$ represents the covariance of the Gaussian before rectified.\\
\noindent
4. (Theorem 4 \& 5) We will rewrite Theorem 4 \& 5 with all assumptions stated clearly and emphasize on their implications (as stated in the general response) and relations to the conclusions.
\newpage

\subsection*{Comments to area chair}
We believe that Reviewer \#3 does not understand our paper. In particular, the reviewer missed the following two basic points: 1. even though NTK analysis gives convergence guarantees, it does not say anything about the top eigenspace of the Hessian, so our Theorems 4 and 5 are still novel; 2. PAC-Bayes bounds have been applied successfully applied to neural networks (though adding noise, see Dziugate and Roy (2017)) and we improve the explicit bounds computed in Dziugate and Roy (2017). Therefore, Reviewer's claim that the KL divergence is always infinity is just due to the fact that the reviewer did not understand how noise is added, and the claim that the bound is proven in the previous paper is just due to the lack of understanding (we are not claiming that we proved a new PAC-Bayes bound, instead, our contribution is to use the existing bound but constructing a better distribution $P$ to compute a stronger explicit generalization bound). 

Due to the large gap in understanding and the page limit in the author's response, we feel it is very difficult to answer every question that this reviewer raised (especially since the answer to most points are just there in the paper but the reviewer has completely missed them). We also worry that this reviewer is not going to take the effort in understanding how the PAC-Bayes framework can be applied to compute explicit generalization bounds and will continue to misunderstand our paper. We hope you will take these into consideration.



\subsubsection*{ Reviewer 2}
\begin{itemize}
\item 5. What are $\mW_i$, $\mW$, and $\mM$ in Theorem 5? It looks like some superscripts are missing?

\item 6. Also, if $R{\mW_i}$ is the span of rows of $W^{(2)}$ it would be a subspace in $\R^n$. But what does it mean to subtract ${\mW \cdot \1}$ here? Isn't $\mW \cdot \1$ in $\R^c$? Also, on page 9 we see both $\1_d$ and $\1$, with slightly different fonts. What is the difference between the two?

\item 7. Are the plots in Figure 2 obtained at initialization or after training?

\item 8. It seems that having E[layer input] $>$ 0 is very crucial for the conjecture to hold. In the appendix the authors also performed experiments with batch-normalized networks and observed that the approximation does not align as well with the exact Hessian. Have the authors considered including (in some way) this requirement of E[layer input] $>$ 0 to the statement of the conjecture?
\end{itemize}
\subsubsection*{ Reviewer 3 }

The paper studies the structure of Hessian of the loss function for training neural networks. The authors rely on the approximation of the Hessian using Kronecker factorizations. The main theorem is not convincing since it is for two-layer neural networks with infinite width. Note that neural networks with infinite width are equivalent to a linear model with a fixed kernel, which can be trained by convex optimization . Thus, it does not capture the essential nature of the Hessian of the nonconvex optimization of the neural networks. PAC-Bayes bound is also not a contribution of this paper, but it is from previous papers. I am also not convinced by how the authors use the PAC-Bayes bound. The KL divergence is infinite and thus this bound is not useful unless we artificially change the definition of the neural networks to be a random function with the distribution overlapping with the initial model and the final model. This is unlikely the case in these experiments to provide good test accuracies. Also, I am not sure if the generalization bound actually holds, because the approximation likely does not hold.

In sum, the authors are using the generalization bound without specifying under what condition this is valid. It does not make sense to compare the generalization bound with and without the conjecture, which seems to be the case here. In this logic, we can conjecture many favorable properties that can sharpen the generalization bounds further. Thus, the conjecture needs to be turned into some assumption that is provable (theoretically or empirically). The experiments support the possibility of the conjecture being true but does not prove or verify the conjecture.


\subsubsection*{ Reviewer 4 }

\begin{itemize}
\item Introduction

Output Hessian and input autocorrelation is mentioned multiple times but is a bit difficult to understand what is meant by these until much further in the paper.

\item Related work

While the authors have done a relatively thorough review of related work one line of research which is based on the low-rank approximation of the Jacobian is missing. This is relevant as the dominant term in the Hessian is typically a Jacobian term and thus these results may also be relevant. Two such results that come to mind:

Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks
 Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian


\item Related work layer wise kronecker…section

It is not clear what the distinction is with the approximation used for deriving the KFAC algorithm. If they are exactly the same I think this should be stated more clearly in the intro as this is currently somewhat claimed as a novel contribution.
\item Section 4.1

Why is a rectified Gaussian input used. I assume this is due to passage through ReLU but in this case wouldn’t the standard deviation of the entries be different?

- Theorem 4 \& 5 should be rewritten with all the assumptions stated clearly (some of which appear before the theorem). The conclusions and implications should also be explained clearly after the theorems. It is a bit difficult to connect the formal statements to the claims made in the intro.

\item Theorem 4 \& 5

It is a bit difficult for me to see the relevance of low-rank approximation at initialization for understanding generalization as the random weights get updated as things are trained. Isn’t the Hessian after training the quantity we are interested in?
\end{itemize}
\end{document}