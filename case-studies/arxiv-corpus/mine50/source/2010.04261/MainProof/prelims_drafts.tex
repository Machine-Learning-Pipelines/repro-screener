This is the complete proof for the two main theorems sketched in \sectionref{sec:theoretical}.
\subsection{Preliminaries}
\label{sec:proof-prelim}
\subsubsection{Notations}
In this section, we generally follow the notation standard by \citet{goodfellow2016deep}. We will use bold italic lowercase letters ($\vv$) to denote vectors, bold non-italic lowercase letters to denote random vectors ($\rvv$), bold italic uppercase letters ($\mA$) to denote matrices, and bold italic uppercase letters ($\rmA$) to denote random matrices.

Moreover, we use $[n]$ for positive integer $n$ to denote the set $\{1,\cdots,n\}$, and $\norm{\mM}$ to denote the spectral norm of a matrix $\mM$. We use $\innerf{\mA, \mB}$ to denote the Frobenius inner product of two matrices $\mA$ and $\mB$, namely $\innerf{\mA, \mB} \triangleq\sum_{i,j}\mA_{i,j}\mB_{i,j}$. We use $\tr(\mM)$ to denote the trace of a matrix $\mM$, and we use $\textbf{1}_c$ to denote the all-one vector of dimension $c$ (the subscript may be omitted when it's clear from the context).

For probability distributions, we use $\rectNormal(\mu,\sigma)$ to denote the rectified Gaussian distribution which has density function\begin{equation}
    f_\rectNormal(x;\mu,\sigma) = \Phi\rbr{\frac\mu\sigma}\delta(x)+\frac{1}{\sqrt{2\pi\sigma^2}}\exp\rbr{-\frac{(x-\mu)^2}{2\sigma^2}}\ind\sbr{x> 0}.
\end{equation}
Here $\Phi$ is the CDF of standard normal distribution, $\delta(x)$ is the Dirac delta function. Note that when $\mu=0$, the density function simplifies to\begin{align}
    f_\rectNormal(x;0,\sigma) = 
    \frac12\delta(x)+\frac{1}{\sqrt{2\pi\sigma^2}}\exp\rbr{-\frac{x^2}{2\sigma^2}}\ind\sbr{x> 0}.
\end{align}
We will use the same notation for multivariate rectified Gaussian distribution, which will be used to characterize the inputs of the network.
% \znote{Move this to somewhere after we define the network?}
\subsubsection{Problem Setting}

Consider a two layer fully connected ReLU activated neural network with input dimension $d$, hidden layer dimension $n$ and output dimension $c$. In particular, $n$ goes to infinity, $d=n^{1+\alpha}$ for some $\alpha>0$, and $c$ is a finite constant. Let network be trained with cross-entropy objective $\gL$. Let $\sigma$ denote the element-wise ReLU activation function which acts as $\sigma(x) = x\cdot\ind_{x\geq 0}$ and the product here is applied element-wise.
Let $\mW^{(1)}\in\R^{n\times d}$ and $\mW^{(2)}\in\R^{c\times n}$ denote the weight matrices of the first and second layer respectively. 
%Let $\vb^{(1)}\in\R^{n}$ and $\vb^{(2)}\in\R^{c}$ denote the weight matrices of the first and second layer respectively.

We consider the case that the neural network has rectified standard Gaussian input $\rvx\sim \rectNormal(0, \mI_d)$. 
Denote the output of the first and second layer as $\rvy$ and $\rvz$ respectively. We have $\rvy = \sigma(\mW^{(1)}\rvx)$ and $\rvz = \mW^{(2)}\rvy.$ Let $\rvp=\mbox{softmax}(\rvz)$ denote the softmax output of the network and let $\rmA\triangleq\diag(\rvp)-\rvp\rvp^\T$.

In this problem, we look into the state of random Gaussian initialization, in which entries of both matrices are i.i.d. sampled from a standard normal distribution, and then re-scaled such that each row of $\mW^{(1)}$ and $\mW^{(2)}$ has norm 1. When taking $n$ and $d$ to infinity, with the concentration of norm in high-dimensional Gaussian random variables, we assume in this problem that entries of $\mW^{(1)}$ are iid sampled from a zero-mean distribution with variance $1/d$, and entries of $\mW^{(2)}$ are iid sampled from a zero-mean distribution with variance $1/n$. This initialization is standard in training neural networks. From the previous analysis of Hessian, the output Hessian corresponding to the first layer has closed form
\begin{equation}
    \mM^{(1)} \triangleq \exop{\rvx\sim \rectNormal(0, \mI_d)}{\rmD\mW^{(2)\\T}\rmA\mW^{(2)}\rmD},
\end{equation}
where $\rmD\triangleq\diag(\ind\sbr{\rvy\geq 0})\in\R^{n\times n}$ is the random 0/1 diagonal matrix representing the activations of ReLU function after the first layer. Note that the output Hessian of the second layer is simply $\mM^{(2)}\triangleq\ex{\rmA}$.

By the Kronecker decomposition, the closed form of the layer-wise Hessians of the first and the second layer are\begin{align*}
    \mH^{(1)} &\triangleq \exop{\rvx\sim \rectNormal(0, \mI_d)}{\rmD\mW^{(2)\T}\rmA\mW^{(2)}\rmD\otimes \rvx\rvx^\T},\\
    \mH^{(2)} &\triangleq \exop{\rvx\sim \rectNormal(0, \mI_d)}{\rmA\otimes \rmD\mW^{(1)}\rvx\rvx^\T\mW^{(1)\T}\rmD}.
\end{align*}
Following the decoupling conjecture, let the Kronecker approximation of the Hessians above be\begin{align*}
    \hat\mH^{(1)} &\triangleq \exop{\rvx\sim \rectNormal(0, \mI_d)}{\rmD\mW^{(2)\T}\rmA\mW^{(2)}\rmD}\otimes\exop{\rvx\sim \rectNormal(0, \mI_d)}{\rvx\rvx^\T},\\
    \hat\mH^{(2)} &\triangleq \exop{\rvx\sim \rectNormal(0, \mI_d)}{\rmA}\otimes\exop{\rvx\sim \rectNormal(0, \mI_d)}{\rmD\mW^{(1)}\rvx\rvx^\T\mW^{(1)\T}\rmD}.
\end{align*}
The decoupling conjecture is then equivalent to $\mH^{(1)}\approx \hat\mH^{(1)}$, $\mH^{(2)}\approx \hat\mH^{(2)}$.

Since our formulae for the Hessians are going to depend on the weight matrices, throughout the section we will condition on the value of $\mW^{(1)}$ and $\mW^{(2)}$ when we take expectation (i.e. the expectation is only taken over the input $\rvx\sim \rectNormal(0, \mI_d)$). We will neglect this under-script of the expectation operator $\E$ as there will be no confusion. When we are discussing the Hessians of a certain layer, we will also neglect the upper-script and just use $\mH$ and $\mM$ when there is no confusion. Moreover, we denote $\rmX\triangleq\ex{\rvx\rvx^\T}$ as the autocorrelation of the input.

Furthermore, for simplicity of notations, we will sometimes use the verbal description ``with probability 1 over $\mW^{(1)}$/$\mW^{(2)}$, event $E$ is true'' to denote
\begin{equation}
    \lim_{n\to\infty}\prop{\mW^{(1)}\sim\gN(0,\frac{1}{d}\mI_{nd}), \mW^{(2)}\sim\gN(0,\frac{1}{n}\mI_{cn})}{E} = 1.
\end{equation}
