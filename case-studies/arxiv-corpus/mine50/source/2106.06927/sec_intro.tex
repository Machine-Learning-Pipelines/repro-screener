\section{Introduction}
\label{sec:introduction}

Deep classifiers trained on large-scale datasets extract meaningful high-level features of natural images, making them an essential tool for manipulation tasks such as style transfer~\cite{gatys_2016_image,li_2017_universal,yoo_2019_photorealistic}, image inpainting~\cite{yang_2017_high,nguyen2017plug}, image composition~\cite{shocher_2020_semantic,nguyen2016synthesizing}, among others \cite{rombach_2020_network,santurkar_2019_image,zhang_2018_unreasonable}. State-of-the-art image manipulation techniques use a decoder \cite{nguyen2017plug,zhang_2018_unreasonable}, \ie, an \emph{image generator}, to create natural images from high-level features. Extensive work has explored how to train image generators, leading to models with photorealistic results \cite{goodfellow2016deep}. Moreover, by learning how to invert deep features, image generators enable impressive synthesis use cases such as anomaly detection \cite{deecke_2018_image,golan_2018_deep} and neural network visualization \cite{nguyen2016synthesizing,nguyen2019understanding,ponce2019evolving,rombach2022invertible}. 

Inverting ImageNet features is a challenging task that often requires the generator to be more complex than the encoder~\cite{bigbigan,dosovitskiy2015inverting,dosovitskiy_2016_generating,nguyen2017plug}, incurring in a high computational cost. Donahue et al.~\cite{bigbigan} explained this shortcoming by the fact that the encoder bottleneck learns entangled representations that are hard to invert. An alternative state-of-the-art technique for inverting ImageNet features requires, in addition to the encoder and decoder CNNs, \emph{an extra} autoregressive model and vector quantization \cite{esser2021taming,esser2021imagebart} or a separate invertible network \cite{rombach2022invertible}.

In this paper, we propose \textbf{a novel mechanism for training effective ImageNet autoencoders that do not require extra decoding layers or networks besides the encoder and its mirror decoder}. Specifically, we adopt a pre-trained classifier as encoder and train an image generator to invert its features, yielding an autoencoder for real data. Unlike existing works that use feature extractors trained on natural images, we train the encoder on adversarial examples~\cite{madry_2018_towards}. This fundamental difference equips our \emph{adversarially robust} (AR) autoencoder with representations that are perceptually-aligned with human vision~\cite{engstrom_2019_adversarial,santurkar_2019_image}, resulting in favorable inversion properties.

To show the advantages of learning how to invert AR features, our generator corresponds to the \emph{mirror} architecture of the encoder, without additional decoding layers \cite{bigbigan,shocher_2020_semantic} or extra components \cite{razavi2019generating,esser2021taming,rombach2022invertible,esser2021imagebart,van2017neural}. To the best of our knowledge, we are the first to show the benefits of training an autoencoder on both adversarial and real images. Our main findings are as follows:

\begin{itemize}
    \item A generator trained to invert AR features has a substantially higher reconstruction quality than those trained on standard features. 
    Our method generalizes to different models (AlexNet~\cite{krizhevsky_2012_imagenet}, VGG-16~\cite{simonyan_2014_very}, and ResNet~\cite{he_2016_deep}) and datasets (CIFAR-10~\cite{krizhevsky_2009_learning} and ImageNet~\cite{russakovsky_2015_imagenet})(\secref{sec:experimental_inverting}).
    \item Our proposed AR autoencoder is remarkably robust to resolution changes, as shown on natural and upscaled high-resolution images (Fig.~\ref{fig:hires}). Experiments on DIV2K \cite{agustsson_2017_ntire} show it accurately reconstructs high-resolution images without any finetuning, despite being trained on low-resolution images (\secref{sec:experimental_scale}).
    \item  Our generator outperforms state-of-the-art inversion methods based on iterative optimization techniques \cite{engstrom_2019_adversarial} in terms of PSNR, SSIM, and LPIPS \cite{zhang_2018_unreasonable}. It also attains comparable accuracy to the well-established DeepSiM model \cite{dosovitskiy_2015_inverting} with a much lower model complexity (\secref{sec:experimental_comparison}).
    \item Our AR model outperforms standard baselines on three downstream tasks: style transfer \cite{li_2017_universal}, image denoising \cite{nguyen2017plug} (Sec.~\ref{sec:tasks}) and anomaly detection  \cite{deecke_2018_image}. The latter is covered in detail in the Appendix (Sec.~\ref{sec:supp_anomaly_detection}).
\end{itemize}
