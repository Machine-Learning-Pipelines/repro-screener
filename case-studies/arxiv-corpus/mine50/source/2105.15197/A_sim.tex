\section{Simulations}

We present simulations for Example~\ref{ex:CATE}: heterogeneous treatment effect estimated by neural network. In addition, we present results for heterogeneous treatment effect estimated by random forest and lasso.

Recall that the localized functional is 
$$
\textsc{CATE}(v)=\lim_{h\rightarrow 0}E[\ell_{h,v}(V)\{\gamma_0(1,V,X)-\gamma_0(0,V,X)\}],$$ where
$
\ell_{h,v}(V)=(h\omega)^{-1}K\left\{(V-v)/h\right\}$ and $\omega=E [h^{-1} K\left\{(V-v)/h\right\}].
$
$V$ is an interpretable, low dimensional characteristic such as age, race, or gender. We implement the heterogeneous treatment effect design of \cite{abrevaya2015estimating}, 
where $\textsc{CATE}(v)=v(1+2v)^2(v-1)^2$ and $v$ is a value in the interval $(-0.5,0.5)$. A single observations consists of the tuple $(Y_i,D_i,V_i,X_i)$ for outcome, treatment, covariate of interest, and other covariates. In this design, $Y_i,D_i,V_i$ are in $\mathbb{R}$ and $X_i$ is in $\mathbb{R}^3$.

A single observation is generated as follows. Draw the latent variables $(\epsilon_{ij})$ $(j=1,...,4)$ independently and identically from the uniform distribution $ \mathcal{U}(-0.5,0.5)$. Then set the covariates $(V_i,X_i)$ according to
$$
V_i=\epsilon_{i1},\quad X_{i1}=1+2V_i+\epsilon_{i2},\quad X_{i2}=1+2V_i+\epsilon_{i3},\quad X_{i3}=(V_i-1)^2+\epsilon_{i4}.
$$
Under the assumption of selection on observables, treatment assignment is as good as random conditional on $(V_i,X_i)$. Draw the treatment $D_i$ from the Bernoulli distribution with parameter $\Lambda \{1/2(V_i+X_{i1}+X_{i2}+X_{i3})\}$ where $\Lambda$ is the logistic link function. Finally, calculate outcome $Y_i$ as $0$ if $D_i=0$ and $V_i X_{i1} X_{i2} X_{i3}+\nu_i$ if $D_i=1$, where the response noise $\nu_i$ is independently drawn from the Gaussian distribution $\mathcal{N}(0,1/16)$. A random sample consists of $n=100$ such observations $(Y_i,D_i,V_i,X_i)$ $(i=1,...,n)$.

We implement different variations of Algorithm~\ref{alg:target} with $L=5$ folds. Across variations, we use a lasso estimator $\hat{\alpha}$ for the minimal Riesz representer $\alpha_0^{\min}$ \cite{chernozhukov2018learning}. We consider different  estimators $\hat{\gamma}$ for the nonparametric regression $\gamma_0$: neural network, random forest, and lasso. We consider both low dimensional and high dimensional variations. In the low dimensional variation, the estimators $(\hat{\alpha}_{\ell},\hat{\gamma}_{\ell})$ use $(D_i,V_i,X_i)$ $(i \text{ \normalfont in } I^c_{\ell})$ as well as their interactions. In the high dimensional variation, the estimators $(\hat{\alpha}_{\ell},\hat{\gamma}_{\ell})$ use fourth order polynomials of $(D_i,V_i,X_i)$ $(i \text{ \normalfont in } I^c_{\ell})$.

Some tuning choices are necessary. We follow the default hyperparameter settings to tune the lasso Riesz representer and lasso regression from \cite{chernozhukov2018learning}. We implement the neural network with a single hidden layer of eight neurons and the random forest with 1000 trees as in \cite{chernozhukov2018original}.  Finally, to tune the bandwidth, we use the heuristic $h=c_h\hat{\sigma}_vn^{-0.2}$ \cite{colangelo2020double}, where $\hat{\sigma}^2_v$ is the sample variance of $(V_i)$ $(i=1,...,n)$. The bandwidth hyperparameter $c_h$ is chosen by the analyst. We evaluate robustness of coverage with respect to hyperparameter values $c_h=0.25, 0.50, 1.00$ below. Empirically, we find that $c_h=0.25$ and $c_h=0.50$ work well.

For each choice of nonparametric regression estimator $\hat{\gamma}$, whether neural network, random forest, or lasso, and for each choice of specification, whether low or high dimensional, we report a coverage table summarizing 500 simulations. The initial columns denote the grid value $v$, the corresponding heterogeneous treatment effect $\textsc{CATE}(v)$, and the bandwidth hyperparamter value $c_h$. The subsequent columns calculate the average point estimate and the average standard error across the 500 simulations for this choice of $\{v,\textsc{CATE}(v),c_h\}$. The final columns report what percentage of the 500 confidence intervals contain the true value $\textsc{CATE}(v)$ compared to the theoretical benchmarks of $80\%$ and $95\%$, respectively.

For the low dimensional regime, Tables~\ref{tab:nn_low},~\ref{tab:rf_low}, and~\ref{tab:lasso_low} summarize results for neural network, random forest, and lasso, respectively. With bandwidth hyperparameter values $c_h=0.25$ and $c_h=0.50$, coverage is close to the nominal level across $\hat{\gamma}$ estimators and across grid values $v=-0.25, 0.00, 0.25$. Neural network and random forest have comparable performance. Lasso has higher bias and compensates with higher variance for the grid value $v=0.25$.

\begin{table}[H]
\centering
  \begin{threeparttable}
    \caption{Low dimensional coverage simulation with neural network}
     \begin{tabular}{lcccccc}
       $v$& $\textsc{CATE}(v)$ & Tuning &  Ave. Est. & Ave. S.E. &  80\% Cov. & 95\% Cov. \\[5pt]
-0.25 & -0.10 & 0.25 & -0.10 & 0.05 &  83\% & 94\% \\
-0.25 & -0.10 & 0.50 & -0.10 & 0.04  & 85\% & 95\% \\
-0.25 & -0.10 & 1.00 & -0.08 & 0.03  & 71\% & 88\% \\
0.00 & 0.00 & 0.25 & 0.00 & 0.04 &  78\% & 95\% \\
0.00 & 0.00 & 0.50 & 0.00 & 0.03  & 78\% & 94\% \\
0.00 & 0.00 & 1.00 & 0.02 & 0.02  & 62\% & 85\% \\
0.25 & 0.32 & 0.25 & 0.31 & 0.12 &  85\% & 92\% \\
0.25 & 0.32 & 0.50 & 0.30 & 0.09  & 85\% & 93\% \\
0.25 & 0.32 & 1.00 & 0.28 & 0.06  & 76\% & 88\% 
     \end{tabular}
     \label{tab:nn_low}
    \begin{tablenotes}
      \small
      \item Ave., average; Est., estimate; S.E., standard error; Cov., coverage. The largest standard error for the results in column 6 is 2\%. The largest standard error for the results in column 7 is 2\%.
    \end{tablenotes}
  \end{threeparttable}
\end{table}


\begin{table}[H]
\centering
  \begin{threeparttable}
    \caption{Low dimensional coverage simulation with random forest}
     \begin{tabular}{lcccccc}
       $v$& $\textsc{CATE}(v)$ & Tuning &  Ave. Est. & Ave. S.E. &  80\% Cov. & 95\% Cov. \\[5pt]
-0.25 & -0.10 & 0.25 & -0.10 & 0.05 &  86\% & 93\% \\
-0.25 & -0.10 & 0.50 & -0.09 & 0.03  & 83\% & 94\% \\
-0.25 & -0.10 & 1.00 & -0.08 & 0.02  & 60\% & 79\% \\
0.00 & 0.00 & 0.25 & 0.00 & 0.02 &  70\% & 91\% \\
0.00 & 0.00 & 0.50 & 0.01 & 0.02  & 72\% & 91\% \\
0.00 & 0.00 & 1.00 & 0.02 & 0.02  & 48\% & 75\% \\
0.25 & 0.32 & 0.25 & 0.30 & 0.12 &  83\% & 91\% \\
0.25 & 0.32 & 0.50 & 0.29 & 0.08  & 82\% & 91\% \\
0.25 & 0.32 & 1.00 & 0.28 & 0.06  & 71\% & 86\% 
     \end{tabular}
     \label{tab:rf_low}
    \begin{tablenotes}
      \small
      \item Ave., average; Est., estimate; S.E., standard error; Cov., coverage. The largest standard error for the results in column 6 is 2\%. The largest standard error for the results in column 7 is 2\%.
    \end{tablenotes}
  \end{threeparttable}
\end{table}

\begin{table}[H]
\centering
  \begin{threeparttable}
    \caption{Low dimensional coverage simulation with lasso}
     \begin{tabular}{lcccccc}
       $v$& $\textsc{CATE}(v)$ & Tuning &  Ave. Est. & Ave. S.E. &  80\% Cov. & 95\% Cov. \\[5pt]
-0.25 & -0.10 & 0.25 & -0.08 & 0.08 &  81\% & 95\% \\
-0.25 & -0.10 & 0.50 & -0.08 & 0.05  & 81\% & 95\% \\
-0.25 & -0.10 & 1.00 & -0.06 & 0.04  & 63\% & 88\% \\
0.00 & 0.00 & 0.25 & 0.00 & 0.06 &  79\% & 94\% \\
0.00 & 0.00 & 0.50 & 0.01 & 0.04  & 83\% & 96\% \\
0.00 & 0.00 & 1.00 & 0.02 & 0.03  & 73\% & 92\% \\
0.25 & 0.32 & 0.25 & 0.30 & 0.11 &  86\% & 94\% \\
0.25 & 0.32 & 0.50 & 0.29 & 0.08  & 85\% & 95\% \\
0.25 & 0.32 & 1.00 & 0.28 & 0.06  & 71\% & 89\% 
     \end{tabular}
     \label{tab:lasso_low}
    \begin{tablenotes}
      \small
      \item Ave., average; Est., estimate; S.E., standard error; Cov., coverage. The largest standard error for the results in column 6 is 2\%. The largest standard error for the results in column 7 is 1\%.
    \end{tablenotes}
  \end{threeparttable}
\end{table}

For the high dimensional regime, Tables~\ref{tab:nn_high},~\ref{tab:rf_high}, and~\ref{tab:lasso_high} summarize results for neural network, random forest, and lasso, respectively. With bandwidth hyperparameter values $c_h=0.25$ and $c_h=0.50$, coverage is close to the nominal level across $\hat{\gamma}$ estimators and for grid values $v=-0.25$ and $v=0.00$. Across $\hat{\gamma}$ estimators, the grid value $v=0.25$ is more challenging. Compared to the low dimensional regime, each estimator in the high dimensional regime has higher bias and compensates with higher variance for the grid value $v=0.25$.



\begin{table}[H]
\centering
  \begin{threeparttable}
    \caption{High dimensional coverage simulation with neural network}
     \begin{tabular}{lcccccc}
       $v$& $\textsc{CATE}(v)$ & Tuning &  Ave. Est. & Ave. S.E. &  80\% Cov. & 95\% Cov. \\[5pt]
-0.25 & -0.10 & 0.25 & -0.09 & 0.04 &  84\% & 91\% \\
-0.25 & -0.10 & 0.50 & -0.09 & 0.03  & 78\% & 91\% \\
-0.25 & -0.10 & 1.00 & -0.07 & 0.02  & 49\% & 74\% \\
0.00 & 0.00 & 0.25 & 0.00 & 0.03 &  75\% & 95\% \\
0.00 & 0.00 & 0.50 & 0.01 & 0.02  & 74\% & 91\% \\
0.00 & 0.00 & 1.00 & 0.04 & 0.03  & 52\% & 75\% \\
0.25 & 0.32 & 0.25 & 0.39 & 0.20 &  90\% & 97\% \\
0.25 & 0.32 & 0.50 & 0.39 & 0.15  & 88\% & 97\% \\
0.25 & 0.32 & 1.00 & 0.38 & 0.13  & 81\% & 95\% 
     \end{tabular}
     \label{tab:nn_high}
    \begin{tablenotes}
      \small
      \item Ave., average; Est., estimate; S.E., standard error; Cov., coverage. The largest standard error for the results in column 6 is 2\%. The largest standard error for the results in column 7 is 2\%.
    \end{tablenotes}
  \end{threeparttable}
\end{table}



\begin{table}[H]
\centering
  \begin{threeparttable}
    \caption{High dimensional coverage simulation with random forest}
     \begin{tabular}{lcccccc}
       $v$& $\textsc{CATE}(v)$ & Tuning &  Ave. Est. & Ave. S.E. &  80\% Cov. & 95\% Cov. \\[5pt]
-0.25 & -0.10 & 0.25 & -0.09 & 0.05 &  81\% & 91\% \\
-0.25 & -0.10 & 0.50 & -0.09 & 0.03  & 78\% & 91\% \\
-0.25 & -0.10 & 1.00 & -0.07 & 0.02  & 53\% & 75\% \\
0.00 & 0.00 & 0.25 & 0.00 & 0.02 &  76\% & 94\% \\
0.00 & 0.00 & 0.50 & 0.01 & 0.02  & 74\% & 92\% \\
0.00 & 0.00 & 1.00 & 0.04 & 0.03  & 44\% & 71\% \\
0.25 & 0.32 & 0.25 & 0.37 & 0.18 &  91\% & 96\% \\
0.25 & 0.32 & 0.50 & 0.40 & 0.16  & 88\% & 97\% \\
0.25 & 0.32 & 1.00 & 0.39 & 0.14  & 81\% & 95\% 
     \end{tabular}
     \label{tab:rf_high}
    \begin{tablenotes}
      \small
      \item Ave., average; Est., estimate; S.E., standard error; Cov., coverage. The largest standard error for the results in column 6 is 2\%. The largest standard error for the results in column 7 is 2\%.
    \end{tablenotes}
  \end{threeparttable}
\end{table}


\begin{table}[H]
\centering
  \begin{threeparttable}
    \caption{High dimensional coverage simulation with lasso}
     \begin{tabular}{lcccccc}
       $v$& $\textsc{CATE}(v)$ & Tuning &  Ave. Est. & Ave. S.E. &  80\% Cov. & 95\% Cov. \\[5pt]
-0.25 & -0.10 & 0.25 & -0.08 & 0.06 &  75\% & 90\% \\
-0.25 & -0.10 & 0.50 & -0.07 & 0.04  & 74\% & 90\% \\
-0.25 & -0.10 & 1.00 & -0.06 & 0.03  & 50\% & 74\% \\
0.00 & 0.00 & 0.25 & 0.01 & 0.05 &  78\% & 96\% \\
0.00 & 0.00 & 0.50 & 0.02 & 0.04  & 80\% & 97\% \\
0.00 & 0.00 & 1.00 & 0.04 & 0.04  & 59\% & 84\% \\
0.25 & 0.32 & 0.25 & 0.41 & 0.20 &  89\% & 96\% \\
0.25 & 0.32 & 0.50 & 0.45 & 0.18  & 87\% & 97\% \\
0.25 & 0.32 & 1.00 & 0.43 & 0.17  & 82\% & 95\% 
     \end{tabular}
     \label{tab:lasso_high}
    \begin{tablenotes}
      \small
      \item Ave., average; Est., estimate; S.E., standard error; Cov., coverage. The largest standard error for the results in column 6 is 2\%. The largest standard error for the results in column 7 is 2\%.
    \end{tablenotes}
  \end{threeparttable}
\end{table}