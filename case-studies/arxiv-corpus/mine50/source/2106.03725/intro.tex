%!TEX root = stability_manifold_TSP.tex

Graph convolutional filters\cite{ortega2018graph, isufi2016autoregressive, gama2020graphs} and graph neural networks (GNNs) \cite{scarselli2008graph, gama2019convolutional, zhou2020graph} have become the tool of choice for signal and information processing on graphs, e.g., \cite{fan2019graph, wu2019session, chowdhury2021unfolding, wang2020unsupervised}. In several applications, graphs can be considered as samples of a manifold. This is sometimes quite explicit as in the case of, e.g., point clouds \cite{bronstein2017geometric, bronstein2021geometric} and sometimes somewhat implicit as in the case of, e.g., wireless communication networks \cite{fan2019graph, wu2019session}. In this context we can think of convolutions on graphs and GNNs as discretizations of their manifold counterparts. This is important because one can often gain valuable insights about discrete (graph) signal processing by studying continuous (manifold) signal processing. 

The main technical contribution of this paper is analyzing the stability of manifold filters and manifold neural networks (MNNs) to smooth deformations of the manifold. Prior to developing stability analyses we define \emph{manifold} convolutional filters and frequency responses that are consistent with \emph{graph} convolutional filters and frequency responses. 

\myparagraph{Manifold Filters} Consider a manifold with Laplace-Beltrami (LB) operator $\ccalL$ and formulate the corresponding manifold diffusion equation with respect to an auxiliary time variable $t$. For initial condition $f(x)$, manifold diffusions generate the time varying manifold function $u(x,t) = e^{-t\ccalL}f(x)$ in which $e^{-t\ccalL}$ is the LB operator exponential (Section \ref{sec_lb}). A manifold convolutional filter $\tdh(t)$ then acts on the function $f(x)$ as the integral over time of the product of $\tdh(t)$ with the diffusion sequence $u(x,t)$ (Section \ref{sec_manifold_filters}),
%
\begin{equation} \label{eqn:manifold-conv-spatial_in_the_intro}
   g(x) = (\bbh f)(x) =\int_0^\infty \tdh(t)e^{-t\ccalL}f(x)\text{d}t  .
\end{equation}
%
A manifold convolutional filter is such that we can recover graph filters through discretization of the manifold and discretization of the auxiliary time variable $t$ (Section \ref{sec:discre_nn}). The reason for this connection is that graph filters are linear combinations of the elements of the graph diffusion sequence \cite{shuman2013emerging, ortega2018graph, sandryhaila2013discrete} and \eqref{eqn:manifold-conv-spatial_in_the_intro} defines manifold filters as linear combinations of the elements of the manifold diffusion sequence. Manifold filters are also generalizations of standard time convolutions. This requires consideration of the wave equation on the real line so that the exponential of the derivative operator $e^{-t\partial/\partial x}$ appears in \eqref{eqn:manifold-conv-spatial_in_the_intro} (Appendix \ref{def:manifold-convolution}).



\myparagraph{Frequency Response of Manifold Filters} We define a filter's frequency response as  
%
\begin{equation}\label{eqn:operator-frequency_in_the_intro}
    \hat{h}(\lambda)=\int_0^\infty \tdh(t) e^{- t \lambda  }\text{d}t \text{.}
\end{equation}
%
This definition is motivated by the fact that manifold convolutions can be decomposed on separate spectral components (Section \ref{sec_spectral_representation}). Indeed, let $\bm\phi_i$ be an eigenfunction of the LB operator associated with eigenvalue $\lambda_i$. If $[\hat{f}]_i = \langle f, \bm\phi_i \rangle_{L^2(\ccalM)}$ and $[\hat{g}]_i = \langle g, \bm\phi_i \rangle_{L^2(\ccalM)}$ are projections of the functions $f$ and $g$ in \eqref{eqn:manifold-conv-spatial_in_the_intro} on this eigenfunction, $[\hat{g}]_i$ depends only on $[\hat{f}]_i$ and can be written as $[\hat{g}]_i = \hat{h}(\lambda_i) [\hat{f}]_i$ (Proposition \ref{prop:filter-spectral}). 

This definition of the frequency response of a manifold filter generalizes the frequency response of a standard time filter. This is because the definition in \eqref{eqn:operator-frequency_in_the_intro} is a Laplace transform, which reduces to the Fourier transform of a filter's impulse response when restricted to the imaginary axis $\lambda=j\omega$. It is also a generalization of the frequency response of a graph filter, which is a z-transform \cite{oppenheim1997signals}.

\myparagraph{Stability to Deformations} We establish stability of manifold filters and MNNs with respect to domain deformations formally defined as manifold diffeomorphisms. We consider a signal $f$ defined on a manifold $\ccalM$ and the signal $f\circ\tau$ made up of the composition of $f$ with a diffeomorphism. The signals $f$ and $f \circ \tau$ are passed through the same MNN $\bm\Phi$. In this paper we prove that if $\tau$ is $\epsilon$-small and $\epsilon$-smooth the respective MNN outputs satisfy
%
\begin{align}\label{eqn_bound_for_intro}
   \| \bm\Phi(f \circ \tau)-\bm\Phi(f)\|_{L^2(\ccalM)}=O(\epsilon)\|f\|_{L^2(\ccalM)},    
\end{align}
%
provided that the manifold filters in the layers of the MNN satisfy certain spectral properties (Sections \ref{sec_manifold_stability} and \ref{sec:stability_nn}). 

The bound in \eqref{eqn_bound_for_intro} is a generalization of the standard convolutional neural network (CNN) bound in \cite{mallat2012group}, which studies diffeomorphisms of the real line and its effect on convolutional filter banks and CNNs. The bound is also a limit version of the GNN bounds of \cite{gama2020stability} and related literature \cite{gama2019stability, verma2019stability, zou2020graph, ruiz2021graph, ruiz2021graphon}. As is the case of \cite{mallat2012group, gama2020stability, gama2019stability, verma2019stability, zou2020graph, ruiz2021graph, ruiz2021graphon} the bound in \eqref{eqn_bound_for_intro} holds when the frequency response $\hat{h}(\lambda)$ of the filters that make up the layers of the MNN have decreasing variability with increasing $\lambda$ (Section \ref{sec_manifold_stability}). Thus, stability requires \emph{layers} with limited ability to discriminate high frequency components and implies that one may expect \emph{multi-layered} MNNs to outperform filters in learning tasks in which high frequency components are important (Section \ref{subsec:discussion}).

\myparagraph{Related Work and Significance} We focus on the stability analysis of MNNs -- the limit version of GNNs -- because all of the existing GNN stability results have bounds that grow with the number of nodes \cite{gama2019stability, ruiz2021graph} in the graph. To overcome this limitation, many works have studied neural networks on graphons \cite{ruiz2021graphon, maskey2021transferability, ruiz2021transferability} and more general graph models with variable sparsity \cite{keriven2020convergence}. Results in these settings are independent of graph size â€“ same as the results for a limit object presented here. It needs to be pointed out that graphons are limits of dense graphs in the sense that they have growing degrees. This is different from graphs sampled from manifolds that can have finite degrees as the discretization becomes finer \cite{calder2019improved}. Moreover, even if there exist random graph models allowing to model moderately sparse graphs (e.g., \cite{keriven2020convergence}), these models do not have the physical interpretation of a manifold, which is often a better descriptor of real-world domains.
Of particular relevance to our paper is the work on GNN transferability for graphs that are sampled from a general topological space \cite{levie2019transferability}. 


%\red{END HERE?}



% We start by proposing the definition of the manifold convolution and constructing MNNs with this operation. To study the stability properties of MNNs, we first show that manifold deformations lead to absolute and relative perturbations of the manifold's Laplace-Beltrami (LB) operator (Theorem \ref{thm:perturb}). Since perturbations of the LB operator perturb its spectrum, we introduce two types of manifold convolutional filters, the frequency difference threshold (FDT) filter (Definition \ref{def:alpha-filter}) and the frequency ratio threshold (FRT) filter (Definition \ref{def:frt-filter}). These filters mitigate the spectral effects of absolute and relative eigenvalue perturbations respectively, grouping eigenvalues that are less than a certain threshold apart to make sure that they have similar frequency responses. When satisfying certain Lipschitz continuity assumptions (Definitions \ref{def:lipschitz} and \ref{def:int-lipschitz}), it is possible to show that these manifold filters are stable to absolute and relative perturbations of the LB operator respectively, because they mitigate the difference of frequency responses (Theorems \ref{thm:stability_abs_filter} and \ref{thm:stability_rela_filter}). Using FDT and FRT filters along with pointwise nonlinear functions, we can thus build MNNs that inherit these stability properties (Theorem \ref{thm:stability_nn}). Therefore, MNNs are stable to manifold deformations as stated in the following theorem. A formal statement of Theorem \ref{thm:main_result} is also presented in Section \ref{sec:stability_nn}.

% \begin{theorem}[MNN stability to deformations] \label{thm:main_result}
% Let $\bm\Phi(f)$ be a MNN with fixed parameters on the manifold $\ccalM$ and assume that the convolutional filters of this MNN are FDT, FRT, Lipschitz and integral Lipschitz. Suppose that $\ccalM$ undergoes a deformation $\tau(x): \ccalM\rightarrow \ccalM$ where the geodesic distance satisfies $\text{dist}(x,\tau(x))=\epsilon$. The Jacobian matrix of the deformation satisfies $J(\tau_*)= I + \Delta$ with $\|\Delta\|_F=\epsilon$. Then, under mild assumptions, 
% $$\| \bm\Phi(f \circ \tau)-\bm\Phi(f)\|_{L^2(\ccalM)}=O(\epsilon)\|f\|_{L^2(\ccalM)}.$$
% \end{theorem}

% This is the main result of our paper. It is worth noting that the Lipschitz continuity assumptions on the FDT and FRT filters inevitably result in a trade-off between their stability and discriminability. This means that manifold filters cannot be both stable and discriminative. Since MNNs inherit their stability properties from manifold filters, we could expect them to also inherit this incompatibility. Interestingly, they do not, because pointwise nonlinearities improve discriminability by scattering frequency components all over the eigenvalue spectrum. MNNs are hence as stable as but more discriminative than manifold filters or, alternatively, as discriminative as but more stable than manifold filters. These considerations are discussed in detail in Section \ref{subsec:discussion}. Using GNNs---as we discuss in Section \ref{sec:discre_nn}, MNNs are implemented as GNNs in practice by discretization of the manifold and time domains---we further illustrate this result numerically in Section \ref{sec:simu}. 
%\blue{We also illustrate the practical implementation of the continous MNN model, from which we also show the consistency of the defined manifold convolution to the graph convolution.}
%This is the main result of this paper and it is a direct consequence of two results of independent interest that we prove in Sections \ref{sec:stability_mnn_abs} and \ref{sec:stability_mnn_rel}: that MNNs with FDT filters satisfying a Lipschitz continuity assumption (Definition \ref{def:lipschitz}) are stable to absolute perturbations (Theorem \ref{thm:stability_nn}); and that MNNs with FRT filters satisfying an integral Lipschitz continuity assumption (Definition \ref{def:int-lipschitz}) are stable to relative perturbations (Theorem \ref{thm:stability_nn_rela}). 

\myparagraph{Organization} Section \ref{sec:stability_filter} introduces preliminary definitions (Section \ref{sec_lb}), defines manifold convolutions (Section \ref{sec_manifold_filters}), and introduces the spectral domain representation of manifold filters (Section \ref{sec_spectral_representation}). Section \ref{sec_manifold_stability} studies the stability of manifold filters to manifold deformations. It shows that a diffeomorphism results in a perturbation of the LB operator that involves additive and multiplicative terms (Theorem \ref{thm:perturb}). It then goes on to study the effect on manifold filters of additive (Section \ref{subsec:filter-absolute}) and multiplicative (Section \ref{subsec:filter-relative}) perturbations of the LB operator. Section \ref{sec:stability_nn} extends the analysis of manifold filter stability to manifold neural networks and Section \ref{subsec:discussion} discusses the implications of the results derived in Sections \ref{sec_manifold_stability} and \ref{sec:stability_nn}. Section \ref{sec:discre_nn} explains how to recover graph filters and GNNs from the discretization of a manifold. Section \ref{sec:simu} illustrates the results of Sections \ref{sec_manifold_stability} and \ref{sec:stability_nn} with numerical examples. Section \ref{sec:conclusion} concludes the paper. Proofs are deferred to appendices and supplmenetary material.