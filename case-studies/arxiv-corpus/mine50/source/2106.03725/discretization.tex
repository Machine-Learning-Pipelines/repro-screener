%!TEX root = stability_manifold_TSP.tex

MNNs are built from manifold convolutional filters (Definition \ref{def:manifold-convolution}) operating on a continuous manifold and over an infinite time horizon. 
This makes it impractical to implement directly the architecture described by \eqref{eqn:mnn} in applications. In this section, we discuss how MNNs are implemented in practice over a set of discrete samples from the manifold in a finite and discrete time frame.
%Manifold neural networks are built upon continuous and infinite manners in both spatial and time domains. This makes it intractable in practical implementation. Therefore we first discritize the manifold with uniform sampling points and propose the convolution on the discritized manifold. We further discretize the time domain and parameterize the pre-defined manifold convolutional filter with discrete parametrization. Then we could recover the definition of graph convolutions \cite{gama2020graphs} which is commonly used to build graph neural networks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% SUBSECTION %%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{{Discretization in the Space Domain}}

%\red{L: I think this subsection is good in general, but it needs to be proofread for typos, and some sentences need to be written in a more direct, clear way. I will do it for the first paragraph, and then you can rewrite the others in the image of this one.}

In practice, the explicit form of the manifold and of its LB operator are unknown. What we typically have access to is a point cloud representation of the manifold, i.e., a discrete set of sampling points.
%Instead of accessing the underlying manifold directly, 
%it is common to use the point cloud formed by sampling points of the manifold to approximate the manifold structure 
From these points' coordinates, the structure of the manifold is approximated by a geometric or a nearest neighbor graph \cite{dunson2021spectral,belkin2008towards,calder2019improved}. The LB operator is then approximated by the graph Laplacian,
%it is intuitive to look into the signal processing over the manifold by utilizing the graph Laplacians to study the information propagation over the graphs.
which can be shown to converge to the LB operator as the number of sampling points grows \cite{dunson2021spectral} \cite{calder2019improved}.
%Previous works also provide theoretical results to justify the convergence of the graph Laplacians to the underlying manifold LB operator as the number of sampling points grows under certain conditions \cite{dunson2021spectral} \cite{calder2019improved}.

%\red{L: A few comments about the paragraph below: a) avoid writing fractions as in-line equations; create a numbered equation for the definition of the edge weights $w_{ij}$; (b) define the graph adjacency matrix $\bbA$ first, and then the graph Laplacian as $\bbL = \mbox{diag}(\bbA \boldsymbol{1})-\bbA$; c) no need to repeat that the Laplacian converges; you already said that in the paragraph above.}
Explicitly, suppose that $X= \{x_1, x_2,\dots, x_n\}$ is a set of $n$ points sampled i.i.d. from measure $\mu$ of manifold $\ccalM$, which is embedded in $\reals^N$. We can construct a complete weighted symmetric graph $\bbG_n$ by taking the sampled points to be the vertices of the graph and setting the edge weights based on the Euclidean distance between pairs of points. Specifically, the weight $w_{ij}$ associated with edge $(i,j)$ is given by
\begin{equation}\label{eqn:weight}
    w_{ij}=\frac{1}{n}\frac{1}{t_n(4\pi t_n)^{k/2}}\exp\left(-\frac{\|x_i-x_j\|^2}{4t_n}\right),
\end{equation}
where $\|x_i-x_j\|$ is the Euclidean distance between points $x_i$ and $x_j$ while $t_n$ is a parameter associated with the chosen Gaussian kernel \cite{belkin2008towards}. The adjacency matrix $\bbA_n \in \reals^{n\times n}$ is thus defined as $[\bbA_n]_{ij}=w_{ij}$ for $1 \leq i,j\leq n$ and the corresponding graph Laplacian matrix $\bbL_n$ \cite{merris1995survey} is given by
\begin{equation}
    \bbL_n = \mbox{diag}(\bbA_n \boldsymbol{1})-\bbA_n.
\end{equation}
We interpret $\bbL_n$ the Laplacian operator of the constructed graph $\bbG_n$. 
%With the number of sampling points grows, the discrite Laplacian will asymptotically recover the LB operator. 
%\red{L: Expand the paragraph by first introducing the discrete approximations of manifold signals (the vectors $\bbx$), which are obtained by evaluating these signals at the sample points $x_i$. Then say that using the discrete approximation of the signals and of the LB operator, we can approximate manifold filters by the filter in (33), which is parametric on $\bbL_n$.}
Similarly, we define a uniform sampling operator $\bbP_n: L^2(\ccalM)\rightarrow L^2(\bbG_n)$ to sample manifold signals. Given a manifold signal $f$, we can use operator $\bbP_n$ to sample graph signals $\bbx_n \in \reals^n$ as
\begin{equation}
\label{eqn:sampling}
    \bbx_n = \bbP_n f\text{ with }[\bbx_n]_i = f(x_i), \quad  x_i \in X,
\end{equation}
where the $i$-th entry of the graph signal $\bbx_n$ is the manifold signal $f$ evaluated at the sample point $x_i$. 
%The discrete approximations of the manifold signals and the LB operator therefore can be constructed based on the sampling point set. 

In Proposition \ref{prop:manifold-filter}, we have shown that the manifold filter $\bbh$ is parametric on the LB operator. Therefore, we can also parameterize $\bbh$ by the discrete graph Laplacian operator $\bbL_n$, i.e.,
\begin{equation}\label{eqn:sample_manifold_convolution}
    \bbz_n = \int_{0}^\infty \tdh(t) e^{-t\bbL_n} \text{d}t \bbx_n=\bbh(\bbL_n)\bbx_n, \; \bbx_n, \bbz_n \in\reals^n, 
\end{equation}
where $\bbz_n$, the output of the filter, is now a discrete graph signal.
By cascading these discrete filters operated on graph $\bbG_n$ and pointwise nonlinearities layer after layer, we can then approximate the MNN on $\bbG_n$ as
\begin{equation}\label{eqn:dis-mnn}
    \bbx_l^p = \sigma\left(\sum_{q=1}^{F_{l-1}} \bbh_l^{pq}(\bbL_n) \bbx^q_{l-1} \right),
\end{equation}
where $\bbh_l^{pq}(\bbL_n)$ maps the $q$-th feature in the $l-1$-th layer to the $p$-th feature in the $l$-th layer, $1\leq q \leq F_{l-1}$ and $1\leq p\leq F_l$, and $F_l$ denotes the number of features in the $l$-th layer (we have dropped the subscript $n$ in $\bbx_l^p$ and $\bbx_{l-1}^q$ for simplicity).
%similar to the notations used in the manifold neural network representations. 
After gathering the filter functions in the set $\bbH$, this neural network %on the discretized manifold $\bbG_n$ 
can be represented more succinctly as $\bm\Phi(\bbH, \bbL_n, \bbx)$. 

Equation \eqref{eqn:dis-mnn} is a consistent approximation of the MNN because, as $n$ goes to infinity, the discrete graph Laplacian operator $\bbL_n$ of the graph $\bbG_n$ converges to the LB operator $\ccalL$ of the manifold $\ccalM$, and the sampled graph signal $\bbx_n$ converges to the manifold signal $f$ \cite{belkin2008towards}. These facts combinely imply that the output of the neural network on the graph $\bbG_n$ converges to the output of the neural network on the continuous manifold as stated in the following.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% PROPOSITION %%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proposition}
\label{prop:convergence} 
Let $X=\{x_1, x_2,...x_n\}$ be $n$ points sampled i.i.d. from measure $\mu$ of $d$-dimensional manifold $\ccalM \subset \reals^N$, with corresponding sampling operator $\bbP_n$ \eqref{eqn:sampling}. Let $\bbG_n$ be a discrete graph approximation of $\ccalM$ constructed from $X$ as in \eqref{eqn:weight} with $t_n = n^{-1/(d+2+\alpha)}$ and $\alpha>0$. Let $\bm\Phi(\bbH, \cdot, \cdot)$ be a neural network parametrized either by the LB
operator $\ccalL$ of the manifold $\ccalM$ \eqref{eqn:mnn} or the graph Laplacian operator $\bbL_n$ of $\bbG_n$. It holds that
\begin{equation}
    \lim_{n\rightarrow \infty} \|\bm\Phi(\bbH,\bbL_n,\bbP_n f) - \bbP_n \bm\Phi(\bbH,\ccalL,f) \|_{L^2(\bbG_n)} = 0,
\end{equation}
with the limit taken in probability.
%where the limit is taken in probability.
%and $\bbP_n$ is a uniform sampling operator over the manifold with measure $\mu$. 
\end{proposition}
\begin{proof}
See Section \ref{app:convergence} in supplementary material.
\end{proof}

% Graph convolutions can be defined with graph shift operator $\exp(-\bbL_n)$ as
% \begin{equation}
%     \bbz = \sum_{k=0}^\infty h_k e^{-k\bbL_n} \bbx=\bbh(\bbL_n)\bbx, \qquad \bbx, \bbz \in\reals^n, 
% \end{equation}
% where $\bbh(\bbL_n)$ represents the graph convolutional filter with filter coefficients $\{h_k\}_{k=0}^\infty$. Compared with \eqref{eqn:manifold_convolution_discrete}, we can see that the same set of filter coefficients can be implemented in both manifold and graph convolutional filters. By cascading these graph filters and nonlinearities layer by layer, the graph neural network can be represented as
% \begin{equation}
%     \bbx_l^p = \sigma\left(\sum_{q=1}^{F_{l-1}} \bbh_l^{pq}(\bbL_n) \bbx^q_{l-1} \right),
% \end{equation}
% where $\bbh_l^{pq}(\bbL_n)$ maps the $q$-th feature in the $l-1$-th layer to the $p$-th feature in the $l$-th layer with $1\leq q \leq F_{l-1}$ and $1\leq p\leq F_l$. The number of features in the $l$-th layer is denoted as $F_l$ similar to the notations used in the manifold neural network representations. 
% By gathering the filter coefficients in a tensor $\bbH$, the GNN map can be defined as $\bm\Phi(\bbH, \bbL_n, \bbx)$. 

% \begin{proposition}
% Put $t_n=n^{-\frac{1}{d+2+\alpha}}$ where $\alpha>0$, it holds that:
% \begin{equation}
%     \lim_{n\rightarrow \infty} \|\bm\Phi(\bbH,\bbL_n,\bbP_n f) - \bbP_n \bm\Phi(\bbH,\ccalL,f) \| =0,
% \end{equation}
% where the limit is taken in probability and $\bbP_n$ is a uniform sampling operator over the manifold with measure $\mu$. 
% \end{proposition}

%\red{L: ``Consistency'' here is very vague. The idea of the paragraph is here, but rewrite it in a more clear way.}
%With the theoretical support of the convergence of neural networks on discrete manifold, we can also claim that the neural networks constructed based on the discrete Laplacians would inherit the stability property of MNNs.
This proposition provides theoretical support to state that neural networks constructed from the discrete Laplacian $\bbL_n$ converge to MNN and thus can inherit the stability properties of the MNN.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% SUBSECTION %%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Discretization in the Time Domain}

In order to learn an MNN \eqref{eqn:mnn}, we need to learn the manifold convolutional filters $\bbh_l^{pq}$. This means that we need to learn the impulse responses $\tilde{h}(t)$ in Definition \ref{def:manifold-convolution}. However, learning continuous functions $\tilde{h}$ is computationally infeasible,
%The parametric mapping of manifold filter defined in Definition \ref{def:manifold-convolution} implies a filter function over infinite time domain. For trainable and practical filter function representations, 
so we sample $\tilde{h}$ over fixed intervals of duration $T_s$ and parameterize the filter with coefficients $h_k = \tilde{h}(k T_s)$, $k =0 ,1, 2\dots$. Setting the sampling interval to $T_s=1$ for simplicity, the discrete-time manifold convolution can be written as
\begin{equation}
\label{eqn:manifold_convolution_discrete}
    \bbh(\ccalL) f(x)= \sum_{k=0}^{\infty} h_k e^{-k\ccalL}f(x) 
\end{equation}
where $\{h_k\}_{k=0}^\infty$ are called the filter coefficients or taps.

Yet, learning \eqref{eqn:manifold_convolution_discrete} is still impractical because there is an uncountable number of parameters $h_k$. To address this, we fix a time horizon of $K$ time steps and rewrite \eqref{eqn:manifold_convolution_discrete} as
\begin{equation}
\label{eqn:manifold_convolution_discrete}
    \bbh(\ccalL) f(x)= \sum_{k=0}^{K-1} h_k e^{-k\ccalL}f(x)
\end{equation}
which can be seen as a finite impulse response (FIR) filter with shift operator $e^{-\ccalL}$. 
%The set of filter coefficients $\{h_k\}_{k=0}^\infty$ thus can be seen as filter taps and can define a manifold filter for any given LB operator $\ccalL$.
Indeed, the frequency response of this filter [cf. Proposition \ref{prop:filter-spectral}] is given by
\begin{equation}
    \hat{h}(\lambda)= \sum_{k=0}^{K-1} h_k e^{-k\lambda}.
\end{equation}

Combining \eqref{eqn:sample_manifold_convolution} and \eqref{eqn:manifold_convolution_discrete}, we can bring the discretization over the spatial and time domains together to rewrite the convolution operation on the discretized manifold and in the discrete-time domain, explicitly,
\begin{equation}
\label{eqn:discrete_manifold_convolution_discrete}
  \bbz_n=  \bbh(\bbL_n) \bbx_n = \sum_{k=0}^{K-1} h_k e^{-k\bbL_n}\bbx_n.
\end{equation}
Equation \eqref{eqn:discrete_manifold_convolution_discrete} recovers the definition of the graph convolution \cite{gama2020graphs} with graph shift operator $e^{-\bbL_n}$. This means that in practice we implement MNNs as graph neural networks (GNNs). Therefore, the stability behavior of the GNN can be seen as a proxy for the stability behavior of the MNN. We will leverage this idea in the numerical experiments of Section \ref{sec:simu}.
