\vspace{-0.4cm}
\section{Related Work}
\label{sec:relatedwork}
\vspace{-0.4cm}
Our contribution shares connections with different families of approaches. First of all, it focuses on  the problem of online adaptation in Reinforcement Learning which has been studied under different terminologies: \textit{Multi-task Reinforcement Learning}~\citep{multitask_RL, distral}, \textit{Transfer Learning}~\citep{transferrl, lazaric2012transfer} and \textit{Meta-Reinforcement Learning}~\citep{MAML,Hausman2018LatentSpace,task_inference}. Many different methods have been proposed, but the best majority considers that the agent is trained over multiple environments such that it can identify variations (or invariant) at train time. For instance, \cite{Duan2016RL2} assume that the agent can sample multiple episodes over the same environments and methods like \citep{DBLP:journals/corr/abs-2005-02934, DBLP:conf/icml/LiuRLF21} consider that the agent has access to a task identifier at train time. \\
\indent More recently, diversity-based approaches have been adapted to focus on the setting where only one training environment is available. They share with our model the idea of learning multiple policies instead of a single one.  For instance, DIAYN \citep{diayn} learns a discrete set of policies that can be reused and fine-tuned over new environments. It has been adapted to online adaptation in \citep{DBLP:conf/nips/KumarKLF20} where the authors propose to combine the intrinsic diversity reward together with the training task reward. This trade-off is obtained through a threshold-based method (instead of a simple weighted sum) with good results. But this method suffers from a major drawback identified in \citep{Tokyo}: it necessitates to sample complete episodes at each epoch which is painful and not adapted to all the RL learning algorithms. \cite{Tokyo} also proposed an alternative based on learning a continuous set of policies instead of a discrete one without using any intrinsic reward.\\
\indent The method we propose is highly connected to recent researches on mode connectivity with neural networks. Mode connectivity is a set of approaches and analyses that focus on the shape of the parameter space. It has been used as a tool to study generalization in the supervised learning setting \citep{DBLP:conf/nips/GaripovIPVW18}, but also as a way to propose new algorithms in different settings \citep{DBLP:conf/iclr/MirzadehFGP021}. Obviously, the work that is the most connected to our approach is the model proposed in \citep{LearningSubspaces} that provides a way to learn a subspace of models in the supervised learning setting. Our contribution adapts this approach to RL for learning policies in a completely different setting which is online adaptation. \\
\indent At last, our work is sharing similarities with robust RL which aims at discovering policies robust to variations of the training environment \citep{robustdeeprl,robustRL}. The main difference is that robust RL techniques learn policies efficient ‘in the worst case’ and are not focused on the online adaptation to test environments (the objective is usually to learn a single policy efficient on different variations while we are learning multiple policies, just selecting one at test time).


