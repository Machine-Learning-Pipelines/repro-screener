\vspace{-0.2cm}
\section{Conclusion and Perspectives}
\vspace{-0.4cm}

We investigate  the idea of learning a subspace of policies in the reinforcement learning setting, and describe how this approach can be used for online adaptation. While simple, our method allows to obtain policies that are robust to variations of the training environments. Contrarily to other techniques, LoP does not need any particular tuning or definition of additional architectures to handle diversity, which is a critical aspect in the online adaptation setting where hyper-parameters tuning is impossible or at least very difficult. Future work includes the extension of this family of approaches in the continual reinforcement learning setting, the deeper understanding of the the built subspace and the investigation of different auxiliary losses to better control the shape of such a subspace. 

\section{Reproducibility Statement}
We have made several efforts to ensure that the results provided in the paper are fully reproducible. In Appendix, we provide a full list of all hyperparameters and extra information needed to reproduce our experiments. The source code is available on SaLinA repository such that everyone can reproduce the experiments\footnote{\url{https://github.com/facebookresearch/salina/tree/main/salina_examples/rl/subspace_of_policies}}.