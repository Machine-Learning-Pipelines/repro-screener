%\vspace{-0.1in}
\section{Related Work}
%\vspace{-0.05in}
%first, general metal learning
Meta-learning \citep{schmidhuber1987evolutionary, thrun2012learning, naik1992meta} \cmt{[51,55, 41]} can be (roughly) classified into three categories: (1) metric-learning methods that learn a metric space (in the outer lever), where the tasks (in the inner level) make predictions by simply matching the training points, \eg nonparametric nearest neighbors \citep{koch2015siamese, vinyals2016matching, snell2017prototypical, oreshkin2018tadam, allen2019infinite}\cmt{[29, 57, 54, 45, 3]}, (2) black-box methods that train feed-forward or recurrent NNs to take the hyperparameters and task dataset as the input and  outright predict the optimal model parameters or parameter updating rules \citep{hochreiter2001learning, andrychowicz2016learning, li2016learning, ravi2016optimization, santoro2016meta, duan2016rl, wang2016learning, munkhdalai2017meta, mishra2017simple}\cmt{[25, 5, 33, 48,50, 12, 58, 40, 38]}, and   (3) optimization-based methods that conduct a bi-level optimization, where the inner level is to estimate the model parameters given the hyperparameters (in each task) and the outer level is to optimize the hyperparameters via a meta-loss~\citep{finn2017model, finn2018learning, bertinetto2018meta, lee2019meta,zintgraf2019fast, li2017meta, finn2018probabilistic, zhou2018deep, harrison2018meta}\cmt{[15, 13, 8, 60, 34, 17, 59, 23]}. Other approaches include \citep{rusu2018meta, triantafillou2019meta}, \etc A successful application of meta-learning is few-shot learning, for which important models include \citep{lake2011one, vinyals2016matching,snell2017prototypical,sung2018learning}, to name a few.  
An excellent survey about meta-learning for neural networks is given in~\citep{hospedales2020meta}.

\maml~\citep{finn2017model} is a popular optimization-based meta-learning method. In addition to FOMAML and Reptile, there are many variants, such as probabilistic versions~\citep{grant2018recasting,yoon2018bayesian,finn2018probabilistic}, and ones improving reinforcement learning~\citep{song2020maml,liu2019taming}. Recently, \citet{denevi2020advantage,wang2020structured,denevi2021conditional} proposed conditional meta learning to leverage side information (when available) to learn task-specific initializations. %In both theory and empirical evaluations, they have shown that conditional meta learning can improve upon the standard \maml that assumes the same initialization across the task family. 
The recent work of \citep{im2019model,xu2021meta} also introduces an ODE view for \maml. However, they use the ODE theory and methods to analyze/improve the outer level optimization, where the inner level still performs one step gradient descent as in standard \maml.  They do not consider long training trajectories in the inner level. \citet{im2019model} pointed out the \maml update is a special case of (second-order) Runge-Kutta gradients, and suggested using more refined nodes, weights and even higher-order updates. \citet{xu2021meta} showed that if the outer-level optimization of \maml is considered as solving an ODE, it enjoys a linear convergence rate for strongly convex task losses. Based on their analysis, they proposed a bi-phase algorithm to further reduce the cost and improve efficiency.  Our work uses the ODE view for inner-level optimization.  The adjoint method is a classical and popular framework to estimate the parameters of ODE or dynamic control models~\citep{chen2018neural,eichmeir2021adjoint}. If we use Euler method to solve the adjoint ODE, it reduces to the reverse mode differentiation method~\citep{bengio2000gradient,baydin2014automatic}, yet leaving first-order global accuracy ($\Ocal(h)$). Another excellent related work is ~\citep{domke2012generic} that provides a general bi-level optimization framework. It can optimize  hyper-parameters that explicitly show up in the loss (\eg regularization strength, yet not including the parameter initialization) with the inner-optimization procedure taken into account.
% accelerate the learning.
%Our work is also related to neural ODE~\citep{chen2018neural}, which uses neural networks to construct ODE models. To estimate the ODE parameters, it also uses the framework of the adjoint method. A major difference is that, our work is not to design any learning model; the model is given by the meta learning task, which can be arbitrary, as long as is trained by gradient-based optimization. Our work conducts efficient inner-optimization for  meta learning the parameter initialization, rather than train the model itself. In general, neural ODE does not need to handle Hessian matrices as well. 

\cmt{
	\maml~\citep{finn2017model} is a popular optimization-based meta-learning method to estimate model initializations. The inner optimization is to train the model parameters on the sampled tasks stating from the initialization. The outer level evaluates the trained parameters on the validation data, and back-propagate the gradient from the model parameters to optimize the initialization. Due to challenges in the complexity of building the computation graphs, \maml usually performs one-step or a few steps of gradient descent in the inner optimization.  To bypass this issue, the authors also proposed first-order \maml, which ignores the Jacobians in the back-propagation, and simply uses the gradient w.r.t. the trained parameters to update the initialization. \citet{nichol2018first} proposed \rap, which subtracts the gradient w.r.t. the trained parameters by the initialization to adjust the updating direction. The recent work of \citet{rajeswaran2019meta}  
	developed implicit \maml (\imaml). By adding a proximal regularizer in the training loss, \citet{rajeswaran2019meta} show that the gradient w.r.t. the initialization has an analytical form when the training reaches the (local) optimum of the regularized loss. For efficient computation, \imaml uses the conjugate gradient (CG) algorithm to avoid a naive matrix inversion. There are also probabilistic versions of \maml.  \citet{grant2018recasting} reinterpreted \maml  as a hierarchical Bayesian model, and used Laplace's method~\citep{laplace1986memoir,mackay1992evidence,mackay1992practical} to conduct  posterior inference. \citep{yoon2018bayesian} developed  Bayesian \maml (\bmaml) that uses a set of particles to approximate the posterior of the initialization. They proposed a chaser loss, and used Stein variational gradient descent (SVGD)~\citep{liu2016stein} to update the particles.  To alleviate the task ambiguity, \citet{finn2018probabilistic} proposed probabilistic \maml (\pmaml), which constructs a task-specific prior of the initialization by performing one-step gradient descent (GD) on the task training dataset. Several works were proposed to improve \maml in reinforcement learning, such as ES-MAML~\citep{song2020maml} and T-MAML~\citep{liu2019taming}. Recently, \citet{denevi2020advantage,wang2020structured,denevi2021conditional} proposed conditional meta learning to leverage side information (when available) to learn task-specific initializations. In both theory and empirical evaluations, they have shown that conditional meta learning can improve upon the standard \maml that assumes the same initialization across the task family. %A similar work includes~\citep{zintgraf2019fast}. 
}