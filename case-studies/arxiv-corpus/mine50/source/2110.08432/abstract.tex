\begin{abstract}
	 Model Agnostic Meta Learning (MAML) is widely used to find a good initialization for a family of tasks. Despite its success, a critical challenge in MAML is to calculate the gradient w.r.t. the initialization of a long training trajectory for the sampled tasks, because the computation graph can rapidly explode and the computational cost is very expensive. 
	 To address this problem, we propose Adjoint MAML (A-MAML). We view gradient descent in the inner optimization as the evolution of an Ordinary Differential Equation (ODE). To efficiently compute the gradient of the validation loss w.r.t. the initialization, we use the adjoint method to construct a companion, backward ODE. To obtain the gradient w.r.t. the initialization, we only need to run the standard ODE solver twice --- one is forward in time that evolves a long trajectory of gradient flow for the sampled task; the other is backward and solves the adjoint ODE. We need not create or expand any intermediate computational graphs, adopt aggressive approximations, or impose proximal regularizers in the training loss.  Our approach is cheap, accurate, and adaptable to different trajectory lengths. We demonstrate  the advantage of our approach in both synthetic and real-world meta-learning tasks. 
\end{abstract}

%Model Agnostic Meta Learning (MAML) is widely used to find a good initialization for a family of tasks. Despite its success, a critical challenge in MAML is to calculate the gradient w.r.t the initialization of a long training trajectory for the sampled tasks, because the computation graph can rapidly explode and subsequent optimization suffers from vanishing gradients.
%To address this problem, we propose Adjoint MAML (A-MAML). We view gradient descent in the inner optimization as the evolution of an Ordinary Differential Equation (ODE). To efficiently compute the gradient of the validation loss w.r.t the initialization, we use the adjoint method to construct a companion, backward ODE. To obtain the gradient w.r.t the initialization, we only need to run the standard ODE solver twice --- one is forward in time that evolves a long trajectory of gradient flow for the sampled task; the other is backward and solves the adjoint ODE. We need not create or expand any intermediate computational graphs, adopt aggressive approximations, or impose proximal regularizers in the training loss.  Our approach is cheap, accurate, and adaptable to different trajectory lengths. We demonstrate  the advantage of our approach in both synthetic and real-world meta-learning tasks. 


%Model Agnostic Meta Learning (MAML) is widely used to find a good initialization for a family of tasks. Despite its success, a critical challenge in MAML is to backpropagate the gradient w.r.t the initialization from a long trajectory of the gradient descent for the sampled tasks, because the computation graph will rapidly explode. To allow easy computation,  MAML in practice usually performs only one or a few steps of gradient descent in the inner optimization, which can be too close to the initialization, and fail to reflect the actual learning performance of using the specific initialization. To overcome this problem, we propose Adjoint MAML (A-MAML). We view the gradient descent in the inner optimization as the evolution of an ODE. To efficiently compute the gradient of the task-specific loss w.r.t the initialization (i.e., the initial state of the ODE), we use the adjoint method to construct a companion ODE, which runs backward. To calculate the gradient w.r.t the initialization, we only need to run the standard ODE solver twice --- one is forward and evolves a long trajectory of gradient descent for the sampled task; the other is backward and solves the adjoint ODE. We do not need to create and store any intermediate computational graphs. It is fast, accurate, and scales to large models.
%To allow easy computation, MAML and its variants, either performs only one a few steps of gradient descent in the inner-optimization, or ignore high-order terms, or impose an explicit regularization
%In practice,  MAML usually performs only one or a few steps of gradient descent in the inner optimization, which can be too close to the initialization, and fail to reflect the actual learning performance of using that specific initialization. 
