%Meta learning, and then MAML --> challenge and MAML, First-order and raptile issues , iMAML and issue --> our solution --> result 
\section{Introduction}
Meta-learning paradigms~\citep{schmidhuber1987evolutionary, thrun2012learning} intend to  develop methods that can quickly adapt a learning model to new tasks or environments, like human learning. A prominent example is the recent model-agnostic meta-learning (\maml) algorithm~\citep{finn2017model}, which is particularly successful in learning the model initialization for a family of tasks. \maml is a bi-level optimization approach. The inner level starts from the initialization, and optimizes the training loss of the sampled tasks via gradient descent. At the trained model parameters, the outer-level uses back-propagation to calculate the gradient of the validation loss w.r.t. the initialization, and optimizes the initialization accordingly. 

While successful, a critical challenge of \maml is to back-propagate the gradient from a long training trajectory of the sampled tasks, because the resulting computation graph grows quickly, can easily explode, and is computationally expensive.\cmt{the resulting optimization also suffers from the vanishing gradient problem.} To combat these issues, practical usage of \maml performs only one or a few steps of gradient descent in the inner optimization; unfortunately this propagates a trajectory only close to the initialization, and fails to reflect the longer-term learning performance of using that initialization. To bypass this issue, first-order \maml (\fomaml)~\citep{finn2017model} and \rap ~\citep{nichol2018first} employ dropout on the Jacobian to obtain an aggressive approximation. While this is efficient, the approach loses accurate gradient information. The recent \imaml approach~\citep{rajeswaran2019meta} uses an implicit method to calculate an accurate gradient w.r.t. the initialization. This approach is elegant and successful, but imposes several restrictions. First, an additional regularizer that encourages proximity of the model parameters and the initialization must be added into the training loss. Second, the gradient is accurate only when training reaches the optimum of the regularized loss. 
\cmt{
\setlength{\columnsep}{5pt}
\begin{wrapfigure}{r}{0.5\textwidth}
	\centering
	\includegraphics[width=0.5\textwidth]{./figs/amaml-idea-trim.pdf}
	%\vspace{-0.15in}
	\caption{\small  Illustration of \ours, where $\btheta$ is the initialization, $J_n$ is the validation loss for task $n$ ($n=1, 2, \ldots$),  $\u_n$ are the model parameters for task $n$, and also the state of the corresponding forward ODE. \ours solves the forward ODE to optimize the meta-training loss, and solves the adjoint ODE backward to obtain the gradient of the meta-validation loss w.r.t. $\btheta$. } 	
	\label{fig:amaml}
	\vspace{-0.15in}
\end{wrapfigure}
}
\begin{figure}
	\centering
	\includegraphics[width=0.7\textwidth]{./figs/amaml-idea-trim.pdf}
%\vspace{-0.15in}
	\caption{\small  Illustration of \ours, where $\btheta$ is the initialization, $J_n$ is the validation loss for task $n$ ($n=1, 2, \ldots$),  $\u_n$ are the model parameters for task $n$, and also the state of the corresponding forward ODE. \ours solves the forward ODE to optimize the meta-training loss, and then solves the adjoint ODE backward to obtain the gradient of the meta-validation loss w.r.t. $\btheta$. } 	
	\label{fig:amaml}
\end{figure}

In this paper, we propose \ours, an efficient and accurate approach to differentiate long paths of the inner-optimization in meta-learning. See Fig. \ref{fig:amaml} for an illustration. Our method does not require additional regularizers and can adapt to different trajectory lengths, hence it is well suited to commonly used training strategies, such as early stopping.  Specifically, we view the inner optimization (training) as evolving a forward ordinary differential equation (ODE) system, where the states are the model parameters. The standard gradient descent is equivalent to solving this ODE with the forward Euler method.  To calculate the gradient of the validation loss w.r.t. the model initialization, \ie the initial state of the ODE, we use the adjoint method to construct a companion  ODE. In effect, we only need to run the standard ODE solver twice: First, we solve the forward ODE to evolve a long training trajectory, based on which we compute the initial state of the adjoint ODE. Next, we solve the adjoint ODE backward to obtain the gradient w.r.t. the model initialization. To avoid divergence when solving backward, we use high-order solvers in the forward pass and track the states in the trajectory, based on which we use the modified Euler method (second-order) to solve backward. Throughout the procedure, we do not create and grow any intermediate computation graphs, nor do we apply any gradient approximation. The memory cost is linear in the number of model parameters. The accuracy is determined by the numerical precision of the ODE solver, which we can explicitly trade for speed.

For evaluation, we first examined \ours in two synthetic benchmark tests, regressing Alpine and Cosine mixture functions. In both task populations, we examined, starting from the given initialization, how the prediction error of the target model varies along with the increase of training epochs. \ours leads to much better prediction accuracy and training behavior compared against \maml, \fomaml, \rap, and \imaml.  Meanwhile, \ours dramatically reduces the memory usage and can easily scale to long training trajectories, compared with \maml which utilizes computation graphs.   The running time of \ours is comparable to \fomaml, \rap, and \imaml. We then applied \ours in three real-world applications of collaborative filtering and two image-classification tasks.  In several few-shot learning settings, \ours nearly always provides the best initialization, which leads to smaller prediction errors than the competing approaches during the meta-tests. The improvement is often significant. 
%physics informed neutral networks (PINN)~\citep{raissi2019physics} . The learning of PINNs requires very long training trajectories. In two typical families of tasks, with the initializations provided by \ours, the PINN takes much less iterations to achieve a small error ($^\sim[10^{-4},10^{-3}]$), as compared with  using the initializations given by MAML, \imaml and other competing methods.  

