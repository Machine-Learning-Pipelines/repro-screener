<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exact Recovery of Community Structures Using DeepWalk and Node2vec</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-24">24 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yichi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution" key="instit1">Yichi Zhang and Minh Tang</orgName>
								<orgName type="institution" key="instit2">North Carolina State University</orgName>
								<address>
									<postCode>27606</postCode>
									<settlement>Raleigh</settlement>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Minh</forename><surname>Tang</surname></persName>
							<email>mtang8@ncsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution" key="instit1">Yichi Zhang and Minh Tang</orgName>
								<orgName type="institution" key="instit2">North Carolina State University</orgName>
								<address>
									<postCode>27606</postCode>
									<settlement>Raleigh</settlement>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exact Recovery of Community Structures Using DeepWalk and Node2vec</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-24">24 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">188C69E4E21BE128A34FF69407A792AE</idno>
					<idno type="arXiv">arXiv:2101.07354v2[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-10-27T19:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Stochastic blockmodel</term>
					<term>network embedding</term>
					<term>perfect community recovery</term>
					<term>node2vec</term>
					<term>DeepWalk</term>
					<term>matrix factorization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Random-walk based network embedding algorithms like DeepWalk and node2vec are widely used to obtain Euclidean representation of the nodes in a network prior to performing downstream inference tasks. However, despite their impressive empirical performance, there is a lack of theoretical results explaining their large-sample behavior. In this paper, we study node2vec and DeepWalk through the perspective of matrix factorization. In particular we analyze these algorithms in the setting of community detection for stochastic blockmodel graphs (and their degree-corrected variants). By exploiting the row-wise uniform perturbation bound for leading singular vectors, we derive high-probability error bounds between the matrix factorization-based node2vec/DeepWalk embeddings and their true counterparts, uniformly over all node embeddings. Based on strong concentration results, we further show the perfect membership recovery by node2vec/DeepWalk, followed by K-means/medians algorithms. Specifically, as the network becomes sparser, our results guarantee that with large enough window size and vertices number, applying K-means/medians on the matrix factorization-based node2vec embeddings can, with high probability, correctly recover the memberships of all vertices in a network generated from the stochastic blockmodel (or its degree-corrected variants). The theoretical justifications are mirrored in the numerical experiments and real data applications, for both the original node2vec and its matrix factorization variant.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>G IVEN a network G, a popular approach for analyzing G is to first map or embed its vertices into some low dimensional Euclidean space and then apply machine learning and statistical inference procedures in this space. Through this embedding process, multiple tasks could be conducted on the network such as community detection (e.g., <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>), link prediction (e.g., <ref type="bibr" target="#b2">[3]</ref>), node classification (e.g., <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>) and network visualization (e.g., <ref type="bibr" target="#b5">[6]</ref>). There has been a large and diverse collection of network embedding algorithms proposed in the literature, including those based on spectral embedding <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, multivariate statistical dimension reduction <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, and neural network <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. See <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b17">[18]</ref> for recent surveys of network embedding and graph representation learning.</p><p>In recent years there has been significant interest in network embeddings based on random-walks. The most well-known examples include DeepWalk <ref type="bibr" target="#b3">[4]</ref> and node2vec <ref type="bibr" target="#b18">[19]</ref>. These algorithms are computationally efficient and furthermore yield impressive empirical performance in many different scientific applications including recommendation systems <ref type="bibr" target="#b19">[20]</ref>, biomedical natural language processing <ref type="bibr" target="#b20">[21]</ref>, human protein identification <ref type="bibr" target="#b21">[22]</ref>, traffic prediction <ref type="bibr" target="#b22">[23]</ref> and city road layout modeling <ref type="bibr" target="#b23">[24]</ref>. Nevertheless, despite their wide-spread use, there is still a lack of theoretical results on their large-sample properties. In particular it is unclear what the node embeddings represents as well as their behavior as the number of nodes increases.</p><p>Theoretical properties for DeepWalk, node2vec, and related algorithms had been studied previously in the computer science community. The focus here had been mostly on the convergence of the entries of the co-occurrence matrix as the lengths and/or number of random walks go to infinity. For example, motivated by the analysis in <ref type="bibr" target="#b24">[25]</ref> for word2vec, the authors of <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> showed that DeepWalk and node2vec using the skip-gram model with negative sampling is equivalent to factorizing a matrix whose entries are obtained by taking the entry-wise logarithm of a cooccurrence matrix, provided that the embedding dimension d is sufficiently large (possibly exceeding the number of nodes n). These authors also derived the limiting form of the entries of this matrix as the length of the random walks goes to infinity. These results were further extended in <ref type="bibr" target="#b27">[28]</ref> to yield finite-sample concentration bounds for the co-occurrence entries. Note, however, that the above cited works focused exclusively on the case of a fixed graph and thus do not provide results on the large sample behavior of these algorithms as n increases.</p><p>The statistical community, in contrast, had extensively studied the large-sample properties of graph embeddings based on matrix factorization. However the embedding algorithms considered are almost entirely based on singular value decomposition (SVD) of either the adjacency matrix or the Laplacian matrix and its normalized and/or regularized variants. For example, in the setting of the popular stochastic blockmodel random graphs, <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b7">[8]</ref> derived consistency results for a truncated SVD of the normalized Laplacian matrix and the adjacency matrix. Subsequently <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> strengthened these results by providing central limit theorems for the components of the eigenvectors of either the adjacency matrix or the normalized Laplacian matrix under the more general random dot product graphs model. As DeepWalk and node2vec are based on taking the entry-wise logarithm of a random-walk co-occurrence matrix, the techniques used in these cited results do not readily translate to this setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions of the current paper</head><p>The current paper studies large-sample properties of random-walk based embedding algorithms. We first present convergence results for the embeddings of DeepWalk and node2vec in the case of stochastic blockmodel graphs and their degree-correctd variant. We then show that running K-means or K-medians on the resulting embeddings is sufficient for exact recovery of the latent community assignments. Our theoretical results thus provide a bridge between previous results in the computer science community and their statistics counterpart.</p><p>We emphasize that our focus on stochastic blockmodel graphs is done purely for ease of exposition. Indeed, most of our results continue to hold for the more general inhomogeneous Erdős-Rényi (IER) random graphs model <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, provided that the edge probabilities are sufficiently homogeneous, i.e., the minimum and maximum values for the edge probabilities are of the same order (possibly converging to 0) as n increases; recall that IER is one of the most general model for edge independent random graphs. In particular we can show that the co-occurrence matrices constructed from the sampled networks is uniformly close (entrywise) to that for the true but unknown edge probabilities matrices. However, as IER random graphs need not possess lowdimensional structure (even when n increases), it is not clear what the embeddings obtained from these co-occurrence matrices represent. See Section 6 for further discussion.</p><p>We now outline our approach. The original node2vec and DeepWalk algorithms are based on optimizing a nonconvex skip-gram model using stochastic gradient descent (SGD); this optimization problem has multiple local minimima and the obtained embeddings can thus be numerically unstable (see e.g., <ref type="bibr" target="#b32">[33]</ref>). We instead consider, for each embedding dimension d, the optimal low-rank approximation of an observed transformed co-occurrence matrix similar to that used in <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b27">[28]</ref>, and recently <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b33">[34]</ref>. We first show that the entries of the co-occurrence matrix computed using the observed adjacency matrix is uniformly close to the entries of the co-occurrence matrix computed using the true but unknown edge probabilities matrix. This uniform bound implies that the entry-wise logarithm of the two cooccurrence matrices are also uniformly close and thus, with high probability, the co-occurrence matrix constructed using the observed graph is well-defined. In the case of stochastic blockmodel graphs the true edge probabilities matrix give rise to a (transformed) co-occurrence matrix with rank at most K where K is the number of blocks and thus for stochastic blockmodel graphs with K n blocks. By leveraging both classical (e.g., the celebrated Davis-Kahan theorem <ref type="bibr" target="#b34">[35]</ref>) as well as recent results on matrix perturbations in the 2 → ∞ norm (e.g., <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>), we show that the truncated low-rank representation of both matrices are uniformly close, i.e., the embeddings of the observed graph is, up to orthogonal transformation, approximately the same as that for the true edge probabilities matrix. Therefore, by running K-means or K-medians on the embeddings of the observed graph, we can with high probability recover the latent community structures for every vertices.</p><p>Our paper is organized as follows. In Section 2, we give a brief introduction of node2vec <ref type="bibr" target="#b18">[19]</ref> and DeepWalk <ref type="bibr" target="#b3">[4]</ref>, and describe the matrix factorization perspective for these algorithms. In particular, DeepWalk can be treated as a special case of node2vec by setting the 2 nd -order randomwalk parameters (p, q) to be (1, 1), which will be assumed in Section 3 for simplicity of theoretical analysis. In Section 3 we provide uniform entry-wise error bounds for the entries of the t-step random-walk transition matrix and their implications for community recovery. The theoretical results in Section 3 hold for both the dense and sparse regimes where the average degree grows linearly and sublinearly in the number of nodes, respectively. In Section 4 we present simulations to corroborate our theoretical results. In Section 5, we apply node2vec to three real-world network datasets and show its remarkable practical performances.We conclude the paper in Section 6 with a discussion of some open questions and potential improvements. All proofs of the stated results, associated technical lemmas, and additional numerical results are provided in the Supplementary File.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Notation</head><p>We first introduce some general notations that are used throughout this paper. For a given positive integer K, we denote by [K] the set {1, 2, . . . , K}. We denote a graph on n vertices by G = (V, E) where V = {v i } n i=1 and E = {e ii } n i,i =1 are the vertices and edge sets, respectively. Unless specified otherwise, all graphs in this paper are assumed to be undirected, unweighted and loop-free. For each node v i we denote by N (v i ) the set of nodes v i adjacent to v i . If G is a graph on n vertices then its n × n adjacency matrix is denoted as A = [a ii ]. In the subsequent discussion we often assume that the upper triangular entries of A are independent Bernoulli random variables with E[a ii ] = p ii when i &lt; i. As A is symmetric we also set a ii = a i i for i &gt; i and denote by P = [p ii ] the corresponding n × n matrix of edge probabilities.</p><p>Given a graph G with adjacency matrix A, let D A = diag(d 1 , . . . , d n ) be a diagonal matrix with d i = n i =1 a ii as its ith diagonal element. Assuming G is connected, we define a random walk on G with a 1-step transition matrix Ŵ = AD −1 A . Correspondingly, when appropriate, we also define W = PD −1 P where D P = diag(p 1 , . . . , p n ) is the diagonal matrix with p i = n i =1 p ii . We use • , • F , • ∞ and • max to denote the spectral norm, Frobenius norm, maximum absolute row sum, and maximum entry-wise value of a matrix, respectively. We also use • max,off and • max,diag to denote the maximum value for the off-diagonal and diagonal entries of a matrix, i.e., for a square matrix M = [m ii ],</p><p>M max,off = max For two terms a and b, let a ∧ b := min{a, b}. We write a b and a b if there exists a constant c not depending on a and b such that a ≤ cb and a ≥ cb, respectively. If a b and a b then a b. We say an event A depending on n happens with high probability (whp) if P(A) ≥ 1 − O(n −c ) for some constant c &gt; 3. Finally, for random sequences A n , B n , we write A n = O P (B n ) if A n /B n is bounded whp and A n = o P (B n ) if A n /B n → 0 whp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SUMMARY OF NODE2VEC AND SBM</head><p>In this section we first provide a brief overview of the node2vec algorithm. We then discuss the popular stochastic blockmodel (SBM) for random graphs. Finally we discuss a matrix factorization perspective to node2vec and show that, for a graph G generated from a stochastic blockmodel, this matrix factorization approach leads to a low-rank approximation of an elementwise non-linear transformation of the random walk transition matrix for G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Node2vec with negative sampling</head><p>First introduced in <ref type="bibr" target="#b18">[19]</ref>, node2vec is a computationally efficient and widely-used algorithm for network embedding. Motivated by the ideas behind word2vec for text documents <ref type="bibr" target="#b37">[38]</ref>, node2vec generates sequences of nodes using random walks which are then feed into a skip-gram model <ref type="bibr" target="#b38">[39]</ref> to yield the node embeddings. The original skip-gram model is quite computationally demanding for large networks and hence, in practice, usually replaced by a skip-gram with negative sampling (SGNS). The resulting algorithm is summarized below. 1) (Sampling Random Paths): First generates r independent 2 nd order random walks on G with each having a fixed length L. A 2 nd order random walk of length L starting at v i with parameters p and q is generated as follows. First let v</p><formula xml:id="formula_0">(i) 1 = v i . Next sample v (i) 2 from N (v (i)</formula><p>1 ) uniformly at random. Then for</p><formula xml:id="formula_1">3 ≤ ≤ L, sample v (i) ∈ N (v<label>(i)</label></formula><p>−1 ) with probability,</p><formula xml:id="formula_2">P(v (i) = v 0 ) =        1 p J(v 0 ) if v 0 = v (i) −2 , J(v 0 ) if v 0 ∈ N (v (i) −2 ), 1 q J(v 0 ) if v 0 ∈ N (v (i) −2 ),</formula><p>where J(v 0 ) is given by</p><formula xml:id="formula_3">1 J(v 0 ) = p −1 + |N (v (i) −2 ) ∩ N (v (i) −1 )| + q −1 |N (v (i) −2 ) c ∩ N (v (i) −1 )| (2.1)</formula><p>The form of J(v 0 ) allows for v (i) to have possibly unbalanced probabilities of reaching three different types of nodes in the neighborhood of v  </p><formula xml:id="formula_4">(i) −1 but not the neighborhood of v (i) −2 .</formula><p>The parameters p &gt; 0 and q &gt; 0 provide weights for these three different type of nodes and hence control the speed at which the random walk leaves the neighborhood of the original node v i . In this paper we assume that the starting vertex v i of any random walk is sampled according to a stationary distribution S = (S 1 , . . . , S n ) on G with</p><formula xml:id="formula_5">P Starting Vertex is v i = S i = d i 2|E| (2.2)</formula><p>for all v i ∈ V. For a given i ∈ [n] we denote by r i the number of random walks starting from v i , (i) j</p><p>as the jth random walk starting from v i and</p><formula xml:id="formula_6">L i = { (i) j , j ∈ [r i ]</formula><p>} as the set of all random walks starting from v i . Remark 1. We consider only the case of p = q = 1 for our theoretical analysis. The choice p = q = 1 is the default setting for node2vec as suggested in the original paper <ref type="bibr" target="#b18">[19]</ref> and leads to a sampling scheme equivalent to that of DeepWalk <ref type="bibr" target="#b3">[4]</ref>; the subsequent analysis thus also applies to DeepWalk.</p><p>2) (Calculating C) : Borrowing ideas from word2vec <ref type="bibr" target="#b37">[38]</ref>, node2vec creates a n × n node-context matrix C = [C ii ] n×n whose ii th entry records the number of times the pair (v i , v i ) appears among all random paths in n i=1 L i . More specifically, for a given window size (t L , t U ), C ii is the number of times that</p><formula xml:id="formula_7">(v i , v i ) appears within a sequence . . . , v i , . . . . . . t−1vertices , v i , . . . or . . . , v i , . . . . . . t−1vertices , v i , . . .<label>(2.3)</label></formula><p>among all random paths in n i=1 L i ; here t is any integer satisfying t L ≤ t ≤ t U ≤ L − 1 Remark 2. The original node2vec algorithm fixed t L = 1 while in this paper we allow for varying t L for a more flexible theoretical analysis. In Section 3 we show that different values for (t L , t U ) could lead to different convergence rates for the embedding and furthermore appropriate values for (t L , t U ) depend intrinsically on the sparsity of the network.</p><p>3) (Skip-gram model with negative sampling) : Given the n × n matrix C and an embedding dimension d, node2vec uses the SGNS model to learn the node embedding matrix F ∈ R n×d and the context embedding matrix F ∈ R n×d . The ith row of F is the d-dimensional embedding vector of node v i .</p><p>In slight contrasts to the original node2vec, in this paper we do not require the constraint F = F . The objective function of SGNS model for a given C is defined as</p><formula xml:id="formula_8">g(F, F ) = ij C ij log σ(f i f j ) + κE f N ∼Pns log σ(−f i f N ) .</formula><p>(2.4)</p><p>Here f i (resp. f j ) are the i (resp. j) row of F (resp. F ), κ is the ratio of negative to positive samples,</p><formula xml:id="formula_9">P ns (f N ) = n i =1 C N i i,i C ii</formula><p>is the empirical unigram distribution for the negative samples, and σ is the logistic function. The original node2vec algorithm solves for ( F, F ) by minizing Eq. (2.4) over (F, F ) using SGD. In this paper we use a matrix factorization approach, described in section 2.3, to find ( F, F ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Stochastic blockmodel</head><p>The stochastic blockmodel (SBM) of <ref type="bibr" target="#b39">[40]</ref> is one of the most popular generative model for network data. It often serves as a benchmark for evaluating community detection algorithms <ref type="bibr" target="#b40">[41]</ref>. Our theoretical analysis of node2vec/DeepWalk is situated in the context of this model. We parametrize a K-blocks SBM in terms of two parameters (B, Z) where B = [b uu ] is a symmetric matrix of blocks connectivity and Z ∈ {0, 1} n×K is a matrix whose rows denote the block assignments for the nodes; we use τ (i) ∈ [K] to represent the community assignment for node i, i.e., the ith row of Z contains a single 1 in the τ k(i)th element and 0 everywhere else. Given B and Z, the edges a ii of G are independent Bernoulli random variables with P[a ii = 1] = B τ (i),τ (i ) , i.e., the probability of connection between i and i depends only on the communities assignment of i and i . Denote by</p><formula xml:id="formula_10">P = [p ii ] = ZBZ (2.5)</formula><p>the matrix of edge probabilities. We denote a graph with adjacency matrix A sampled from a stochastic blockmodel as A ∼ SBM(B, Z), and, for any stochastic blockmodel graph, we denote by n k the number of vertices assigned to block k. We shall also assume, without loss of generality, that Z is ordered by blocks:</p><formula xml:id="formula_11">Z :=      1 n1 0 . . . 0 0 1 n2 . . . 0 . . . . . . . . . . . . 0 0 . . . 1 n K      . (2.6)</formula><p>In real-world applications the average degree of a networks usually grows at a slower rate than Θ(n). To model this phenomenon we introduce a sparse parameter ρ n that can vanish as n → ∞. For ease of exposition we use the following parametrization of B that is commonly used in the literature (see e.g., <ref type="bibr" target="#b41">[42]</ref>).</p><p>Assumption 1. There exists a fixed</p><formula xml:id="formula_12">K × K matrix B 0 such that B = ρ n B 0 with ρ n n −β for some β ∈ [0, 1).</formula><p>The parameter ρ n scales the edge probabilities in B. As ρ n n −β , the average degree of the nodes in G grows at rate n 1−β so that larger values of β lead to sparser network. It is well known that, for sufficiently large n, if G satisfies Assumption 1 then G is connected with high probability (see e.g. Section 7.1 of <ref type="bibr" target="#b30">[31]</ref>). Then P = ZBZ has a K × K block structure and thus has rank at most K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Node2vec and matrix factorization</head><p>In general, for a fixed given embedding dimension d &lt; n, minimization of the objective function in Eq. (2.4) leads to a non-convex optimization problem and the potential convergence of SGD into local minima makes the asymptotic analysis of F quite complicated. Indeed, almost all existing results for non-convex optimization using gradient descent or SGD only guarantees convergence to a local minima provided that the initial estimate is sufficiently close to this local minima, see e.g., <ref type="bibr" target="#b42">[43,</ref><ref type="bibr">Section 5]</ref> and <ref type="bibr" target="#b43">[44]</ref>. We thus desire a different approach for finding F, namely one for which the form of F is more readily apparent. One such approach is the use of matrix factorization. For example, in the context of word2vec embedding, <ref type="bibr" target="#b24">[25]</ref> showed that minimization of Eq. (2.4) when C is a word-context matrix is equivalent to a matrix factorization problem on some elementwise nonlinear transformation of C and that this transformation can be related to the notion of pointwise mutual information between the words. Motivated by this line of inquiry, we consider a formulation of node2vec wherein F F is a lowrank approximation of some elementwise transformation M of Ŵ; recall that Ŵ is the 1-step transition matrix for the canonical random walk on G. We emphasize that this approach had been considered previously in <ref type="bibr" target="#b25">[26]</ref> and recently by <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b33">[34]</ref>. The main contribution of our paper is in showing that this matrix factorization leads to consistent community recovery for stochastic blockmodel graphs.</p><p>We now describe the matrix M. In the context of the word2vec algorithm, <ref type="bibr" target="#b24">[25]</ref> showed that there exists some embedding dimension d such that the minimizer of Eq. (2.4) over F ∈ R n×d and F ∈ R n×d satisfies</p><formula xml:id="formula_13">F F = M(C, κ) := log C ij ( ij C ij ) κ i C ij j C ij n×n<label>(2.7)</label></formula><p>Using the same idea for our analysis of node2vec, we first fixed n and show that if the number of sampled random paths increases then M(C, k) converges, elementwise, to a limiting matrix M0 defined below. Note that the entries of M0 can be interpreted as point-wise mutual information (PMI) between the nodes. Theorem 1. Let n be fixed but arbitrary. Suppose G is a connected graph on n vertices and t U is large enough such that the entries of</p><formula xml:id="formula_14">t U t=t L</formula><p>Ŵt are all positive. Applying the node2vec sampling strategy introduced in Section 2.1 on G we have</p><formula xml:id="formula_15">M(C, κ) a.s. −→ M0 (G, t L , t U , κ, L) := log 2|A| κγ t U t=t L (L − t)D −1 A Ŵt (2.8)</formula><p>as the number of random paths r = n i=1 r i → ∞; recalling that Ŵ = AD −1 A . The convergence of M(C, κ) to M0 is elementwise and uniform over all entries of M(C, κ). Here |A| denote the sum of the entries in A and the constant γ is defined as</p><formula xml:id="formula_16">γ := 1 2 (L − t L − t U )(t U − t L + 1).</formula><p>To reduce notation clutter, we will henceforth drop the dependency of M0 on the parameters G, t L , t U , κ, L. As the value of r is chosen purely for computational expediency, i.e., smaller values of r require sampling fewer random walks, we will thus take the conceptual view that r → ∞ so that M(C, κ) → M0 ; note that M0 can be constructed explicitly from A without needing to sample any random walk. Combining Eq. (2.7) and Theorem 1, we have that, for any fixed n, there exists an embedding dimension d such that for r → ∞, the matrices F and F are exact factors for factorizing M0 . Note that D −1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A</head><p>Ŵt is symmetric for any t ≥ 1 and hence M0 is symmetric.</p><p>In practice one usually chooses d n to reduce the noise in the embeddings as well as combat the curse of dimensionality in downstream inference. Obviously if d &lt; n then exact factors ( F, F ) for factorizing M0 might no longer exist (see e.g., <ref type="bibr" target="#b24">[25]</ref>). The requirement that F F = M0 is, however, both misleading and unnecessary. Indeed, as the observed graph is but a single noisy sample generated from some true but unobserved edge probabilities matrix P, what we really want to recover is the factorization induced by P. More specifically, replacing Ŵt and |A| with W t and |P| in M0 , we define</p><formula xml:id="formula_17">M 0 = log 2|P| κγ t U t=t L (L − t)D −1 P W t (2.9)</formula><p>as the underlying-truth counterpart of M0 ; note that, similar to M0 , we had dropped the parameters associated with M 0 for simplicity of notations. Under the SBM setting, the true signal matrices P and M 0 are both low-rank and hence an embedding dimension of d = rk(M 0 ) n is sufficient to recover the factorization induced by M 0 .</p><p>To be more precise, recall from Eq. (2.6) that for stochastic blockmodel graphs, the matrix P has a K × K block structure. Thus both W t and D −1 P W t also have K × K block structures. Eq. (2.9) then implies that M 0 also has a K × K block structure and hence rank(M 0 ) ≤ K. Most importantly, the K × K block structure of M 0 is also sufficient for recovering the community structure in G. We will show in Section 3 that the relative error, in the rowwise maximum norm, between M0 and M 0 converges to 0 as n → ∞. This convergence, together with results for perturbation of eigenspaces, implies the existence of an embedding dimension d ≤ K for which the n × d matrices F and F obtained by factorizing M0 lead to exact recovery of the community structure in G.</p><p>Remark 3. If P does not arise from a stochastic blockmodel graph then M 0 need not have a low-rank structure. Nevertheless we can still consider a rank-d approximation to M 0 for some d &lt; rk(M 0 ). Furthermore, as we will clarify in Section 6, the bound for M0 − M 0 max in Section 3 also holds for general edge independent random graphs, provided that the entries of P is reasonably homogeneous. Hence M0 has an approximate low-rank structure if and only if M 0 also has an approximate low-rank structure.</p><p>In summary, motivated by the low-rank structure of M 0 in the case of SBM graphs, we view the matrix factorization approach for node2vec as finding the best rank d &lt; n approximation F • F to M0 under Frobenius norm, i.e., ( F , F ) = arg min</p><formula xml:id="formula_18">(F ,F )∈R n×d •R n×d M0 − F • F F . (2.10)</formula><p>The minimizer of Eq. (2.10) is obtained by truncating the SVD of M0 . More specifically, let</p><formula xml:id="formula_19">M0 = Û Σ V (2.11)</formula><p>with a decreasing order of singular values in Σ. Then for a given d ≤ rk(M 0 ), let</p><formula xml:id="formula_20">F = Ûd , F = Vd Σd (2.12)</formula><p>where Ûd ∈ R n×d , Vd ∈ R n×d are the first d columns of Û and V, respectively, and Σd ∈ R d×d is the diagonal matrix containing the d largest singular values in Σ.</p><p>Remark 4. The appropriate embedding dimension d for factorizing M0 depends on knowing rank(M 0 ). but the convergence of M0 to that of M 0 does not require knowing rank(M 0 ). For ease of exposition we will assume that rank(M 0 ) is known; in practice it can be estimated consistently using an eigenvalue thresholding procedure provided that M 0 has a low-rank structure. Finally, in the context of SBM graphs and their degree-corrected variant, community recovery using F also depends on knowing K. For simplicity we also assume that K is known, noting that consistent estimates for K are provided in <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THEORETICAL ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Entry-wise concentration of Ŵt and M0</head><p>Recall that F is obtained from the eigendecomposition of M0 while the true embedding is obtained from the eigendecomposition of M 0 (see Eq. (2.9)). Therefore, before studying the community recovery using F , we first study the convergence of M0 to M 0 . In particular we derive concentration bounds for M0 − M 0 in both Frobenius and infinity norms. These bounds are facilitated by the following Theorem 2 which provides a precise uniform bound for the entry-wise difference between the t-step transition matrix Ŵt and W t defined using the adjacency matrix A and the edge probabilities matrix P, respectively. Theorem 2. Let G ∼ SBM(B, Z) where B satisfies Assumption 1. We then have the following bounds.</p><p>1) (Dense regime) Suppose ρ n 1. Then</p><formula xml:id="formula_21">Ŵ − W max = O P (n −1 ), (3.1) Ŵ2 − W 2 max,diag = O P (n −1 ),<label>(3.2)</label></formula><formula xml:id="formula_22">Ŵ2 − W 2 max,off = O P log 1/2 n n 3/2 ,<label>(3.3)</label></formula><p>Furthermore, for t ≥ 3,</p><formula xml:id="formula_23">Ŵt − W t max = O P log 1/2 n n 3/2 ,<label>(3.4)</label></formula><p>2) (Sparse regime) Let ρ n → 0 with ρ n n −β for some β ∈ [0, 1). Then for t ≥ 4 satisfying t−3 t−1 &gt; β we have</p><formula xml:id="formula_24">Ŵt − W t max = O P log 1/2 n n 3/2 ρ 1/2 n . (3.5) In addition if 0 ≤ β &lt; 1/2 then Ŵ2 − W 2 max,off = O P log 1/2 n n 3/2 ρ n , Ŵ3 − W 3 max = O P log 1/2 n n 3/2 ρ n . (3.6)</formula><p>Remark 5. Throughout this paper we assume that t L ≥ 2 instead of t L ≥ 1 as used in the original node2vec formulation. The rationale for this assumption is as follows. Recall the definition of M0 in Eq. (2.8). If we allow t to starts from</p><formula xml:id="formula_25">1 in the sum t U t=t L (L − t) • D −1 A</formula><p>Ŵt then the term Ŵ might lead to a convergence rate of M0 to M 0 that is slower than that given in Eq. (3.7). For example in the dense regime Eq. (3.1) and Eq. <ref type="bibr">(3.3)</ref> show that the entries of Ŵ − W are of larger magnitude than the entries of Ŵt − W t for t ≥ 2.</p><p>Before discussing the convergence rate of M0 to M 0 we first find a value of t U such that, for large values of n, M0 is well defined with high probability. We note that the entries of {W t } t≥1 are uniformly of order Θ(n −1 ). Then, under the dense regime, t = 2 is sufficient to guarantee that all the off-diagonal entries of Ŵt are uniformly of order Ω(n −1 − n −3/2 log 1/2 n) = Ω(n −1 ) with high probability (c.f. Eq. (3.2)) while t = 3 is sufficient to guarantee that all entries of Ŵt are of order Ω(n −1 ) with high probability (c.f. Eq. <ref type="bibr">(3.3)</ref>). If we are under the sparse regime with β &lt; 1/2 then these same values of t ≥ 2 are still sufficient to guarantee that the entries of Ŵt are of order Ω(n −1 ) (c.f. Eq. (3.5) and Eq. (3.6)). Finally, if we are under the sparse regime with β ≥ 1/2 then choosing t ≥ 4 with t−3 t−1 &gt; β is sufficient to guarantee that the entries Ŵt are uniformly of order Ω(n</p><formula xml:id="formula_26">−1 − n −3/2 ρ −1/2 n log 1/2 n) = Ω(n −1 ) with high probability. Now recall that the matrix M0 is of the form log 2|A| κγ t U t=t L (L − t)D −1 A Ŵt</formula><p>We therefore have, for t U ≥ 3 in the dense regime, t U ≥ 2 in the not too sparse regime of β &lt; 1/2, or for t U −3 t U −1 &gt; β in general, that the entries of the inner sum are bounded away from 0 with high probability. For the dense regime, the condition can be further relaxed to t U ≥ 2, as a dense graph has a diameter of 2 and thus all entries of Ŵ2 are uniformly larger than 0 with high probability; see Theorem 10.10 in <ref type="bibr" target="#b30">[31]</ref>. Therefore, with high probability, the elementwise logarithm is well-defined for all entries of M0 . Given the existence of M0 , the following result shows the convergence rate of M0 to M 0 . Theorem 3. Suppose G ∼ SBM(B, Θ) satisfies Assumption 1, and t U ≥ t L ≥ 2 where t L is chosen as described above. Then M0 is well-defined with high probability. Denote</p><formula xml:id="formula_27">∆ = max{ M0 − M 0 F , M0 − M 0 ∞ }.</formula><p>We then have the following bounds.</p><p>1) (Dense regime) Let ρ n 1. Then for t L ≥ 2 we have</p><formula xml:id="formula_28">∆ = O P (n 1/2 log 1/2 n). (3.7) 2) (Sparse regime) Let ρ n → 0 with ρ n n −β for some β ∈ [0, 1). Then for t L satisfying t L −3 t L −1 &gt; β we have ∆ = O P n 1/2 ρ −1/2 n log 1/2 n .</formula><p>(3.8)</p><p>In addition if 0 ≤ β &lt; 1/2 then for t L ≥ 2 we have</p><formula xml:id="formula_29">∆ = O P n 1/2 ρ −1 n log 1/2 n . (3.9)</formula><p>In both regimes we have M 0 F = Θ(n) and M 0 ∞ = Θ(n).</p><p>Theorem 3 indicates that as β increases (equivalently, as ρ n decreases) so that the graph G becomes sparser, we could (1) still guarantee the existence of M0 when t U is sufficiently large, and (2) control the convergence rate of M0 − M 0 F relative to M 0 F by increasing t L .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Subspace perturbations and exact recovery</head><p>Theorem 3 implies that M0 is close to M 0 under both Frobenius and infinity norms, i.e., M0 − M 0 / M 0 = o P (1) for ∈ {F, ∞} and sufficiently large n. Now, by Eq. (2.6), M 0 has a K × K block structure and hence rk(M 0 ) ≤ K. Furthermore the eigenvectors of M 0 associated with its nonzero eigenvalues is sufficient for recovering the community assignments induced by Z. The following result, which follows from bounds for M0 − M 0 ∞ given in Theorem 3 together with perturbations bounds for invariant subspaces using 2 → ∞ norm <ref type="bibr" target="#b35">[36]</ref>, shows that the embedding F given by the leading eigenvectors of M0 is uniformly close to that of the leading eigenvectors of M 0 . Therefore K-means or K-medians clustering on the rows of F will recover the community membership for every nodes, i.e, attain strong or exact recovery of Z. Theorem 4. Under the condition of Theorem 3, let Û Σ Û and UΣU be the eigen-decomposition of M0 and M 0 , respectively. Let d = rk(M 0 ) and note that U is a n × d matrix. Let F = Ûd be the matrix formed by the columns of Û corresponding to the d largest-in-magnitude eigenvalues of M0 . For a n × d matrix Z with rows Z 1 , Z 2 , . . . , Z n let Z 2→∞ denote the maximum 2 norms of the {Z i }, i.e.,</p><formula xml:id="formula_30">Z 2→∞ = max i Z i 2 .</formula><p>We then have the following results.</p><p>(i) (Dense regime) Let ρ n 1. Then for t L ≥ 2 we have min</p><formula xml:id="formula_31">T∈O d F T − U F = O P log 1/2 n n 1/2 min T∈O d F T − U 2→∞ = O P log 1/2 n n .</formula><p>(3.10)</p><formula xml:id="formula_32">(ii) (Sparse regime) Let ρ n → 0 with ρ n n −β for some β ∈ [0, 1/2). If t L ≥ 2, we have min T∈O d F T − U F = O P log 1/2 n n 1/2 ρ n min T∈O d F T − U 2→∞ = O P log 1/2 n nρ n (3.11) (iii) (Sparse regime) Let ρ n → 0 with ρ n n −β for some β ∈ [0, 1). If t L −3 t L −1 &gt; β, we have, min T∈O d F T − U F = O P log 1/2 n (nρ n ) 1/2 min T∈O d F T − U 2→∞ = O P log 1/2 n nρ 1/2 n .</formula><p>(3.12)</p><p>Given the above convergence rates, clustering the rows of F using either K-means or K-medians will, with high probablity, recover the memberships of every nodes in G.</p><p>Remark 6. Settings (ii) and (iii) in Theorem 4 both consider the sparse regime but setting (ii) focuses on the case where ρ n = ω(n −1/2 ) and exact recovery is achieved whenever t L ≥ 2 while setting (iii) considers the more general scenario of ρ n = ω(n −β ) for any fixed but arbitrary β &lt; 1. We note that for ease of exposition we had impose t L −3 t L −1 &gt; β for setting (iii) but this condition can be relaxed to</p><formula xml:id="formula_33">t L − 2 t L &gt; β,<label>(3.13)</label></formula><p>under which we still have M0 is well-defined with high probability, and have a more complicated bound of min</p><formula xml:id="formula_34">T∈O d F T − U 2→∞ O P log 1/2 n n 3/2 ρ 1/2 n + (nρ n ) −t L /2</formula><p>(see (B.71)). The above bound is still sufficient to guarantee that running K-means or K-medians on the rows of F will recover the memberships of every nodes in G with high probability; see Section B.4 in the Supplementary File for a rigorous proof. A recent preprint <ref type="bibr" target="#b33">[34]</ref> which appeared on arXiv after the first version of our paper also studied community recovery using SVD-based DeepWalk/node2vec and they have a similar requirement for t L as Eq. (3.13); see <ref type="bibr">(3.1)</ref> in <ref type="bibr" target="#b33">[34]</ref>. For comparison we note that <ref type="bibr" target="#b33">[34]</ref> only derived the convergence rate of F under Frobenius norm, and thereby prove a weak recovery result which allows at most o(n 1/2 ) nodes to be misclassified. In contrast the max-norm concentration of Ŵt in Theorem 2 helps us derive a 2 → ∞ norm convergence for F , based on which we achieved the much stronger exact recovery (i.e., there are no mis-classified nodes). Finally we conjecture that Eq. (3.13) for t L is sufficient but not necessary. Our simulation results in Section 4 agree with this conjecture and we leave its verification for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 7.</head><p>The exact recovery results in Theorem 4 can also be extended to the case of degree-corrected SBM graphs <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>. Recall that the edge probabilities for a DCSBM is P = ΘZBZ T Θ where Θ = diag(θ 1 , . . . , θ n ) are the degreecorrection parameters. DCSBM allows heterogeneous edge probabilities within each community and thus yields a more flexible model in comparison with SBM. Section A.4 and B.5 in the Supplementary File demonstrates how to extend the technical derivations for Theorem 4 to the DCSBM case provided that the {θ i } are sufficiently homogeneous, i.e., that max i θ i / min i θ i = O(1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SIMULATION</head><p>We now present simulations experiments for the matrix factorization perspective of node2vec/DeepWalk. These experiments complement our theoretical results in Section 3 and illustrate the interplay between the sparsity of the graphs, the choice of window sizes, and their combined effects on the nodes embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Error bounds for M0 − M 0 F</head><p>We first compare the large-sample empirical behavior of M0 − M 0 F against the theoretical bounds given in Theorem 3. We shall simulate undirected graphs generated from a 2-blocks SBM with parameters</p><formula xml:id="formula_35">B(ρ n ) := 0.8ρ n 0.3ρ n 0.3ρ n 0.8ρ n , π = (0.4, 0.6),<label>(4.1)</label></formula><p>and sparsity ρ n ∈ {1, 3n −1/3 , 3n −1/2 , 3n −2/3 }. While this two blocks setting is quite simple it nevertheless displays the effect of the sparsity ρ n and the window size (t L , t U ) on the upper bound for M0 − M 0 F . For each value of n and sparsity ρ n we run 100 independent replications where, in each replicate, we generate G ∼ SBM(B(ρ n ), Θ n ) and calculate M0 for different choices of (t L , t U ). In particular we consider two types of window size, namely t U = t L + 1 and t U = t L + 3. While t U = t L +1 is not commonly used in practice, for simulation purpose this choice clearly show the effects of the random walks' length t on the error M0 − M 0 F . In contrast the choice t U = t L + 3 is more realistic but also partially obfuscate the effect of t on M0 − M 0 F . Recall that, from the discussion prior to Theorem 3, sparser values of ρ n requires larger values of t U to guarantee that M0 is welldefined. The choices for ρ n , n, (t L , t U ) in the simulations are summarized below.</p><formula xml:id="formula_36">• If ρ n ≥ 3n −1/2 then n ∈ {100, 200, 300, . . . 1500}. We chose 2 ≤ t L ≤ 7 when t U = t L + 1 and chose 2 ≤ t L ≤ 5 when t U = t L + 3. • If ρ n = 3n −2/3 then n ∈ {800, 900, . . . , 4000}. We chose 4 ≤ t L ≤ 7 when t U = t L + 1 and 3 ≤ t L ≤ 5 when t U = t L + 3.</formula><p>We calculate two relative error criteria for M0 , namely</p><formula xml:id="formula_37">ε 1 ( M0 ) = M0 − M 0 F M 0 F and ε 2 ( M0 ) = M0 − M 0 F n 1/2 ρ −1/2 n log 1/2 n .</formula><p>We expect that, as n increases, the first criteria converges to 0 while the second criteria remains bounded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relative Error 1:</head><p>We first confirm the convergence of ε 1 ( M0 ) to 0. Figures <ref type="figure" target="#fig_11">1 and D1</ref> shows the means and 95% confidence intervals for ε 1 ( M0 ) based on 100 Monte Carlo replicates for different values of ρ n , (t L , t U ). These figures indicate the following general patterns as predicted by the theoretical results in Theorem 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The error ε 1 ( M0 ) is smallest in the dense case and deteriorates as the sparsity factor ρ n decreases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The error also depends on (t L , t U ) with larger values of t U − t L leading to smaller ε 1 ( M0 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>If the window size is too small, e.g., (t L , t U ) = (2, 3) or (t L , t U ) = (2, 5), then M0 is often times not welldefined.</p><p>Relative Error 2: Figures 1 and Figure <ref type="figure" target="#fig_11">D1</ref> (see the Supplementary File) corroborate our theoretical results in Section 3. Nevertheless there are two additional questions we should consider. The first is whether or not the bound</p><formula xml:id="formula_38">M0 −M 0 F = O P (n 1/2 ρ −1/2 n log 1/2 n</formula><p>) in Theorem 3 is tight and, if it is tight, the second is whether or not the condition</p><formula xml:id="formula_39">t L −3 t L −1 &gt; β is</formula><p>necessary to achieve this rate. Analogous to the previous two figures, Figures <ref type="figure" target="#fig_20">2 and D2</ref> show the means and 95% empirical confidence intervals for the relative error ε 2 ( M0 ) over 100 Monte Carlo replicates for different values of ρ n and (t L , t U ). From these simulations we can answers the above questions as follows.</p><p>•</p><formula xml:id="formula_40">If ρ n n −β is such that β ≤ t L −3 t L −1 then ε 2 ( M0 )</formula><p>appears to converge to a constant as n increases. There is thus evidence that the rate n  t L ≥ 6, then ε 2 ( M0 ) appears to converges to 0 which suggests that for a fixed β the error rate for M0 − M 0 F can be smaller than n 1/2 ρ −1/2 n log 1/2 n; this might be due to the convergence of Ŵt and W t towards the stationary distributions as t increases.</p><formula xml:id="formula_41">1/2 ρ −1/2 n log 1/2 n for M0 − M 0 F is optimal. Nevertheless if t L is large relative to ρ n , e.g., ρ n ∈ {3n −1/3 , 3n −1/2 } and</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>For cases such as (t L , t U ) ∈ {(3, 4), <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b5">6)</ref>} and ρ n = 3n −1/2 or (t L , t U ) ∈ {(4, 5), <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b5">6)</ref>} and ρ n = 15n −2/3 , the t L 's do not satisfy t L −3 t L −1 &gt; β. Nevertheless ε 2 ( M0 ) still appears to converge to a constant as n increases. This suggests that t L −3 t L −1 &gt; β is sufficient but possibly not necessary for the bound in Eq. (3.8) to hold. On the other hand, for fixed n and ρ n , the error M0 − M 0 F generally decreases as t U − t L increases.</p><formula xml:id="formula_42">• Finally if (t L , t U ) ∈ {(2, 3), (2, 5)} and ρ n ∈ {3n −1/3 , 3n −1/2 } then ε 2 ( M0 ) increases with n.</formula><p>This supports the claim in Theorem 3 of a phase transition for the error rate of M0 − M 0 F as t L increases.</p><p>In summary Figure <ref type="figure" target="#fig_2">1</ref> through Figure <ref type="figure" target="#fig_20">D2</ref> supports the conclusion of Theorem 3. In particular the error rate in Theorem 3 is sharp and the condition t L −3 t L −1 &gt; β is sufficient but perhaps not necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Exact recovery of community structure</head><p>Theorem 4 together with Remark 6 showed that F combined with K-means/medians can correctly recover the memberships of all nodes in a SBM with high probability. We demonstrate this result for two-blocks SBMs with block probabilities being either B(ρ n ) as given in Eq. (4.1) or</p><formula xml:id="formula_43">B (ρ n ) := 0.3ρ n 0.8ρ n 0.8ρ n 0.3ρ n .</formula><p>Note that B(ρ n ) and B (ρ n ) corresponds to an assortative and a dis-assortative structure, respectively. Given specific setting of B, n, ρ n , we randomly sample 100 graphs where each vertex is randomly assigned to one of the two blocks with equal probability and evaluate the membership recovery performances of the original node2vec <ref type="bibr" target="#b18">[19]</ref> (based on SGD) and node2vec using matrix factorization (as described <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b33">[34]</ref> and this paper) followed by clustering using K-means. We set the window sizes to t U ∈ {5, 8} and choose κ = 5 and L = 200. For the original node2vec we also set t L = 1 as the default and r 1 = • • • = r n = 200, while for the SVD-based node2vec we set t L = t U − 3. We report in Tables <ref type="table" target="#tab_2">1 and 2</ref> the proportions of times for the 100 simulated graphs that these two variants of the node2vec algorithm correctly recover the memberships of all nodes.</p><p>The numerical results show that as n increases, both the original and SVD-based node2vec are more likely to perfectly recover memberships of all nodes in the graph, under all different settings of ρ n , B, t U . Furthermore the accuracy when ρ n = 3n −1/3 is considerably higher than that for ρ n = 3n −1/2 . This is consistent with the results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>n</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVD-based node2vec</head><p>Original node2vec   </p><formula xml:id="formula_44">t U = 5 t U = 8 t U = 5 t U =</formula><formula xml:id="formula_45">ρ n = 3n −1/2 (right table).</formula><p>in Theorem 4 as a smaller magnitude for ρ n results in a slower convergence rate for F under both the Frobenius and 2 → ∞ norms. In addition the exact recovery performance of SVD-based node2vec when ρ n n −1/2 and (t L , t U ) = (2, 5) suggests that the t L threshold for Theorem 4 in Eq. (3.13) is possibly not sharp as t L −2 t L = 0 &lt; β = 1/2. Finally we note that the SVD-based node2vec has better empirical performance than the original node2vec in these experiments as well as in the experiments for three-blocks SBMs and DCSBMs in Section 4.3. This is consistent with the discussion in Section 2. Indeed, the entries of M0 are the limit of those for the original node2vec when the number of sampled paths r → ∞ and furthermore M0 has an approximately low-rank structure as n increases. In other words, at least for SBM and DCSBM graphs, we can view the original node2vec as a computationally efficient approach to approximate the embeddings based on SVD of M0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Embedding performance</head><p>In this section we perform more numerical experiments to take a closer look at the finite-sample performance of community detection, using both the original and SVDbased node2vec embeddings. We consider both three-blocks SBM and three-blocks DCSBM. We will vary the sample size n, window sizes t U , and sparsity ρ n in these simulations and investigate the effect of these parameters on the community detection accuracy.</p><p>More specifically, for each simulation with a specified value of n and ρ n , we run 100 Monte Carlo replications where, for each replicate, we apply both the original and SVD-based node2vec algorithms with different window sizes on the simulated random graph to obtain the embeddings followed by community detection using K-means on these embeddings. Let the true and estimated cluster labels be denoted by {τ (i)} n i=1 and {τ (i)} n i=1 . We calculate the accuracy of τ as (here ξ(•) denotes an arbitrary permutation of {1, 2, . . . , K}),</p><formula xml:id="formula_46">Accuracy = min ξ(•) #{i|ξ(τ (i)) = τ (i)} n . (4.2)</formula><p>Before presenting the formal numerical results, we first fix ρ n = 3n −1/2 , n = 600 and sample one random realization from both the SBM and the DCSBM to illustrate the node2vec embedding performances. These visualizations, which are depicted in Figure <ref type="figure">D6</ref> and Figure <ref type="figure" target="#fig_13">D3</ref> in the Supplementary File, provide us with some intuitions, namely that (i) the original and SVD-based node2vec variants yield similar embeddings (ii) for SVD-based node2vec, increasing the window size could help separate nodes from different communities and thereby improve the community detection accuracy; (iii) although the embeddings appear similar, Kmeans clustering yields more accurate membership recovery for the SVD-based node2vec compared to the original SVD-based node2vec embeddings. We now describe the settings of the network generation models used in these simulations. Stochastic Blockmodel: We consider three-blocks SBMs with block probabilities being either and block assignment probabilities π = (0.3, 0.3, 0.4). Degree-Corrected Stochastic Blockmodel: DCSBMs are direct generalization of SBMs with the only difference being that each node i has a degree-correction parameter θ i and that the probability of connection between nodes i and j is</p><formula xml:id="formula_47">p ij = θ i θ j B τ (i)τ (j)</formula><p>instead of p ij = B τ (i)τ (j) as in the case of SBMs. For more on DCSBMs and their inference, see <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>. We generate the degree correction parameters θ i as (4.4)   This procedure for generating θ i is the same as that in <ref type="bibr" target="#b48">[49]</ref>.</p><formula xml:id="formula_48">θ i = |Z i | + 1 − (2π) −1/2 , Z 1 , . . . , Z n iid ∼ N (0, 0.25)</formula><p>For each simulated graph, we test both the original node2vec and the SVD-based node2vec with t U = 5, 6, 7. Other settings of the node2vec algorithms are similar to Section 4.2. The simulation results for the SBMs and the DCSMBs are presented in Figure <ref type="figure">4</ref> and Figure <ref type="figure">D4</ref> in the Supplementary File. We now summarize the main trend in these figures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The box plots when ρ n = 1 (dense regime) have large interquartile ranges because there are a few replicates where, due to sampling variability, the simulated graphs are quite noisy and the community detection algorithm has low accuracy while for most of the remaining graphs we achieved exact recovery using both the original and SVD-based node2vec algorithms. Furthermore if ρ n = 1 then increasing the window size from t L = 2 to t L &gt; 2 does not yields noticeable improvement in accuracy for the SVD-based node2vec. The condition t L ≥ 2 in Theorem 4 (i) is thus sufficient for exact recovery in the dense regime. On the other hand, when ρ n = 1, we see that the accuracy increases with n as indicated by the large-sample results in Theorem 4.</p><p>• When ρ n → 0 faster (i.e., the network is more sparse), we need a larger n to achieve the same level of accuracy. This is consistent with Theorem 4 as the convergence rate for F depends on nρ n .</p><p>• When B = B 2 the original node2vec and SVD-based node2vec have very similar accuracy and thus our theoretical analysis of SVD-based node2vec closely reflects the performance of the original node2vec.</p><p>• When B = B 1 the SVD-based node2vec has higher accuracy compared to the original node2vec. However the embeddings generated by these algorithms are still quite similar. A plausible reason for why the original node2vec has lower accuracy is because the downstream K-means clustering is sub-optimal for these embeddings. For example, comparing panels (c) and (d) in Fig. <ref type="figure">D6</ref> we can see that K-means clustering correctly recovers most of the membership assignments for embeddings from the SVD-based node2vec. In contrast, panels (g) and (h) in Fig. <ref type="figure">D6</ref> show that K-means clustering is less accurate for embeddings from the original node2vec. Indeed, when replacing K-means with Gaussian mixtures model (GMM) <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref> in panels (g) and (h) of Fig. <ref type="figure">D6</ref> we increase the clustering accuracy from 0.58 to 0.84 which is close to that of 0.88 for the SVD-based node2vec (see Fig. <ref type="figure" target="#fig_22">D5</ref> of the Supplementary File).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">APPLICATIONS TO REAL-WORLD NETWORKS</head><p>We test the membership recovery performance of node2vec on three real-world networks, namely, the Zachary's karate graph (henceforth, ZK) <ref type="bibr" target="#b51">[52]</ref>, political blogs graph (henceforth, PB) <ref type="bibr" target="#b52">[53]</ref>, and Wikipedia graph (henceforth, WIKI) <ref type="bibr" target="#b7">[8]</ref>.</p><p>In each of the three graphs, the memberships of all vertices have been assigend baed on specific real-world meanings without missing. Both ZK and PB contain 2 communities, while WIKI contains 6 communities. ZK is connected with 34 vertices. By conventions <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b46">[47]</ref>, we ignore the directions of edges and focus on the largest connected components of PB and WIKI, which contain 1222 and 1323 vertices, respectively. We refer interested readers to the references above for more detailed information about the three realworld network datasets. Fig. <ref type="figure">4</ref>: Community detection accuracy of node2vec followed by K-means for SBM graphs. The boxplots of the accuracy for each value of n, ρ n and t U are based on 100 Monte Carlo replications. Boxplots with the slash pattern (resp. dot pattern) summarized the results for the original (resp. SVD-based) node2vec. Different colors (yellow, green, blue) represent the algorithms implemented for different choices of t U ∈ {5, 6, 8}. The first and second row plot the results when the block probabilities for the SBM is B 1 and B 2 , respectively.</p><p>For each network dataset, we embed the vertices into the K-dimensional Euclidian space through both the SVDbased and original node2vec, and then cluster the embeddings by K-means to estimate the memberships of each vertex; K is chosen as the exact number of memberships in each graph. We test three window sizes t U ∈ {10, 15, 20}. Similar to Section 4, we set t L = t U − 5 for the SVD-based node2vec and t L = 1 for the original node2vec by default. To measure the membership recovery performances, we calculate the accuracies between the estimated memberships and the real memberships for ZK and PB; see the definition of accuracy in Eq. (4.2). For WIKI, because the criteria of accuracy becomes computationally inflexible, we alternatively use the adjusted rand index (ARI). Similar to the accuracy, ARI = 1 indicates the estimated memberships perfectly recover the real memberships, while ARI = 0 indicates the estimated memberships are assigned randomly. We also compare performances of node2vec algorithms with other popular spectral embedding algorithms, including the spectral clustering based on adjacency and normalized Laplacian <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, and the spectral clustering with projection onto the sphere <ref type="bibr" target="#b53">[54]</ref>; for all methods we use K-means for the downstream clustering.</p><p>The recovery results are summarized in Table <ref type="table" target="#tab_4">3</ref>. The SVD-based and original node2vec algorithms have similar performances, which are generally better than or equivalently to other methods in all three datasets. In addition, we note the PB dataset is better modeled as a DCSBM <ref type="bibr" target="#b46">[47]</ref>. Recall that, as shown in Remark 4, node2vec can theoretically attain exact recovery for DCSBMs and hence the high-accuracy of node2vec on the PB dataset is expected. Similarly, <ref type="bibr" target="#b53">[54]</ref> shows a valid theoretical guarantee of the spectral clustering with a spherical projection, when applying to the DCSBM graph. This can also be verified by the high accuracy of ASE+SP on PB as shown in Table <ref type="table" target="#tab_4">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>In this paper we derive perturbation bounds and show exact recovery for the DeepWalk and node2vec (with p = q = 1) algorithms under the assumption that the observed graphs are instances of the stochastic blockmodel graphs. Our results are valid under both the dense and sparse regimes for sufficient large t L and n. The simulation results corroborate our theoretical findings; in particular they show that increasing the sample size and window size can improve the community detection accuracy for both sparse SBM and DCSBM graphs.</p><p>We emphasize that our paper only include real data analysis on simple graphs with a small number of nodes just to illustrate the agreement between our theoretical results and the empirical performance of DeepWalk/node2vec. This is intentional as DeepWalk and node2vec are widelyused algorithms with numerous papers demonstrating their uses for analyzing real graphs in diverse applications. In contrast, our paper is one of a few that addresses the theory underpinning these algorithms and is, to the best of our knowledge, the first paper to establish consistency and exact   <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, respectively. ASE+SP denote spectral clustering using the truncated eigendecomposition of the adjacency matrix together with a spherical projection step <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>.</p><p>recovery for SBMs and DCSBMs using these random-walk based embedding algorithms. Note that exact recovery for SBMs can also be achieved using other algorithms such as those based on semidefinite programming, variational Bayes, and spectral embedding; see <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref> for a few examples.</p><p>There are several open questions for future research:</p><p>1) In this paper we only consider the case of p = q = 1 for node2vec embedding (recall that p = q = 1 is the default parameter values for node2vec). If p = 1 and/or q = 1 then the transformed co-occurrence matrix M0 can no longer be expressed in terms of the adjacency matrix A or the transition matrix Ŵt ; this renders the theoretical analysis for general values of p and q substantially more involved. One potential approach to this problem is to consider, similar to the notion of the non-backtracking matrix in community detection for sparse SBM <ref type="bibr" target="#b57">[58]</ref>, a transition matrix associated with the edges of G as opposed to the transition matrix associated with the vertices in G. Indeed, if p = q then the transition probability from a vertex v to another vertex w depends also on the vertex, say u, preceding v in the random walk. i.e., the transition probability for (v, w) depends on the choice of (u, v). 2) In this paper we focus on error bounds (in Frobenius and infinity norms) of node2vec/DeepWalk embedding for stochastic blockmodel graphs and their degree-corrected variant. An important question is whether or not stronger limit results are available for these algorithms. For example spectral embeddings of stochastic blockmodel graphs obtained via eigendecompositions of either the adjacency or the normalized Laplacian matrices are well-approximated by mixtures of multivariate Gaussians; see <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> for more precise statements of these results and their implications for statistical inference in networks. It is thus natural to inquire if normal approximations also holds node2vec/Deepwalk. We ran several one-round simple simulations to visualize the embeddings of node2vec/DeepWalk when the graphs are sampled The results are summarized in Fig. <ref type="figure">D6</ref> in the Supplementary File. In particular when n is large these embeddings are also well-approximated by a mixture of multivariate Gaussians. We leave the theoretical justification of this phenomenon for future work. 3) As we allude to in the introduction, for simplicity we only consider (degree-corrected) stochastic blockmodel graphs in this paper. For the more general inhomogeneous Erdős-Rényi random graphs model, we expect that Theorem 2 and Theorem 3 still hold, provided that the edge probabilities are sufficiently homogeneous, i.e., the minimum and maximum values for the edge probabilities values are of the same order as n increases. However, the error bounds in Theorem 4 might no longer apply since the entry-wise logarithmic transformation of the co-occurrence matrices can lead to the setting wherein M 0 is no longer low-rank, e.g., the rank of M 0 can be as large as n the number of vertices. Furthermore, even when M 0 have an approximate low-rank structure, due to the logarithmic transformation there is still the question of how the embedding of M 0 relates to the underlying latent structure in P. 4) Finally, in this paper we mainly focus on the node2vec and DeepWalk embedding through matrix factorization (SVD-based node2vec), but also compare the SVD-based node2vec with the original node2vec in the numerical experiments. As we mentioned in the introduction the original node2vec algorithm uses (stochastic) gradient descent (GD/SGD) to optimize Eq. (2.4) and obtain the embeddings. As Eq. (2.4) is non-convex there can be a large number of local-minima, thereby making the theoretical analysis intractable unless we assume that the initial estimates for GD/SGD are sufficiently close to the global minima; see e.g., <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref> for some examples of results relating the closeness of the initial estimates and the convergence rate of GD/SGD. One popular initialization scheme for GD/SGD is via spectral methods and thus we can consider using the SVD-based embedding F as a "warm-start" for Eq. (2.4). We leave the precise convergence analysis of the resulting GD/SGD iterations to the interested reader. We note, however, that while this is certainly an interesting technical problem, the practical benefits might be limited. Indeed, the theoretical results in Section 3 guaranteed perfect recovery using F while the empirical evaluations in Sections 4.2 and Section 4.3 suggest that clustering based on F is comparable or even better than that of the original node2vec. In other words as the main objective is to recover the structure in M 0 induced by P, it is certainly possible that optimizing Eq. (2.4) does not lead to better inference performance due to the noise in using A as a replacement for P.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A PROOFS UNDER THE DENSE REGIME</head><p>In this section we provide proofs of our main theorems under the dense regime of ρ n = Θ(1). This is done for ease of exposition as the proofs for the sparse regime follows the same conceptual ideas and proof structure but with substantially more involved and tedious technical derivations. We first list two basic lemmas that will be used repeatedly in the subsequent proofs. Proofs of these lemmas are deferred to Appendix C.</p><p>Lemma A1. Let d = (d 1 , . . . , d n ) and p = (p 1 , . . . , p n ). Then for any integer t &gt; 0 we have</p><formula xml:id="formula_49">1 n Ŵt = 1 n W t = 1 n , Ŵt d = d, W t p = p. (A.1)</formula><p>Lemma A2. Under Assumption 1, we have</p><formula xml:id="formula_50">D A = max i d i = O P (n), D −1 A = max i 1/d i = O P (1/n), D A − D P = max i |d i − p i | = O P (n 1/2 log 1/2 n), (A.2) D −1 A − D −1 P = max i |d −1 i − p −1 i | = O P log 1/2 n n 3/2 , Ŵt max = O P (n −1 ), max i,i w (t) ii 1/n, min i,i w (t) ii 1/n, for any fixed t ≥ 1. Here w (t)</formula><p>ii is the ii th entry of W t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Proof of Theorem 2 (Dense regime)</head><p>We use the following three steps to bound Eq. (3.1)-Eq. (3.4) in turn.</p><p>Step 1 (Bounding Ŵ − W max ): We start with the decomposition</p><formula xml:id="formula_51">Ŵ − W = AD −1 A − PD −1 P = AD −1 P D −1 A (D P − D A ) ∆ (1)<label>1</label></formula><formula xml:id="formula_52">+ (A − P)D −1 P ∆<label>(1) 2 . (A.3)</label></formula><p>For the first term, we have ∆</p><p>(1)</p><formula xml:id="formula_53">1 = AD −1 P D −1 A (D P − D A ) = a ii d i • p i (d i − p i )</formula><p>n×n and hence</p><formula xml:id="formula_54">∆ (1) 1 max = max i,i a ii d i • p i (d i − p i ) max i 1 n d i − p i d i by |a ii | ≤ 1 and c 0 &lt; p ii &lt; c 1 . Lemma A2 then implies ∆ (1) 1 max 1 n max i |d i − p i | • max i 1 d i = O P n −3/2 log 1/2 n . (A.4)</formula><p>For the second term we have, by Assumption 1, that</p><formula xml:id="formula_55">∆ (1) 2 max = max i,i a ii − p ii p i ≤ max i 1 p i = O P n −1 . (A.5)</formula><p>Combining Eq. (A.4) and Eq. (A.5) yields</p><formula xml:id="formula_56">Ŵ − W max ≤ ∆ (1) 1 max + ∆ (1) 2 max = O P n −1 . (A.6) Step 2 (Bounding Ŵ2 − W 2 max,diag , Ŵ2 − W 2 max,off ): We first decompose Ŵ2 − W 2 as Ŵ2 − W 2 = ( Ŵ − W)W ∆ (2) 1 + Ŵ( Ŵ − W) ∆ (2) 2 .</formula><p>(A.7)</p><p>As with Eq (A.3) we have, ∆</p><formula xml:id="formula_57">1 = (AD −1 A − PD −1 P )W = {AD −1 P D −1 A (D P − D A ) + (A − P)D −1 P }W = {AD −1 P D −1 A (D P − D A ) − AD −2 P (D P − D A )}W ∆ (2,1) 1 + AD −2 P (D P − D A ) ∆ (2,2) 1 W + (A − P)D −1 P W ∆ (2,3) 1 . (A.8)<label>(2)</label></formula><p>The ii th element of ∆</p><p>(2,1) 1</p><p>is given by</p><formula xml:id="formula_58">n i * =1 a ii * (p i * − d i * ) 2 p 2 i * d i * • w i * i . (A.9)</formula><p>We therefore have, by Assumption 1 and Lemma A2,</p><formula xml:id="formula_59">∆ (2,1) 1 max ≤ max i,i n i * =1 a ii * (p i * − d i * ) 2 p 2 i * d i * w i * i ≤ n max i |p i − d i | 2 max i 1 p 2 i d i (max i,i w ii ) = O P (n −2 log n). (A.10)</formula><p>Similarly, for ∆</p><p>(2,2) 1</p><p>we have</p><formula xml:id="formula_60">∆ (2,2) 1 max = max i,i n i * =1 a ii * p 2 i * (p i * − d i * )w i * i ≤ n(max i |p i − d i |) • max i,i w ii • max i 1 p 2 i = O P n −3/2 log 1/2 n . (A.11)</formula><p>We now consider the term ∆</p><p>(2,3) 1</p><p>. We have </p><formula xml:id="formula_61">∆ (2,3) 1 max = max i,i n i * =1 (a ii * − p ii * ) p i * w i * i (A.</formula><formula xml:id="formula_62">max i,i n i * =1 (a ii * − p ii * ) p i * w i * i = O P (n −3/2 log 1/2 n). (A.13)</formula><p>Combining the above bounds we obtain</p><formula xml:id="formula_63">∆ (2) 1 max ≤ ∆ (2,1) 1 max + ∆ (2,2) 1 max + ∆ (2,3) 1 max = O P n −3/2 log 1/2 n . (A.14)</formula><p>We now consider the term</p><formula xml:id="formula_64">∆ (2) 2 = Ŵ( Ŵ − W). We have ∆ (2) 2 max = (W − Ŵ) 2 − W(W − Ŵ) max ≤ (W − Ŵ) 2 max + W(W − Ŵ) max . (A.15)</formula><p>The same argument for bounding ∆</p><p>(1) 2 max as given above also yields</p><formula xml:id="formula_65">W(W − Ŵ) max = O P (n −3/2 log n).</formula><p>We then bound (W − Ŵ) 2 max through the following expansion</p><formula xml:id="formula_66">(W − Ŵ) 2 max = max i,i n i * =1 p ii * p i * − a ii * d i * p i * i p i − a i * i d i = max i,i n i * =1 p ii * p i * − a ii * p i * + a ii * p i * − a ii * d i * p i * i p i − a i * i p i + a i * i p i − a i * i d i ≤ max i,i n i * =1 a ii * p i * − a ii * d i * p i * i p i − a i * i p i δ (2,1)<label>2</label></formula><formula xml:id="formula_67">+ max i,i n i * =1 p ii * p i * − a ii * d i * a i * i p i − a i * i d i δ (2,2) 2 + max i,i n i * =1 p ii * p i * − a ii * p i * p i * i p i − a i * i p i δ (2,3) 2 . (A.16) Since |a ii | ≤ 1 and |a ii − p ii | ≤ 1, we have δ (2,1) 2 ≤ max i n i * =1 1 p i * − 1 d i * 1 p i n • 1 n max i * |p i * − d i * | p i * d i * = O P (n −3/2 log 1/2 n).</formula><p>Similar reasoning also yields</p><formula xml:id="formula_68">δ (2,2) 2 ≤ max i,i n i * =1 p ii * p i * − a ii * d i * • 1 p i − 1 d i ≤ n max i * 1 p i * + max i * 1 d i * • max i 1 p i − 1 d i n • n −1 + O P (1/n) • O P n −3/2 log 1/2 n = O P n −3/2 log 1/2 n .</formula><p>We now bound δ</p><p>(2,3) 2</p><p>by considering the diagonal and off-diagonal terms respectively. For the diagonal terms we have</p><formula xml:id="formula_69">max i n i * =1 (a ii * − p ii * ) 2 p i * p i ≤ max i n i * =1 1 p i p i * = O(n −1 ) (A.17)</formula><p>Eq. (A.14) together with the above bounds for δ</p><formula xml:id="formula_70">(2,1) 2 through δ (2,3) 2 yield Ŵ2 − W 2 max,diag ≤ ∆ (2) 1 max + W(W − Ŵ) max + δ (2,1) 2 + δ (2,2) 2 + max i n i * =1 (p ii * − a ii * ) 2 p i * p i = O P (n −1 ). (A.18)</formula><p>We now consider the off-diagonal terms with i = i for δ</p><p>(2,3) 2</p><p>. First define</p><formula xml:id="formula_71">ζ ii i * = 1 p i * p i (p ii * − a ii * )(p i * i − a i * i ).</formula><p>We now make an important observation that if i = i then the collection of random variables ζ ii i * for i * = 1, 2 . . . , n are independent mean 0 random variables. Indeed, when i = i then a ii * and a i * i are independent and hence</p><formula xml:id="formula_72">Eζ ii i * = 1 p i * p i E(p ii * − a ii * ) • E(p i * i − a i * i ) = 0.</formula><p>We thus have</p><formula xml:id="formula_73">max i =i n i * =1 (p ii * − a ii * )(p i * i − a i * i ) p i * p i = max i =i n i * =1 ζ ii i * = max i =i n i * =1 i * =i,i ζ ii i * + ζ ii i + ζ ii i . (A.19)</formula><p>Now fix a pair {i, i } with i = i . Then by Bernstein inequality, we have, for any t &gt; 0,</p><formula xml:id="formula_74">P n i * =1 i * =i,i ζ ii i * &gt; t ≤ 2 exp − t2 2σ 2 1 + M 3 t (A.20)</formula><p>where the variance proxy σ 2 1 is bounded as</p><formula xml:id="formula_75">σ 2 1 = n i * =1 i * f =i,i Var(ζ ii i * ) = 1 p 2 i * p 2 i n i * =1 i * =i,i Var(p ii * − a ii * ) • Var(p i i * − a i i * ) ≤ n 16p 2 i * p 2 i ≤ 1 16c 4 0 n 3 ,</formula><p>and M is any constant bigger than max i * | ζii i * |. In particular</p><formula xml:id="formula_76">|ζ ii i * | = p ii * p i * − a ii * p i * p i * i p i − a i * i p i ≤ 1 c 2</formula><p>0 n 2 and we can take M = (c 0 n) −2 . Let ϑ n = n −3/2 log 1/2 n. Plugging the above bounds for σ 2  1 and M into Eq. (A.20), we have, for any C 1 &gt; 0,</p><formula xml:id="formula_77">P n i * =1 i * =i,i ζii i * &gt; C 1 ϑ n ≤ 2 exp − C 1 ϑ n 2 2 16c 4 0 n 3 + 1 3c 2 0 n 2 C 1 ϑ n = 2 exp − C 1 log n 1 8c 4 0 + log 1/2 n n 1/2 3c 2 0 C1 n −8C1c 4 0 . Now choose C 1 such that 8C 1 c 4 0 &gt; 5.</formula><p>We then have, by a union bound over all pairs {i, i } with i = i , that</p><formula xml:id="formula_78">P max i =i n i * =1 i * =i,i ζii i * &gt; C 1 n −3/2 log 1/2 n n 2−8C1c 4 0 n −3 .</formula><p>We thus conclude</p><formula xml:id="formula_79">max i =i n i * =1 (p ii * − a ii * )(p i * i − a i * i ) p i * p i ≤ max i =i n i * =1 i * =i,i ζ ii i * + 2M = O P (n −3/2 log 1/2 n).</formula><p>A similar argument to Eq. (A.18) then yields</p><formula xml:id="formula_80">Ŵ2 − W 2 max,off ≤ ∆ (2) 1 max + W(W − Ŵ) max + δ (2,1) 2 + δ (2,2) 2 + max i =i n i * =1 p ii * − a ii * p i * p i * i − a i * i p i = O P (n −3/2 log 1/2 n).</formula><p>(A.21)</p><p>Step 3 (Bounding Ŵt − W t max , t ≥ 3): We first consider t = 3. We have</p><formula xml:id="formula_81">Ŵ3 − W 3 max ≤ Ŵ2 − W 2 Ŵ max + W 2 Ŵ − W max (A.22)</formula><p>For the first term on the RHS of Eq. (A.22) we have, by Eq. (A.18), Eq. (A.21) and Lemma A2, that</p><formula xml:id="formula_82">( Ŵ2 − W 2 ) Ŵ max ≤ n Ŵ max Ŵ2 − W 2 max,off + Ŵ max Ŵ2 − W 2 max,diag = O P (n −3/2 log 1/2 n). (A.23)</formula><p>For the second term in the RHS of Eq. (A.22) we use the same argument as that for bounding ( Ŵ − W)W max in Step 2.</p><p>In particular we have</p><formula xml:id="formula_83">W 2 ( Ŵ − W) max = O P (n −3/2 log 1/2 n). (A.24)</formula><p>Combining Eq. (A.22) through Eq. (A.24) yields</p><formula xml:id="formula_84">Ŵ3 − W 3 max = O P (n −3/2 log 1/2 n). (A.25)</formula><p>The case when t = 4 is analogous. More specifically,</p><formula xml:id="formula_85">Ŵ4 − W 4 max = ( Ŵ2 − W 2 ) Ŵ2 max + W 2 ( Ŵ2 − W 2 ) max ≤ n Ŵ2 − W 2 max,off ( Ŵ2 max + W 2 max ) + Ŵ2 − W 2 max,diag ( Ŵ2 max + W 2 max ) = O P (n −3/2 log 1/2 n) (A.26)</formula><p>We now consider a general t ≥ 5. We start with the decomposition</p><formula xml:id="formula_86">Ŵt − W t = ( Ŵt − W 2 Ŵt−2 ) + (W 2 Ŵt−2 − W t ) = ( Ŵ2 − W 2 ) Ŵt−2 ∆ (3) 1 + W 2 ( Ŵt−2 − W t−2 ) ∆ (3) 2 ,</formula><p>We now have, by Lemma A2, Eq. (A.18), and Eq. (A.21), that</p><formula xml:id="formula_87">∆ (3) 1 max ≤ n Ŵ2 − W 2 max,off Ŵt−2 max + Ŵ2 − W 2 max,diag Ŵt−2 max = O P (n −3/2 log 1/2 n). (A.27)</formula><p>Once again, by Lemma A2, we have</p><formula xml:id="formula_88">∆ (3) 2 max ≤ n W 2 max Ŵt−2 − W t−2 max Ŵt−2 − W t−2 max . (A.28)</formula><p>Combining Eq. (A.27) and Eq. (A.28), we have</p><formula xml:id="formula_89">Ŵt − W t max O P (n −3/2 log 1/2 n) + Ŵt−2 − W t−2 max (A.29)</formula><p>As t is finite, iterating the above argument yields</p><formula xml:id="formula_90">Ŵt − W t max O P (n −3/2 log 1/2 n) + Ŵ4 − W 4 max (when t is even) O P (n −3/2 log 1/2 n) + Ŵ3 − W 3</formula><p>max (when t is odd)</p><p>Recalling Eq. (A.25) and Eq. (A.26), we conclude that Ŵt − W t max = O P (n −3/2 log 1/2 n) for all t ≥ 3 as desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Theorem 3 (Dense regime)</head><p>We will only present the proof of bounding M0 − M 0 F and M0 − M 0 ∞ here as the rates for M 0 F and M 0 ∞ are derived in the proof of Theorem 4, Eq. (A.50). Under the dense regime of Assumption 1 and for a sufficiently large n, the diameter of G is 2 with high probability, see e.g., Corollary 10.11 in <ref type="bibr" target="#b30">[31]</ref>. Therefore, with high probability, all entries of Ŵ2 are positive and hence</p><formula xml:id="formula_91">M0 = log 2|A| κγ t U t=t L (L − t) • D −1 A</formula><p>Ŵt is well-defined with high probability. Next recall the definition of M 0 given in Eq. (2.9). We have</p><formula xml:id="formula_92">M0 − M 0 = log |A| t U t=t L (L − t) • D −1 A Ŵt I A − log |P| t U t=t L (L − t) • D −1 P W t I P . (A.30)</formula><p>By the mean value theorem, the absolute value of ii th entry in M0 − M 0 is</p><formula xml:id="formula_93">1 α ii t U t=t L (L − t) • |A| • ŵ(t) ii d i − |P| • w (t) ii p i I ii A −I ii P</formula><p>where α ii ∈ (I ii A , I ii P ) and I ii A and I ii P are the ii th entry of I A and I P , respectively. We therefore have</p><formula xml:id="formula_94">M0 − M 0 max,off ≤ max i =i 1 α ii • I A − I P max,off , M0 − M 0 max,diag ≤ max i 1 α ii • I A − I P max,diag .</formula><p>We now bound α −1 ii , I A − I P max,off and I A − I P max,diag .</p><p>Step 1 (Bounding max ii α −1 ii ): As α ii ∈ (I ii A , I ii P ) we have</p><formula xml:id="formula_95">max i,i 1 α ii ≤ max i,i 1 I ii A , 1 I ii P . (A.31)</formula><p>We first bound (I ii P ) −1 . In particular 1</p><formula xml:id="formula_96">I ii P = 1 t U t=t L (L − t)|P| • w (t) ii pi ≤ p i |P|w (t L ) ii as L − t ≥ 1 for all t L ≤ t ≤ t U ≤ L − 1. Now there exists constants c 0 and c 1 such that c 0 ≤ p ii ≤ c 1 for all (i, i ) ∈ [n] 2 .</formula><p>Therefore, by Lemma A2, we have</p><formula xml:id="formula_97">max i,i 1 I ii P n n 2 • 1 n = 1. (A.32)</formula><p>We then bound (I ii A ) −1 . We have</p><formula xml:id="formula_98">1 I ii A = 1 t U t=t L (L − t)|A| • ŵ(t) ii di ≤ d i |A| ŵ(t L )</formula><p>ii First suppose i = i . Then by Theorem 2, we have</p><formula xml:id="formula_99">max i =i |w (t L ) ii − ŵ(t L ) ii | = O P (n −3/2 log n) which implies, whp, that 0 ≤ w (t L ) ii − max i =i |w (t L ) ii − ŵ(t L ) ii | for all i = i . (A.33)</formula><p>As min i,i w</p><formula xml:id="formula_100">(t L ) ii max i,i w (t L ) ii</formula><p>n −1 we also have ŵ(t L )</p><p>ii n −1 for all i = i . Hence, by Lemma A2, we have</p><formula xml:id="formula_101">max i =i 1 I ii A ≤ max i =i d i |A| ŵ(t L ) ii max i d i • max i 1 nd i • max i =i 1 ŵ(t L ) ii = O P (1). (A.34)</formula><p>Now suppose that i = i . Then for t L = 2, we have</p><formula xml:id="formula_102">1 ŵ(2) ii = 1 n i =1 ŵii ŵi i = 1 n i =1 a ii d i a i i di = d i n i =1 a ii /d i .</formula><p>Once again, by Lemma A2, we have</p><formula xml:id="formula_103">max i 1 ŵ(2) ii ≤ max i d i 1 maxi di n i =1 a ii ≤ (max i d i ) 2 • max i 1 d i = O P (n),</formula><p>and hence max</p><formula xml:id="formula_104">i 1 I ii A max i d i • max 1 nd i • max i 1 ŵ(2) ii = O P (1). If t L ≥ 3, then max i |w (t L ) ii − ŵ(t L )</formula><p>ii | = O P n −3/2 log 1/2 n (see Theorem 2) and hence, using the same argument as that for deriving Eq. (A.34), we also have max i (I ii A ) −1 = O P (1). In summary, we have</p><formula xml:id="formula_105">max i,i 1 α ii = O P (1). (A.35)</formula><p>Step 2 (Bounding I A − I P max,off ): We start with the inequality</p><formula xml:id="formula_106">max i =i |I ii A − I ii P | = max i =i t U t=t L (L − t) |A| ŵ(t) ii d i − |P| w (t) ii p i ≤ t U t=t L (L − t) max i =i |A| ŵ(t) ii d i − |P| w (t) ii p i . (A.36)</formula><p>We now bound each of the summand in the RHS of the above display. Consider a fixed value of t ≥ 2. We have</p><formula xml:id="formula_107">max i =i |A| ŵ(t) ii d i − |P| w (t) ii p i ≤ (|A| − |P|) • max i =i w (t) ii d i + |P| max i =i (p i − d i )w (t) ii p i d i + |A| max i =i ŵ(t) ii − w (2) ii d i . (A.37)</formula><p>By Lemma A2, the first term in RHS of Eq. (A.37) is bounded as</p><formula xml:id="formula_108">(|A| − |P|) max i =i w (t) ii d i ≤ n max i |d i − p i | • max i,i w (t) ii • max i 1 d i = O P (n −1/2 log 1/2 n). (A.38)</formula><p>The second term in the RHS of Eq. (A.37) is also bounded by Lemma A2 as</p><formula xml:id="formula_109">|P| max i =i (p i − d i )w (t) ii p i d i ≤ |P| • max i p i − d i p i d i • max i,i w (t) ii = O P (n −1/2 log 1/2 n) (A.39)</formula><p>The third term is bounded by Theorem 2 and Lemma A2 as</p><formula xml:id="formula_110">|A| max i =i ŵ(t) ii − w (t) ii d i ≤ |A| max ı =i | ŵ(t) ii − w (t) ii | • max i 1 d i = O P n −1/2 log 1/2 n .</formula><p>Collecting the above terms and summing over the finite values of t L ≤ t ≤ t U , we obtain</p><formula xml:id="formula_111">I A − I P max,off = O P n −1/2 log 1/2 n . (A.40)</formula><p>Step 3 (Bounding I A − I P max,diag ): With a similar argument as Step 2, we consider</p><formula xml:id="formula_112">I A − I P max,diag ≤ t U t=2 (L − t) max i |A| ŵ(t) ii d i − |P| w (t) ii p i . (A.41)</formula><p>We once again bound each summand in the above display. Similar to Eq. (A.37), we have</p><formula xml:id="formula_113">max i |A| ŵ(t) ii d i − |P| w (t) ii p i ≤ (|A| − |P|) max i w (t) ii d i + |P| max i (p i − d i )w (t) ii p i d i + |A| max i ŵ(t) ii − w (t) ii d i . (A.42)</formula><p>The first two terms in the RHS of Eq. (A.42) is bounded via O P (n −1/2 log 1/2 n); see the arguments for Eq. (A.38) and Eq. (A.39). For the third term, we consider the cases t = 2 and t &gt; 2 separately. For t = 2, we have</p><formula xml:id="formula_114">|A| max i ŵ(2) ii − w (2) ii d i ≤ |A| max i | ŵ(2) ii − w (2) ii | • max i 1 d i = O P (1).</formula><p>In contrast, for t &gt; 2, we have</p><formula xml:id="formula_115">|A| max i ŵ(t) ii − w (t) ii d i = O P n −1/2 log 1/2 n</formula><p>Combining the above terms, we have</p><formula xml:id="formula_116">I A − I P max,diag = O P (1) if t L = 2, O P (n −1/2 log 1/2 n) if t L ≥ 3. (A.43)</formula><p>Step 4 (Bounding M0 − M 0 F ): In summary, Eq. (A.35), Eq. (A.40) and Eq. (A.43) imply</p><formula xml:id="formula_117">M0 − M 0 max,off ≤ max i =i 1 α ii • I A − I P max,off = O P (n −1/2 log 1/2 n), (A.44) M0 − M 0 max,diag ≤ max i 1 α ii • I A − I P max,diag = O P (1). (A.45)</formula><p>We thus conclude that</p><formula xml:id="formula_118">M0 − M 0 F ≤ n 2 M0 − M 0 2 max,off + n • M0 − M 0 2 max,diag 1/2 = O P (n 1/2 log 1/2 n), M0 − M 0 ∞ ≤ n M0 − M 0 max,off + M0 − M 0 max,diag = O P (n 1/2 log 1/2 n).</formula><p>(A.46) as desired. .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Proof of Theorem 4 (Dense Regime)</head><p>Recall that n k denote the number of vertices assigned to block k. For ease of exposition we shall assume that</p><formula xml:id="formula_119">n k = nπ k , for all k ∈ [K]. (A.47)</formula><p>The case where the n k are random with E[n k ] = nπ k is, except for a few slight modifications, identical. Note that M0 and M 0 are symmetric matrices. Indeed</p><formula xml:id="formula_120">(D −1 A Ŵt ) T = (D −1 A AD −1 A • • • D −1 A AD −1 A ) T = D −1 A Ŵt</formula><p>and similarly for (D −1 P W t ) = (D −1 P W t ). Recall from Eq. (2.6) that P = ZBZ has a K × K blocks structure. Now let B = VΥV be the eigendecomposition of B where V is a K × rank(B) matrix with orthornormal columns, i.e., V V = I and Υ is a diagonal matrix containing the non-zero eigenvalues of B. We then have, for any t ≥ 1,</p><formula xml:id="formula_121">D −1 P W t = D −1 P (ZBZ D −1 P t = D −1 P (ZVΥV Z D −1 P t = D −1 P ZV(ΥV Z D −1 P ZV) t−1 ΥV Z D −1 P (A.48)</formula><p>Now ZD −1 P Z is a K × K diagonal matrix and is positive definite. Therefore V Z D −1 P ZV is also positive definite. Let D := ΥV Z D −1 P ZV. Note that D is invertible and all of its eigenvalues are real-valued as it is similar to the matrix</p><formula xml:id="formula_122">(V Z D −1 P ZV) 1/2 Υ(V ZD −1 P ZV) 1/2 .</formula><p>We then have</p><formula xml:id="formula_123">M 0 := 2|P| κγ t U t=t L (L − t)D −1 P W t = 2|P| κγ D −1 P ZV t U t=t L (L − t)D t−1 f (D) ΥV Z D −1 P .</formula><p>Recall that t L ≤ t U ≤ L. Then f (D) is singular if and only if there is an eigenvalue λ of D such that t U t=t L (L − t)λ t−1 = 0. As the set of roots for any fixed polynomial equation has measure 0, we conclude that f (D) is invertible for almost every D, or equivalently that the set of matrix B whose induced f (D) is singular has Lebesgue measure 0 in the space of K × K symmetric matrices. Note that f (D) is guaranteed to be invertible whenever B is positive semidefinite.</p><p>We thus assume, without loss of generality, that f (D) is invertible. Then f (D)Υ is also invertible and hence Vf (D)ΥV is a symmetric K × K matrix with distinct rows. Indeed, suppose that there exists two rows k and k of Vf (D)ΥV that are the same. Let V k and V k be the kth and k th row vectors for V . Then the kth and kth rows of Vf (D)ΥV are V k f (D)ΥV and V k f (D)ΥV . We then have</p><formula xml:id="formula_124">V k f (D)ΥV = V k f (D)ΥV =⇒ (V k − V k )f (D)ΥV VΥf (D) (V k − V k ) = 0 =⇒ (V k − V k )f (D)Υ 2 f (D) (V k − V k ) = 0 which is only possible if V k − V k = 0 as f (D)Υ 2 f (D)</formula><p>is positive definite. As B has distinct rows, the rows of V are also distinct and hence V k − V k = 0 if and only if k = k . The matrix M 0 thus have the same K × K block structure as P.</p><p>Recall that the entries of M 0 are the logarithm of the entries of M 0 and hence M 0 is of the form</p><formula xml:id="formula_125">M 0 =    ξ 11 1 nπ1 1 T nπ1 . . . ξ 1K 1 nπ1 1 T nπ K . . . . . . . . . ξ K1 1 nπ K 1 T nπ1 . . . ξ KK 1 nπ K 1 T nπ K    = ZΞZ T ,</formula><p>where Ξ = (ξ ii ) K×K is a symmetric matrix with distinct rows. The non-zero eigenvalues of M 0 coincides with that of</p><formula xml:id="formula_126">Z ZΞ = ndiag(π 1 , π 2 , . . . , π K )Ξ. (A.49)</formula><p>As diag(π 1 , . . . , π K ) is fixed, it is left to study Ξ. Since the entries in P are Θ(1), we have |P| = Θ(n 2 ), and entries in D −1 P and W t are of order Θ(n −1 ) (see Lemma A2). Therefore each entry of M 0 is of order Θ(1). Indeed, with some more careful book-keeping, one can show that under Eq. (A.47) each entry of M 0 can take on one of K 2 possible values (with these values not depending on n). Thus Ξ is a fixed matrix not depending on n and hence, by Eq. (A.49), all non-zero singular values of M 0 grows at order n since diag(π 1 , π 2 , . . . , π K ) • Ξ is fixed. In summary we have</p><formula xml:id="formula_127">σ d (M 0 ) = Θ(n).</formula><p>In addition as M 0 has rank at most K, we have</p><formula xml:id="formula_128">M 0 F n, M 0 ∞ n (A.50)</formula><p>Therefore, by the Davis-Kahan Theorem <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b58">[59]</ref> and Theorem 3 (dense regime), we have</p><formula xml:id="formula_129">min T∈O d F • T − U F ≤ M0 − M 0 F σ d (M 0 ) = O P (n −1/2 log 1/2 n). (A.51)</formula><p>To show the 2 → ∞ norm concentration of F , we need to further bound U 2→∞ . Since M 0 shares the exact same K × K block structure as B (with B assumed to have distinct rows), we can follow the same argument as that in Section 5.1 in <ref type="bibr" target="#b36">[37]</ref> and show that [U] i = [U] i if and only if k(i) = k(i ) which also implies U 2→∞ n −1/2 . Summarizing above results, by Theorem 3 (dense regime) and Theorem 4.2 in <ref type="bibr" target="#b35">[36]</ref>, we have</p><formula xml:id="formula_130">min T∈O d F • T − U 2→∞ ≤ 14 M0 − M 0 ∞ σ d (M 0 ) U 2→∞ = O P n −1 log 1/2 n , (A.52)</formula><p>with high probability. We thus have</p><formula xml:id="formula_131">n 1/2 • min T∈O d F • T − U 2→∞ = O P n −1/2 log 1/2 n .</formula><p>Hence, by combining Lemma 5.1 and the arguments in Section 5.1 of <ref type="bibr" target="#b36">[37]</ref>, we can show that running K-medians or K-means on the rows of Û will, with high probability, correctly recover the memberships of every nodes in G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Strong recovery for DCSBM</head><p>We now extend the proof of Theorem 4 to the setting of DCSBM. Recall that the edge probabilities matrix of a DCSBM is of the form P = ΘZBZ Θ where Θ = diag(θ 1 , . . . , θ n ) contains the degree correction factors. Substituting the above P into Eq. (A.48) we obtain</p><formula xml:id="formula_132">D −1 P W = D −1 P (ΘZBZ ΘD −1 P ) t = D −1 P ΘZV(ΥV Z ΘD −1 P ΘZV) t−1 ΥV Z ΘD −1 P (A.53)</formula><p>The matrix Z ΘD −1 P ΘZ is now a K × K positive definite diagonal matrix and hence V Z ΘD −1 P ΘZV is also positive definite. Let D = ΥV Z ΘD −1 P ΘZV. We then have</p><formula xml:id="formula_133">M 0 := 2|P| κγ t U t=t L (L − t)D −1 P W t = 2|P| κγ D −1 P ΘZVf (D)ΥV Z ΘD −1 P</formula><p>Once again the set of matrices B for which f (D) is singular has Lebesgue measure 0 in the space of K × K matrices and hence we can assume, without loss of generality, that f (D) is invertible. Using the same reasoning as that in the proof of Theorem 4 we conclude that M 0 is of the form</p><formula xml:id="formula_134">M 0 = D −1 P ΘZΞ Z ΘD −1 P</formula><p>where Ξ is a K × K matrix with distinct rows. Now for any vertices i we have</p><formula xml:id="formula_135">p i = j p ij = j θ i θ j b τ (i),τ (j) = θ i K k=1 τ (j)=k θ j b τ (i),k</formula><p>and hence θ i /p i = θ i /p i whenever i and i belong to the same community. The ith entry of D −1 P Θ is θ i /p i and thus</p><formula xml:id="formula_136">D −1 P ΘZ = Z Θ</formula><p>for some K × K positive definite diagonal matrix Θ. In summary we have</p><formula xml:id="formula_137">M 0 = D −1 P ΘZΞZ D −1 P = Z ΘΞ ΘZ .</formula><p>where ΘΞ Θ is a K × K matrix with distinct rows. Again recall that the entries of M 0 are the logarithm of the entries of M 0 and hence M 0 is of the form M 0 = ZΞZ T , for some K × K matrix Ξ. Let F be the truncated SVD of M0 and U be the leading singular vectors of M 0 . We can then follow the remaining steps in the proof of Theorem 4 to show that</p><formula xml:id="formula_138">min T∈O d F • T − U F ≤ M0 − M 0 F σ d (M 0 ) = O P (n −1/2 log 1/2 n), min T∈O d F • T − U 2→∞ ≤ 14 M0 − M 0 ∞ σ d (M 0 ) U 2→∞ = O P n −1 log 1/2 n ,</formula><p>noting that the involved high-probability bounds still hold for DCSBM as long as max i θ i / min i θ i = O(1) and θ i ∈ (0, 1), because we are using entry-wise arguments in the main proofs. Thus clustering the rows of F will, with high probability, exactly recover the memberships of every nodes in G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B PROOFS UNDER THE SPARSE REGIME</head><p>The approach used in the proofs of our main theorems under the sparse regime is similar to that in the dense regime. However, if we simply use the same technique as in Section A then we only obtain a convergence rate of O P {(nρ n ) −3/2 log 1/2 n} for Ŵ2 − Ŵ2 max,off and Ŵt − Ŵt max (with t ≥ 3), which is too loose. More specifically the bounds for M0 −M F and M0 −M ∞ Eq. (A.33) is currently valid only when ρ n = ω(n −1/3 log 1/3 n). Before getting into the formal proofs, we first state Lemma B1 and Lemma B2 as the main technical results for bounding Ŵ2 − Ŵ2 max,off and Ŵt − Ŵt max for t ≥ 3 under the sparse regime. We summarize the motivation behind these lemmas below.</p><p>• Lemma B1 is an analogue of Lemma A2 and is used repeatedly for bounding several important terms that frequently appear in our proofs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>In Section A we show that the bound for Ŵt − Ŵt max when t ≥ 3 is of the same order as that for Ŵ2 − Ŵ2 max,off . For the sparse regime these bounds are generally of different magnitude as nρ n increases. This difference is the main distinguish feature between the two regimes. Therefore, we derive a more accurate bound for Ŵt − Ŵt max when t ≥ 3 in Step 4 and Step 5 of the proof of Theorem 3 (sparse regime) presented below. The main challenge is in controlling the term A t max as given in Lemma B2. Thus Lemma B2 is the main technical contribution of this section and might be of independent interest.</p><p>For ease of exposition we will only present the proof of Lemma B2 in this section; the proofs of the other lemmas are deferred to Section C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma B1.</head><p>Under Assumption 1 we have, for any t ≥ 1 and sufficiently large n,</p><formula xml:id="formula_139">D A = max 1≤i≤n d i = O P (nρ n ), (B.1)</formula><formula xml:id="formula_140">D −1 A = max 1≤i≤n 1/d i = O P {(nρ n ) −1 }, (B.2) D A − D P = max i |d i − p i | = O P ( nρ n log n), (B.3) D −1 A − D −1 P = max i |d −1 i − p −1 i | = O P {(nρ n ) −3/2 log n}, (B.4) Ŵt max = O P {(nρ n ) −1 }, (B.5) max i,i w (η)</formula><p>ii , min Lemma B2. Under Assumption 1, suppose ρ n satisfies ρ n → 0 and,</p><formula xml:id="formula_141">n −1/2 log β2 n ρ n (B.7)</formula><p>for some β 2 &gt; 1/2. Then we have</p><formula xml:id="formula_142">A 2 max,off = O P (nρ 2 n ), A t max = O P (nρ n ) t/2 + n t−1 ρ t n , (B.8)</formula><p>for any fixed t ≥ 3. Furthermore, if ρ n → 0 and n</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2−t t</head><p>ρ n for some t ≥ 3, then above bounds can be sharpened to</p><formula xml:id="formula_143">A t max = O P n t−1 ρ t n .</formula><p>Proof. Suppose ρ n satisfies Eq. (B.7) for some β 2 &gt; 1/2. Define</p><formula xml:id="formula_144">ζ ii i * := a ii * a i i * , ζ ii := max i,i i * =i,i a ii * a i i * = max i,i i * =i,i ζ ii i * . (B.9) Given i = i , the {ζ ii i * } i * ∈[n],i * =i,i are a set of independent Bernoulli variables with c 2 2 ρ 2 n ≤ P(ζ ii i * = 1) ≤ c 2 3 ρ 2 n .</formula><p>A similar argument to that for deriving Eq. (C.4) yields (recall that n −1/2 log β2 n ρ n )</p><formula xml:id="formula_145">log P ζ ii n − 1 ≤ c 2 2 2 ρ 2 n ≤ log P ζ ii ≤ 1 2 Eζ ii −C 1 nρ 2 n −C 1 log 2β2 n (B.<label>10</label></formula><formula xml:id="formula_146">)</formula><p>for all i = i ; here C 1 ≥ 0 is a constant not depending on n or ρ n . Eq. (B.10) together with a union bound then implies</p><formula xml:id="formula_147">P max i =i ζ ii ≤ c 2 2 2 nρ 2 n ≤ n 2 max i =i P ζ ii ≤ c 2 2 2 nρ 2 n exp 2 log n − C 1 log 2β2 n → 0 as n −→ ∞. We thus have max i =i i * =i,i a ii * a i i * = O P (nρ 2 n ) and hence A 2 max,off = max i =i n i * =1 a ii * a i i * ≤ max i =i i * =i,i a ii * a i i * + 2 = O P (nρ 2 n ).</formula><p>We next consider the case when t ≥ 3 and ρ n satisfies n 2−t t ρ n . We have</p><formula xml:id="formula_148">A t max ≤ P t max + A t − P t max .</formula><p>Under Assumption 1 we have</p><formula xml:id="formula_149">P t max ≤ n P t−1 max P max ≤ n 2 P t−2 max P 2 max ≤ • • • ≤ n t−1 P t max = O(n t−1 ρ t n ).</formula><p>We now focus on bounding A t − P t max . Consider the following expansion for A t − P t</p><formula xml:id="formula_150">A t − P t = (A t−1 − P t−1 )(A − P) + P t−1 (A − P) + t−1 b0=1 A t−1−b0 (A − P)P b0 . (B.11) Let E = A − P.</formula><p>Applying the same expansion to A t−1 − P t−1 , . . . , A 2 − P 2 , we obtain</p><formula xml:id="formula_151">A t − P t = (A t−1 − P t−1 )E + P t−1 E + t−1 b0=1 A t−1−b0 (A − P)P b0 = t−2 b1=0 A t−2−b1 EP b1 E + P t−1 E + t−1 b0=1 A t−1−b0 EP b0 = (A t−2 − P t−2 )E 2 + t−2 b1=1 A t−2−b1 EP b1 E + t−1 b0=1 A t−1−b0 EP b0 + 2 b =1 P t−b E b = • • • = E t + t−1 c=0 t−c−1 b=1 A t−c−b−1 EP b E c L1 + t−1 b =1 P t−b E b L2 . (B.12)</formula><p>Now for any summand appearing in L 1 , if both c = 0 and t − c − b − 1 = 0 then</p><formula xml:id="formula_152">A t−c−b−1 EP b E c max ≤ A 1 • A t−c−b−2 EP b E c max ≤ • • • ≤ A t−c−b−1 1 EP b E c max ≤ A t−c−b−1 1 EP b E c−1 max ( A 1 + P 1 ) ≤ A t−c−b−1 1 EP b max ( A 1 + P 1 ) c . (B.13)</formula><p>Here M 1 denote the maximum of the absolute column sum of a matrix M. The bound in Eq. (B.13) also holds when c = 0 or t − c − b − 1 = 0. A similar argument yields</p><formula xml:id="formula_153">EP b max ≤ P max ( A 1 + P 1 ) P b−1 1 . (B.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14)</head><p>Observe that A 1 = max i d i and P 1 = max i j p ij . Combining Eq. (B.13), Eq. (B.14) and Lemma B1, we have</p><formula xml:id="formula_154">L 1 max ≤ t−1 a=0 t−c−1 b=1 A t−c−b−1 EP b E c max = O P (n t−1 ρ t n ).</formula><p>Similarly, we also have L 2 max = O P (n t−1 ρ t n ). We therefore have</p><formula xml:id="formula_155">A t max ≤ P t max + E t max + L 1 max + L 2 max = E t max + O P (n t−1 ρ t n ). (B.15)</formula><p>Finally we bound E t max . Note that M max ≤ M 2 for any matrix M. Now, under Assumption 1, the maximal expected degree of G is of order nρ n n 1−β log 4 n. We can thus apply the spectral norm concentration in <ref type="bibr" target="#b59">[60]</ref> to obtain</p><formula xml:id="formula_156">E t max ≤ A − P t 2 = O P (nρ n ) t/2 . (B.16)</formula><p>We thus have, after a bit of algebra, that E t max = O P (n t−1 ρ t n ) whenever n 2−t t ρ n . Combining Eq. (B.15), we have</p><formula xml:id="formula_157">A t max = O P (n t−1 ρ t n ) whenever n 2−t t ρ n ≺ 1 holds.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Proof of Theorem 3 (Sparse Regime)</head><p>The proof is organized as follows. In Step 1 through Step 4, we bound Ŵt −W t max under the general sparse condition as specified in Assumption 1. These arguments are generalizations of the corresponding arguments in the proof of Theorem 2 in Section A.1. In Step 5, we provide an improved bound for Ŵt − W t max when t ≥ 3 and t−3 t−1 &gt; β. For ease of exposition we omitted some of the more mundane technical details from the current proof and refer the interested reader to Section C.4.</p><p>Step 1 (Bounding Ŵ − W max ): Similar to Step 1 in the proof of Theorem 2, we have</p><formula xml:id="formula_158">Ŵ − W = AD −1 A − PD −1 P = AD −1 P D −1 A (D P − D A ) ∆ (1)<label>1</label></formula><formula xml:id="formula_159">+ (A − P)D −1 P ∆<label>(1) 2</label></formula><p>(B.17 and hence ∆</p><p>(1)</p><formula xml:id="formula_160">1 max = O P (nρ n ) −3/2 log 1/2 n , ∆<label>(1) 2 max</label></formula><formula xml:id="formula_161">= O P (nρ n ) −1 . We therefore have Ŵ − W max ≤ ∆ (1) 1 max + ∆ (1) 2 max = O P (nρ n ) −3/2 log 1/2 n + O P (nρ n ) −1 = O P (nρ n ) −1 . (B.18)</formula><p>See Section C.4.1 for additional details in deriving the above inequalities.</p><p>Step 2 (Bounding Ŵ2 − W 2 max,diag ): Similar to Step 2 in the proof of Theorem 2,</p><formula xml:id="formula_162">Ŵ2 − W 2 = ( Ŵ − W)W ∆ (2) 1 + Ŵ( Ŵ − W) ∆ (2) 2 , ∆<label>(2)</label></formula><formula xml:id="formula_163">1 = AD −1 P D −1 A (D P − D A ) − AD −2 P (D P − D A ) W ∆ (2,1) 1 + AD −2 P (D P − D A ) ∆ (2,2) 1 W + (A − P)D −1 P W ∆ (2,3) 1 . (B.19)</formula><p>The bounds in Section C.4.2 imply</p><formula xml:id="formula_164">∆ (2) 1 max = O P n −3/2 ρ −1/2 n log 1/2 n , W(W − Ŵ) max = O P (n −3/2 ρ −1/2 n log 1/2 n).</formula><p>Furthermore, we also have</p><formula xml:id="formula_165">∆ (2) 2 max = (W − Ŵ)(W − Ŵ) − W(W − Ŵ) max ≤ (W − Ŵ) 2 max + W(W − Ŵ) max . (B<label>.20)</label></formula><p>Replacing • max with • max,diag in Eq. (A.16), and following the same argument as that for Eq. (A.16) and Eq. (A.17), we have, by Lemma B1,</p><formula xml:id="formula_166">(W − Ŵ) 2 max,diag = O P {(nρ n ) −1 }, (B.21)</formula><p>and thus</p><formula xml:id="formula_167">W 2 − Ŵ2 max,diag = O P {(nρ n ) −1 }. Step 3 (Bounding Ŵ2 − W 2</formula><p>max,off ): Similar to Eq. (A.16),</p><formula xml:id="formula_168">(W − Ŵ) 2 max,off = max i =i n i * =1 p ii * p i * − a ii * d i * p i * i p i − a i * i d i ≤ max i =i n i * =1 a ii * p i * − a ii * d i * p i * i p i − a i * i p i δ<label>(2,1) 2,off</label></formula><formula xml:id="formula_169">+ max i =i n i * =1 p ii * p i * − a ii * d i * a i * i p i − a i * i d i δ (2,2) 2,off + max i =i n i * =1 p ii * p i * − a ii * p i * p i * i p i − a i * i p i δ<label>(2,3) 2,off . (B.22)</label></formula><p>We then have the bounds</p><formula xml:id="formula_170">δ (2,1) 2,off = O P (nρ n ) −2 log n , δ<label>(2,2) 2</label></formula><formula xml:id="formula_171">,off = O P (nρ n ) −2 log n , δ<label>(2,3) 2</label></formula><formula xml:id="formula_172">,off = O P n −3/2 ρ −1 n log 1/2 n . (B.23)</formula><p>For succinctness we only derive the bound for δ </p><formula xml:id="formula_173">n i * =1 a ii * p i * − a ii * d i * p i * i p i − a i * i p i = 1 p i n i * =1 a ii * (d i * − p i * ) 2 p 2 i * d i * + p i * − d i * p 2 i * a i * i − p i * i , (B.24)</formula><p>and hence</p><formula xml:id="formula_174">δ (2,1) 2,off ≤ max i =i 1 p i n i * =1 a ii * (d i * − p i * ) 2 p 2 i * d i * p i * i − a i * i ξ ii + max i =i 1 p i n i * =1 a ii * d i * − p i * p 2 i * p i * i − a i * i ζ ii . (B.25)</formula><p>For the term ξ ii , by Lemma B1, we have</p><formula xml:id="formula_175">max |ξ ii | ≤ max i 1 p i max i * (d i * − p i * ) 2 p 2 i * d i * 2 + max i =i n i * =1 i * =i,i |a ii * ||p i * i − a i * i | ≤ O P (nρ n ) −3 log n 2 + max i =i i * : a ii * = a i i * = 1 + P max • max i =i i * : a ii * = 1, a i i * = 0 ≤ O P (nρ n ) −3 log n 2 + max i d i + P max max i d i = O P (nρ n ) −2 log n .</formula><p>For the term ζ ii , first let e ii = a ii − p ii . Then expanding d i * − p i * as i * * e i * i * * , we have</p><formula xml:id="formula_176">ζ ii = 1 p i n i * =1 a ii * p 2 i * i * * ∈{i ,i} −e i * i * * e i * i − i * * ∈{i ,i} e i * i * * e i * i . (B.26)</formula><p>Now, for fixed i, i , since a ii = p ii = 0 for all i, we have</p><formula xml:id="formula_177">1 p i n i * =1 a ii * p 2 i * i * * ∈{i ,i} e i * i * * e i * i = 1 p i i * ∈{i,i } i * * ∈{i ,i,i * } 1 p 2 i * a ii * e i * i * * e i * i = 1 p i (i * ,i * * )∈T (i,i ) a ii * e i * i p 2 i * + a ii * * e i * * i p 2 i * * e i * i * * := S(a i , a i ). (B.27) Here T (i, i ) = (i * , i * * )|i * &lt; i * * , i * ∈ {i, i }, i * * ∈ {i, i }} and a i = (a i1 , . . . , a in ).</formula><p>Let us now, in addition to conditioning on P, also condition on both a i and a i . Then the sum for S(a i , a i ) in Eq. (B.27) is a sum of independent, mean zero random variables, i.e., once we conditioned on P, a i , and a i , the e i * i * * for (i * , i * * ) ∈ T (i, i ) are independent. We therefore have</p><formula xml:id="formula_178">Var 1 p i T (i,i ) a ii * e i * i p 2 i * + a ii * * e i * * i p 2 i * * e i * i * * a, a = 1 p 2 i T (i,i ) a ii * e i * i p 2 i * + a ii * * e i * * i p 2 i * * 2 Var[e i * ,i * * ] (nρ n ) −6 • (nρ n ) • (d i + d i ) (B.28)</formula><p>Furthermore, we also have</p><formula xml:id="formula_179">1 p i a ii * e i * i p 2 i * + a ii * * e i * * i p 2 i * * e i * i * * (nρ n ) −3 .</formula><p>Therefore, by Bernstein inequality, for any c &gt; 0 there exists a constant C &gt; 0 such that, for all sufficiently large n,</p><formula xml:id="formula_180">P S(a i , a i ) ≥ C (nρ n ) −5/2 (d i + d i ) 1/2 log 1/2 n | a i , a i ≤ n −c (B.29)</formula><p>We can now uncondition with respect to a i and a i . More specifically, for any t, we have</p><formula xml:id="formula_181">P S(a i , a i ) ≥ t ≤ P S(a i , a i ) ≥ t | max{d i , d i } &lt; Cnρ n P max{d i , d i } &lt; Cnρ n + P max{d i , d i } ≥ Cnρ n ,</formula><p>where C is any arbitrary positive constant. Now let c be arbitrary. Then by Lemma B1 and Eq. (B.29), together with taking t = C (nρ n ) −2 log 1/2 n for some constant C , we have</p><formula xml:id="formula_182">P S(a i , a i ) ≥ C (nρ n ) −2 log 1/2 n ≤ 2n −c .</formula><p>A union bound over the n 2 possible choices of a i and a i yields</p><formula xml:id="formula_183">max i =i S(a i , a i ) = O P (nρ n ) −2 log 1/2 n . (B.30)</formula><p>Finally we also have</p><formula xml:id="formula_184">max i,i 1 p i n i * =1 a ii * p 2 i * i * * ∈{i,i } e i * i * * e i * i max i d i • max i 1/p i 3 = O P {(nρ n ) −2 }. (B.31)</formula><p>In summary, we have</p><formula xml:id="formula_185">max ii ζ ii = O P ((nρ n ) −2 log 1/2 n) and hence δ (2,1) 2,off ≤ max ii ξ ii + max ii ζ ii = O P (nρ n ) −2 log n . (B.32)</formula><p>Given the previous claim together with an argument similar to that for Eq. (A.18), we obtain</p><formula xml:id="formula_186">W 2 − Ŵ2 max,off ≤ ∆ (2) 1 max + W(W − Ŵ) max + δ (2,1) 2,off + δ (2,2) 2,off + δ (2,3) 2,off = O P max (nρ n ) −2 log n, n −3/2 ρ −1 n log n = O P (n −3/2 ρ −1 n log 1/2 n) if 0 ≤ β &lt; 1/2 O P ((nρ n ) −2 log n) otherwise. (B.33)</formula><p>Step 4 (Bounding Ŵt − W t max for t ≥ 3): The following argument is a generalization of the argument in Step 3 of Section A.1. We first consider t = 3. We have</p><formula xml:id="formula_187">Ŵ3 − W 3 max ≤ Ŵ2 − W 2 Ŵ max + W 2 Ŵ − W max (B.34)</formula><p>For the first term in the RHS, by Eq. (B.21) and Lemma B1, we have</p><formula xml:id="formula_188">( Ŵ2 − W 2 ) Ŵ max ≤ max i d i • Ŵ max Ŵ2 − W 2 max,off = O P (1) Ŵ2 − W 2 max,off = O P max (nρ n ) −2 log n, n −3/2 ρ −1 n log 1/2 n . (B.35)</formula><p>For the second term in the RHS of Eq. (B.34), we apply the similar technique for ∆</p><p>(2,1) 1</p><p>in Step 2 and deduce </p><formula xml:id="formula_189">W 2 ( Ŵ − W) max = O P (1) W 2 − Ŵ2 max,</formula><formula xml:id="formula_190">Ŵ3 − W 3 max = O P max (nρ n ) −2 log n, n −3/2 ρ −1 n log 1/2 n . (B.37)</formula><p>Now for t = 4, we have</p><formula xml:id="formula_191">Ŵ4 − W 4 = ( Ŵ2 − W 2 ) Ŵ2 + W 2 ( Ŵ2 − W 2 ) = ( Ŵ2 − W 2 ) Ŵ2 − (W 2 − Ŵ2 ) 2 + Ŵ2 ( Ŵ2 − W 2 ). (B<label>.38)</label></formula><p>For ( Ŵ2 − W 2 ) Ŵ2 , by Eq. (B.35) and Lemma B1</p><formula xml:id="formula_192">( Ŵ2 − W 2 ) Ŵ2 max ≤ max i d i • Ŵ max • ( Ŵ2 − W 2 ) Ŵ max = O P (nρ n • (nρ n ) −1 ) • O P (1) W 2 − Ŵ2 max,off = O P (1) W 2 − Ŵ2 max,off . (B.39)</formula><p>Similarly, we have Ŵ2 ( Ŵ2 − W 2 ) max = O P (1) W 2 − Ŵ2 max,off . Furthermore,</p><formula xml:id="formula_193">( Ŵ2 − W 2 ) 2 max ≤ n Ŵ2 − W 2 2 max,off + Ŵ2 − W 2 2 max,diag = O P (nρ n ) −2 log n . (B.40)</formula><p>Combining Eq. (B.38), Eq. (B.39) and Eq. (B.40), we obtain</p><formula xml:id="formula_194">Ŵ4 − W 4 max ≤ O P (1) ( Ŵ2 − W 2 ) Ŵ2 max + (W 2 − Ŵ2 ) 2 max = O P max (nρ n ) −2 log n, n −3/2 ρ −1 n log 1/2 n . (B.41)</formula><p>For any t ≥ 5, we can write Ŵt − W t as</p><formula xml:id="formula_195">Ŵt − W t = ( Ŵ2 − W 2 ) Ŵt−2 ∆ (3) 1 + W 2 ( Ŵt−2 − W t−2 ) ∆<label>(3) 2</label></formula><p>.</p><p>We first consider ∆</p><p>1 . By Lemma B1 and Eq. (B.35)</p><formula xml:id="formula_197">∆ (3) 1 max ≤ max i d i • Ŵ max • ( Ŵ2 − W 2 ) Ŵt−3 max ≤ • • • ≤ max i d i Ŵ max t−3 ( Ŵ2 − W 2 ) Ŵ max = O P max (nρ n ) −2 log n, n −3/2 ρ −1 n log 1/2 n . (B.42)</formula><p>For ∆</p><p>2 , by Lemma B1</p><formula xml:id="formula_199">∆ (3) 2 max ≤ n • W 2 max Ŵt−2 − W t−2 max = O(1) Ŵt−2 − W t−2 max . (B.43)</formula><p>Similar to Eq. (A.29), we obtain</p><formula xml:id="formula_200">Ŵt − W t max = O P max (nρ n ) −2 log n, n −3/2 ρ −1 n log 1/2 n + O(1) Ŵ4 − W 4 max if t is even O(1) Ŵ3 − W 3 max , if t is odd (B.44)</formula><p>Eq. (B.37) and Eq. (B.41) then implies</p><formula xml:id="formula_201">Ŵt − W t max = O P max (nρ n ) −2 log n, n −3/2 ρ −1 n log 1/2 n = O P (n −3/2 ρ −1 n log 1/2 n) if 0 ≤ β &lt; 1/2 O P ((nρ n ) −2 log n) otherwise.</formula><p>Step 5 (Bounding Ŵt − W t max for t ≥ 4 and t−3 t−1 ≥ β): Similar to Eq. (B.11) and Eq. (B.12) in the proof of Lemma B2, we write</p><formula xml:id="formula_202">Ŵt − W t = ( Ŵ − W) t + t−1 r=0 t−r−1 s=1 Ŵt−r−s−1 ( Ŵ − W)W s ( Ŵ − W) r + t−1 r =1 W t−r ( Ŵ − W) r . (B.45)</formula><p>In Step 2, we shown</p><formula xml:id="formula_203">( Ŵ − W)W max = O P n −3/2 ρ −1/2 n √ log n . Let d max = max i d i .</formula><p>A similar argument to that for L 1 in the proof of Lemma B2 yields, for 0 ≤ r ≤ t − 1 and 1</p><formula xml:id="formula_204">≤ s ≤ t − r − 1, that Ŵt−r−s−1 ( Ŵ − W)W s ( Ŵ − W) r ≤ d max Ŵ max t−r−s−1 ( Ŵ − W)W s max (n W max + d max Ŵ − W max ) r = O P ( Ŵ − W)W s max = O P n s−1 ( Ŵ − W)W max W s−1 max = O P ( Ŵ − W)W max = O P n −3/2 ρ −1/2 n log 1/2 n ,</formula><p>where the second inequality follows from Lemma B1 and Eq. (B.18) and the last inequality follows from Lemma B1. As t is finite, we have</p><formula xml:id="formula_205">t−1 r=0 t−r−1 s=1 Ŵt−r−s−1 ( Ŵ − W)W s ( Ŵ − W) r max = O P n −3/2 ρ −1/2 n log 1/2 n . Similarly, t−1 r =1 W t−r ( Ŵ − W) r max = O P n −3/2 ρ −1/2 n log 1/2 n .</formula><p>Recalling Eq. (B.45), we obtain</p><formula xml:id="formula_206">Ŵt − W t max ≤ ( Ŵ − W) t max + O P n −3/2 ρ −1/2 n log 1/2 n . (B.46)</formula><p>Now we focus on ( Ŵ − W) t . We start with the polynomial expansion</p><formula xml:id="formula_207">( Ŵ − W) t = A(D −1 A − D −1 P ) + (A − P)D −1 P t = (A − P)D −1 P t + c∈{1,2} t c =(1,••• ,1) t r=1 Ξ cr .</formula><p>Here c r represents the rth coordinate of c = (c 1 , . . . , c t ) ∈ {1, 2} t and</p><formula xml:id="formula_208">Ξ cr = (A − P)D −1 P , if c r = 1 A(D −1 A − D −1 P ), if c r = 2.</formula><p>We note that there are 2 t − 1 distinct c = (1, 1, . . . , 1). Now for any given c, let r * = r * (c) be the smallest value of r such that c r = 2. We emphasize that r * depends on c; however, for simplicity of notation we make this dependency implicit. We further denote </p><formula xml:id="formula_209">(Ξ 1 cr , Ξ 2 cr ) := (AD −1 A , −AD −1 P ), if c r = 1, (AD −1 P , −PD −1 P ), if c r = 2, so that Ξ cr = Ξ 1 cr + Ξ 2 cr . Given c, we could write t r=1 Ξ cr = j&lt;r * (Ξ 1 cj + Ξ 2 cj ) • Ξ c r * • k&gt;r * (Ξ 1 c k + Ξ 2 c k ) = m∈{1,2} t j&lt;r * Ξ mj cj • Ξ c r * • k&gt;r * Ξ m k c k m,c<label>(</label></formula><formula xml:id="formula_210">ξ cj ,mj ij−1ij ϑ cj ,mj ij−1ij • a i r * −1 ir * 1 d i r * − 1 p i r * k&gt;r * ξ c k ,m k i k−1 i k • ϑ c k ,m k i k−1 i k ,</formula><p>where, with a slight abuse of notation, we denote i = (i 1 , . . . , i t−1 ) ∈ {1, . . . , n} t−1 , i 0 = i and i t = i . We thus have</p><formula xml:id="formula_211">| m,c ii | ≤ i a i r * −1 i r * j =r * ξ cj ,mj ij−1ij • 1 d i r * − 1 p i r * j =r * ϑ cj ,mj ij−1ij ≤ D −1 A s(c,m) D −1 P t−1−s(c,m) • D −1 A − D −1 P i a i r * −1i r * j =r * ξ cj ,mj ij−1ij .</formula><p>Here s(c, m) is the number of indices r with r = r * and (c r , m r ) = (1, 1). Now, by Lemma B1 and Assumption 1, we have</p><formula xml:id="formula_212">m,c max = O P (nρ n ) −1/2−t (log n) 1/2 • max ii i a i r * −1i r * j =r * ξ cj ,mj ij−1ij . (B.49)</formula><p>with the convention i 0 = i and i t = i . Now define a matrix-valued function ξ A,P (•) by</p><formula xml:id="formula_213">ξ A,P (c r , m r ) = A if (c r , m r ) ∈ {(1, 1), (1, 2), (2, 1)}, P if (c r , m r ) = (2, 2), Also define ξ m,c A,P = j&lt;r * ξ A,P (c j , m j ) • A • k&gt;r * ξ A,P (c k , m k ) .</formula><p>Then by the definition of the ξ cr,m ii in Eq. (B.48), we have</p><formula xml:id="formula_214">ξ m,c A,P max = max ii i a i r * −1i r * j =r * ξ cj ,mj ij−1ij . (B.50)</formula><p>We now consider two cases to bound ξ m,c A,P max . Case 1: Suppose that for the given m, c, there exists at least one index r = r * with (c r , m r ) = 2, i.e., the matrix P appears at least once among the collection of ξ A,P (c r , m r ) for r = r * . Then ξ m,c</p><p>A,P must have the form</p><formula xml:id="formula_215">• • • • • • g(c,m) matrices PA • • • • • • t−2−g(c,m) matrices , or • • • • • • g(c,m) matrices AP • • • • • • t−2−g(c,m) matrices . (B.51)</formula><p>Note that it is possible that g(c, m) = 0 or g(c, m) = t − 2 as A could be either the first or last matrix in the product ξ m,c A,P . Consider the first form in Eq. (B.51). We have</p><formula xml:id="formula_216">ξ m,c A,P max = • • • • • • g(c,m) matrices PA • • • • • • t−2−g(c,m) matrices max (1) ≤                  n max i,i p ii • • • • • • • g(c,m)−1 matrices PA • • • • • • t−2−g(c,m) matrices (if first matrix is P) max i d i • • • • • • • g(c,m)−1 matrices PA • • • • • • t−2−g(c,m) matrices (if first matrix is A) (1) = O P nρ n • • • • • • g(c,m−1) matrices PA • • • • • • t−2−g(c,m) matrices<label>(1)</label></formula><p>≤ . . .</p><formula xml:id="formula_217">≤ O P (nρ n ) g(c,m) PA • • • • • • t−2−g(c,m) matrices<label>(1)</label></formula><formula xml:id="formula_218">≤ O P (nρ n ) g(c,m)+1 PA • • • • • • t−3−g(c,m) matrices<label>(2)</label></formula><p>≤ . . .</p><formula xml:id="formula_220">≤ O P (nρ n ) t−2 PA max ≤ O P (nρ n ) t−2 max i,i p ii max i d i = O P (n t−1 ρ t n ).<label>(2)</label></formula><p>(B.52)</p><p>In the above inequality, all relationships with</p><p>≤ and</p><p>(2)</p><p>≤ are when we removed either P or A from before and after the term PA, respectively. An identical argument also yields</p><formula xml:id="formula_222">ξ m,c A,P max = O P (n t−1 ρ t n ) (B.53)</formula><p>for the second form in Eq. (B.51).</p><p>Case 2: Suppose now that, for all r = r * , we have (c r , m r ) = (2, 2), i.e., ξ A,P (c r , m r ) = A for all r = r * . Then ξ m,c A,P = A t and, by the fact that t− </p><formula xml:id="formula_223">= O P 2 t ξ m,c A,P max (nρ n ) −1/2−t log 1/2 n = O P (nρ n ) −1/2−t log 1/2 n • O P n t−1 ρ t n = O P n −3/2 ρ −1/2 n log 1/2 n . (B.55)</formula><p>We then have, by Eq. (B.46),</p><formula xml:id="formula_224">Ŵt − W t max ≤ ( Ŵ − W) t max + O P n −3/2 ρ −1/2 n log 1/2 n ≤ {(A − P)D −1 P } t max + c∈{1,2} t c =(1,••• ,1) t r=1 Ξ cr max + O P n −3/2 ρ −1/2 n log 1/2 n ≤ (A − P)D −1 P t max + O P n −3/2 ρ −1/2 n log 1/2 n . (B.56)</formula><p>The last inequality in the above display follows from the fact that the number of distinct c is 2 t − 1 which is finite and does not depend on n. Finally we focus on (A − P)D −1 P t max . An argument similar to that for (A − P) t max in the proof of Lemma B2 yields </p><formula xml:id="formula_225">(A − P)D −1 P t max ≤ A − P t 2 • D −1 P t 2 (nρ n ) −t A − P t 2 (By Lemma B1) = O P (nρ n ) −t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Proof of Theorem 3 (Sparse Regime)</head><p>We will only present the proof of bounding M0 − M 0 F and M0 − M 0 ∞ here as the rates for M 0 F and M 0 ∞ are derived in the proof of Theorem 4 (c.f. Eq. (B.65)). For sufficient large t U &gt; 0 such that, w.h.p. M0 is well defined, we use the same notations of I A , I P as defined in Eq. (A.30). Then we also have</p><formula xml:id="formula_226">M0 − M 0 max,off ≤ max i,i 1 α ii • I A − I P max,off , M0 − M 0 max,diag ≤ max i,i 1 α ii • I A − I P max,diag , (B.59)</formula><p>where α ii ∈ (min{I ii A , I ii P }, max{I ii A , I ii P }), I ii A , I ii P are ii th entries of I A , I P . We now extend the argument in the dense regime to the sparse regime.</p><p>Step 1 (Bounding max i,i α −1 ii ): We also have α ii is between I ii A and I ii P and max i,i</p><formula xml:id="formula_227">1 α ii ≤ max i,i 1 I ii A ,<label>1</label></formula><formula xml:id="formula_228">I ii P</formula><p>. By Lemma B1 and Assumption 1, we have</p><formula xml:id="formula_229">max i,i 1 I ii P = max i,i 1 t U t=t L (L − t) ii p ii • w (t) ii pi ≤ max i,i p i (L − t L )( ii p ii )(w (t L ) ii ) nρ n n 2 ρ n • 1 n = 1.</formula><p>We also have 1</p><formula xml:id="formula_230">I ii A ≤ di (L−t L )( ii a ii )( ŵ(t L ) ii )</formula><p>and we consider off-diagonal and diagonal cases separately.</p><p>(1) When i = i : By Theorem 3 and Lemma B1, we have min i,i w</p><formula xml:id="formula_231">(t L ) ii , max i,i w (t L ) ii n −1 for fixed 2 ≤ t L and max i =i |w (t L ) ii − ŵ(t L ) ii |/ min i,i w (t L ) ii =    O P n −3/2 ρ −1 n log 1/2 n/(1/n) = o P (1) (when t L ≥ 2 and β &lt; 1 2 ) O P n −3/2 ρ −1/2 n log 1/2 n/(1/n) = o P (1) (when t L ≥ 4 and β &lt; t L −3 t L −1 ) (B.<label>60</label></formula><formula xml:id="formula_232">)</formula><formula xml:id="formula_233">d i = O P (nρ n ) −1/2 log n (B.64)</formula><p>for all t ≥ t L ≥ 3, which implies, in this case,</p><formula xml:id="formula_234">I A − I P max,off , I A − I P max,diag = O P (nρ n ) −1/2 log n .</formula><p>Step </p><formula xml:id="formula_235">M0 − M 0 max,off = O P n −1/2 ρ −1 n √ log n when t L ≥ 2 and β &lt; 1 2 , O P (nρ n ) −1/2 √ log n when t L ≥ 4 and β &lt; t L −3 t L −1 , M0 − M 0 max,diag = O P ρ −1 n when t L ≥ 2 and β &lt; 1 2 , O P (nρ n ) −1/2 √ log n when t L ≥ 4 and β &lt; t L −3 t L −1 .</formula><p>and hence</p><formula xml:id="formula_236">M0 − M 0 F ≤ n 2 M0 − M 0 2 max,off + n M0 − M 0 2 max,diag 1/2 = O P n 1/2 ρ −1 n log 1/2 n when t L ≥ 2 and β &lt; 1 2 , O P n 1/2 ρ −1/2 n log 1/2 when t L ≥ 4 and β &lt; t L −3 t L −1 , M0 − M 0 ∞ ≤ n M0 − M 0 max,off + M0 − M 0 max,diag = O P n 1/2 ρ −1 n log 1/2 n when t L ≥ 2 and β &lt; 1 2 , O P n 1/2 ρ −1/2 n log 1/2 n when t L ≥ 4 and β &lt; t L −3 t L −1 . as desired .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Proof of Theorem 4 (Sparse Regime)</head><p>Same as the dense regime, we shall assume n k = nπ k , for all k ∈ [K]. From Eq. (2.9), one key observation is that when we write (1) B = ρ n B 0 and B 0 is a constant matrix; (2) n k = nπ k , for all k ∈ [K]; the M 0 built on B or B 0 are identical and do not depend on n. We therefore have</p><formula xml:id="formula_237">M 0 =    ξ 11 1 nπ1 1 T nπ1 . . . ξ 1K 1 nπ1 1 T nπ K . . . . . . . . . ξ K1 1 nπ K 1 T nπ1 . . . ξ KK 1 nπ K 1 T nπ K    = ΘΞΘ T</formula><p>where Ξ := (ξ ii ) K×K is a fixed matrix. The remaining steps in the proof of Theorem 4 (dense regime) still hold, e.g.,</p><formula xml:id="formula_238">M 0 F = Θ(n), M 0 ∞ = Θ(n), σ d (M 0 ) = Θ(n), U 2→∞ n −1/2 . (B.65)</formula><p>Applying Theorem 3 (sparse regime) and similar to Eq. (A.51) and Eq. (A.52), one has</p><formula xml:id="formula_239">min T∈O d F • T − U F ≤ M0 − M 0 F σ d (M 0 ) = O P (nρ n ) −1/2 ρ −1/2 n log 1/2 n if t L ≥ 2 &amp; β &lt; 1 2 , O P (nρ n ) −1/2 log 1/2 n if t L ≥ 4 &amp; β &lt; t L −3 t L −1 , (B.66) min T∈O d F • T − U 2→∞ ≤ 14 M0 − M 0 ∞ σ d (M 0 ) U 2→∞ = O P n −1 ρ −1 n log 1/2 n if t L ≥ 2 &amp; β &lt; 1 2 , O P n −1 ρ −1/2 n log 1/2 n if t L ≥ 4 &amp; β &lt; t L −3 t L −1 . (B.67)</formula><p>Finally it is easy to check that if t L ≥ 2 and β &lt; 1/2 then</p><formula xml:id="formula_240">n 1/2 min T∈O d F • T − U 2→∞ (nρ n ) −1/2 ρ −1/2 n log 1/2 n n β−1/2 log 1/2 n → 0 with high probability. When t L ≥ 4 and β &lt; t L −3 t L −1 then n 1/2 • min T∈O d F • T − U 2→∞ (nρ n ) −1/2 log 1/2 n → 0 (B.68)</formula><p>with high probability. Similar to the dense regime, the above 2 → ∞ results together with the technical arguments in <ref type="bibr" target="#b36">[37]</ref> show that applying the K-medians algorithm to cluster the rows of F will recover the memberships for all nodes with high probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Proof of Remark 6</head><p>We now show that the condition β &lt; t L −2 t L , which is weaker than that assumed in Theorem 4, is actually sufficient to achieve exact recovery under the sparse regime of ρ n n −β for β ∈ (0, 1]. We will follow the same proof strategy as that presented earlier with the main changes being that we perform more careful book-keeping when bounding the important terms in Section B. Noting above equation implies that Ŵt − W t max = o P (1) as implied by the following Eq. (B.70), then similar to the discussion in Section 3.1, M0 is thus well-defined with high probability. We can then follow the same arguments as that presented in Section B.  </p><formula xml:id="formula_241">i =i |w (t L ) ii − ŵ(t L ) ii |/ min i,i w (t L ) ii = O P log 1/2 n n 1/2 ρ 1/2 n + n • (nρ n ) −t L /2 = o P (1) (B.70) as (log 1/2 n)/(n 1/2 ρ 1/2 n ) log 1/2 n (β−1)/2 → 0 and n • (nρ n ) −t L /2 n {2−(1−β)t L }/2 → 0 for β &lt; t L −2 t L . Meanwhile Eq. (B.64) becomes max i,i ii a ii ŵ(t) ii − w (t) ii d i = O P log 1/2 n n 1/2 ρ 1/2 n + n • (nρ n ) −t L /2</formula><p>and in turn Eq. (B.65) becomes</p><formula xml:id="formula_242">M0 − M 0 ∞ = O P n 1/2 log 1/2 n ρ 1/2 n + n 2 • (nρ n ) −t L /2 .</formula><p>Finally, using the above bound M0 − M 0 ∞ , we can proceed with the same proof of Theorem 4 in Section B.3 and obtain, in place of Eq. (B.67), the bound</p><formula xml:id="formula_243">min T∈O d F • T − U 2→∞ ≤ 14 M0 − M 0 ∞ σ d (M 0 ) U 2→∞ = O P log 1/2 n n 3/2 ρ 1/2 n + (nρ n ) −t L /2 . (B.71)</formula><p>Eq. (B.68) then becomes</p><formula xml:id="formula_244">n 1/2 • min T∈O d F • T − U 2→∞ n −1/2 ρ −1/2 n log 1/2 n + n −t L /2+1 ρ −t L /2 n o(1) + n βt L /2−t L /2+1 = o(1), (B.72)</formula><p>The technical arguments in <ref type="bibr" target="#b36">[37]</ref> once again show that clustering the rows of F using K-medians will, with high probability recovers the memberships of all nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Strong recovery for DCSBM (Sparse Regime)</head><p>The exact same arguments as that presented in Section A.4 also apply to the sparse regime where ρ n n −β for some β ∈ [0, 1). Indeed, the matrix M 0 and M 0 in Section A.4 does not depend on the sparsity ρ n . Rather ρ n only affects the convergence rate of M0 to M 0 which in turns lead to a slower convergence rate for F to U. This convergence rate is, however, still the same as that in Theorem 4 provided that the θ i are homogeneous (so that max i θ i / min i θ i 1). Clustering the rows of F will thus, with high probability, recover the membership of all nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C ADDITIONAL PROOFS AND DERIVATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Proof of Lemma A1</head><p>(1): The sum of ith row of AD</p><formula xml:id="formula_245">−1 A is n i=1 a ii d i = n i=1 a ii n i=1 a ii = 1. So 1 n • Ŵ = 1 n • AD −1 A = 1 n<label>and</label></formula><formula xml:id="formula_246">1 n • Ŵt = 1 n AD −1 A . . . AD −1 A t items = (1 n • AD −1 A ) AD −1 A . . . AD −1 A t−1 items = 1 n AD −1 A . . . AD −1 A t−1 items = • • • = 1 n .</formula><p>With similar argument we have </p><formula xml:id="formula_247">1 n W t = 1 n . (<label>2</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Proof of Lemma A2</head><p>(1) D A , D −1 A : With Chernoff bound, since c 0 ≤ p ii ≤ c 1 for any fixed i we can get</p><formula xml:id="formula_248">P d i n &gt; 3c 1 2 ≤ P d i &gt; 3 2 p i exp − C 0 • n P 1 d i 1 n &gt; 2 c 0 ≤ P d i ≤ p i 2 exp − C 1 • n</formula><p>for some C 0 , C 1 &gt; 0 by taking appropriate constant in general Chernoff bound. So we have</p><formula xml:id="formula_249">P max 1≤i≤n d i n ≤ 3c 1 2 n exp − C 0 • n −→ 0 P max 1≤i≤n 1 d i 1 n ≤ 2 c 0 n exp − C 1 • n −→ 0 as n → ∞. We therefore have D A = max 1≤i≤n d i = O P (n), D −1 A = max 1≤i≤n 1/d i = O P (1/n).<label>(2</label></formula><p>) D A − D P : For a given i the a i1 , . . . , a in are independent and Ea ii = p ii for any</p><formula xml:id="formula_250">1 ≤ i, i ≤ n. Also |a ii − p ii | ≤ 1.</formula><p>By Bernstein inequality, we have</p><formula xml:id="formula_251">P n j=1 a ii − n j=1 p ii &gt; t ≤ 2 exp − t2 2σ 2 0 n + 2 3 t , where σ 2 0 = 1 n n j=1 Var(a ii − p ii ) = 1 n n j=1 p ii (1 − p ii ). So we have σ 2 0 ≤ 1 n n j=1 (p ii + 1 − p ii ) 2 /4 = 1/4 and P n j=1 a ii − n j=1 p ii &gt; t ≤ 2 exp − t2 1 2 n + 2 3 t . (C.1) Take t = c √ n log n in Eq. (C.1), we have P n j=1 a ii − n j=1 p ii &gt; c n log n ≤ 2 exp − c 2 n log n 1 2 n + 2 3 c √ n log n exp − 2c 2 log n = 1 n 2c 2</formula><p>. Combining all the events among i ∈ {1, . . . , n}, we get</p><formula xml:id="formula_252">P D A − D P &gt; c n log n = P max 1≤i≤n n j=1 a ii − n j=1 p ii &gt; c n log n = P n i=1 n j=1 a ii − n j=1 p ii &gt; c n log n n • 1 n 2c 2 = 1 n 2c 2 −1 , which implies D A − D P = O P ( n log(n)). (3) D −1 A − D −1 P :</formula><p>With the results in (1) and (2), we have </p><formula xml:id="formula_253">D −1 A − D −1 P = max i 1 d i − 1 p i = max i p i − d i d i p i ≤ D A − D P • D −1 A • D −1 P = O P n −3/2 log n .<label>(4</label></formula><formula xml:id="formula_254">Ŵ max = AD −1 A max ≤ A max • D −1 A ≤ max 1≤i≤n 1/d i = O P (n −1 ). (C.2) For any t ≥ 1, Ŵt max ≤ n Ŵt−1 max Ŵ max ≤ n 2 Ŵt−2 max Ŵ 2 max ≤ • • • ≤ n t−1 Ŵ t max = O P (n −1 ). Since c 0 ≤ p ii ≤ c 1 , we could also see c t 0 c t 1 n ≤ w (t) ii ≤ c t 1 c t 0 n (C.3)</formula><p>for any given t ≥ 1 and 1 ≤ i, i ≤ n, which implies min i,i w</p><formula xml:id="formula_255">(t) ii , max i,i w (t) ii n −1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Proof of Lemma B1</head><p>(1) Bounding D A , D −1 A : As c 2 ρ n ≤ p ii ≤ c 3 ρ n and Ed i = p i , we have by Chernoff bounds that</p><formula xml:id="formula_256">log P d i n &gt; 3c 3 2 ρ n ≤ log P d i &gt; 3 2 p i ≤ −C 0 nρ n , log P n d i &gt; 1 2c 2 ρ n ≤ log P d i ≤ p i 2 ≤ −C 0 nρ n , (C.4)</formula><p>for any i ∈ {1, 2, . . . , n} where C 0 &gt; 0 is a constant not depending on i or n. Eq. (C.4) together with a union bound imply</p><formula xml:id="formula_257">P max i d i n &gt; 3c 3 ρ n 2 ≤ n max i P d i &gt; 3p i 2 ≤ exp log n − C 0 nρ n −→ 0, P max i n d i &gt; 1 2c 2 ρ n ≤ n max i P d i ≤ p i 2 ≤ exp log n − C 0 nρ n −→ 0, (C.5) as n −→ ∞. So D A = max i d i = O P (nρ n ), D −1 A = max i 1/d i = O P (nρ n ) −1 . (2) Bounding D A − D P , D −1 A − D −1 P :</formula><p>For a given vertex i the {a i1 , . . . , a in } are independent Bernoulli random variables with Ea ii = p ii . Therefore, by Bernstein inequality, we have</p><formula xml:id="formula_258">P n i =1 a ii − n i =1 p ii &gt; t ≤ 2 exp − t2 2σ 2 0 n + 2 3 t , (C.6) where σ 2 0 = n −1 i Var(a ii − p ii ) = n −1 i p ii (1 − p ii ).</formula><p>As min i,i p ii ρ n and max i,i p ii ρ n where ρ n → 0, we have σ 2 0 ρ n . Letting t = C(nρ n log n) 1/2 in Eq. (C.6), we have</p><formula xml:id="formula_259">σ 2 0 n t ρ n n −1 log n 1/2 → ∞ by Assumption 1. We therefore have t2 2σ 2 0 n + 2 3 t t2 2σ 2 0 n C 2 log n.</formula><p>There thus exists some constant C &gt; 0 such that</p><formula xml:id="formula_260">P n j=1 a ii − n j=1 p ii &gt; C(nρ n log n) 1/2 ≤ 2 exp − 2 log n and hence P max 1≤i≤n n j=1 a ii − n j=1 p ii &gt; C(nρ n log n) 1/2 ≤ 2n exp − 2 log n → 0 (C.7) as n → ∞, which implies D A − D P = O P (nρ n log n) 1/2 .</formula><p>Combining the above results we obtain</p><formula xml:id="formula_261">D −1 A − D −1 P = max i 1 d i − 1 p i = max i p i − d i d i p i ≤ D A − D P • D −1 A • D −1 P = O P (nρ n ) −3/2 log 1/2 n .<label>(</label></formula><formula xml:id="formula_262">) Bounding Ŵt max , max i,i w (t)<label>3</label></formula><p>ii and min i,i w</p><formula xml:id="formula_263">(t)</formula><p>ii As the elements of A and P are non-negative and bounded above by 1, we have</p><formula xml:id="formula_264">Ŵ max = AD −1 A max ≤ A max D −1 A ≤ max i d −1 i = O P ((nρ n ) −1 ). (C.8)</formula><p>We thus have, for any t ≥ 1, that We first write</p><formula xml:id="formula_265">Ŵt max ≤ (max i d i ) Ŵt−1 max Ŵ max ≤ (max i d i ) 2 Ŵt−2 max Ŵ 2 max ≤ • • • ≤ (max i d i ) t−1 Ŵ t max = O P ((nρ n ) −1</formula><formula xml:id="formula_266">∆ (1) 1 max = max i,i a ii d i • p i (d i − p i ) max i 1 nρ n d i − p i d i</formula><p>by |a ii | ≤ 1 and Assumption 1. Applying Lemma B1, we obtain</p><formula xml:id="formula_267">∆ (1) 1 max 1 nρ n max i |d i − p i | • max i 1 d i = O P (nρ n ) −3/2 log 1/2 n (C.9) ∆ (1) 2 max = max i,i a ii − p ii p i ≤ max i 1 p i = O P (nρ n ) −1 . (C.10) Since ρ n log n n , we have (nρ n ) −3/2 log 1/2 n (nρ n ) −1 . C.4.2 Bounding ∆ (2) 1 max</formula><p>The ii th element of ∆</p><p>(2,1) 1</p><p>is given by</p><formula xml:id="formula_268">n i * =1 a ii * d i * p i * (p i * − d i * ) − a ii * p 2 i * (p i * − d i * ) • w i * i = n i * =1 a ii * (p i * − d i * ) 2 w i * i p 2 i * d i * .</formula><p>We therefore have, by Lemma B1, that</p><formula xml:id="formula_269">∆ (2,1) 1 max ≤ max i,i n i * =1 a ii * (p i * − d i * ) 2 w i * i p 2 i * d i * ≤ max i d i max i |p i − d i | 2 • max i 1 p i 2 max i 1 d i (max i,i w ii ) = O P (n −2 ρ −1 n log n)</formula><p>Similar to Eq. (A.11) and Eq. (A.13), we also have</p><formula xml:id="formula_270">∆ (2,2) 1 max = max i,i n i * =1 a ii * p 2 i * (p i * − d i * )w i * i ≤ max i d i (max i |p i − d i |) • max i,i</formula><p>w ii max  =: S 2 (a i , a i ).</p><p>Here T (i, i ) = {(i * , i * * )|i * &lt; i * * , i * ∈ {i, i }, i * * ∈ {i, i }} and a i = (a i1 , . . . , a in ).</p><p>Conditioning on the event that D A = max i d i ≤ cnρ n with some c &gt; 0 and a i , a i , it is easy to check that the summands in S 2 (a i , a i ) are independent mean 0 random variables with Var[S 2 (a i , a i )] n −2 (C.17)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Proof of Theorem 1</head><p>To give a detailed analysis for all components in C ii , we firstly denote</p><formula xml:id="formula_271">C (t)</formula><p>ii as the times that the structure, . . . , v i , . . . . . .    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX D ADDITIONAL FIGURES</head><p>This section contains the figures of all additional simulation results for Section 4 and Section 6 in the main paper.   Fig. <ref type="figure">D6</ref>: Scatter plots of node2vec/DeepWalk embeddings for a two-blocks SBM with B and π as defined in Eq. (6.1) as n varies. The points are colored according to their community membership. The dashed ellipses are the 95% level curves for the block-conditional empirical distributions. The two black points are the two distinct embedding vectors obtained by factorizing M 0 ; note that these points had been transformed by the appropriate orthogonal matrices so as to align them with the node2vec/DeepWalk embedding obtained from the observed graphs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>i =i |m ii |, M max,diag = max i |m ii |. (1.1) We use | • | to denote the absolute value of a real number as well as the cardinality of a finite set. The vectors 0 d and 1 d ∈ R d are d dimensional vectors with all elements equal to 0 and 1, respectively. The set of d × d matrices with orthonormal columns is denoted as O d,d while the set of d × d orthogonal matrices is denoted as O d .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>− 2 ;</head><label>2</label><figDesc><ref type="bibr" target="#b1">(2)</ref> nodes belonging to both the neighborhoods of v</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>− 1 ;</head><label>1</label><figDesc><ref type="bibr" target="#b2">(3)</ref> nodes belonging only to the neighborhood of v</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 Fig. 1 :</head><label>31</label><figDesc>Fig. 1: Sample means and 95% empirical confidence intervals for ε 1 ( M0 ) based on 100 Monte Carlo replicates for different values of n, ρ n and (t L , t U ) with t U − t L = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 Fig. 2 :</head><label>32</label><figDesc>Fig. 2: Sample means and 95% empirical confidence intervals for ε 2 ( M0 ) based on 100 Monte Carlo replicates for different values of of n, ρ n , and (t L , t U ) with t U − t L = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 :</head><label>3</label><figDesc>Fig.3: Visualizations of the SVD-based node2vec embeddings (first row) and original node2vec embeddings (second row) for different choices of t U . The plots are for a single realization of a SBM graph on n = 600 vertices with block probabilities matrix B 1 (see eq. (4.3)), sparsity ρ n = 3n −1/2 , and block assignment probabilities π = (0.3, 0.3, 0.4). The embeddings in panels (a)-(c) and (e)-(g) are colored using the true membership assignments while the embeddings in panels (d) and (h) are colored using the K-means clustering. Accuracy of the recovered memberships (by K-means clustering) are also reported for panels (a)-(c) and (e)-(g).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>ρn = 6n −2/3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>The bounds for Ŵt max given in Eq. (B.5) is generally sub-optimal for t ≥ 2. Nevertheless we use this bound purely as a stepping stone in proving Theorem 3. Once Theorem 3 is established we can improve the bound for Ŵt max by applying triangle inequality incorporated with the tight bounds of W t max and Ŵt − W t max .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Claim 1 .</head><label>1</label><figDesc>are derived similarly; see Section C.4.3 for more details. Under the setting of Theorem 3 we have δ (2,1) 2,off = O P (nρ n ) −2 log n . Proof. We start by writing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>B. 47 )</head><label>47</label><figDesc>Now for any c r ∈ {1, 2} and m ∈ {1, 2}, the ii th entry ofΞ m cr is ξ cr,m ii • ϑ cr,mii where(ξ cr,m ii , ϑ cr,m ii ii , 1/d i ) if c r = 1 and m = 1, (a ii , −1/p i ) if c r = 1 and m = 2, (a ii , 1/p i ) if c r = 2 and m = 1, (p ii , −1/p i ) if c r = 2 and m = 2. (B.48)Using the above notations, we can now write the ii th entry of m,c as m,c ii = i∈{1,...,n} t−1 j&lt;r *</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>3 (</head><label>3</label><figDesc>Bounding M0 − M 0 F and M0 − M 0 ∞ ): From Eq. (B.59), Eq. (B.61), Eq. (B.63) and Eq. (B.64), we obtain</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>2, taking care to replace the bound in Eq. (B.58) with that in Eq. (B.69). Eq. (B.69) to the Proof of Theorem 3 in Section B.2. In particular Eq. (B.60) now becomes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>max</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>): The ith element of Ŵd is n j=1 a ii d i d i = d i and hence Ŵd = d. This implies Ŵt d = d. The same argument yields W t p = p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>i 1 p i 2 = 2 n log 1 a 2 n</head><label>2212</label><figDesc>O P n −3/2 ρ −1/ii * − p ii * p i * w i * i = O P n −3/2 ρ −1/log 1/2 n .By Assumption 1, we haven −3/2 ρ −1/2 n log 1/2 n n −2 ρ −1 n log n.Combining the above results we obtain∆ = O P n −3/2 ρ −1/2 n log</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>3 Fig. D1 :</head><label>3D1</label><figDesc>Fig. D1: Sample means and 95% empirical confidence intervals for ε 1 ( M0 ) based on 100 Monte Carlo replicates for different settings of n, ρ n , (t L , t U ). Here we set t U − t L = 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>ρn = 15n −2/3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. D2 :</head><label>D2</label><figDesc>Fig. D2: Sample means and 95% empirical confidence intervals for ε 2 ( M0 ) based on 100 Monte Carlo replicates for different values of of n, ρ n , and (t L , t U ) with t L − t U = 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>(a) tU = 5 ,</head><label>5</label><figDesc>Fig.D3: Visualizations of the SVD-based node2vec embeddings (first row) and original node2vec embeddings (second row) for different choices of t U . The plots are for a single realization of a DCSBM graph on n = 600 vertices with block probabilities matrix B 1 (see eq. (4.3)), sparsity ρ n = 3n −1/2 , and block assignment probabilities π = (0.3, 0.3, 0.4). The embeddings in panels (a)-(c) and (e)-(g) are colored using the true membership assignments while the embeddings in panels (d) and (h) are colored using the K-means clustering. Accuracy of the recovered memberships (by K-means clustering) are also reported for panels (a)-(c) and (e)-(g).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig. D5 :</head><label>D5</label><figDesc>Fig. D5: The colors of embeddings represent the recovered memberships of corresponding vertices, by applying the Gaussian mixture model-based (GMM) clustering on the embeddings shown in Fig. D6 (g). Comparing Fig. D5 and Fig. D6 (g), one can see that GMM clustering correctly recovers most of vertices' memberships (accuracy of 0.84).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>(a) n = 1000 (b) n = 2000 (c) n = 3000</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>Proportions of times that SGD-based and SVD-based node2vec variants perfectly recover all nodes memberships. The graphs are generated from B(ρ n ) with sparsity ρ n = 3n −1/3 (left table) and ρ n = 3n −1/2 (right table).</figDesc><table><row><cell>n</cell><cell cols="2">SVD-based node2vec</cell><cell cols="2">original Node2vec</cell><cell>n</cell><cell cols="2">SVD-based node2vec</cell><cell cols="2">original Node2vec</cell></row><row><cell></cell><cell>t U = 5</cell><cell>t U = 8</cell><cell>t U = 5</cell><cell>t U = 8</cell><cell></cell><cell>t U = 5</cell><cell>t U = 8</cell><cell>t U = 5</cell><cell>t U = 8</cell></row><row><cell>600</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>0.45</cell><cell>600</cell><cell>0.25</cell><cell>0.40</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>900</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>0.95</cell><cell>900</cell><cell>0.58</cell><cell>0.61</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>1500</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>1500</cell><cell>0.82</cell><cell>0.83</cell><cell>0.13</cell><cell>0.37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc>Proportions of times that SGD-based and SVD-based node2vec variants perfectly recover all nodes memberships. The graphs are generated from B (ρ n ) with sparsity ρ n = 3n −1/3 (left table) and</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 :</head><label>3</label><figDesc>The upper table reports the membership recovery accuracy of different embedding methods on the ZK and PB network datasets. The lower table reports the ARI of different embedding methods on the WIKI network dataset. ASE and LSE denote spectral clusterings using the truncated eigendecomposition of the adjacency and normalized Laplacian matrix<ref type="bibr" target="#b0">[1]</ref>,</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>w i * i /p i * n−2, and min i i * w i * i /p i * n −2 . and hence, by Bernstein inequality, we have</figDesc><table><row><cell>12)</cell></row><row><cell>Assumption 1 and Lemma A2 then imply</cell></row><row><cell>max i ,i</cell></row></table><note>*  </note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>1 and Section B.2. More specifically all the arguments in Section B.1 are still valid up to (and including) Eq. (B.57). The rate shown in Eq. (B.57) is, however, only negligible compared to the term O P (n −3/2 ρ</figDesc><table><row><cell cols="5">−1/2 n t L &gt; β then we have to include the error term in Eq. (B.57) by replacing Eq. (B.58) the following bound log 1/2 n) in Eq. (B.56)for t L −3 t L −1 &gt; β, i.e., if we only assume t L −2</cell></row><row><cell>Ŵt − W t</cell><cell>max = O P</cell><cell>log 1/2 n n n 3/2 ρ 1/2</cell><cell>+ (nρ n ) −t/2 .</cell><cell>(B.69)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>)</head><label></label><figDesc>Ŵt max , max i,i w</figDesc><table><row><cell>(t) ii</cell><cell>1/n and min i,i w ii (t)</cell><cell>1/n: By result in (1) and A, P are bounded by 1 elementwisely, it is</cell></row><row><cell>easy to see that</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>i * − a ii * d i * a i * i . We have For the first term on the RHS of Eq. (C.12), we have i* i * * p ii * − p i * i * * a ii * p 2 =1 i * * =i,i a i * i * * p ii * − p i * i * * a ii * p 2 =1 i * * =i,i a i * i * * p ii * − p i * i * * a ii * = (i * ,i * * )∈T (i,i ) a i * i p ii * p 2</figDesc><table><row><cell>n</cell><cell>n</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>n</cell><cell></cell><cell>n</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>a i  *  i =</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>a i  *  i</cell></row><row><cell>i  *  =1</cell><cell>i  *  *  =1</cell><cell></cell><cell></cell><cell>i  *</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">i  *  =1 i  *  =i,i</cell><cell></cell><cell cols="4">i  *  *  i  *</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>=</cell><cell cols="2">n i  *  =1 i  *  =i,i</cell><cell cols="2">a i  *  i p 2 i  *</cell><cell cols="2">n i  *  *  i  *</cell><cell>+</cell><cell>a i  *  *  i p ii  *  *  p 2 i  *  *</cell><cell>a i  *  i  *  *  −</cell><cell>a i  *  i a ii  *  p 2 i  *</cell><cell>+</cell><cell>a i  *  *  i a ii  *  *  i  *  *  p 2</cell><cell>p i</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1/2 n .</cell><cell>(C.11)</cell></row><row><cell cols="3">C.4.3 Bounding δ</cell><cell cols="4">(2,2) 2,off and δ</cell><cell cols="2">(2,3) 2,off</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="13">Step 1 (Bounding δ 2,off ): By Lemma B1, we have (2,2)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">δ 2,off = max (2,2) i,i</cell><cell cols="2">n i  *  =1</cell><cell cols="2">p ii  *  p i  *</cell><cell cols="2">−</cell><cell>a ii  *  d i  *</cell><cell>a i  *  i p i</cell><cell>−</cell><cell cols="2">a i  *  i d i</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>≤ max i,i</cell><cell cols="2">n i  *  =1</cell><cell cols="2">p ii  *  p i  *</cell><cell cols="2">−</cell><cell>a ii  *  d i  *</cell><cell cols="3">a i  *  i • max i</cell><cell>1 p i</cell><cell>−</cell><cell>1 d i</cell><cell cols="2">= O P (n −3/2 ρ −3/2 n</cell><cell>log 1/2 n) • max i,i</cell><cell>n i  *  =1</cell><cell>p ii  *  p i  *</cell><cell>−</cell><cell>a ii  *  d i  *</cell><cell>a i  *  i .</cell></row><row><cell cols="2">Now we focus on</cell><cell></cell><cell cols="2">n i  *  =1</cell><cell cols="3">p ii  *</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>p n i * =1 p ii * p i * − a ii * d i * a i * i = n i * =1 d i * p i * + 1 − d i * p i * p ii * d i * − a ii * p i * p i * d i * a i * i = n i * =1 p ii * d i * − a ii * p i * p 2 i * a i * i + n i * =1 1 − d i * p i * • p ii * d i * − a ii * p i * p i * d i * a i * i = n i * =1 n i * * =1 a i * i * * p ii * − p i * i * * a ii * p 2 i * a i * i + n i * =1 (p ii * d i * − a ii * p i * )(p i * − d i * )a i * i p 2 i * d i * .(C.12)a * i * *</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>. Similar arguments to Eq. (B.28)-Eq. (B.<ref type="bibr" target="#b29">30)</ref> yield ,i * * =1 a i * i * * p ii * − p i * i * * a ii * p 2 i * a i * i = O P (n −1 ). p ii * d i * − a ii * p i * )(p i * − d i * ) p 2 i * d i * a i * i = O P {(nρ n ) −1/2 log 1/2 n}, as max i,i |p ii d i − a ii p i | ≤ max i,i p ii d i + p i = O P (nρ n ).Combining the above results we obtainδ (2,2) 2,off = O P ((nρ n ) −2 log n). Var(p ii * − a ii * ) • Var(p i i * − a i i * ) 1 n 4 ρ 2Therefore, by Bernstein inequality, for any given c &gt; 0 there exists a constant C &gt; 0 not depending on n such that By Eq. (C.16), we also have max i,i | ζii i * | = O((nρ n ) −2 ) and thus P (n −3/2 ρ −1 n log 1/2 n).</figDesc><table><row><cell>max i,i i  In addition, by Lemma B1, we also have max i,i n i  *  =1 Step 2 (Bounding δ (2,3) 2,off ): We first note that n δ (2,3) 2,off = max i =i n i  *  =1 p ii  *  p i  *  − a ii  *  p i  *  p i  *  i p i p ii  n − a i  *  i p i = max i =i n i  *  =1 1 p i  *  p i i  *  =1 p ii  *  p i  *  − a ii  *  p i  *  p i  *  i p i − a i  *  i p i = n i  *  =1 i  *  =i,i ζii i  *  + ζii i + ζii i . Next observe that the { ζii i  1 n − 2 n i  *  =1 i  *  =i,i Var( ζii i  *  ) = 1 n − 2 n i  *  =1 i  *  =i,i Var p ii  *  p i  *  − a ii  *  p i  *  p i  *  i p i − a i  *  i p i = 1 (n − 2) n i  *  =1 i  *  =i,i 1 p 2 i  *  p 2 i We also have max i =i | ζii i  *  | = p ii  *  p i  *  − a ii  *  p i  *  p i  *  i p i − a i  *  i p i ≤ 1 nρ n c 2 2 (nρ n ) −2 . P max i  *  =i,i i  *  =1 i =i n ζii (2,3) 2,off = max i =i n i  *  =1 p ii  *  p i  *  − a ii  *  p i  *  p i  *  i p i − a i  *  i p i n i  δ ≤ max i =i i  *  =i,i i  *  =1 ζii i  *  + max i,i ζii i i,i + max ζii</cell><cell>n</cell><cell>.</cell><cell>(C.14) (C.15) (C.16)</cell></row></table><note>*(* − a ii * p i i * − a i i * .(C.13)Denote ζii i * := 1 p i * p i p ii * − a ii * p i i * − a i i * . We then have * } for i * ∈ {i, i } are independent mean 0 random variables. Hence * &gt; C n −3/2 ρ −1 n log 1/2 n ≤ n −cIn summary we havemax i =i n i * =1 i * =i,i ζii i * = O P (n −3/2 ρ −1 n log 1/2 n). i = O</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>t appears among all random paths in n i=1 L i . As C ii counts all structures defined in Eq. (2.3), we have ,i,i denote the number of times the following structure appears among all random paths in Let {R i } i≥1 represents a stationary simple random walk on G. Since all random paths are stationary and independent simple random walks over G, the strong law of large numbers for Markov chain implies+1 = v i ) • P(R i * +t+1 = v i |R i * +1 = v i ) = S i • P(R t+1 = v i |R 1 = v i ) =almost surely, where we denote γ :=(2L−t L −t U )(t U −t L +1) 2. Note that the second equality in Eq. (C.22) is due to Lemma A1. Now for (t L , t U ) satisfying that all entries in t U t=t L Ŵt are positive, the ii th entry in M(C, κ) satisfieslog C ii • ( i,i C ii ) κ i C ii • i C ii = log (C ii /r) • (κ i,i C ii /r)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">t−1terms , v i , . . .</cell><cell>(C.18)</cell></row><row><cell cols="18">with a fixed C ii =</cell><cell></cell><cell cols="2">t U</cell><cell></cell><cell>C ii + (t)</cell><cell>t U</cell><cell>C</cell><cell>(t) i i .</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">t=t L</cell><cell>t=t L</cell></row><row><cell>Let C</cell><cell>(t)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(C.19)</cell></row><row><cell cols="3">We then have</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">L−t−1</cell></row><row><cell cols="23">C i  lim (t) ii = i  *  =0 (t) C r→∞ 1 r C (t) i  d i 2|A|</cell><cell>•</cell><cell>ŵ(t) i i</cell><cell>(C.21)</cell></row><row><cell cols="12">almost surely. Furthermore we also have</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>lim r→∞</cell><cell>1 r</cell><cell cols="2">C</cell><cell cols="3">(t) ii = lim r→∞</cell><cell>1 r</cell><cell cols="3">L−t−1 i  *  =0</cell><cell>C</cell><cell cols="4">(t) i  *  ,i,i =</cell><cell cols="3">(L − t)d i 2|A|</cell><cell>ŵ(t) i i ,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>lim r→∞</cell><cell cols="2">1 r</cell><cell cols="4">C ii = lim r→∞</cell><cell>1 r</cell><cell cols="2">t U t=t L</cell><cell cols="4">C ii + (t)</cell><cell cols="3">t U t=t L</cell><cell cols="2">C</cell><cell>(t) i i</cell><cell>=</cell><cell>t U t=t L</cell><cell>(L − t)</cell><cell>d i 2|A|</cell><cell>ŵ(t) i i +</cell><cell>d i 2|A|</cell><cell>ŵ(t) ii .</cell></row><row><cell cols="23">almost surely. Combining the above two convergences, we have</cell></row><row><cell></cell><cell>lim r→∞</cell><cell>1 r</cell><cell>n i=1</cell><cell>C ii =</cell><cell cols="4">t U t=t L</cell><cell>(L − t)</cell><cell cols="3">1 2|A|</cell><cell cols="2">n i=1</cell><cell cols="2">d i</cell><cell cols="3">ŵ(t) i i +</cell><cell cols="3">d i 2|A|</cell><cell>n i=1</cell><cell>ŵ(t) ii</cell><cell>=</cell><cell>t U t=t L</cell><cell>(L − t)</cell><cell>d i 2|A|</cell><cell>+</cell><cell>d i 2|A|</cell><cell>=</cell><cell>γd i |A|</cell><cell>(C.22)</cell></row><row><cell cols="5">Similar reasoning yields</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">lim r→∞</cell><cell>1 r</cell><cell cols="2">n i =1</cell><cell cols="4">C i i =</cell><cell cols="3">γd i |A|</cell><cell>, lim r→∞</cell><cell>1 r</cell><cell>n i=1</cell><cell>n i =1</cell><cell>C ii = 2γ</cell><cell>(C.23)</cell></row><row><cell cols="23">almost surely. t U t=t L</cell><cell>(L − t)</cell><cell>ŵ(t) i i d i</cell><cell>+</cell><cell>ŵ(t) ii d i</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">= log</cell><cell>2|A| κγ</cell><cell>t U t=t</cell></row></table><note>kn i=1 L i , . . . . . . k nodes , v i , . . . . . . t−1 nodes , v i , . . . * ,i,i . (C.20) * ,i,i = P(R i * i (C ii /r) • i (C ii /r) a.s. − − → log |A| κγ L (L − t) ŵ(t) ii d i , (C.24)</note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Furthermore we have</p><p>(2) When i = i : Similarly, when t L = 2, we have max i</p><p>ii ]</p><p>= O P (1).</p><p>In summary we have both max i,i</p><p>under the conditions of Theorem 3 (sparse regime). We thus conclude</p><p>Step 2 (Bound of I A − I P max,off and I A − I P max,diag ): We consider two cases for (t L , β) Case 1 (t L ≥ 2 and β &lt; 0.5): For t ≥ 2 we have</p><p>by Lemma B1, Theorem 3 and Eq. <ref type="bibr">(3.6)</ref>. Thus a similar argument as the proof under dense regime gives</p><p>We note that (a) and (b) in Eq. (B.62) do not change for t ≥ t L ≥ 3. By Eq. (B.8) we further have  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Community preserving network embedding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="203" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liben-Nowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Network visualization and analysis of gene expression data using biolayout express 3d</title>
		<author>
			<persName><forename type="first">A</forename><surname>Theocharidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Dongen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Enright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Protocols</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">1535</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spectral clustering and the highdimensional stochastic blockmodel</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rohe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1878" to="1915" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A consistent adjacency spectral embedding for stochastic blockmodel graphs</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Sussman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Fishkind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Priebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="1119" to="1128" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A typology of deviant workplace behaviors: A multidimensional scaling study</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Academy of Management Journal</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="555" to="572" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Two-dimensional linear discriminant analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Janardan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1569" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Line graph neural networks for link prediction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="5103" to="5113" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">PINE: Universal deep embedding for graph nodes via partial permutation invariant set functions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="770" to="782" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph embedding and extensions: A general framework for dimensionality reduction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="40" to="51" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Engineering Bulletin</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="52" to="74" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A survey on network embedding</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="833" to="852" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph representation learning: a survey</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">APSIPA Transactions on Signal and Information Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">e15</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Knowledge graph embeddings with node2vec for item recommendation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Palumbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Troncy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Baralis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Osella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ferro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="117" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Biowordvec, improving biomedical word embeddings with subword information and mesh</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Identification of human membrane protein types by incorporating network embedding methods</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="140" to="794" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gman: A graph multiattention network for traffic prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1234" to="1241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural turtle graphics for modeling city road layouts</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shugrina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4522" to="4530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
				<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="459" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Ergodic limits, relaxations, and geometric properties of random walk node embeddings</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Sussman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>arXiv preprint #2109.04526</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Concentration bounds for co-occurrence matrices of markov chains</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>arXiv preprint #2008.02464</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Limit theorems for eigenvectors of the normalized laplacian for random graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Priebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="2360" to="2415" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A statistical interpretation of spectral embedding: the generalised random dot product graph</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rubin-Delanchy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Priebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="1446" to="1473" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Random graphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bollobás</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Latent space approaches to social network analysis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Handcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="1090" to="1098" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the suprising behavior of node2vec</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rieck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022, ICML Workshop on Topology, Algebra, and Machine Learning</title>
				<imprint/>
	</monogr>
	<note>arXiv preprint #2206.082525</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Community detection using low-dimensional network embedding algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhamidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dhara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>arXiv preprint #2111.05267</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The rotation of eigenvectors by a pertubation. III</title>
		<author>
			<persName><forename type="first">C</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Siam Journal on Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The two-to-infinity norm and singular subspace geometry with applications to highdimensional statistics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Priebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="2405" to="2439" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Unified two-to-infinity eigenspace perturbation theory for symmetric random matrices</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>arXiv preprint #1909.04798</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>arXiv preprint #1301.3781</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stochastic blockmodels: First steps</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="109" to="137" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Community detection and stochastic block models: recent developments</title>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Abbe</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="6446" to="6531" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A nonparametric view of network models and newman-girvan and other modularities</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="21" to="068" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Nonconvex optimization meets low-rank matrix factorization: An overview</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="5239" to="5269" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Convergence rates for the stochastic gradient descent method for non-convex objective function</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fehrman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jentzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hypothesis testing for automated community detection in networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Bickel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Association, Series B</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="253" to="273" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A goodness-of-fit test for stochastic block models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="401" to="424" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Stochastic blockmodels and community structure in networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">16107</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Consistency of community detection in networks under degree-corrected stochastic block models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Levina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2266" to="2292" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Community detection in degree-corrected block models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="2153" to="2185" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mclust 5: clustering, classification and density estimation using gaussian finite mixture models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Scrucca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The R Journal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">289</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Model-based clustering, discriminant analysis, and density estimation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fraley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">458</biblScope>
			<biblScope unit="page" from="611" to="631" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">An information flow model for conflict and fission in small groups</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Zachary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Anthropological Research</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="452" to="473" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The political blogosphere and the 2004 us election: divided they blog</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Glance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd international workshop on Link discovery</title>
				<meeting>the 3rd international workshop on Link discovery</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="36" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Spectral clustering under degree heterogeneity: a case for the random walk laplacian</title>
		<author>
			<persName><forename type="first">A</forename><surname>Modell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rubin-Delanchy</surname></persName>
		</author>
		<idno>arXiv preprint #2105.00987</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">On spectral clustering: analysis and an algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Achieving optimal mis-classification proportion in stochastic blockmodels</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="45" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Perfect clustering for stochastic blockmodel graphs via adjacency spectral embedding</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lyzinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Sussman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Athreya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Priebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="2905" to="2922" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Non-backtracking spectrum of random graphs: community detection and nonregular ramanujan graphs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bordenave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lelarge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Massoulié</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th IEEE Annual Symposium on Foundations of Computer Science</title>
				<meeting>the 56th IEEE Annual Symposium on Foundations of Computer Science</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1347" to="1357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A useful variant of the Davis-Kahan theorem for statisticians</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Samworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="315" to="323" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Spectra of edge-independent random graphs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Combinatorics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">P27</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
