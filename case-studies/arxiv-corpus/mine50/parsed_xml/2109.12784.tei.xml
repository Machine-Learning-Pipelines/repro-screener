<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning from Few Samples: Transformation-Invariant SVMs with Composition and Locality at Multiple Scales</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-22">22 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Tao</forename><surname>Liu</surname></persName>
							<email>tliu@tamu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<settlement>College Station</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Kumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<settlement>College Station</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruida</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<settlement>College Station</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xi</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Applied Machine Learning</orgName>
								<orgName type="department" key="dep2">Facebook AI</orgName>
								<address>
									<settlement>Menlo Park</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning from Few Samples: Transformation-Invariant SVMs with Composition and Locality at Multiple Scales</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-22">22 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">26E7986481AC4DDF493E38845E7946D0</idno>
					<idno type="arXiv">arXiv:2109.12784v6[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-10-27T19:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Motivated by the problem of learning with small sample sizes, this paper shows how to incorporate into support-vector machines (SVMs) those properties that have made convolutional neural networks (CNNs) successful. Particularly important is the ability to incorporate domain knowledge of invariances, e.g., translational invariance of images. Kernels based on the maximum similarity over a group of transformations are not generally positive definite. Perhaps it is for this reason that they have not been studied theoretically. We address this lacuna and show that positive definiteness indeed holds with high probability for kernels based on the maximum similarity in the small training sample set regime of interest, and that they do yield the best results in that regime. We also show how additional properties such as their ability to incorporate local features at multiple spatial scales, e.g., as done in CNNs through max pooling, and to provide the benefits of composition through the architecture of multiple layers, can also be embedded into SVMs. We verify through experiments on widely available image sets that the resulting SVMs do provide superior accuracy in comparison to well-established deep neural network benchmarks for small sample sizes.</p><p>36th Conference on Neural Information Processing Systems (NeurIPS 2022).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the goal of learning when the number of training samples is small, and motivated by the success of CNNs <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b30">31]</ref>, we wish to endow SVMs with as much a priori domain knowledge as possible.</p><p>One such important domain property for image recognition is translational invariance. An image of a dog remains an image of the same dog if the image is shifted to the left. Similarly, it is also rotation-invariant. More generally, given a group of transformations under which the classification of images is invariant, we show how to endow SVMs with the knowledge of such invariance.</p><p>One common approach is data augmentation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b5">6]</ref>, where several transformations of each training sample are added to the training set. When applying SVMs on this augmented training set, it corresponds to a kernel that defines the similarity between two vectors X 1 and X 2 as the average similarity between X 1 and all transformations of X 2 . However the average also includes transformations that are maximally dissimilar, and we show that it leads to poor margins and poor classification results. More appealing is to construct a kernel that defines similarity as the maximum similarity between X 1 and all transformations of X 2 . We show that this kernel is positive definite with high probability in the small sample size regime of interest to us, under a probabilistic model for features. We verify this property on widely available datasets and show that the improvement obtained by endowing SVMs with this transformation invariance yields considerably better test accuracy.</p><p>Another important domain property for image recognition is the "locality" of features, e.g., an edge depends only on a sharp gradient between neighboring pixels. Through operations such as maxpooling, CNNs exploit locality at multiple spatial scales. We show how one may incorporate such locality into polynomial SVMs.</p><p>Finally, their multi-layer architecture provides CNNs the benefits of composition <ref type="bibr" target="#b6">[7]</ref>. We show how one can iteratively introduce multiple layers into SVMs to facilitate composition. The introduction of multiple layers increases computational complexity, and we show how this can be alleviated by parallel computation so as to achieve a reduction of computation time by increasing memory.</p><p>We show experimentally that the resulting SVMs provide significantly improved performance for small datasets. Translational and rotational invariance embedded into SVMs allows them to recognize objects that have not already been centered in images or oriented in upright positions; referred to as transformed datasets in the sequel. The transformation-invariant SVMs provide significant improvements over SVMs as well as CNN benchmarks without data augmentation when the training set is small. For 100/200/500 training samples, the recognition accuracy of the MNIST dataset <ref type="bibr" target="#b16">[17]</ref> is increased, respectively, from the figures of 68.33%/83.20%/91.33% reported by the CNNs optimized over architectures and dimensions <ref type="bibr" target="#b9">[10]</ref> to, respectively, 81.55%/89.23%/93.11%. Similar improvements are also obtained in the EMNIST Letters <ref type="bibr" target="#b3">[4]</ref> and Transformed MNIST datasets. Computational results reported here are for small datasets, handled efficiently by LIBSVM <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Background</head><p>In the early 2000s, SVMs <ref type="bibr" target="#b4">[5]</ref> were one of the most effective methods for image classification <ref type="bibr" target="#b18">[19]</ref>. They had a firm theoretical foundation of margin maximization <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b0">1]</ref> that is especially important in high dimensions, and a kernel method to make high-dimensional computation tractable. However, with the advent of CNNs and the enhanced computing power of GPUs, SVMs were replaced in image classification. One reason was that CNNs were able to incorporate prior knowledge. The pioneering paper <ref type="bibr" target="#b34">[35]</ref> emphasizes that "It is usually accepted that good generalization performance on real-world problems cannot be achieved unless some a priori knowledge about the task is built into the system. Back-propagation networks provide a way of specifying such knowledge by imposing constraints both on the architecture of the network and on its weights. In general, such constraints can be considered as particular transformations of the parameter space." It further mentions specifically that, "Multilayer constrained networks perform very well on this task when organized in a hierarchical structure with shift-invariant feature detectors." Indeed, CNNs have successfully incorporated several important characteristics of images. One, mentioned above (called shift-invariance), is translational invariance, which is exploited by the constancy, i.e., location independence, of the convolution matrices. A second is locality. For example, an "edge" in an image can be recognized from just the neighboring pixels. This is exploited by the low dimensionality of the kernel matrix. A third characteristic is the multiplicity of spatial scales, i.e., a hierarchy of spatial "features" of multiple sizes in images. These three properties are captured in modern CNNs through the "pooling" operation at the ( + 1)-th layer, where the features of the -th layer are effectively low-pass filtered through operations such as max-pooling. More recently, it has been shown that depth in neural networks (NNs) of rectified linear units (ReLUs) permits composition, enhances expressivity for a given number of parameters, and reduces the number needed for accurate approximations <ref type="bibr" target="#b6">[7]</ref>.</p><p>Generally, neural networks have become larger over time with the number of parameters ranging into hundreds of millions, concomitantly also data-hungry, and therefore inapplicable in applications where data is expensive or scarce, e.g., medical data <ref type="bibr" target="#b19">[20]</ref>. For these reasons, there is an interest in methodologies for learning efficiently from very few samples, which is the focus of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related Work</head><p>There are mainly two sets of studies: on transformation-invariant kernels and on local correlations.</p><p>Transformation-Invariant Kernels. There are two major routes to explore transformation invariant kernels <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b15">16]</ref>. The most widely used is based on Haar-integration kernels, called "average-fit" kernels in this paper, which average over a transformation group <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>. Although trivially positive definite, they appear to produce poor results in our testing (Section 4). Mairal et al. <ref type="bibr" target="#b20">[21]</ref> reported some good results when a large dataset is available for pre-training bottom network layers unsupervised <ref type="bibr" target="#b23">[24]</ref>, but our focus is on data-scarce situations where no such large dataset is available. This paper concentrates on "best-fit" kernels, which are based on the maximum similarity over transformations. "Jittering kernels" <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> calculate the minimum distance between one sample and all jittered forms of another sample, analogous to this paper. Instead of measuring the minimum distance between samples, tangent distance kernels <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b12">13]</ref> use a linear approximation to measure the minimum distance between sets generated by the transformation group, which can only guarantee local invariance. In comparison, our "best-fit" kernel is defined as the maximum inner product between one sample and all transformed forms of another sample, which enjoys global invariance and differs from jittering kernels for non-norm-preserving transformations (e.g., scaling transformations). Although "best-fit" kernels enjoy a good performance, they are not guaranteed to be positive definite, thus the global optimality of the SVM solution cannot be guaranteed theoretically. We address this lacuna and show that positive definiteness indeed holds with high probability in the small training sample set regime for the "best-fit" kernel. Additionally, we show that locality at multiple scales can further improve the performance of "best-fit" kernels. We note that there were also some works that treat an indefinite kernel matrix as a noisy observation of some unknown positive semidefinite matrix <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36]</ref>, but our goal is to analyze conditions that make the "best-fit" kernel positive definite.</p><p>Local Correlations. Since "local correlations," i.e., dependencies between nearby pixels, are more pronounced than long-range correlations in natural images, Scholkopf et al. <ref type="bibr" target="#b24">[25]</ref> defined a two-layer kernel utilizing dot product in a polynomial space which is mainly spanned by local correlations between pixels. We extend the structure of two-layer local correlation to multilayer architectures by introducing further compositions, which gives the flexibility to consider the locality at multiple spatial scales. We also analyze the corresponding time and space complexity of multilayer architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Kernels with Transformational Invariance</head><p>To fix notation, let S = {(X 1 , y 1 ), . . . , (X n , y n )} be a set of n labeled samples with m-dimensional feature vectors</p><formula xml:id="formula_0">X i = (X (1) i , . . . , X (m) i ) T ∈ X ⊂ R m and labels y i ∈ Y.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transformation Groups and Transformation-Invariant Best-Fit Kernels</head><p>We wish to endow the kernel of the SVM with the domain knowledge that the sample classification is invariant under certain transformations of the sample vectors. Let G be a transformation group that acts on X , i.e., for all S, T, U ∈ G: (i) T maps X into X ; (ii) the identity map I ∈ G; (iii) ST ∈ G; (iv) (ST )U = S(T U ); (v) there is an inverse</p><formula xml:id="formula_1">T −1 ∈ G with T T −1 = T −1 T = I.</formula><p>We start with a base kernel K base : X × X → R that satisfies the following properties:</p><p>1. Symmetry, i.e., K base (X i , X j ) = K base (X j , X i ); 2. Positive definiteness, i.e., positive semi-definiteness of its Gram matrix:</p><formula xml:id="formula_2">n i=1 n j=1 α i α j K base (X i , X j ) ≥ 0 for all α 1 , . . . , α n ∈ R; 3. K base (T X i , X j ) = K base (X i , T −1 X j ), ∀T ∈ G.</formula><p>Define the kernel with the "best-fit" transformation over G by</p><formula xml:id="formula_3">K G,best,base (X i , X j ) := sup T ∈G K base (T X i , X j ).</formula><p>(1)</p><formula xml:id="formula_4">Lemma 1. K G,best,base is a symmetric kernel that is also transformation-invariant over the group G, i.e., K G,best,base (T X i , X j ) = K G,best,base (X i , X j ).</formula><p>Proof. The symmetry follows since</p><formula xml:id="formula_5">K G,best,base (X i , X j ) = sup T ∈G K base (T X i , X j ) = sup T ∈G K base (X i , T −1 X j ) = sup T ∈G K base (T −1 X j , X i ) = sup T −1 ∈G K base (T −1 X j , X i ) = K G,best,base (X j , X i ).</formula><p>The transformational invariance follows since</p><formula xml:id="formula_6">K G,best,base (T X i , X j ) = sup S∈G K base (ST X i , X j ) = sup U T −1 ∈G K base (U X i , X j ) = sup U ∈G K base (U X i , X j ) = K G,best,base (X i , X j ).</formula><p>The Translation Group: Of particular interest in image classification is the group of translations, a subgroup of the group of transformations. Let</p><formula xml:id="formula_7">X i = {X (p,q) i : p ∈ [m 1 ], q ∈ [m 2 ]} denote a two-dimensional m 1 × m 2 array of pixels, with m = m 1 m 2 . Let T rs X i := {X ((p−r) mod m1,(q−s) mod m2) i : p ∈ [m 1 ], q ∈ [m 2 ]</formula><p>} denote the transformation that translates the array by r pixels in the x-direction, and by s pixels in the y-direction. The translation group is</p><formula xml:id="formula_8">G trans := {T rs : r ∈ [m 1 ], s ∈ [m 2 ]}.</formula><p>For notational simplicity, we will denote the resulting kernel K Gtrans,best,base by K T I,best,base .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Positive Definiteness of Translation-Invariant Best-Fit Kernels</head><p>There are two criteria that need to be met when trying to embed transformational invariance into SVM kernels. (i) The kernel will need to be invariant with respect to the particular transformations of interest in the application domain. (ii) The kernel will need to be positive definite to have provable guarantees of performance. K T I,best,base satisfies property (i) as established in Lemma 1. Concerning property (ii) though, in general, K T I,best,base is an indefinite kernel. We now show that when the base kernel is a normalized linear kernel, K linear (X i , X j ) := 1 m X T i X j , then it is indeed positive definite in the small sample regime of interest under a probabilistic model for dependent features. Assumption 1. Suppose that n samples {X 1 , X 2 , . . . , X n } are i.i.d., with</p><formula xml:id="formula_9">X i = {X (1) i , X (2) i , . . . , X (m) i } being a normal random vector with X i ∼ N (0, Σ), ∀i ∈ [n]. Suppose also that λ 2 / λ ∞ ≥ (ln m) 1+ 2 /2 for some ∈ (0, 1], where λ := (λ (1) , . . . , λ (m) ) is comprised of the eigenvalues of Σ. Note that X (p) i may be correlated with X (q) i , for p = q. Example 1. When Σ = I m , i.e., X (p) i ∼ N (0, 1), ∀p ∈ [m], the condition λ 2 / λ ∞ ≥ (ln m) 1+ 2 /2 holds trivially since λ 2 = √ m and λ ∞ = 1.</formula><p>We note that for independent features, we can relax the normality (Assumption 1) to sub-Gaussianity. Theorem 1. Let</p><formula xml:id="formula_10">K T I,best,linear (X i , X j ) := sup T ∈Gtrans 1 m (T X i ) T X j<label>(2)</label></formula><p>be the best-fit translation invariant kernel with the base kernel chosen as the normalized linear kernel. Under Assumption 1, if n ≤</p><formula xml:id="formula_11">λ 1 2 λ 2 (ln m) 1+ 2</formula><p>, then K T I,best,linear is a positive definite kernel with probability approaching one, as m → ∞.</p><p>Outline of proof. We briefly explain ideas behind the proof, with details presented in Appendix B. For brevity, we denote K T I,best,linear (X i , X j ) and K linear (X i , X j ) by K T I,ij and K ij , respectively. From Gershgorin's circle theorem <ref type="bibr" target="#b32">[33]</ref> every eigenvalue of K T I,best,linear lies within at least one of the Gershgorin discs</p><formula xml:id="formula_12">D(K T I,ii , r i ) := {λ ∈ R | |λ − K T I,ii | ≤ r i }, where r i := j =i |K T I,ij |. Hence if K T I,ii &gt; j =i |K T I,ij |, ∀i, then K T I is a positive definite kernel.</formula><p>Accordingly, we study a tail bound on j =i |K T i,ij | and an upper bound on K T I,ii , respectively, to complete the proof.</p><p>Note that λ 1 ≤ √ m λ 2 for an m-length vector λ, which implies that n = Õ( √ m).</p><p>We now show that positive definiteness in the small sample regime also holds for the polynomial kernels which are of importance in practice:</p><formula xml:id="formula_13">K poly (X i , X j ) := (1 + γ m X T i X j ) d for γ ≥ 0 and d ∈ N.</formula><p>Theorem 2. For any γ ∈ R + and d ∈ N, the translation-invariant kernels,</p><formula xml:id="formula_14">K T I,best,poly (X i , X j ) := sup T ∈Gtrans (1 + γ m (T X i ) T X j ) d<label>(3)</label></formula><p>are positive definite with probability approaching one as m → +∞, under Assumption 1, when λ ∞ ≤ 1, and n ≤</p><formula xml:id="formula_15">λ 1 2 λ 2(ln m) 1+ 2</formula><p>.</p><p>Proof. Note that λ ∞ ≤ 1 can be satisfied simply by dividing all entries of the X i s by λ ∞ . The detailed proof is presented in Appendix C. Remark. Since Gershgorin's circle theorem is a conservative bound on the eigenvalues of a matrix, the bound n = Õ( √ m) on the number of samples for positive definiteness is also conservative. In practice, positive definiteness of K T I,best,linear holds for larger n. Even more usefully, K T I,best,poly is positive definite for a much larger range of n than K T I,best,linear , as reported in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Comparison with the Average-Fit Kernel and Data Augmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Average-Fit Kernel</head><p>In <ref type="bibr" target="#b11">[12]</ref>, the following "average-fit kernel" kernel is considered</p><formula xml:id="formula_16">K Gtrans,avg,linear (X i , X j ) := 1 |G] T ∈G K linear (T X i , X j ),<label>(4)</label></formula><p>which seeks the "average" fit over all transformations. We denote it by K T I,avg,linear (X i , X j ) for short. It is trivially invariant with respect to the transformations in G and positive definite. However, it is not really a desirable choice for translations when the base kernel is the linear kernel. Note that</p><formula xml:id="formula_17">1 |Gtrans] T ∈Gtrans T X i = α(1, 1, . . . , 1) T , where α = 1 m p∈[m1],q∈[m2] X (p,q) i</formula><p>is the average brightness level of X i . Therefore K T I,avg,linear (X i , X j ) = m× (Avg brightness level of X i ) × (Avg brightness level of X j ). The kernel solely depends on the average brightness levels of the samples, basically blurring out all details in the samples. In the case of rotational invariance, it depends only on the average brightness along each concentric circumference. As expected, it produces very poor results, as seen in the experimental results in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Data Augmentation</head><p>Data augmentation is a popular approach to learn how to recognize translated images. It augments the dataset by creating several translated copies of the existing samples. (A special case is the virtual support vectors method which augments the data with transformations of the support vectors and retrains <ref type="bibr" target="#b25">[26]</ref>.) SVM with kernel K base applied to the augmented data is equivalent to employing the average-fit kernel as <ref type="bibr" target="#b3">(4)</ref>. Consider the case where the augmented data consists of all translates of each image. The resulting dual problem for SVM margin maximization <ref type="bibr" target="#b4">[5]</ref> is:</p><formula xml:id="formula_18">max λ − 1 2 i,j,T1,T2 λ i,T1 λ j,T2 y i y j K base (T 1 X i , T 2 X j ) + i,T λ i,T s.t. λ i,T ≥ 0, ∀i ∈ [n], ∀T ∈ G trans ; i,T λ i,T y i = 0.</formula><p>The corresponding classifier is sign( i,T λ * i,T y i K base (T X i , X) + b * ), where λ * is the optimal dual variable and b * = y j − i,T λ * i,T y i K base (T X i , T X j ), for any j and T satisfying λ * j,T &gt; 0. When no data augmentation is implemented, i.e., |G trans | = 1, we use λ i as shorthand for λ i,1 . As shown in Theorem 4.1 <ref type="bibr" target="#b17">[18]</ref>, this is simply the dual problem for the SVM with K T I,avg,base , and i λ i K T I,avg,base (X i , X j ) = i,T ∈G λ i,T K base (T X i , X j )∀j. Hence data augmentation is mathematically equivalent to a kernel with average similarity over all transformations. This yields a poor classifier since it only leverages the average brightness level of an image. a translation in the image space is a right shift of vector elements. (So a translation of X 1 = (1, 2) yields the augmented datum X 3 = (2, 1) in this two-dimensional context because of wraparound.) Two new samples X 3 = (2, 1) and X 4 = (2, 5) are therefore generated through data augmentation, shown in green. The decision boundary of the base kernel with augmented data (equivalent to the average-fit kernel K T I,avg ) is shown by the blue solid line in Figure <ref type="figure" target="#fig_0">1</ref>. Note that the linear decision boundary X (1) + X (2) = 5 depends solely on the brightness level X (1) + X (2) . However, the decision boundary of the best-fit kernel K T I,best,linear (X i , X j ) = sup T ∈Gtrans 1 2 (T X i ) T X j is piecewise linear due to the "sup" operation. Each piece focuses only on the half of the samples that are on the same side of the symmetry axis (black dashed line), leading to the red piecewise linear separatrix with a larger margin. (The red margin is √ 10 2 , which is larger than the blue margin √ 2.) The best-fit kernels thereby maximize the margin over the larger class of piecewise linear separatrices by exploiting the invariances. Even when data augmentation is used, it only produces linear separatrices and thus still has smaller margins. Benefits will be even more pronounced in higher dimensions. For other kernels (e.g., polynomial kernels), the shape of the decision boundary will be altered correspondingly (e.g., piecewise polynomial), but the TI best-fit kernel will still provide a larger margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Rotation-Invariant Kernels</head><p>To mathematically treat rotation-invariant kernels, consider images that are circular, of radius r, with each concentric annulus from radius (p−1)r m1 to pr m1 , for p ∈ [m 1 ], comprised of m 2 pixels spanning the sectors [2kπ/m 2 , 2(k + 1)π/m 2 ) for k = 0, 1, . . . , m 2 − 1. Denote the element in the p-th annulus and q-th sector as "pixel" X (p,q) , and define the rotation group G rotate := {T 1 , T 2 , ..., T m2 }, where T q X (p,q) = X (p,q+q ) . The rotation-invariant (RI) best-fit kernel is</p><formula xml:id="formula_19">K RI,best,base (X i , X j ) := sup T ∈Grotate K base (T X i , X j ).</formula><p>Lemma 2. Under Assumption 1 , the rotational invariant kernel K RI,best,poly is positive definite with probability approaching one as m → +∞, under the same conditions as in Theorem 1.</p><p>In Section 4, we report on the large performance gain of K RI,best,poly .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Incorporating Locality at Multiple Spatial Scales</head><p>To better illustrate the property of "locality" and its incorporation into SVMs, consider the simple context of a polynomial kernel and a one-dimensional real-valued pixel sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Let us regard the individual pixel values in the sequence</head><formula xml:id="formula_20">{X (1) i , X<label>(2)</label></formula><p>i , . . . , X (m) i } as the primitive features at "Layer 0". Consider now a "local" feature depending only on the nearby pixels {X</p><formula xml:id="formula_21">( ) i , X ( +1) i , . . . , X<label>( +k1) i</label></formula><p>} that can be modeled by a polynomial of degree d 1 . We refer to k 1 as the locality parameter. </p><formula xml:id="formula_22">L,ij := [ m−k1 =1 ( +k1 p= X (p) i X (p) j + 1) d1 + 1] d2 .<label>(5)</label></formula><p>We regard "Layer 1" as comprised of such local features of locality parameter k 1 , at most k 1 apart.</p><p>"Layer 2" allows the composition of local features at Layer 1 that are at most k 2 apart:</p><formula xml:id="formula_23">K (2) L,ij := { m−k1−k2 g=1 [ g+k2 =g ( +k1 p= X (p) i X (p) j + 1) d1 1] d2 + 1} d3 .<label>(6)</label></formula><p>This can be recursively applied to define deeper kernels with locality at several coarser spatial scales.</p><p>The above procedure extends naturally to two-dimensional images {X</p><formula xml:id="formula_24">(p,q) i : p ∈ [m 1 ], q ∈ [m 2 ]}. Then the kernel at Layer 1 is simply ( m2−k1 s=1 m1−k1 =1 ( s+k1 q=s +k1 p= X (p,q) i X (p,q) j + 1) d1 + 1) d2 .</formula><p>The resulting kernels are always positive definite: Lemma 3. K L is a positive definite kernel.</p><p>Proof. Note that if K 1 and K 2 are positive definite kernels, then the following kernels K obtained by Schur products <ref type="bibr" target="#b27">[28]</ref>, addition, or adding a positive constant elementwise, are still positive definite kernels: (i)</p><formula xml:id="formula_25">K ij = αK 1,ij + βK 2,ij , (ii) K ij = (K 1,ij ) 1 (K 2,ij ) 2 , (iii) K ij = K 1,ij + γ, ∀α, β ≥ 0, 1 , 2 ∈ N, γ ≥ 0.</formula><p>The kernel K L can be obtained by repeatedly employing the above operations with α = β = γ = 1, starting with a base linear kernel which is positive definite with high probability under the conditions of Theorem 2.</p><p>One difference from CNNs is that, for the same input layer, one cannot have multiple output channels. If we design multiple channels with different degrees, then the channel with a larger degree will automatically subsume all terms generated by the channel with a smaller degree. Therefore, it is equivalent to having only one output channel with the largest degree. However, if the images have multiple channels to start with (as in R, G, and B, for example), then they can be handled separately. But after they are combined at a layer, there can only be one channel at subsequent higher layers.</p><p>Combining Locality at Multiple Spatial Scales with Transformational Invariance. To combine both locality at multiple spatial scales and transformational invariance, a kernel with locality at multiple spatial scales can be introduced as a base kernel into transformation-invariant kernels.</p><p>Complexity Analysis and Memory Trade-off. One may trade off between the memory requirement and computation time when it comes to the depth of the architecture. Supported by adequate memory space, one can store all kernel values from every layer, with both computation time and memory space increasing linearly with depth. In contrast, when limited by memory space, one can store only the kernel values from the final layer. In that case, although the memory requirement does not increase with depth, computation time grows exponentially with depth.</p><p>The time complexity of computing the polynomial kernel is between O(n 2 m) and O(n 3 m) based on LIBSVM <ref type="bibr" target="#b1">[2]</ref>, while space complexity is O(n 2 ). With sufficient memory of order O(n 2 m), the computations of kernel values can be parallelized so that the time complexity of the locality kernel is considerably reduced to between O(n 2 kd) and O(n 3 kd), where k and d are the locality parameter and the depth, respectively, with kd m. Note that since our focus is on small sample sizes, the O(n 2 ) complexity is acceptable. We evaluate the performance of the methods developed on four datasets:</p><p>1. The Original MNIST Dataset <ref type="bibr" target="#b16">[17]</ref> 2. The EMNIST Letters Dataset <ref type="bibr" target="#b3">[4]</ref> 3. The Translated MNIST Dataset: Since most generally available datasets appear to have already been centered or otherwise preprocessed, we "uncenter" them to better verify the accuracy improvement of TI kernels. We place the objects in a larger (64*64*1) canvas, and then randomly translate them so that they are not necessarily centered but still maintain their integrity.</p><p>In addition, we add a Gaussian noise (µ = 0, σ = 0.1) to avoid being able to accurately center the image by calculating the center-of-mass. We call the resulting dataset the "Translated dataset". Figure <ref type="figure" target="#fig_0">1</ref>(b) shows some samples from different classes of the Translated MNIST dataset. 4. The Rotated MNIST Dataset: We place the original image in the middle of a larger canvas and rotate it, with the blank area after rotation filled with Gaussian noise. RI kernels are not designed to and cannot distinguish between equivalent elements (e.g., 6 versus 9), and so we skip them. Figure <ref type="figure" target="#fig_0">1</ref>(c) displays samples from different classes of the Rotated MNIST dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results and Observations</head><p>Table <ref type="table" target="#tab_1">2</ref> and Figure <ref type="figure" target="#fig_3">2</ref> provide the test accuracy of all methods on the Original and Transformed MNIST Datasets, respectively, while Table <ref type="table" target="#tab_1">2</ref> shows the test accuracy for the EMNIST Letters Dataset <ref type="bibr" target="#b3">[4]</ref>. The letters L, TI, and RI represent Locality at multiple spatial scales, TI kernels, and RI kernels, respectively. A combination such as L-TI represents an SVM with both Locality and Translational Invariance. Note that the intended application of our proposed methods is to learn when data is scarce and there is no large database of similar data, which precludes the possibility of pre-training. We provide code at https://github.com/tliu1997/TI-SVM.</p><p>For the Original MNIST dataset with 100/200/500 training samples (Table <ref type="table" target="#tab_1">2</ref>), after introducing locality and transformational invariance, the classification accuracy is improved from 68.33%/83.20%/91.33% reported by the best CNNs optimized over architectures and dimensions <ref type="bibr" target="#b9">[10]</ref> to 81.55%/89.23%/93.11% respectively. The improvements indicate that the original dataset does not center and deskew objects perfectly. Larger improvements can be observed on the EMNIST Letters dataset <ref type="bibr" target="#b3">[4]</ref> compared with the original SVM, two-sided tangent distance (TD) nearest neighbors <ref type="bibr" target="#b29">[30]</ref>, RI kernel based on Average-Fit, and ResNet. (Since we find that the test accuracy of TD-based  nearest neighbors <ref type="bibr" target="#b29">[30]</ref> is better than that of TD-based SVMs <ref type="bibr" target="#b12">[13]</ref>, we only report results of TD-based nearest neighbors as one of the baselines.) All results are multi-class classification accuracies.</p><p>In Figure <ref type="figure" target="#fig_3">2</ref>, we present the obtained test accuracy as a function of the number of training samples, for two different transformed datasets. Experiments are performed 5 times with mean and standard deviation denoted by the length of the bars around the mean, for 100/300/500/700/1000 training samples respectively. (L-)TI-SVM and (L-)RI-SVM outperform ResNet in many cases when there is no data augmentation since they embed useful domain knowledge for classifiers, especially for the small size regime of training samples. However, with the increase in the number of training samples, the benefits brought by domain knowledge gradually decrease, as shown in Figure <ref type="figure" target="#fig_3">2</ref>. Additionally, the test accuracy of the newly proposed methods has a smaller variance than ResNet's in general.</p><p>From the experimental results, we see that all SVMs with newly defined kernels improve upon the test accuracy of the original SVM method, whether they are original datasets or transformed datasets. They also greatly outperform the best CNNs in the small training sample regime of interest. For transformed datasets, improvements are more significant. Note that the performance improvement of proposed methods comes at the cost of longer computation time. When computation time is critical, we may simply use new single methods, i.e., L-SVM, TI-SVM, and RI-SVM, which enjoy a relatively good performance at a small cost of additional computation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Details of Experimental Evaluation</head><p>With consideration of computational speed and memory space, we utilize a two-layer structure <ref type="bibr" target="#b4">(5)</ref> as well as a k1−1 2 -zero padding and a stride of 1 to implement locality. In order to compare the test of L-SVM, TI-SVM, RI-SVM and further combine them, we select a polynomial kernel with a fixed degree (8 in our experiments) to realize the proposed methods. Note that degree 8 is not necessarily the optimal degree; one can tune the specific degree for different datasets.</p><p>We compare our results with <ref type="bibr" target="#b9">[10]</ref>, which adopts a tree building technique to examine all the possible combinations of layers and corresponding dimensionalities to find the optimal CNN architecture. As for the DNN benchmark of the EMNIST Letters and the Transformed MNIST datasets, we select ResNet <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, a classic CNN architecture, as a DNN benchmark. Plus, for fairness, we do not implement data augmentation for any models and train all from scratch. Note that all experimental results are based on LIBSVM <ref type="bibr" target="#b1">[2]</ref> and are carried out on an Intel Xeon E5-2697A V4 Linux server with a maximum clock rate of 2.6 GHz and a total memory of 512 GB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Concluding Remarks</head><p>In this paper, we have developed transformation-invariant kernels that capture domain knowledge of the invariances in the domain. They can additionally incorporate composition and locality at multiple spatial scales. The resulting kernels appear to provide significantly superior classification performance in the small sample size regime that is of interest in this paper. Experiments demonstrate that for the same polynomial kernel, incorporating locality and transformational invariance improves accuracy, especially for situations where data is scarce.</p><p>A Supporting Definitions and Lemmas Definition 1. A random variable X with mean µ = E[X] is sub-exponential if there are non-negative parameters (ν, b) such that <ref type="bibr" target="#b33">[34]</ref> </p><formula xml:id="formula_26">E[exp (λ(X − µ))] ≤ exp( ν 2 λ 2 2 ), ∀|λ| &lt; 1 b .</formula><p>We denote this by X ∼ SE(ν, b). Lemma 4 (Sub-exponential tail bound <ref type="bibr" target="#b33">[34]</ref>). Suppose that X ∼ SE(ν, b). Then,</p><formula xml:id="formula_27">P[|X − µ| ≥ t] ≤ 2 exp(− t 2 2ν 2 ) if 0 ≤ t ≤ ν 2 b , 2 exp(− t 2b ) if t &gt; ν 2 b .</formula><p>Some properties related to sub-exponential random variables are listed below. Lemma 5 <ref type="bibr">([34]</ref>).</p><p>1. For a standard normal random variable X, X 2 is SE(2, 4);</p><p>2. For random variable X ∼ SE(ν, b), aX ∼ SE(aν, ab);</p><p>3. Consider independent random variables X 1 , . . . , X n , where</p><formula xml:id="formula_28">X i ∼ SE(ν i , b i ). Let ν = (ν 1 , . . . , ν n ) and b = (b 1 , . . . , b n ). Then n i=1 X i ∼ SE( ν 2 , b ∞ ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof of Theorem 1</head><p>Theorem 3 (Restatement of Theorem 1). Let</p><formula xml:id="formula_29">K T I,best,linear (X i , X j ) := sup T ∈Gtrans 1 m (T X i ) T X j</formula><p>be the best-fit translation invariant kernel with the base kernel chosen as the normalized linear kernel. Under Assumption 1, if n ≤</p><formula xml:id="formula_30">λ 1 2 λ 2 (ln m)<label>1+ 2</label></formula><p>, then K T I,best,linear is a positive definite kernel with probability approaching one, as m → ∞.</p><p>Proof of Theorem 1. For brevity, we denote K T I,best,linear (X i , X j ) and K linear (X i , X j ) by K T I,ij and K ij , respectively. From Gershgorin's circle theorem <ref type="bibr" target="#b32">[33]</ref> every eigenvalue of K T I,best,linear lies within at least one of the Gershgorin discs D</p><formula xml:id="formula_31">(K T I,ii , r i ) := {λ ∈ R | |λ − K T I,ii | ≤ r i }, where r i := j =i |K T I,ij |. Hence if K T I,ii &gt; j =i |K T I,ij |, ∀i, then K T I,best,linear is a positive definite kernel.</formula><p>Note that the dimension of the vector X i is m = m 1 × m 2 in the case of a two-dimensional array. Under Assumption 1, X i ∼ N (0, Σ), ∀i ∈ [n]. Since Σ is symmetric and positive semi-definite, there exists an orthogonal matrix O ∈ R m×m with Σ = O • diag λ (1) , . . . , λ (m) • O T , where λ (1) , . . . , λ (m) ≥ 0 are the eigenvalues of Σ. Define Σ</p><formula xml:id="formula_32">1 2 := O • diag √ λ (1) , . . . , √ λ (m) • O T , then X i = Σ 1 2 Z i , where Z i ∼ N (0, I m ). Define H i := O T Z i ∼ N (0, I m ), then X i 2 = O diag λ (1) , . . . , λ (m) O T Z i 2 = diag λ (1) , . . . , λ (m) H i 2 = m p=1 λ (p) (H (p) i ) 2 ,</formula><p>and 1) , . . , λ (m) ). Based on Lemma 5, we have (H</p><formula xml:id="formula_33">E[ X i 2 ] = m p=1 λ (p) E[(H (p) i ) 2 ] = λ 1 . Let λ := (λ<label>(</label></formula><formula xml:id="formula_34">(p) i ) 2 ∼ SE(2, 4), λ (p) (H (p) i ) 2 ∼ SE(2λ (p) , 4λ (p) ), and X i 2 ∼ SE(2 λ 2 , 4 λ ∞ ). According to Lemma 4, P 1 m X i 2 ≤ λ 1 m − t m ≤    exp(− t 2 8 λ 2 2 ) if 0 ≤ t ≤ λ 2 2 λ ∞ , exp(− t 8 λ ∞ ) if t &gt; λ 2 2 λ ∞ .</formula><p>Let t = λ 1 /2, then we have</p><formula xml:id="formula_35">P K ii ≤ 1 2m λ 1 ≤ max exp − λ 2 1 32 λ 2 2 , exp − λ 1 16 λ ∞ (a) ≤ exp − λ 1 32 λ ∞ ,</formula><p>where (a) holds due to Holder's inequality. Noting that</p><formula xml:id="formula_36">K T I,ii = max T ∈G K linear (T X i , X i ) ≥ K ii , ∀i, P(K T I,ii ≤ 1 2m λ 1 ) ≤ P(K ii ≤ 1 2m λ 1 ) ≤ exp − λ 1 32 λ ∞ .<label>(7)</label></formula><p>Now we turn to the off-diagonal terms K T I,ij for i = j. For p ∈ [m], one can write</p><formula xml:id="formula_37">X (p) i X (p) j = (OΛ 1 2 H i ) T (OΛ 1 2 H j ) = H T i ΛH j = m p=1 λ (p) H (p) i H (p) j .</formula><p>Note that .</p><formula xml:id="formula_38">H (p) i H (p) j = 1 2 (Y (p) +,ij − Y (p) −,ij )(Y (p) +,ij + Y (p) −,ij ), where Y (p) +,ij := 1 √ 2 (H (p) j + H (p) i ) and Y (p) −,ij := 1 √ 2 (H (p) j − H (p) i ) are independent N (0,</formula><p>Proof. Define event A γ,m,n := { γ m (T X i ) X j ≥ −1, ∀T ∈ G, ∀i = j}, and event B m,n := {K T I,best,linear is pd}. Denote K(•, •) = (1 + γK T I,best,linear (•, •)) d . Conditioned on event A γ,m,n , K T I,best,poly (X i , X j ) = K(X i , X j ) for off-diagonal entries, and K T I,best,poly (X i , X i ) ≥ K(X i , X i ). This implies K T I,best,poly K. Conditioned on event B m,n , the three properties in the proof of <ref type="bibr">Lemma</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Extensions to other kernels</head><p>The property of transformational invariance can be extended to other kernels with the guarantee of positive definiteness, e.g., radial basis function (RBF) kernels with norm-preserving transformations (i.e., T X i = X i , ∀i ∈ [n]). Define the normalized base kernel as</p><formula xml:id="formula_39">K RBF (X i , X j ) := exp(− X i − X j 2 2mσ 2 ) = exp(− X i 2 + X j 2<label>2mσ</label></formula><p>2 ) exp( X T i X j mσ 2 ).</p><p>If transformations are norm-preserving, then K T I,best,RBF (X i , X j ) := exp(−</p><formula xml:id="formula_40">X i 2 + X j 2 2mσ 2 ) sup T ∈Gtrans exp( (T X i ) T X j mσ 2 ) = exp(− X i 2 + X j 2<label>2mσ</label></formula><p>2 ) exp( sup</p><formula xml:id="formula_41">T ∈Gtrans (T X i ) T X j mσ 2 ).</formula><p>By Theorem 1 and three properties in the proof of Lemma 3, K T I,best,rbf is still positive definite since e x = ∞ i=0</p><p>x i i! . However, the design of locality at multiple scales doesn't apply to RBF kernels since the kernel tricks cannot be utilized anymore. Since we merge two designs in the experimental part, we choose polynomial kernels to illustrate the designs in the paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>AFigure 1 :</head><label>1</label><figDesc>Figure 1: (a) The kernel K T I,best,linear produces the piecewise linear separatrix shown in red. It yields a larger margin than the blue linear separatrix, i.e., decision boundary, that data augmentation and K T I,avg yield; (b) &amp; (c) Transformed MNIST with random translation/rotation and Gaussian noise (µ = 0, σ = 0.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Such a local feature is a linear combination of monomials of the form min( +k1,m) j= (X (j) i ) cj with min( +k1,m) j= c j ≤ d 1 where each integer c j ≥ 0. This gives rise to a kernel K (1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Test accuracy vs. Number of training samples, for Transformed MNIST datasets. L-TI-SVM yields considerable improvement at small sizes in the case of translated samples, while, similarly, L-RI-SVM does so in the case of rotated samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 ] = 1 √ 1 − 1 − 1 √ 1 − 1 1C Proof of Theorem 2 Theorem 4 (≤ λ 1 2 λ 2 (</head><label>21111112412</label><figDesc>1) random variables. Hence (Y (p) +,ij ) 2 and (Y (p) −,ij ) 2 are chi-squared random variables, and their moment generating functions are E[e r(Y (p)+,ij ) 2 ] = E[e r(Y (p) −,ij ) 2r for r &lt; 1 2 . Hence for any |r| ≤ 1/ λ ∞ , we know E[e rX i Xj ] = E exp r (λ (p) ) 2 r 2 ,where (b) is true since the random variables {Y } p∈[m] are mutually independent. It can be verified that x ≤ e x for 0 ≤ x ≤ 1/2. We can then upper bound the moment generating function of E[e rX i Xj ], since for any|r| ≤ 1/ λ ∞ , E[e rX i Xj ] = m p=1 − (λ (p) ) 2 r 2 ≤ 32 λ ∞ + ln n) − exp(− 1 4 (ln m) 1+ + 2 ln n + ln 2m).Above, (e) holds since the probability distributions of |K T I,ij | are identical for all j = i. Sinceλ 1 ≥ λ 2 , the assumption λ 2 λ ∞ ≥ (ln m) (1+ )/2 implies λ 1 λ ∞ ≥ (ln m) (1+ )/2. Note that since n = Õ( Therefore, K T I,best,linear is a positive definite kernel with probability approaching one as m → ∞. Restatement of Theorem 2). For any γ ∈ R + and d ∈ N, the translation-invariant kernels,K T I,best,poly (X i , X j ) := sup T ∈Gtrans (1 + γ m (T X i ) T X j ) dare positive definite with probability approaching 1 as m → +∞, under Assumption 1, λ ∞ ≤ 1, and n ln m)1+ 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 m</head><label>2</label><figDesc>3 indicate K is pd. Thus K T I,best,poly is pd conditioned on A γ,m,n ∩ B m,n . Now B m,n holds w.h.p. by Theorem 1. By the symmetric distribution of K ij and (8), we haveP( γ m (X i ) T X j ≤ − γ λ 2 (ln m) 1+ ) ≤ exp(− (ln m) 1+4).Due to λ 2 ≤ √ m and union bounds, lim m→∞ P(A γ,m,n ) ≥ lim m→∞ 1 − 1 poly(m) = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The value of n up to which the kernel is positive definite. Positive definiteness continues to hold for moderate sample sizes, indicating that the theorem is conservative.</figDesc><table><row><cell>Datasets</cell><cell cols="2">K T I,best,linear K T I,best,poly</cell></row><row><cell>Original MNIST</cell><cell>≈ 45</cell><cell>≈ 375</cell></row><row><cell>EMNIST</cell><cell>≈ 35</cell><cell>≈ 395</cell></row><row><cell>Translated MNIST</cell><cell>≈ 455</cell><cell>≈ 15000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Original MNIST Dataset and EMNIST Letters Dataset (100, 200, 500 training samples): Test accuracy of newly proposed methods compared with the original SVM, the tangent distance (TD) nearest neighbors (two-sided), the RI-SVM based on Average Fit, and the best CNN. Based on the same training set, our fine-tuned ResNet achieves similar performance as in<ref type="bibr" target="#b9">[10]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Original MNIST</cell><cell></cell><cell cols="2">EMNIST Letters</cell></row><row><cell>Method</cell><cell>100</cell><cell>200</cell><cell>500</cell><cell>100</cell><cell>200</cell><cell>500</cell></row><row><cell></cell><cell cols="6">Acc/% Acc/% Acc/% Acc/% Acc/% Acc/%</cell></row><row><cell>L-TI-RI-SVM</cell><cell>81.55</cell><cell>89.23</cell><cell>92.58</cell><cell>44.56</cell><cell>55.18</cell><cell>66.42</cell></row><row><cell>TI-RI-SVM</cell><cell>75.10</cell><cell>86.47</cell><cell>93.11</cell><cell>43.16</cell><cell>52.40</cell><cell>67.42</cell></row><row><cell>L-TI-SVM</cell><cell>78.86</cell><cell>87.02</cell><cell>91.01</cell><cell>42.51</cell><cell>52.81</cell><cell>64.66</cell></row><row><cell>L-RI-SVM</cell><cell>77.96</cell><cell>83.96</cell><cell>89.65</cell><cell>38.39</cell><cell>47.29</cell><cell>59.76</cell></row><row><cell>TI-SVM</cell><cell>69.34</cell><cell>82.34</cell><cell>91.00</cell><cell>39.94</cell><cell>48.12</cell><cell>63.52</cell></row><row><cell>RI-SVM</cell><cell>73.82</cell><cell>83.60</cell><cell>90.19</cell><cell>38.03</cell><cell>45.02</cell><cell>59.04</cell></row><row><cell>L-SVM</cell><cell>75.27</cell><cell>82.11</cell><cell>88.21</cell><cell>37.01</cell><cell>45.08</cell><cell>58.05</cell></row><row><cell>SVM</cell><cell>68.16</cell><cell>78.67</cell><cell>87.14</cell><cell>36.65</cell><cell>42.74</cell><cell>56.38</cell></row><row><cell>TD (two-sided) [30]</cell><cell>73.04</cell><cell>81.68</cell><cell>88.15</cell><cell>37.99</cell><cell>45.31</cell><cell>57.63</cell></row><row><cell>RI-SVM (Average-Fit)</cell><cell>68.05</cell><cell>78.81</cell><cell>87.21</cell><cell>36.82</cell><cell>42.41</cell><cell>56.22</cell></row><row><cell>Best CNN [10] / ResNet</cell><cell>68.33</cell><cell>83.20</cell><cell>91.33</cell><cell>33.82</cell><cell>53.17</cell><cell>57.06</cell></row><row><cell cols="2">4 Experimental Evaluation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4.1 Datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A training algorithm for optimal margin classifiers</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Annual Workshop on Computational Learning Theory</title>
				<meeting>the Fifth Annual Workshop on Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Libsvm: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM transactions on intelligent systems and technology (TIST)</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Training svm with indefinite kernels</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
				<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="136" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emnist: Extending mnist to handwritten letters</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tapson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Schaik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Joint Conference on Neural Networks (IJCNN)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2921" to="2926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Autoaugment</surname></persName>
		</author>
		<title level="m">Learning augmentation policies from data. Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nonlinear approximation and (deep) relu networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Devore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Foucart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Petrova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Constructive Approximation</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distortion-invariant recognition via jittered queries</title>
		<author>
			<persName><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Burl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No. PR00662)</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No. PR00662)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="732" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Training invariant support vector machines</title>
		<author>
			<persName><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="161" to="190" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structural analysis and optimization of convolutional neural networks with a small sample size</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-C</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2020">2020</date>
			<publisher>Nature Publishing Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning translation invariant kernels for classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ghiasi-Shirazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Safabakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shamsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Invariant kernel functions for pattern analysis and machine learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Haasdonk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Burkhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="61" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Tangent distance kernels for support vector machines. In Object recognition supported by user interaction for service robots</title>
		<author>
			<persName><forename type="first">B</forename><surname>Haasdonk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>IEEE</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="864" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Incorporating prior knowledge in support vector machines for classification: A review</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">7-9</biblScope>
			<biblScope unit="page" from="1578" to="1594" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">MNIST handwritten digit database</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00809</idno>
		<title level="m">Enhanced convolutional neural tangent kernels</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A survey of image classification methods and techniques for improving classification performance</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of Remote sensing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="823" to="870" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Will we ever solve the shortage of data in medical applications? https://towardsdatascience.com/ will-we-ever-solve-the-shortage-of-data-in-medical-applications-70da163e2c2d</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2022" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<title level="m">Convolutional kernel networks. Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning with invariances in random features and kernel models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Misiakiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning with group invariant features: A kernel perspective</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Voinea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised learning of invariant feature hierarchies with applications to object recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE conference on computer vision and pattern recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Prior in support vector kernels</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="640" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning with kernels: support vector machines, regularization, optimization, and beyond</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Constructing invariant features by averaging techniques</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schulz-Mirbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th IAPR International Conference on Pattern Recognition</title>
				<meeting>the 12th IAPR International Conference on Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="387" to="390" />
		</imprint>
	</monogr>
	<note>Signal Processing (Cat. No. 94CH3440-5)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Bemerkungen zur theorie der beschränkten bilinearformen mit unendlich vielen veränderlichen</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1911">1911</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Transformation invariance in pattern recognition-tangent distance and tangent propagation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Victorri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: tricks of the trade</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="239" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The nature of statistical learning theory</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Geršgorin and his circles</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Varga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">High-dimensional statistics: A non-asymptotic viewpoint</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Generalization and network design strategies</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Le Cun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="143" to="155" />
		</imprint>
	</monogr>
	<note>Connectionism in perspective</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Analysis of svm with indefinite kernels</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Girolami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
