<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Q-Learning for Nash Equilibria</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-23">23 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nash-Dqn</forename><surname>Philippe</surname></persName>
							<email>¡philippe.casgrain@math.ethz.ch¿</email>
						</author>
						<author>
							<persName><forename type="first">Brian</forename><surname>Ning</surname></persName>
							<email>brian.ning@mail.utoronto.ca</email>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Jaimungal</surname></persName>
							<email>sebastian.jaimungal@utoronto.ca</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Statistical Sciences</orgName>
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Deparment Mathematics</orgName>
								<orgName type="institution">ETH Zurich (Philippe Casgrain</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Statistical Sciences</orgName>
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Q-Learning for Nash Equilibria</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-23">23 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">54E30EB17A016C4EA6C6CB11CC21249D</idno>
					<idno type="arXiv">arXiv:1904.10554v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-10-31T05:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>, http:// sebastian</term>
					<term>statistics</term>
					<term>utoronto</term>
					<term>ca</term>
					<term>)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Model-free learning for multi-agent stochastic games is an active area of research. Existing reinforcement learning algorithms, however, are often restricted to zero-sum games, and are applicable only in small state-action spaces or other simplified settings. Here, we develop a new data efficient Deep-Q-learning methodology for model-free learning of Nash equilibria for general-sum stochastic games. The algorithm uses a local linear-quadratic expansion of the stochastic game, which leads to analytically solvable optimal actions. The expansion is parametrized by deep neural networks to give it sufficient flexibility to learn the environment without the need to experience all state-action pairs. We study symmetry properties of the algorithm stemming from label-invariant stochastic games and as a proof of concept, apply our algorithm to learning optimal trading strategies in competitive electronic markets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1. Introduction. The study of equilibria in systems of interacting agents is ubiquitous throughout the natural and social sciences. The classical approach to studying these equilibria requires building a model of the interacting system, solving for its equilibrium, and studying its properties thereafter. This approach often runs into complications, however, as a fine balance between (i) model tractability and (ii) its ability to capture the main features of the data it aims to represent, must be struck. Rather than taking a model-based approach, it is possible to derive non-parametric reinforcement-learning (RL) methods to study these equilibria. The main idea behind these methods is to directly approximate equilibria from simulations or observed data, providing a powerful alternative to the usual approach.</p><p>The majority of the existing literature on RL is dedicated to single-player games. Most modern approaches follow either a deep Q-learning approach(e.g. <ref type="bibr" target="#b21">[22]</ref>), policy gradient methods (e.g. <ref type="bibr" target="#b25">[26]</ref>), or some mixture thereof (e.g. <ref type="bibr" target="#b10">[11]</ref>). RL methods have also been developed for multi-agent games, but are for the most part restricted to the case of zero-sum games. For a survey see <ref type="bibr" target="#b2">[3]</ref>.</p><p>There are recent efforts on extending RL to general sum games with fictitious play as in <ref type="bibr" target="#b12">[13]</ref>, or with iterative fixed point methods as in <ref type="bibr" target="#b17">[18]</ref>. In the specific context of (discrete stateaction space) mean-field games, <ref type="bibr" target="#b8">[9]</ref> provides a Q-learning algorithm for solving for the Nashequilibria. Many of the existing algorithms suffer either from computational intractability as the size and complexity of a game increases, when state-action space becomes continuous, or from the inability to model complex game behaviour.</p><p>Hu and Wellman <ref type="bibr" target="#b11">[12]</ref> introduce a Q-learning based approach for obtaining Nash equilibria in general-sum stochastic games. Although they prove convergence of the algorithm for games with finite game and action spaces, their approach is computationally infeasible for all but the simplest examples. The main computational bottleneck in their approach is the need to repeatedly compute a local Nash equilibrium over states, which is an NP-hard operation in general. Moreover, the method proposed in <ref type="bibr" target="#b11">[12]</ref> does not extend to games where agents choose continuous-valued controls or to games with either high-dimensional game state representations or with large numbers of players. We instead combine the iLQG framework of of <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b7">8]</ref> and the Nash Q-learning algorithm of <ref type="bibr" target="#b11">[12]</ref> to produce an algorithm which can learn Nash equilibria in these more complex and practically relevant settings.</p><p>In particular, we decompose the state-action value (Q)-function as a sum of the value function and the advantage function. We approximate the value function using a neuralnet, and we locally approximate the advantage function as linear-quadratic in the agents' actions with coefficients that are non-linear functions of the features given by a neural-net. This allows us to compute the Nash equilibrium analytically at each point in feature space (i.e., the optimal action of all agents) in terms of the network parameters. Using this closed form local Nash equilibrium, we derive an iterative actor-critic algorithm to learn the network parameters.</p><p>In principle, our approach allows us to deal with stochastic games with a large number of game state features and a large action space. Moreover, our approach can be easily adapted to mean-field game (MFG) problems, which result from the infinite population limit of certain stochastic games (see <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4]</ref>), such as those developed in, e.g., <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5]</ref> or major-minor agent MFGs such as those studied in, e.g., <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b15">16]</ref>. A drawback of the method we propose is the restriction on the local structure of the proposed Q-function approximator. We find, however, that the proposed approximators are sufficiently expressive in most cases, and perform well in the numerical examples that we include in this paper.</p><p>The remainder of this paper is structured as follows. Section 2 introduces a generic Markov model for a general-sum stochastic game. In Section 3, we present optimality conditions for the stochastic game and motivate our Q-learning approach to finding Nash equilibria. Section 4 introduces our local linear-quadratic approximations to the Q-function and the resulting learning algorithm. We also provide several simplifications that arise in label-invariant games. Section 5 covers implementation details and Section 6 presents some illustrative examples.</p><p>2. Model Setup. We consider a stochastic game with agents i P N :" t1, 2, . . . , N u all competing together. We assume the state of the game is represented via the stochastic process tx t u tPN so that for each time t, x t P X , for some separable Banach space X . At each time t, agent-i chooses an action u i,t P U i , where U i is assumed to be a separable Banach space. In the sequel, we use the notation u ´i,t " pu j,t q jPN{tiu to denote the vector of actions of all agents other than agent-i at time t and the notation u t " pu j,t q jPN to denote the vector of actions of all agents. We assume that the game is a Markov Decision Process (MDP) with a fully-visible game state. The MDP assumption is equivalent to assuming the joint state-action process px t , u t q 8</p><p>t"1 is Markov, whose state transition probabilities are defined by the stationary Markov transition kernel ppx t`1 | x t , u t q and a distribution over initial states p 0 px 0 q.</p><p>At each step of the game, agents receive a reward that varies according to the current state of the game, their own choice of actions, and the actions of all other agents. The agenti's reward is represented by the function px, u i , u ´iq Þ Ñ r i px; u i , u ´iq P R, so that at each time t, agent-i accumulates a reward r i px t ; u i,t , u ´i,t q. We assume that each function r i is continuously differentiable and concave in u i and is continuous in x and u ´i.</p><p>At each time t, agent-i may observe other agents' actions u ´i,t , as well as the state of the game x t . Moreover, each agent-i chooses their actions according to a deterministic Markov policy X Q x Þ Ñ π i pxq P U i . The objective of agent-i is to select the policy π i that maximizes the objective functional R i which represents their personal expected discounted future reward over the remaining course of the game, given a fixed policy π i for themselves and a fixed policy π ´i for all other players. The objective functional for agent-i is</p><formula xml:id="formula_0">(2.1) R i px; π i , π ´iq " E « 8 ÿ t"0 γ ´t i r i px t , π i,t , π ´i,t q ff ,</formula><p>where the expectation is over the process px t q tPN , with x 0 " x, and where we assume γ i P p0, 1q is a fixed constant representing a discount rate. In Equation (2.1), we use the compressed notation π i,t :" π i px t q and π ´i,t :" π ´ipx t q. The agent's objective functional (2.1) explicitly depends on the policy choice of all agents. Each agent, however, can only control their own policy, and must choose their actions while conditioning on the behavior of all other players. Agent-i therefore seeks a policy that optimizes their objective function, but remains robust to the actions of others. In the end, agents' policies form a Nash equilibrium -a collection of policies π ˚pxq " tπ i pxqu iPN such that unilateral deviation from this equilibrium by a single agent will result in a decrease in the value of that agent's objective functional. Formally, we say that a collection of policies π ˚forms a Nash equilibrium if</p><formula xml:id="formula_1">(2.2) R i `x; π i , π ˚i˘ď R i `x; π i , π</formula><p>˚if or all admissible policies π i and for all i P N. Informally, we can interpret the Nash equilibrium as the policies for which each agent simultaneously maximizes their own objective function, while conditioned on the actions of others.</p><p>3. Optimality Conditions. Our ultimate goal is to obtain an algorithm that can attain the Nash equilibrium of the game without a-priori knowledge of its dynamics. In order to do this, we first identify conditions that are more easily verifiable than the formal definition of a Nash equilibrium given above.</p><p>We proceed by extending the well known Bellman equation for Nash equilibria. While leaving π ˚i fixed, we may apply the dynamic programming principle to agents-i reward R i px; π i , π ˚iq resulting in</p><formula xml:id="formula_2">(3.1) R i px; π i , π ˚iq " max uPU i " r i px, u, π ˚ipxqq `γi E x 1 "pp¨|x,uq " R i px 1 ; π i,1 , π ˚i,1 q ‰ * .</formula><p>At the Nash equilibrium, equation (3.1) is satisfied simultaneously for all i P N.</p><p>To express this more concisely, we introduce a vector notation. First define the vectorvalued function Rpx; πq " pR i px; π i , π ´iqq iPN , consisting of the stacked vector of objective functions. We call the stacked objective functions evaluated at their Nash equilibria the stacked value function, which we write as V pxq :" pV i pxqq iN " Rpx; π ˚q.</p><p>Next, we define the Nash state-action value function, also called the Q-Function, which we denote Qpx; uq :" pQ i px; u i , u ´iqq iPN , where <ref type="bibr">(3.2)</ref> Qpx; uq " rpx; uq `γi E</p><p>x 1 "pp¨|x,uq</p><formula xml:id="formula_3">" V px 1 q ‰ ,</formula><p>and where we denote rpx, uq :" pr i px, u i , u ´iqq iPN to indicate the vectorized reward function.</p><p>Each element Q i of Q can be interpreted as the expected maximum value their objective function may take, given a fixed current state x and a fixed (arbitrary) immediate action u taken by all agents. Next, we define the Nash operator as follows.</p><p>Definition 3.1 (Nash Operator). Consider a collection of N concave real-valued functions, f puq " pf i pu i , u ´iqq iPN , where</p><formula xml:id="formula_4">f i : U i Ś U ´i Ñ R.</formula><p>We define the Nash operator N uPU f puq Þ Ñ f pu ˚q, as a map from the collection of functions to their Nash equilibrium value u ˚P R N , where u ˚" arg N uPU f puq is the unique point satisfying,</p><formula xml:id="formula_5">f i `ωi , u ˚i˘ď f i `ui , u ˚i˘,</formula><p>@ω i P U i , and @i P N.</p><formula xml:id="formula_6">(3.3)</formula><p>For a sufficiently regular collection of functions f , the Nash operator corresponds to simultaneously maximizing each of the f i in their first argument u i .</p><p>This definition provides us with a relationship between the value function and agents' Qfunction as V pxq " N uPU Qpx; uq. Using the Nash operator, we may then express the Bellman Equation (3.1) in a concise form as</p><formula xml:id="formula_7">V pxq " N uPU Qpx; uq " N uPU " rpx; uq `γi E x 1 "pp¨|x,uq " V px 1 q ‰ * ,<label>(3.4)</label></formula><p>which we refer to as the Nash-Bellman equation for the remainder of the paper. The definition of the value function equation <ref type="bibr">(3.4)</ref> implies that π ˚" N uPU Qpx; uq. Hence, in order to identify the Nash equilibrium π ˚, it is sufficient to obtain the Q-function and apply the Nash operator to it. This principle will inform the approach we take in the remainder of the paper: rather than directly searching the space of policy collections for the Nash-equilibrium via equations (2.1) and (2.2), we may rely on identifying the function Q satisfying (3.4), and thereafter compute π ˚" N uPU Qpx; uq.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Locally Linear-Quadratic Nash Q-Learning. In this section, we formulate an algorithm which learns the Nash equilibrium of the stochastic game described in the previous section. The principal idea behind the approach we take is to construct a parametric estimator Qθ of agent-i's Q-function, where we search for the set of parameters θ P Θ, which results in estimators Qθ that approximately satisfy the Nash-Bellman equation <ref type="bibr">(3.4)</ref>. Thus, our objective is to minimize the quantity (4.1)</p><formula xml:id="formula_8">E x"ρ x 1 "pp¨|x,uq « › › › › Qθ px; uq ´rpx; uq ´γi N u 1 PU Qθ px 1 ; u 1 q › › › › 2 ff ,</formula><p>over all u, where we define ρ to be an unconditional proability measure over game states x. Equation (4.1) is designed as a measure of the gap between the right and left sides of equation <ref type="bibr">(3.4)</ref>. We may also interpret it as the distance between Qθ and the true value of Q.</p><p>The expression (4.1) is intractable, since we do not know ρ nor pp¨|x, uq a-priori, and we wish to make little to no assumptions on the system dynamics. Therefore, we rely on a simulation based method and approximate (4.1) with</p><formula xml:id="formula_9">(4.2) Lpθq " 1 M M ÿ m"1 Qθ px m ; u m q ´rpx m ; u m q ´γi N u 1 m PU Qθ px 1 m ; u 1 m q 2 ,</formula><p>where for each m " 1, 2, . . . , M , px m , u m ; x 1 m q represents an observed transition triplet from the game. We then search for θ P Θ that minimizes the Lpθq in order to approximate Q.</p><p>Our approach is motivated by Hu and Wellman <ref type="bibr" target="#b11">[12]</ref> and Todorov &amp; Li <ref type="bibr" target="#b26">[27]</ref>. <ref type="bibr" target="#b11">[12]</ref> presents a Q-learning algorithm where Q, which is assumed to take only finitely many values, can be estimated through an update rule that relies on the repeated computation of the Nash operator N uPU Qθ . As the computation of N uPU Qθ is NP-hard in general, this approach proves to be computationally intractable beyond trivial examples. To circumvent this issue and to make use of more expressive parametric models, we generalize and adapt techniques in Gu et al. <ref type="bibr" target="#b7">[8]</ref> to the multi-agent game setting to develop a computational and data efficient algorithm for approximating the Nash equilibria.</p><p>In our algorithm, we make the additional assumption that game states x P X and actions u i P U i are real-valued. Specifically, we assume that X " R dx for some positive integer d x and </p><formula xml:id="formula_10">U i " R d i for each i P N,</formula><formula xml:id="formula_11">µ θ i pxq : X Ñ R d i with µ θ ´ipxq " pµ θ i pxqq iPN , and Ψ i : X Ñ R d ´i . In (4.5), P θ 11,i : X Ñ R d i ˆdi , P θ 12,i : X Ñ R d i ˆd´i , P θ 21,i : X Ñ R d ´iˆd i and P θ 22,i : X Ñ R d ´iˆd ´i</formula><p>are matrix valued functions, for each i P N. We require that P θ 11,i pxq is positive-definite for all x P X and without loss of generality we may choose P 12,i pxq " pP 21,i pxqq , as the advantage function depends only the symmetric combination of P 12,i and P 21,i .</p><p>Hence, rather than modelling Qθ px; uq, we instead model the functions V θ , µ θ , and tP θ i , Ψ θ i u iPN separately as functions of the state space X . Each of these functions can be modeled by universal function approximators such as neural networks. The only major restriction is that P θ 11,i pxq must remain a positive-definite function of x. This restriction is easily attained by decomposing P θ 11,i pxq using Cholesky decomposition, so that we write P θ 11,i pxq " L θ 11,i pxqpL θ 11,i pxqq and instead model the lower triangular matrices L θ 11,i P R d i ˆdi . The model assumption in (4.4) implicitly assumes that agent-i's Q-function can be approximately written as a linear-quadratic function of the actions of each agent. One can equivalently motivate such an approximation by considering a second order Taylor expansion of Q i in the variable u around the Nash equilibrium, together with the assumption that the Q i are convex functions of their input u i . This expansion, however, assumes nothing about the dependence of Q i on the value of game state x.</p><p>The form of (4.4) is designed so that each Qθ i px; u i , u ´iq is a concave function of u i , guaranteeing that N uPU Q is bijective. Moreover, under our model assumption, the Nashequilibrium is attained at the point u ˚pxq " µpxq and at this point, the advantage function is zero, hence we obtain simple expressions for the value function and the equilibrium strategy Consequently, our model allows us to directly specify the Nash equilibrium strategy and the value function of each agent through the functions µ θ and V θ . The outcome of this simplification is that the summand of the loss function in equation (4.2), which contains the Nash equilibria and was itself previously intractable, becomes tractable. For each sample observation m (consisting of a state x m , an u m , and new state x 1 m ) we then have a loss of (4.7a)</p><formula xml:id="formula_12">L m pθq " V θ px m q `p A θ px m ; u m q ´rpx m ; u m q ´γi V θ px 1 m q 2 ,</formula><p>and all that remains is to minimize the total loss (4.7b)</p><formula xml:id="formula_13">Lpθq " 1 M M ÿ m"1</formula><p>L m pθq over the parameters θ given a set of observed state-action triples px m , u m , x 1 m q M m"1 . 4.1. Simplifying Game Structures. Equation (4.4) requires a parametric model of the functions V θ , µ θ ,tP θ i , Ψ i u iPN , which results in potentially a very large parameter space and in principle result in requiring many training steps. In many cases, however, the structure of the game can significantly reduce the dimension of the parameter space and leads to easily learnable model structures. The following subsections enumerate these typical simiplications.</p><p>Label Invariance. Many games have symmetric players, and hence are invariant to a permutation of the label of players. Such label invariance implies that each agent-i does not differentiate among other game participants and the agent's reward functional is independent of any reordering of all other agents' states and/or actions.</p><p>More formally, we assume that for an arbitrary agent-i, the game's state can be represented as x " px 0 , x i , x ´iq, where x 0 represents the part of the game state not belonging to any agent, x i represents the portion of the game state belonging to agent-i and x ´i " tx j u jPN{tiu represents the part of the game state belonging to other agents. Next, let Λ denote the set of permutations over sets of N ´1 indices, where for each λ P Λ, we express the permutation of a collection as λpty j u N ´1 j"1 q " ty kpjq u N ´1 j"1 , where k : t1, . . . , N ´1u Ñ t1, . . . , N ´1u is a one-to-one and onto map from the indices of the collection into itself.</p><p>Label invariance is equivalent to the assumption that for any λ P Λ, each agent's reward function satisfies <ref type="bibr">(4.8)</ref> r i px 0 , x i , λpx ´iq ; u i , λpu ´iqq " r i px 0 , x i , x ´i ; u i , u ´iq .</p><p>With such label invariance, the form of the linear quadratic expansion of the advantage function in (4. for all i P N, where we use the notation }z} 2 M " z M z and xy, zy M " y M z for appropriately sized matrices M . The functional form of (4.9) allows us to drastically reduce the size of the matrices being modelled by an order of N 2 .</p><p>To impose label invariance on states, we require permutation invariance on the inputs the function approximations V θ , µ θ , tP i , , ψ θ u iPN . <ref type="bibr" target="#b27">[28]</ref> provide necessary and sufficient conditions on neural network structures to be permutation invariant. This necessary and sufficient structure is defined as follows. Let φ : R n 1 Ñ R n 2 and σ : R n 2 Ñ R n 3 be two arbitrary functions. From these functions, let f inv : R Jˆn 1 Ñ R n 3 be the composition of these functions, such that (4.10)</p><formula xml:id="formula_14">f inv pzq " σ ˜J ÿ j"1 φpz j q ¸.</formula><p>It is clear that f inv constructed in this manner is invariant to the reordering of the components of z. Equation (4.10) may be interpreted as a layer which aggregates the all dimensions of the inputs (which will corresponding to the state of all agents), through φ, and a layer that transforms the aggregate result to the output, through σ. We assume further that φ and σ are both neural networks with appropriate input and output dimension. This structure can also be embedded as an input later inside of a more complex neural network.</p><p>Identical Preferences. It is quite common that the admissible actions of all agents are identical, i.e., U i " U, @i P N, and agents have homogeneous objectives, or large subpopulations of agents have homogeneous objectives. Thus far, we allowed agents to assign different performance metrics, and the variations are show through the set of rewards and discount rates, tr i , γ i u iPN . If agents have identical preferences, then we simply need to assume r i px; uq " rpx; uq and γ i " γ for all i P N. By the definition of total discounted reward, state-action value function, and value function, identical preferences and admissible actions imply that R i , Q i and V i are independent of i.</p><p>In addition, the assumption of identical preferences, combined with the assumption of label invariance can further reduce the parametrization of the advantage function. Under this additional assumption we have that Vi , p A i must be identical for all i, which reduces modelling of all of the V θ i , µ θ i ,P θ i ,ψ θ to modelling these for a single i. This further reduces the number of functions that must be modeled by an order of N . The combined effect of label invariance and identical preferences has a compounding effect which can have a large impact on the modelling task, particularily when considering large populations of players.</p><p>Remark 4.1 (Sub-population Invariance and Preferences). We can also consider cases where label and preference invariance occur within sub-population of agents, rather than across the entire population. For example, in games in which some agents may cooperate with other agents, we can assume that agents are indifferent to re-labeling of cooperators and noncooperators separately. Similarly, we can consider cases in which groups of agents share the same performance metrics. Such situations, among others, lead to modelling simplifications similar to equation (4.9) and simplifying neural network structures can be developed. In the interest of space, we do not develop further examples simplifying examples, nor do we claim the list we provide is exhaustive as one can easily imagine a multitude of other almost symmetric cases that can be of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation of Nash Actor-Critic Algorithm.</head><p>With the locally linear-quadratic form of the advantage function, and the simplifying assumptions outlined in the previous section, we can now minimize the objective (4.2), which reduces to the sum over (4.7), over the parameters θ through an iterative optimization and sampling scheme. One could in principle apply a simple stochastic gradient descent method using back-propagation on the appropriate loss function. Instead, we propose an actor-critic style algorithm to increase stability and efficiency of the algorithm. Actor-critic methods (see e.g. <ref type="bibr" target="#b16">[17]</ref>) have been shown to provide faster and more stable convergence of reinforcement learning methods towards their optima, and our model lends itself naturally to such methods.</p><p>The decomposition in Equation (4.3) allows us to model the value function V independently from other components. Therefore, we employ an actor-critic update rule to minimize the loss function (4.7) by separating the parameter set θ " pθ V , θ A q, where θ V represents the parameter set for modelling V θ V and θ A represents the parameter set used for modeling p A θ A . Our proposed actor-critic algorithm updates these parameters by minimizing the total loss (5.1a)</p><formula xml:id="formula_15">1 M M ÿ m"1 L py m , θ V , θ A q ,</formula><p>where the individual sample loss corresponding to the error in the Nash-Bellman equation, after already solving for the Nash-equilibria, is</p><formula xml:id="formula_16">(5.1b) Lpy m , θ V , θ A q " V θ V px m q `p A θ A px m ; u m q ´rpx m ; u m q ´γi V θ V px 1 m q 2 ,</formula><p>with u m , y m " px m , u m , x 1 m q, and we minimize the loss by alternating between minimization in the variables θ A and θ V . Observe state transition y t " px t´1 , u, x t q from game.</p><formula xml:id="formula_17">Store D Ð y t " px t´1 , u, x t q Sample Y " ty i u M i"1 randomly from D Optimization Step of 1 M `1 ř yPY Ť tytu Lpy, θ V , θ A q over θ V Optimization Step of 1 M `1 ř yPY Ť tytu Lpy, θ V , θ A q over θ A 5 return pθ A , θ V q</formula><p>Algorithm 5.1 provides an outline of the actor-critic procedure for our optimization problem. We include a replay buffer and employ mini-batching. A replay buffer is a collection of previously experienced transition tuples of the form y t " px t´1 , u, x t q representing the previous state of the system, the action taken in that state, the resulting state of the system, and the reward during the transition. We randomly sample a mini-batch from the replay buffer to update the model parameters using SGD. The algorithm also uses a naïve Gaussian exploration policy, although it may be replaced by any other action space exploration method. During the optimization steps over θ V and θ A , we use stochastic gradient descent, or any other adaptive optimization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.</head><p>Experiments. We test our algorithm on a multi-agent game for statistical arbitrage in electronic exchanges. The game consists of agents trading a single asset with a stochastic price process that is affected by their actions. A simpler version of this model has been studied in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24]</ref>. In these works, under various assumptions, the authors take a mean-field approach and show that the mean field optimal strategy provides an approximate Nash equilibrium for the finite-player case.</p><p>In our setting, an arbitrary agent-i, i P N, may buy or sell assets at a rate of ν i,t P R during the trading horizon t P T :" r0, T s. At t " T , agents must liquidate their holdings, any remaining inventory will be subjected to a terminal penalty. Agents may change their rate of trading at discrete specific points in time t 1 , t 2 , . . . , t M (decision points). The rate of trading is assumed to be constant between any two decision points and thus the total amount of assets traded during the period pt m , t m`1 q is given by ν T i,tm " pt m`1 ´tm qν i,t 0 . Each agent-i keeps track of their inventory q i,tm " q i,0</p><p>`ştm 0 ν i,t dt. Inventories are not visible to other agents however the total order flow, i.e. ř iPN pq i,t ´qi,0 q, is. For ease of notation, we assume t P tt 1 , t 2 , . . . , t M u. We assume the asset price process S t evolves according to the following system of continuous time dynamics: dS t " pµpS t q `gpν t qq dt `dY t `σ dW t , (6.1) dY t " ´ρ Y t dt `γ hpν t q dt, (6.2) with initial condition S 0 , variance σ ą 0, and Brownian motion pW t q tě0 . Furthermore, we denote the vector valued process of trading rates by ν " ppν i,t q iPN q tě0 and denote by νt " ř iPN ν i,t the total trading rate at time t. The above form assumes the cumulative trades of all agents induce both a transient impact through Y :" pY t q tě0 and a permanent impact through g.</p><p>The functions µ and g represents the mean process and permanent price impact respectively. For the experiments conducted here, we assume the process mean-reverts so that µpSq " κpθ ´Sq, where κ ą 0 represents the mean-reversion rate and θ the mean-reversion level. Moreover, we assume a linear permanent price impact so that gpνq " η ν. The cumulative transient price impact is assumed to be square root, i.e. hpνq " signpνq ? ν. Under these model assumptions, the continuous dynamics may be written dS t " pκpθ ´St q `η νt q dt `dY t `σW t (6.3)</p><formula xml:id="formula_18">dY t " ´ρ Y t dt `γ signpν t q ? νt dt (6.4)</formula><p>In addition, each agent pays a transaction cost proportional to the amount they buy or sell during each time period. This represents the cost incurred by the agents from "walking the limit-order-book" and is often dependent on the liquidity of the asset. Agents keep track of their total cash from trading and we denote the corresponding process by X i :" pX i,t q tě0 where X i,t " ´şT 0 ν i,t pS t `b1 ν i,t q dt, and b 1 ą 0 is the transaction cost constant. The agent's objective is to maximize the sum of (i) total cash they possess by time T , (ii) excess exposure at time T , and (iii) a penalty for risk taking. We express agent-i's objective (total expected reward) as (6.5) R i :" E " X i,T `qi,T pS T ´b2 q i,T q ´b3</p><formula xml:id="formula_19">ż T 0 q 2 i,t dt  ,</formula><p>where b 2 , b 3 ą 0. In Equation (6.5), the second term serves as a cost of instantaneously liquidating the inventory at time T and the last term serves as a penalty for taking on excess risk proportional to the square of the holdings at each time period -the so called urgency penalty. In this objective function, the effect of all agent's trading actions appears implicitly through the dynamics of S, and through its effect on the cash process X i . This particular form of objective assumes that agents have identical preferences which are invariant to agent relabeling 2 . Hence, we may employ the techniques discussed in Subsection 4.1 to simplify the form of the advantage function p A. In our example, we model each component of the advantage function p</p><p>A with neural networks that includes a permutation invariant layer. Our experiments assume a total of five agents over a time horizon of five hours (T " 5) and ten decision points (M " 10) of equal duration.</p><p>6.1. Features. We use the following features to represent the state x t of the environment at time t: Price (S t ): Scalar representing the current price of the asset, Time (t): Scalar representing the current time step the agent is at in the time horizon, Total order flow ( ř iPN q i,0 ´qi,t ): Scalar representing the total order from all agents since the beginning of the trading horizon, and Cumulative transient price impact (Y t ): Scalar representing the current level of transient price impact effecting the price.</p><p>All features are assumed to be non-label invariant.</p><p>6.2. Network Details. The network structure for the advantage function approximation p A θ A consists of two network components: (i) a permutation invariant layer that feeds into (ii) a main network layer. The input of the permutation invariant layer are the label invariant features. This layer, as described in Subsection 4.1, is a fully connected neural network with three hidden layers each containing 20 nodes. Layers are connected by SiLU activation functions <ref type="bibr" target="#b6">[7]</ref>. We then combine the output of this permutation invariant later with the nonlabel invariant features and together they form the inputs to the main network. The main network comprises of four hidden layers with 32 nodes each. The outputs of this main network are the parameters µ θ and tP θ i , Ψ i u iPN , of the approximated advantage function defined in Section 4. These parameters fully specify the value of the advantage function.</p><p>The network structure for the value function approximation V θ V contains four hidden layers with 32 nodes each. This network takes the features from all states described in Subsection 6.1 and outputs the approximate value function for all agents.</p><p>We use the Adam optimizer <ref type="bibr" target="#b20">[21]</ref> with mini-batches and a weight decay of 0.001 to optimize the loss functions defined in Section 5. Mini-batch sizes are set to ten full episodes executed on the period [0, T]. Learning rates are set to 0.003 and are held constant throughout training, modified only by the optimizer's weight decay. Training is performed over a maximum of 20,000 iterations, with a early stopping criteria of no improvement in the loss in the last 3,000 iterations.</p><p>6.3. Baseline -Fictitious Play. Fictitious Play (FP), first introduced in <ref type="bibr" target="#b1">[2]</ref>, is a classical method of determining the Nash Equilibrium of multi-agent games. It has been shown to converge in the two player case for zero-sum games <ref type="bibr" target="#b1">[2]</ref>, potential games <ref type="bibr" target="#b22">[23]</ref>, 2 ˆN games with generic payoffs <ref type="bibr" target="#b0">[1]</ref>, and is often used as a basis for many modern methods <ref type="bibr" target="#b9">[10]</ref>. In general, FP assumes each player follows a stationary mixed strategy that is updated at each iteration by taking the best response to the empirical average of the opponents' previous strategies.</p><p>In our experiments we assume identical agents and thus computing the optimal response for a single agent at each iteration is sufficient. Specifically, let u F i pxq iPN be the optimal FP strategies for each agent which we use the algorithm defined in Algorithm 6.1 to obtain.</p><p>To obtain the optimal response of agent-i against other agents' strategies in line 2 of Algorithm 6.1 we use Deep Deterministic Policy Gradient (DDPG) <ref type="bibr" target="#b19">[20]</ref>. DDPG uses a neural network to represent the optimal policy and the state-action value function. As with our Nash-DQN approach, this approach has no knowledge of the agent's reward function nor the transition dynamics, and hence provides a fair comparison for our Nash-DQN. Due, however, to the nature of DDPG combined with FP it is highly data and training inefficient as each  6.4. Optimization Improvements. We find that directly minimizing Equation (5.1) sometimes produce inaccuracies in the final policy due to a difference in magnitudes of the learned parameters used in the advantage function Equation (4.9), specifically the linear term ψ θ pxq. Adding in the L 1 regularization term (6.6) Lpy m , θ V , θ A q :" Lpy m , θ V , θ A q `β }ψ θ pxq} improves the performance significantly.</p><p>The optimal value for the hyperparameter β is determined by minimizing the average loss Equation (5.1) computed over 1,000 randomly generated simulations. Figure <ref type="figure" target="#fig_3">1</ref> indicates an value of β " 100 is optimal.</p><p>6.5. Results. We use the set of model parameters shown in Table <ref type="table" target="#tab_2">1</ref> in our analysis.  Figure <ref type="figure">2</ref> shows the optimal policies obtained through our Nash-DQN method and the FP method. We can see that despite being significantly more data efficient, the optimal policies are essentially the same. To evaluate the performance of the Nash-DQN method, we considering the following two scenarios: (i) Agent 1 following Nash-DQN and all other agents FP vs. all agents following FP, (ii) Agent 1 following FP and all other agents Nash-DQN vs all agents following Nash-DQN. Next, we apply these policies to 1,000 paths using the simulated environment. Finally, we average total rewards, repeat the whole exercise 100 times, and plot the resulting distributions shown in Figure <ref type="figure" target="#fig_7">3</ref>. From the figures, it appears there is no noticeable discernable difference in the distributions. Indeed, the null hypothesis that the means of the results from FP and Nash-DQN differ cannot be rejected at the 5% level.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions.</head><p>Here we present a computationally tractable reinforcement leanring framework for multi-agent (stochastic) games. Our approach utilizes function approximations after decomposing the collection of agents' state-action value functions into the individual value functions and their advantage functions. Further, we approximate the advantage function in a locally linear-quadratic form and use neural-net architectures to approximate both the value and advantage function. Typical symmetries in games allow us to use permutation invariant neural-nets, motivated by the Arnold-Kolmogorov representation theorem, to reduce the dimensionality of the parameter space. Finally, we develop an actor-critic paradigm to estimate parameters and apply our approach to an important application in electronic trading. Our approach is more data efficient than conventional FP policies, and is applicable to large number of players and continuous state-action spaces.</p><p>There are a number of doors left open for exploration including extending our approach to account for latent factors driving the environment, and when the state of all agents are partially (or completely) hidden from any individual agent. As well, our approach can be easily applied to mean-field games which correspond to the infinite population limit of stochastic games that have interactions where any individual agent has only an infinitesimal contribution to the state dynamics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(4. 6 )</head><label>6</label><figDesc>V θ pxq " N uPU Qθ px; uq and µpxq " arg N uPU Qθ px; uq .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 5 . 1 "1 ą 0 1 Initialize: 2 for 4 Select</head><label>51124</label><figDesc>Nash-DQN Actor-Critic Algorithm Input : # Episodes B ą 0, Minibatch Size M ą 0, # of Game Steps N , Exploration Noise tσ b u B bReplay Buffer D, Parameters pθ A , θ V q Episode b Ð 1 to B do 3 Reset Simulation, get initial state x 0 .for Game steps t Ð 1 to N do actions u Ð µ θ A pxq ` , " N p0, σ b Iq.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 6 . 1 1 for</head><label>611</label><figDesc>Fictitious Play Algorithm Initialize: tu F i pxqu iPN,i‰0 Iteration b Ð 1 to B do 2 Optimize u F 0 pxq assuming others act according to tu F i pxqu iPN,i‰0 . Assign all other agents to this iterations optimal: tu F i pxqu iPN,i‰0 Ð u F 0 pxq 3 return u F 0 pxq FP iteration requires the optimization of the DDPG policy -and each iteration of DDPG uses approximately the same amount of resources as a full training cycle of our Nash-DQN method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Average loss for different β values. Optimal value is β " 100.</figDesc><graphic coords="12,337.73,262.55,177.15,125.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 Figure 2 :</head><label>22</label><figDesc>Figure 2: Optimal trade execution heatmaps of the Nash-DQN and Fictitious Play policies as a function of time, inventory, price, and cumulative transient impact (Y t ), other features were assumed to be equal to zero. Within each panel, subplots corresponds to price levels $9.50, $9.75, . . . , $10.50 from left to right. The dotted line shows the threshold where the agent switches from buying to selling.</figDesc><graphic coords="13,293.44,429.71,199.29,140.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results of deviating from the Fictitious Play policy (i) and the Nash-DQN Policy (ii) when taking the total rewards obtained through randomly initialized simulated environments. Each data point represents the average of 1,000 simulations, repeated 100 times.</figDesc><graphic coords="13,94.15,429.71,199.29,140.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where d 1 , . . . d N are all positive integers. For notational convenience we define d ´i :" ř jPN{tiu d j .1  We now define a specific model for the collection of approximate Q-functions Qθ px; uq " p Qθ The advantage function represents the optimality gap between Q and V . We further assume that for each i P N, p</figDesc><table><row><cell></cell><cell cols="4">A θ i has the linear quadratic form</cell><cell></cell><cell></cell></row><row><cell>(4.4) p A θ i px; uq "</cell><cell>´ˆu i u ´i ´µθ ´µθ i pxq ´ipxq</cell><cell>˙</cell><cell cols="2">P θ i pxq ˆui u ´i ´µθ ´µθ i pxq ´ipxq ˙``u</cell><cell>´i ´µθ ´ipxq</cell><cell>˘</cell><cell>Ψ θ i pxq ,</cell></row><row><cell cols="2">where the block matrix</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(4.5)</cell><cell cols="3">P θ i pxq :"</cell><cell>P θ 21,i pxq P θ 22,i pxq ˆP θ 11,i pxq P θ 12,i pxq ˙,</cell><cell></cell><cell></cell></row></table><note>i px; uqq iPN . For each θ P Θ, we have Qθ : X Ś U Ñ R N and decompose the Q-function into two components:(4.3)Qθ px; uq " V θ pxq `p A θ px; uq , where V θ pxq " p V θ i pxqq iPN is a model of the collection of value functions so that V θ : X Ñ R N and where p A θ px; uq " p p A θ i px; uqq iPN is what we refer to as the collection of advantage functions.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc><ref type="bibr" target="#b3">4)</ref> simplifies. Assuming that d j " d, for all j P N, independent label invariance in only the actions of agents requires p A i to have the simplified form</figDesc><table><row><cell>(4.9)</cell><cell>p A θ i px; uq "</cell><cell>´› › ›u i ´ÿ jPN{tiu ´µθ i pxq u j ´µθ › › › 2 P θ 11,i pxq j pxq 2 ´ÿ jPN{tiu P θ 22,i pxq `ÿ jPN{tiu A ´ui ´µi pxq θ ´uj ´µθ ¯, ´uj j pxq ¯ ψ θ pxq , ´µθ j pxq</cell><cell>¯EP θ 12,i pxq</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Asset price process, price impact, and risk preference parameters.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our approach can be easily extended to the case of controls that are restricted to convex subsets of R d i .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We could extend this to include the case of sub-populations with homogeneous preferences, but that are heterogeneous across sub-populations.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fictitious play in 2ˆn games</title>
		<author>
			<persName><forename type="first">U</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Theory</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="139" to="154" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Iterative solution of games by fictitious play, Activity analysis of production and allocation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951">1951</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="374" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A comprehensive survey of multiagent reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">De</forename><surname>Schutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="156" to="172" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Probabilistic theory of mean field games: vol. i, mean field fbsdes, control, and games, Stochastic Analysis and Applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Carmona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Delarue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Casgrain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1803.04094</idno>
		<title level="m">Mean field games with partial information for algorithmic trading</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mean-field games with differing beliefs for algorithmic trading, Mathematical Finance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Casgrain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="995" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sigmoid-weighted linear units for neural network function approximation in reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Elfwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Uchibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Doya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Continuous deep q-learning with model-based acceleration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2829" to="2838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09585</idno>
		<title level="m">Learning mean-field games</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fictitious self-play in extensive-form games</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="805" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rainbow: Combining improvements in deep reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nash q-learning for general-sum stochastic games</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Wellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1039" to="1069" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.09376</idno>
		<title level="m">Deep fictitious play for stochastic differential games</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-population LQG games involving a major player: the Nash certainty equivalence principle</title>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="3318" to="3353" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large population stochastic dynamic games: closed-loop mckean-vlasov systems and the Nash certainty equivalence principle</title>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Malhamé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Caines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Information &amp; Systems</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="221" to="252" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mean-field game strategies for optimal execution</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jaimungal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nourian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Mathematical Finance, Forthcoming</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Actor-critic algorithms</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1008" to="1014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A unified game-theoretic approach to multiagent reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gruslys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pérolat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4190" to="4203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mean field games</title>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Lasry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-L</forename><surname>Lions</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Japanese journal of mathematics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="229" to="260" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<title level="m">Playing atari with deep reinforcement learning</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Potential games</title>
		<author>
			<persName><forename type="first">D</forename><surname>Monderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Shapley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Games and economic behavior</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="124" to="143" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Trading with the crowd</title>
		<author>
			<persName><forename type="first">E</forename><surname>Neuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Voß</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Available at SSRN</title>
		<imprint>
			<biblScope unit="volume">3868708</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Nash mean field game theory for nonlinear stochastic dynamical systems with major and minor agents</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nourian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Caines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="3302" to="3331" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A generalized iterative lqg method for locally-optimal feedback control of constrained nonlinear stochastic systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005</title>
				<meeting>the 2005</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="300" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
