<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TrojanZoo: Towards Unified, Holistic, and Practical Evaluation of Neural Backdoors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-21">21 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ren</forename><surname>Pang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangshan</forename><surname>Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhaohan</forename><surname>Xi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shouling</forename><surname>Ji</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Cheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiapu</forename><surname>Luo</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ting</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TrojanZoo: Towards Unified, Holistic, and Practical Evaluation of Neural Backdoors</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-21">21 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">2967DA3F7D77A565F3DC9892F7A45085</idno>
					<idno type="arXiv">arXiv:2012.09302v4[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-10-27T19:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>backdoor attack</term>
					<term>backdoor defense</term>
					<term>benchmark platform</term>
					<term>deep learning security</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural backdoors represent one primary threat to the security of deep learning systems. The intensive research has produced a plethora of backdoor attacks/defenses, resulting in a constant arms race. However, due to the lack of evaluation benchmarks, many critical questions remain under-explored: (i) what are the strengths and limitations of different attacks/defenses? (ii) what are the best practices to operate them? and (iii) how can the existing attacks/defenses be further improved?</p><p>To bridge this gap, we design and implement TROJAN-ZOO, the first open-source platform for evaluating neural backdoor attacks/defenses in a unified, holistic, and practical manner. Thus far, focusing on the computer vision domain, it has incorporated 8 representative attacks, 14 state-of-theart defenses, 6 attack performance metrics, 10 defense utility metrics, as well as rich tools for in-depth analysis of the attack-defense interactions. Leveraging TROJANZOO, we conduct a systematic study on the existing attacks/defenses, unveiling their complex design spectrum: both manifest intricate trade-offs among multiple desiderata (e.g., the effectiveness, evasiveness, and transferability of attacks). We further explore improving the existing attacks/defenses, leading to a number of interesting findings: (i) one-pixel triggers often suffice; (ii) training from scratch often outperforms perturbing benign models to craft trojan models; (iii) optimizing triggers and trojan models jointly greatly improves both attack effectiveness and evasiveness; (iv) individual defenses can often be evaded by adaptive attacks; and (v) exploiting model interpretability significantly improves defense robustness. We envision that TROJANZOO will serve as a valuable platform to facilitate future research on neural backdoors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Today's deep learning (DL) systems are large, complex software artifacts. With the increasing system complexity and training cost, it becomes not only tempting but also necessary to exploit pre-trained deep neural networks (DNNs) in building DL systems. It was estimated that as of 2016, over 13.7% of DL-related repositories on GitHub re-use at least one pre-trained DNN <ref type="bibr" target="#b26">[27]</ref>. On the upside, this "plug-and-play" paradigm greatly simplifies the development cycles <ref type="bibr" target="#b46">[47]</ref>. On the downside, as most pre-trained DNNs are contributed by untrusted third par-ties <ref type="bibr" target="#b7">[8]</ref>, their lack of standardization or regulation entails profound security implications.</p><p>In particular, pre-trained DNNs can be exploited to launch neural backdoor attacks <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b42">[43]</ref>, one primary threat to the security of DL systems. In such attacks, a maliciously crafted DNN ("trojan model") forces its host system to misbehave once certain pre-defined conditions ("triggers") are met but to function normally otherwise, which can result in consequential damages in securitysensitive domains <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b60">[61]</ref>.</p><p>Motivated by this, intensive research has led to a plethora of attacks that craft trojan model via exploiting properties such as neural activation patterns <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b67">[68]</ref> and defenses that mitigate trojan models during inspection <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b61">[62]</ref> or detect trigger inputs at inference <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b59">[60]</ref>. With the rapid development of new attacks/defenses, a number of open questions have emerged: RQ 1 -What are the strengths and limitations of different attacks/defenses? RQ 2 -What are the best practices (e.g., optimization strategies) to operate them? RQ 3 -How can the existing backdoor attacks/defenses be further improved?</p><p>Despite their importance for understanding and mitigating the vulnerabilities incurred by neural backdoors, these questions are largely under-explored due to the following challenges.</p><p>Non-holistic evaluations -Most studies conduct the evaluation with a fairly limited set of attacks/defenses, resulting in incomplete assessment. For instance, it is unknown whether STRIP <ref type="bibr" target="#b18">[19]</ref> is effective against the newer ABE attack <ref type="bibr" target="#b30">[31]</ref>. Further, the evaluation often uses simple, macro-level metrics, failing to comprehensively characterize given attacks/defenses. For instance, most studies use attack success rate (ASR) and clean accuracy drop (CAD) to assess attack performance, which is insufficient to describe the attack's ability of trading between these two metrics.</p><p>Non-unified platforms -Due to the lack of unified benchmarks, different attacks/defenses are often evaluated under inconsistent settings, leading to non-comparable conclusions. For instance, TNN <ref type="bibr" target="#b37">[38]</ref> and LB <ref type="bibr" target="#b67">[68]</ref> are evaluated with distinct trigger definitions (i.e., shape, size, and transparency), datasets, and DNNs, making it difficult to directly compare their assessment.</p><p>Non-adaptive attacks -The evaluation of the existing defense <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b61">[62]</ref> often assume static, non-adaptive attacks, without fully accounting for the adversary's possible countermeasures, which however is critical for modeling the adversary's optimal strategies and assessing the attack vulnerabilities in realistic settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Work</head><p>To this end, we design, implement, and evaluate TRO-JANZOO, an open-source platform for assessing neural backdoor attacks/defenses in a unified, holistic, and practical manner. Note that while it is extensible to other domains (e.g., NLP), currently, TROJANZOO focuses on the image classification task in the computer vision domain. Our contributions are summarized in three major aspects:</p><p>Platform -To our best knowledge, TROJANZOO represents the first open-source platform specifically designed for evaluating neural backdoor attacks/defenses. At the moment of writing (02/06/2022), focusing on the computer vision domain, TROJANZOO has incorporated 8 representative attacks, 14 state-of-the-art defenses, 6 attack performance metrics, 10 defense utility metrics, as well as a benchmark suite of 5 DNN models, 5 downstream models, and 6 datasets. Further, TROJANZOO implements a rich set of tools for in-depth analysis of the attack-defense interactions, including measuring feature-space similarity, tracing neural activation patterns, and comparing attribution maps.</p><p>Measurement -Leveraging TROJANZOO, we conduct a systematic study of the existing attacks/defenses, unveiling the complex design spectrum for the adversary and the defender. Different attacks manifest delicate tradeoffs among effectiveness, evasiveness, and transferability. For instance, weaker attacks (i.e., lower ASR) tend to show higher transferability. Meanwhile, different defenses demonstrate trade-offs among robustness, utilitypreservation, and detection accuracy. For instance, while effective against a variety of attacks, model sanitization <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b39">[40]</ref> also incur a significant accuracy drop. These observations indicate the importance of using comprehensive metrics to evaluate neural backdoor attacks/defenses, and suggest the optimal practices of applying them under given settings.</p><p>Exploration -We further explore improving existing attacks/defenses, leading to a number of previously unknown findings including (i) one-pixel triggers often suffice (over 95% ASR); (ii) training from scratch often outperforms perturbing benign models to forge trojan models; (iii) leveraging DNN architectures (e.g., skip connects) in optimizing trojan models improves the attack effectiveness; (iv) most individual defenses are vulnerable to adaptive attacks; and (v) exploiting model interpretability significantly improves defense robustness. We envision that the TROJANZOO platform and our findings will facilitate future research on neural backdoors and shed light on designing and building DL systems in a more secure and informative manner. 1   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Roadmap</head><p>The remainder of the paper proceeds as follows. § 3 introduces fundamental concepts and assumptions; § 4 1. All the data, models, and code used in the paper are released at: https://github.com/ain-soph/trojanzoo. details the design and implementation of TROJANZOO and systemizes existing attacks/defenses; equipped with TRO-JANZOO, § 5 conducts a systematic evaluation of existing attacks/defenses; § 6 explores their further improvement; § 7 discusses the limitations of TROJANZOO and points to future directions; the paper is concluded in § 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Some recent studies have surveyed neural backdoor attacks/defenses (e.g., <ref type="bibr" target="#b32">[33]</ref>); yet, none of them provides benchmark implementation or empirical evaluation to explore their strengths/limitations. Compared with the rich collection of platforms for adversarial attacks/defenses (e.g., CLEVERHANS <ref type="bibr" target="#b1">[2]</ref>, DEEPSEC <ref type="bibr" target="#b34">[35]</ref>, and ADVBOX <ref type="bibr" target="#b0">[1]</ref>), only few platforms currently support evaluating neural backdoors. For instance, ART <ref type="bibr" target="#b2">[3]</ref> integrates 3 attacks and 3 defenses.</p><p>In comparison, TROJANZOO differs in major aspects: (i) to our best knowledge, it features the most comprehensive library of attacks/defenses; (ii) it regards the evaluation metrics as a first-class citizen and implements 6 attack performance metrics and 10 defense utility metrics, which holistically assess given attacks/defenses; (iii) besides reference implementation, it also provides rich utility tools for in-depth analysis of attack-defense interactions, such as measuring feature-space similarity, tracing neural activation patterns, and comparing attribution maps. r The work closest to ours is perhaps TROJAI <ref type="bibr" target="#b3">[4]</ref>, which is a contest platform for model-inspection defenses against neural backdoors. While compared with TRO-JANZOO, TROJAI provides a much larger pool of trojan models (over 10K) across different modalities (e.g., vision and NLP), TROJANZOO departs from TROJAI in majors aspects and offers its unique value. (i) Given its contestlike setting, TROJAI is a closed platform focusing on evaluating model-inspection defenses (i.e., detecting trojan models) against fixed attacks, while TROJANZOO is an open platform that provides extensible datasets, models, attacks, and defenses. Thus, TROJANZOO may serve the needs ranging from conducting comparative studies of existing attacks/defenses to exploring and evaluating new attacks/defenses. (ii) While TROJAI focuses on modelinspection defenses, TROJANZOO integrates four major defense categories. (iii) In TROJAI, for its purpose, the concrete attacks behind the trojan models are unknown, which makes it challenging to assess the strengths/limitations of given defenses with respect to different attacks, while in TROJANZOO one may directly evaluate such interactions. (iv) As the attacks are fixed in TROJAI, one may not evaluate adaptive attacks. (v) The main metric used in TROJAI is the accuracy that defenses successfully detect trojan models, while TROJANZOO provides a much richer set of metrics to characterize attacks/defenses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Fundamentals</head><p>We first introduce fundamental concepts and assumptions used throughout the paper. The important notations are summarized in Table <ref type="table" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notation Definition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Deep neural networks (DNNs) -Deep neural networks (DNNs) represent a class of ML models to learn high-level abstractions of complex data. We assume a predictive setting, in which a DNN f θ (parameterized by θ) encodes a function f θ : R n → S m , where n and m denote the input dimensionality and the number of classes. Given input x, f (x) is a probability vector (simplex) over m classes.</p><p>Pre-trained DNNs -Today, it becomes not only tempting but also necessary to reuse pre-trained models in domains in which data labeling or model training is expensive <ref type="bibr" target="#b69">[70]</ref>. Under the transfer learning setting, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, the feature extractor (FE) g of a pre-trained model is often reused and composed with a classifier h to form an end-to-end model f . As the data used to train g may differ from the downstream task, it is often necessary to fine-tune f = h • g in a supervised manner. One may opt to perform full-tuning to train both g and h or partialtuning to train h only with g fixed <ref type="bibr" target="#b26">[27]</ref>.</p><p>Neural backdoor attacks -With the increasing use of DNN models in security-sensitive domains, the adversary is strongly incentivized to forge malicious FEs as attack vectors and lure victim users to re-use them during system development <ref type="bibr" target="#b20">[21]</ref>. Specifically, through a malicious FE, the backdoor attack infects the target model with malicious functions desired by the adversary, which are activated once pre-defined conditions ("triggers") are present. We refer to such infected models as "trojan models". Typically, a trojan model reacts to trigger-embedded inputs (e.g., images with specific watermarks) in a highly predictable manner (e.g., misclassified to a target class) but functions normally otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Specifics</head><p>Trigger mixing operator -For given trigger r, the operator ⊕ mixes a clean input x ∈ R n with r to generate a trigger input x ⊕ r. Typically, r comprises three parts: (i) mask m ∈ {0, 1}</p><p>n specifies where r is applied (i.e., x's i-th feature x i is retained if m i is on and mixed with r otherwise); (ii) transparency α ∈ [0, 1] specifies the mixing weight; and (iii) pattern p(x) ∈ R n specifies r's color intensity, which can be a constant, randomly drawn from a distribution (e.g., by perturbing a template), or dependent on x <ref type="bibr" target="#b44">[45]</ref>. Formally, the trigger embedding operator is defined as:</p><formula xml:id="formula_0">x ⊕ r = (1 − m) [(1 − α)x + αp(x)] + m x (1)</formula><p>where denotes element-wise multiplication.</p><p>Attack objectives -The trojan model satisfies that with high probability, (i) trigger inputs are classified to the target class desired by the adversary and (ii) clean input are still correctly classified. Formally, the adversary forges the malicious FE by optimizing the following objective:</p><formula xml:id="formula_1">min r∈R,θ E (x,y)∈T [ (f θ (x ⊕ r), t) + λ (f θ (x), y)]<label>(2)</label></formula><p>where T represents the training set, t denotes the target class, and trigger r is selected from the feasible set R (which constrains r's shape, transparency, and/or pattern).</p><p>Intuitively, the first and second terms describe (i) and (ii), respectively, and the hyper-parameter λ balances the two objectives.</p><p>Adversary's knowledge -If the downstream classifier h is known to the adversary, f shares the same architecture with the model h • g used by the victim; otherwise, the adversary may resort to a surrogate classifier h * (i.e., h * • g) or re-define the loss (f (x ⊕ r), t) in terms of latent representations <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b67">[68]</ref> as ∆(g(x ⊕ r), φ t ), that is, the difference(e.g., MSE loss) between g(x⊕r) and φ t , where φ t is the average latent representation of class t.</p><p>Malicious FE training -To optimize Eqn. 2, one may perturb a benign FE <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b54">[55]</ref> or train the malicious FE from scratch (details in § 6). To satisfy the trigger constraint, r can be fixed <ref type="bibr" target="#b20">[21]</ref>, partially defined <ref type="bibr" target="#b37">[38]</ref> (e.g., with its mask fixed), or optimized with f jointly <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Platform</head><p>As illustrated in Figure <ref type="figure">2</ref>, TROJANZOO comprises three major components: (i) the attack library integrates a set of representative attacks that, for given benign models and clean inputs, are able to generate trojan models and trigger inputs; (ii) the defense library integrates a set of stateof-the-art defenses that are able to provide model-and input-level protection against trojan models and trigger inputs; and (iii) the analysis engine, equipped with attack performance metrics, defense utility metrics, and featurerich utility tools, is able to conduct unified and holistic evaluation across different attacks/defenses.</p><p>In its current implementation, TROJANZOO has incorporated 8 attacks, 14 defenses, 6 attack performance metrics, and 10 defense utility metrics, which we systematize as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Attack Library</head><p>While neural backdoor attacks can be characterized from a number of aspects, here we focus on 4 key design choices by the adversary that directly impact attack  <ref type="bibr" target="#b20">[21]</ref> ESB <ref type="bibr" target="#b56">[57]</ref> TNN <ref type="bibr" target="#b37">[38]</ref> RB <ref type="bibr" target="#b38">[39]</ref> TB <ref type="bibr" target="#b11">[12]</ref> LB <ref type="bibr" target="#b67">[68]</ref> ABE <ref type="bibr" target="#b30">[31]</ref> IMC <ref type="bibr" target="#b42">[43]</ref> Table <ref type="table">2</ref>. Summary of representative neural backdoor attacks currently implemented in TROJANZOO ( -full optimization, -partial optimization, -no optimization)</p><p>Non-optimization -The attack simply solves Eqn. 2 under pre-defined triggers (i.e., shape, transparency, and pattern) without optimization for other desiderata.</p><p>-BadNet (BN) <ref type="bibr" target="#b20">[21]</ref>, as the representative, pre-defines trigger r, generates trigger inputs {(x ⊕ r, t)}, and crafts the trojan model f * by re-training a benign model f with such data.</p><p>Architecture modifiabilitywhether the attack is able to change the DNN architecture. Being allowed to modify both the architecture and the parameters enables a larger attack spectrum, but also renders the trojan model more susceptible to certain defenses (e.g., model specification checking).</p><p>-Embarrassingly-Simple-Backdoor (ESB) <ref type="bibr" target="#b56">[57]</ref>, as the representative, modifies f 's architecture by adding a module which overwrites the prediction as t if r is recognized. Without disturbing f 's original configuration, f * retains f 's predictive power on clean inputs.</p><p>Trigger optimizabilitywhether the attack uses a fixed, pre-defined trigger or optimizes it during crafting the trojan model. Trigger optimization often leads to stronger attacks with respect to given desiderata (e.g., trigger stealthiness).</p><p>-TrojanNN (TNN) <ref type="bibr" target="#b37">[38]</ref> fixes r's shape and position, optimizes its pattern to activate neurons rarely activated by clean inputs in pre-processing, and then forges f * by re-training f in a manner similar to BN.</p><p>-Reflection-Backdoor (RB) <ref type="bibr" target="#b38">[39]</ref> optimizes trigger stealthiness by defining r as the physical reflection of a clean image x r (selected from a pool): r = x r ⊗k, where k is a convolution kernel, and ⊗ is the convolution operator.</p><p>-Targeted-Backdoor (TB) <ref type="bibr" target="#b11">[12]</ref> randomly generates r's position in training, which makes f * effective regardless of r's position and allows the adversary to optimize r's stealthiness by placing it at the most plausible position (e.g., an eyewear watermark over eyes).</p><p>Fine-tuning survivabilitywhether the backdoor remains effective if the model is fine-tuned. A pre-trained model is often composed with a classifier and fine-tuned using the data from the downstream task. It is desirable to ensure that the backdoor remains effective after finetuning.</p><p>-Latent Backdoor (LB) <ref type="bibr" target="#b67">[68]</ref> accounts for the impact of downstream fine-tuning by optimizing g with respect to latent representations rather than final predictions. Specifically, it instantiates Eqn. 2 with the following loss function: (g(x ⊕ r), t) = ∆(g(x ⊕ r), φ t ), where ∆ measures the difference of two latent representations and φ t denotes the average representation of class t, defined as φ t = arg min φ E (x,t)∈T [g(x)].</p><p>Defense adaptivitywhether the attack is optimizable to evade possible defenses. For the attack to be effective, it is essential to optimize the evasiveness of the trojan model and the trigger input with respect to the deployed defenses.</p><p>-Adversarial-Backdoor-Embedding (ABE) <ref type="bibr" target="#b30">[31]</ref> accounts for possible defenses in forging g * . In solving Eqn. 2, ABE also optimizes the indistinguishability of the latent representations of trigger and clean inputs. Specifically, it uses a discriminative network d to predict the representation of a given input x as trigger or clean. Formally, the loss is defined as ∆(d • g(x), b(x)), where b(x) encodes whether x is trigger or clean, while g * and d are trained using an adversarial learning framework <ref type="bibr" target="#b19">[20]</ref>.</p><p>Multi-optimizationwhether the attack is optimizable with respect to multiple objectives listed above.</p><p>-Input-Model Co-optimization (IMC) <ref type="bibr" target="#b42">[43]</ref> is motivated by the mutual-reinforcement effect between r and f * : optimizing one amplifies the effectiveness of the other. Instead of solving Eqn. 2 by first pre-defining r and then optimizing f * , IMC optimizes r and f * jointly, which enlarges the search spaces for r and f * , leading to attacks satisfying multiple desiderata (e.g., fine-tuning survivability and defense adaptivity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Backdoor Defense Category</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mitigation Detection Target Design Rationale Input Model Input Model Trigger</head><p>Randomized-Smoothing (RS) <ref type="bibr" target="#b13">[14]</ref> Input Reformation A's fidelity (x's and x * 's surrounding class boundaries) Down-Upsampling (DU) <ref type="bibr" target="#b65">[66]</ref> A's fidelity (x's and x * 's high-level features) Manifold-Projection (MP) <ref type="bibr" target="#b40">[41]</ref> A's fidelity (x's and x * 's manifold projections) Activation-Clustering (AC) <ref type="bibr" target="#b9">[10]</ref> Input Filtering distinct activation patterns of {x} and {x * } Spectral-Signature (SS) <ref type="bibr" target="#b58">[59]</ref> distinct activation patterns of {x} and {x * } (spectral space) STRIP (STRIP) <ref type="bibr" target="#b18">[19]</ref> distinct self-entropy of x's and x * 's mixtures with clean inputs NEO (NEO) <ref type="bibr" target="#b59">[60]</ref> sensitivity of f * 's prediction to trigger perturbation Adversarial-Retraining (AR) <ref type="bibr" target="#b39">[40]</ref> Model A's fidelity (x's and x * 's surrounding class boundaries) Fine-Pruning (FP) <ref type="bibr" target="#b35">[36]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sanitization</head><p>A's use of neurons rarely activated by clean inputs NeuralCleanse (NC) <ref type="bibr" target="#b61">[62]</ref> Model Inpsection abnormally small perturbation from other classes to t in f DeepInspect (DI) <ref type="bibr" target="#b10">[11]</ref> abnormally small perturbation from other classes to t in f * TABOR (TABOR) <ref type="bibr" target="#b22">[23]</ref> abnormally small perturbation from other classes to t in f NeuronInspect (NI) <ref type="bibr" target="#b25">[26]</ref> distinct explanations of f and f * with respect to clean inputs ABS (ABS) <ref type="bibr" target="#b36">[37]</ref> A's use of neurons elevating t's prediction </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Attack Performance Metrics</head><p>Currently, TROJANZOO incorporates 6 metrics to assess the effectiveness, evasiveness, and transferability of given attacks.</p><p>Attack success rate (ASR) -which measures the likelihood that trigger inputs are classified to the target class t:</p><formula xml:id="formula_2">Attack Success Rate (ASR) = # successful trials # total trials (3)</formula><p>Typically, higher ASR indicates more effective attacks.</p><p>Trojan misclassification confidence (TMC) -which is the average confidence score assigned to class t of trigger inputs in successful attacks. Intuitively, TMC complements ASR and measures attack efficacy from another perspective. For two attacks with the same ASR, we consider the one with higher TMC a stronger one.</p><p>Clean accuracy drop (CAD) -which measures the difference of the classification accuracy of benign and trojan models; CAD measures whether the attack directs its influence to trigger inputs only.</p><p>Clean classification confidence (CCC) -which is the average confidence assigned to the ground-truth classes of clean inputs; CCC complements CAD by measuring attack specificity from the perspective of classification confidence.</p><p>Efficacy-specificity AUC (AUC) -which quantifies the aggregated trade-off between attack efficacy (measured by ASR) and attack specificity (measured by CAD). As revealed in <ref type="bibr" target="#b42">[43]</ref>, there exists an intricate balance: at a proper cost of specificity, it is possible to significantly improve efficacy, and vice versa; AUC measures the area under the ASR-CAD curve. Intuitively, smaller AUC implies a more significant trade-off effect.</p><p>Neuron-separation ratio (NSR) -which measures the intersection between neurons activated by clean and trigger inputs. In the penultimate layer of the model, we find N c and N t , the top-k active neurons with respect to clean and trigger inputs, respectively, and calculate their jaccard index:</p><formula xml:id="formula_3">Neuron Separation Ratio (NSR) = 1 − |N t ∩ N c | |N t ∪ N c |<label>(4)</label></formula><p>Intuitively, NSR compares the neural activation patterns of clean and trigger inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Defense Library</head><p>The existing defenses against neural backdoors, according to their strategies, can be classified into 4 major categories, as summarized in Table <ref type="table" target="#tab_2">3</ref>. Notably, we focus on the setting of transfer learning or outsourced training, which precludes certain other defenses such as purging poisoning training data <ref type="bibr" target="#b52">[53]</ref>. Next, we detail the 14 representative defenses currently implemented in TROJANZOO.</p><p>Input reformationwhich, before feeding an incoming input to the model, first reforms it to mitigate the influence of the potential trigger, yet without explicitly detecting whether it is a trigger input. It typically exploits the high fidelity of attack A, that is, A tends to retain the perceptual similarity of a clean input x and its trigger counterpart x * .</p><p>-Randomized-Smoothing (RS) <ref type="bibr" target="#b13">[14]</ref> exploits the premise that A retains the similarity of x and x * in terms of their surrounding class boundaries and classifies an input by averaging the predictions within its vicinity (via adding Gaussian noise).</p><p>-Down-Upsampling (DU) <ref type="bibr" target="#b65">[66]</ref> exploits the premise that A retains the similarity of x and x * in terms of their high-level features while the trigger r is typically not perturbation-tolerant. By downsampling and then upsampling x * , it is possible to mitigate r's influence.</p><p>-Manifold-Projection (MP) <ref type="bibr" target="#b40">[41]</ref> exploits the premise that A retains the similarity of x and x * in terms of their projections to the data manifold. Thus, it trains an autoencoder to learn an approximate manifold, which projects x * to the manifold.</p><p>Input filteringwhich detects whether an incoming input is embedded with a trigger and possibly recovers the clean input. It typically distinguishes clean and trigger inputs using their distinct characteristics.</p><p>-Activation-Clustering (AC) <ref type="bibr" target="#b9">[10]</ref> distinguishes clean and trigger inputs by clustering their latent representations. While AC is also applicable for purging poisoning data, we consider its use as an input filtering method at inference time.</p><p>-Spectral-Signature (SS) <ref type="bibr" target="#b58">[59]</ref> exploits the similar property in the spectral space.</p><p>-STRIP <ref type="bibr" target="#b18">[19]</ref> mixes a given input with a clean input and measures the self-entropy of its prediction. If the input is trigger-embedded, the mixture remains dominated by the trigger and tends to be misclassified, resulting in low self-entropy.</p><p>-NEO <ref type="bibr" target="#b59">[60]</ref> detects a trigger input by searching for a position, if replaced by a "blocker", changes its prediction, and uses this substitution to recover its original prediction.</p><p>Model sanitizationwhich, before using a pretrained model f , sanitizes it to mitigate the potential backdoor, yet without explicitly detecting whether f is tampered.</p><p>-Adversarial-Retraining (AR) <ref type="bibr" target="#b39">[40]</ref> treats trigger inputs as one type of adversarial inputs and applies adversarial training over the pre-trained model to improves its robustness to backdoor attacks.</p><p>-Fine-Pruning (FP) <ref type="bibr" target="#b35">[36]</ref> uses the property that the attack exploits spare model capacity. It thus prunes rarely used neurons and then applies fine-tuning to defend against pruning-aware attacks.</p><p>Model inspectionwhich determines whether f is a trojan model and, if so, recovers the target class and the potential trigger, at the model checking stage.</p><p>-NeuralCleanse (NC) <ref type="bibr" target="#b61">[62]</ref> searches for potential triggers in each class t. If t is trigger-embedded, the minimum perturbation required to change the predictions of the inputs in other classes to t is abnormally small.</p><p>-DeepInspect (DI) <ref type="bibr" target="#b10">[11]</ref> follows a similar pipeline but uses a generative network to generate trigger candidates.</p><p>-TABOR <ref type="bibr" target="#b22">[23]</ref> extends NC by adding a new regularizer to control the trigger search space.</p><p>-NeuronInspect (NI) <ref type="bibr" target="#b25">[26]</ref> exploits the property that the explanation heatmaps of benign and trojan models manifest distinct characteristics. Using the features extracted from such heatmaps, NI detects trojan models as outliers.</p><p>-ABS <ref type="bibr" target="#b36">[37]</ref> inspects f to sift out abnormal neurons with large elevation difference (i.e., active only with respect to one specific class) and identifies triggers by maximizing abnormal neuron activation while preserving normal neuron behaviors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Defense Utility Metrics</head><p>Currently, TROJANZOO incorporates 10 metrics to evaluate the robustness, utility-preservation, and genericity of given defenses. The metrics are tailored to the objectives of each defense category (e.g., trigger input detection). For ease of exposition, below we consider the performance of a given defense D with respect to a given attack A.</p><p>Attack rate deduction (ARD) -which measures the difference of A's ASR before and after D. Intuitively, ARD indicates D's impact on A's efficacy. Intuitively, larger ARD indicates more effective defense. We also use A's TMC to measure D's influence on the classification confidence of trigger inputs.</p><p>Clean accuracy drop (CAD) -which measures the difference of the ACC of clean inputs before and after D is applied. It measures D's impact on clean inputs. Note that CAD here is defined differently from its counterpart in attack performance metrics. We also use CCC to measure D's influence on the classification confidence of clean inputs.</p><p>True positive rate (TPR) -which, for input-filtering methods, measures the performance of detecting trigger inputs.</p><p>True Positive Rate (TPR) = # detected trigger inputs # total trigger inputs <ref type="bibr" target="#b4">(5)</ref> Correspondingly, we use false positive rate (FPR) to measure the error of misclassifying clean inputs as trigger inputs.</p><p>Anomaly index value (AIV) -which measures the anomaly of trojan models in model-inspection defenses. Most existing methods (e.g., <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b61">[62]</ref>) formalize finding trojan models as outlier detection: each class t is associated with a score (e.g., minimum perturbation); if its score significantly deviates from others, t is considered to contain a backdoor. AIV, the absolute deviations from median normalized by median absolute deviation (MAD), provide a reliable measure for such dispersion. Typically, t with AIV larger than 2 has over 95% probability of being anomaly.</p><p>Mask L 1 norm (MLN) -which measures the 1 -norm of the triggers recovered by model-inspection methods.</p><p>Mask jaccard similarity (MJS) -which further measures the intersection between the recovered trigger and the ground-truth trigger (injected by the adversary). Let m o and m r be the masks of original and recovered triggers. We define MJS as the Jaccard similarity of m o and m r :</p><formula xml:id="formula_4">Mask Jaccard Similarity (MJS) = |O(m o ) ∩ O(m r )| |O(m o ) ∪ O(m r )|<label>(6)</label></formula><p>where O(m) denotes the set of non-zero elements in m.</p><p>Average running time (ART) -which measures D's overhead. For model sanitization or inspection, which is performed offline, ART is measured as the running time per model; while for input filtering or reformation, which is executed online, ART is measured as the execution time per input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Assessment</head><p>Leveraging TROJANZOO, we conduct a systematic assessment of the existing attacks and defenses and unveil their complex design spectrum: both attacks and defenses tend to manifest intricate trade-offs among multiple desiderata. We begin by describing the setting of the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setting</head><p>Datasets -In the evaluation, we primarily use 5 datasets: CIFAR10 <ref type="bibr" target="#b28">[29]</ref>, CIFAR100 <ref type="bibr" target="#b28">[29]</ref>, ImageNet <ref type="bibr" target="#b15">[16]</ref>, GTSRB <ref type="bibr" target="#b51">[52]</ref>, and VGGFace2 <ref type="bibr" target="#b8">[9]</ref>, with their statistics summarized in Table <ref type="table" target="#tab_3">4</ref>.</p><p>Models -We consider 3 representative DNN models: VGG <ref type="bibr" target="#b50">[51]</ref>, ResNet <ref type="bibr" target="#b23">[24]</ref>, and DenseNet <ref type="bibr" target="#b24">[25]</ref>. Using models of distinct architectures (e.g., residual blocks versus skip connections), we factor out the influence of individual model characteristics. <ref type="bibr">By</ref>  the downstream classifier comprising one fully-connected layer with softmax activation (1FCN). We also consider other types of classifiers, including Bayes, SVM, and Random Forest. The ACC of benign models is summarized in Table <ref type="table" target="#tab_3">4</ref>. Attacks, Defenses, and Metrics -In the evaluation, we exemplify with 8 attacks in Table <ref type="table">2</ref> and 12 defenses in Table <ref type="table" target="#tab_2">3</ref>, and measure them using all the metrics in § 4.2 and § 4.4. In all the experiments, we generate 10 trojan models for a given attack under each setting and 100 pairs of clean-trigger inputs with respect to each trojan model. The reported results are averaged over these cases.</p><p>Implementation -All the models, algorithms, and measurements are implemented in PyTorch. The default parameter setting is summarized in Table <ref type="table" target="#tab_20">20</ref> and 21 ( § A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Attack Evaluation</head><p>We evaluate the existing attacks under the vanilla setting (without defenses), aiming to understand the impact of various design choices on the attack performance. Due to space limitations, we mainly report the results on CIFAR10 and defer the results on other datasets to § B. Overall, different attacks manifest intricate trade-offs among effectiveness, evasiveness, and transferability, as detailed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Effectiveness vs. Evasiveness (Trigger) .</head><p>We start with the effectiveness-evasiveness trade-off. Intuitively, the effectiveness measures whether the trigger inputs are successfully misclassified into the target class, while the evasiveness measures whether the trigger inputs and trojan models are distinguishable from their normal counterparts. Here, we first consider the evasiveness of triggers. Trigger size -Recall that the trigger definition comprises mask m, transparency α, and pattern p. We measure how the attack effectiveness varies with the trigger size |m|. To make fair comparison, we bound the clean accuracy drop (CAD) of all the attacks below 3% via controlling the number of optimization iterations n iter . Figure <ref type="figure" target="#fig_1">3</ref> plots the attack success rate (ASR) and trojan misclassification confidence (TMC) of various attacks under varying |m| (with fixed α = 0.8).</p><p>Observe that most attacks seem insensitive to |m|: as |m| varies from 2×2 to 5×5, the ASR of most attacks increases by less than 10%, except RB with over 30% growth. This may be attributed to its additional constraints: RB defines the trigger to be the reflection of another image; thus, increasing |m| may improve its perturbation spaces. Compared with other attacks, TB and ESB perform poorly because TB aims to force inputs with random triggers to be misclassified while ESB is unable to account for trigger transparency during training. Also observe that the TMC of most attacks remains close to 1.0 regardless of |m|. Trigger transparency -Under the same setting, we evaluate the impact of trigger transparency α. Compared with trigger size, α has a more profound impact. The ASR of most attacks drops sharply once α exceeds 0.6, among which TB approaches 10% if α ≥ 0.8, and ESB works only if α is close to 0, due to its reliance on recognizing the trigger precisely to overwrite the model prediction. Meanwhile, LB and IMC seem insensitive to α. This may be attributed to that LB optimizes trojan models with respect to latent representations (rather than final predictions), while IMC optimizes trigger patterns and trojan models jointly. Both strategies may mitigate α's impact.  Data complexity -The trade-off between attack effectiveness and trigger evasiveness is especially evident for complex data. We compare the ASR and TMC of given attacks on different datasets, with results in Table <ref type="table" target="#tab_5">5</ref> (more in <ref type="bibr">Table 22)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ASR (%)</head><formula xml:id="formula_5">Attack CIFAR10 CIFAR100 ImageNet |m| = 3, α = 0.8 |m| = 3, α = 0.8 |m| = 3, α = 0 |m| = 7, α = 0.</formula><p>We observe that the class-space size (the number of classes) negatively affects the attack effectiveness. For example, the ASR of BN drops by 7.9% from CIFAR10 to CIFAR100. Intuitively, it is more difficult to force trigger inputs from all the classes to be misclassified in larger output space. Moreover, it tends to require more significant triggers to achieve comparable attack performance on more complex data. For instance, for IMC to attain similar ASR on CIFAR10 and ImageNet, it needs to either increase trigger size (from 3×3 to 7×7) or reduce trigger transparency (from 0.8 to 0.0).</p><p>Remark 1 -There exists a trade-off between attack effectiveness and trigger evasiveness (in terms of transparency), which is especially evident for complex data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Effectiveness vs. Evasiveness (Model)</head><p>. Further, we consider the evasiveness of trojan models, which is measured by their difference from benign models in terms of classifying clean inputs. One intriguing property of the attacks is the trade-off between maximizing the attack effectiveness with respect to trigger inputs and minimizing the influence over clean inputs. Here, we characterize this trade-off via varying the fraction of trigger inputs in the training data. For each attack, we bound its CAD within 3%, measure its highest and lowest ASR (which corresponds to its lowest and highest CAD respectively), and then normalize the ASR and CAD measures to Figure <ref type="figure" target="#fig_4">5</ref> visualizes the normalized CAD-ASR trade-off. Observe that the curves of all the attacks manifest strong convexity, indicating the "leverage" effects <ref type="bibr" target="#b42">[43]</ref>: it is practical to greatly improve ASR at a disproportionally small cost of CAD. Also, observe that different attacks feature varying Area Under the Curve (AUC). Intuitively, a smaller AUC implies a stronger leverage effect. Among all the attacks, IMC shows the smallest AUC. This may be explained by that IMC uses the trigger-model cooptimization framework, which allows the adversary to maximally optimize ASR at given CAD.</p><p>Remark 2 -The trade-off between attack effectiveness and model evasiveness demonstrates strong "leverage" effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Effectiveness vs.</head><p>Transferability. Next, we evaluate the transferability of different attacks to the downstream tasks. We consider two scenarios: (i) the pretraining and downstream tasks share the same dataset; and (ii) the downstream task uses a different dataset.</p><p>Transferability (classifier) -In (i), we focus on evaluating the impact of downstream-classifier selection and fine-tuning strategy on the attacks. We consider 5 different classifiers (1/2 fully-connected layer, Bayes, SVM, and Random Forest) and 3 fine-tuning strategies (none, partial tuning, and full tuning). Notably, the adversary is unaware of such settings. 99.9 99.9 99.9 99.8 IMC 100.0 99.9 88.7 99.9 100.0 99.9 99.8 Table <ref type="table">6</ref>. Impact of fine-tuning and downstream-model selection.</p><p>Table <ref type="table">6</ref> compares the ASR of 5 attacks with respect to varying downstream classifiers and fine-tuning strategies. Observe that fine-tuning has a large impact on attack effectiveness. For instance, the ASR of TNN drops by 62.5% from partial-to full-tuning. Yet, LB and IMC are less sensitive to fine-tuning, due to their optimization strategies. Also, note that the attack performance seems agnostic to the downstream classifier. This may be explained by that the downstream classifier in practice tends to manifest "pseudo-linearity" <ref type="bibr" target="#b26">[27]</ref> (details in § A).</p><p>Transferability (data) -In (ii), we focus on evaluating the transferability of the attacks across different datasets. We evaluate the effectiveness of transferring attacks across two datasets, CIFAR10 and ImageNet, with results summarized in Table <ref type="table">7</ref>. We have the following findings. Several attacks (e.g., BN) are able to transfer from ImageNet to CIFAR10 to a certain extent, but most attacks fail to transfer from CIFAR10 to ImageNet. The finding may be justified as follows. A model pre-trained on complex data (i.e., ImageNet) tends to maintain its effectiveness of feature extraction on simple data (i.e., CIFAR10) <ref type="bibr" target="#b16">[17]</ref>; as a side effect, it may also preserve its effectiveness of propagating trigger patterns. Meanwhile, a model pre-trained on simple data may not generalize well to complex data. Moreover, compared with stronger attacks in non-transfer cases (e.g., LB), BN shows much higher transferability. This may be explained by that to maximize the attack efficacy, the trigger and trojan model often need to "over-fit" the training data, resulting in poor transferability. Remark 3 -Most attacks transfer across classifiers; however, weaker attacks demonstrate higher transferability across datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Defense Evaluation</head><p>As the defenses from different categories bear distinct objectives (e.g., detecting trigger inputs versus cleansing trojan models), below we evaluate each defense category separately. Robustness -With the no-defense (vanilla) case as reference, we compare different defences in terms of attack rate deduction (ARD) and trojan misclassification confidence (TMC), with results shown in Table <ref type="table" target="#tab_7">8</ref>. We have the following observations: (i) MP and AR are the most robust methods in the categories of input transformation and model sanitization, respectively. (ii) FP seems robust against most attacks except LB and IMC, which is explained as follows: unlike attacks (e.g., TNN) that optimize the trigger with respect to selected neurons, LB and IMC perform optimization with respect to all the neurons, making them immune to the pruning of FP. (iii) Most defenses are able to defend against ESB (over 85% ARD), which is attributed to its hard-coded trigger pattern and modified DNN architecture: slight perturbation to the trigger input or trojan model may destroy the embedded backdoor.  Utility -We now measure the impact of defenses on the accuracy of classifying clean inputs. Table <ref type="table" target="#tab_8">9</ref> summarizes the results. With the vanilla setting as the baseline, most defenses tend to negatively affect clean accuracy, yet with varying impact. For instance, across all the cases, FP attains the least CAD across all the cases, mainly due to its fine-tuning; RS and AR cause about 0.4% and 11% CAD, respectively. This is explained by the difference of their underlying mechanisms: although both attempt to alleviate the influence of trigger patterns, RS smooths the prediction of an input x over its vicinity, while AR forces the model to make consistent predictions in x's vicinity. Notably, comparing with Table <ref type="table" target="#tab_7">8</ref>, while MP and AR seem generically effective against all the attacks, they also suffer over 10% CAD, indicating the trade-off between robustness and utility preservation.</p><p>Remark 4 -The design of attack-agnostic defenses faces the trade-off between robustness and utility preservation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Detection Accuracy of Different Attacks.</head><p>We evaluate the effectiveness of input filtering by measuring its accuracy in detecting trigger inputs.</p><p>Detection accuracy -For each attack, we randomly generate 100 pairs of trigger-clean inputs and measure the true positive (TPR) and false positive (FPR) rates of STRIP and NEO, two input filtering methods. To make comparison, we fix FPR as 0.05 and report TPR in  <ref type="table" target="#tab_9">10</ref>. TPR of NEO and STRIP (FPR = 0.05, α = 0.0, ± standard deviation).</p><p>We have the following findings. (i) STRIP is particularly effective against LB and IMC (over 0.9 TPR). Recall that STRIP detects a trigger input using the self-entropy of its mixture with a clean input. This indicates that the triggers produced by LB and IMC effectively dominate the mixtures, which is consistent with the findings in other experiments (cf. Figure <ref type="figure">2</ref>). (ii) NEO is effective against most attacks to a limited extent (less than 0.3 TPR), but especially effective against ESB (over 0.6 TPR), due to its requirement for recognizing the trigger pattern precisely to overwrite the model prediction.</p><p>Impact of trigger definition -We also evaluate the impact of trigger definition on input filtering, with results in Figure <ref type="figure" target="#fig_7">6</ref> (results for other defenses in § B). With fixed trigger transparency, NEO constantly attains higher TPR under larger triggers; in comparison, STRIP seems less sensitive but also less effective under larger triggers. This is attributed to the difference of their detection rationale: given input x, NEO searches for the "tipping" position in x to cause prediction change, which is clearly subjective to the trigger size; while STRIP measures the self-entropy of x's mixture with a clean input, which does not rely on the trigger size.  The results are shown in Table <ref type="table" target="#tab_12">11</ref>. We observe: (i) compared with other defenses, ABS is highly effective in detecting trojan models (with largest AIV), attributed to its neuron sifting strategy; (ii) IMC seems evasive to most defenses (with AIV below 2), explainable by its triggermodel co-optimization strategy that minimizes model distortion; (iii) most model-inspection defenses are either ineffective or inapplicable against ESB, as it keeps the original DNN intact but adds an additional module. This contrasts the high effectiveness of other defenses against ESB (cf. Table <ref type="table" target="#tab_7">8</ref>).</p><p>Recovery Capability -For successfully detected trojan models, we further evaluate the trigger recovery of various defenses by measuring the mask 1 norm (MLN) of recovered triggers and mask jaccard similarity (MJS) between the recovered and injected triggers, with results shown in Table <ref type="table" target="#tab_0">12</ref>. While the ground-truth trigger has MLN = 9 (α = 0.0, |m| = 3×3), most defenses recover triggers of varying MLN and non-zero MJS, indicating that they recover triggers different from, yet overlapping with, the injected ones. In contrast to Table <ref type="table" target="#tab_12">11</ref>, NC and TABOR outperform ABS in trigger recovery, which may be explained by that while ABS relies on the most abnormal neuron to recover the trigger, the actual trigger may be embedded into multiple neurons. This may also be corroborated by that ABS attains the highest MJS on LB and IMC, which tend to generate triggers embedded in a few neurons (Table <ref type="table" target="#tab_9">10</ref>).</p><p>Remark 6 -The design of model-inspection defenses faces the trade-off between the accuracy of detecting trojan models and the effectiveness of recovering trigger patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4.">Execution Time.</head><p>We compare the overhead of various defenses by measuring their ART ( § 4.4) on a NVIDIA Quodro RTX6000. The results are listed in Table <ref type="table" target="#tab_2">13</ref>. Note that online defenses (e.g., STRIP) have negligible overhead, while offline methods (e.g., ABS) require longer but acceptable running time (10 3 ∼10 4 seconds).</p><p>Remark 7 -Most defenses have marginal execution overhead with respect to practical datasets and models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Summary</head><p>Although the defense from different categories bear distinct objectives (e.g., detecting trigger inputs versus cleansing trojan models), the evaluation above leads to the following observations: (i) attack-agnostic defenses often face a dilemma of trade-off between robustness and accuracy: input transformation retains high accuracy but is often ineffective against most attacks; model sanitization is effective to mitigate neural backdoors but at the cost of significant accuracy drop; (ii) input-filtering is computationally efficient but only effective against a limited set of attacks; (iii) model-inspection requires extensive optimization but the recovered trigger is able to serve as a guidance for possible backdoor unlearning. These observations may provide guidance for choosing suitable defense strategies for given application scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Exploration</head><p>Next, we examine the current practices of operating backdoor attacks and defenses and explore potential improvement. Table <ref type="table" target="#tab_0">12</ref>. MLN and MJS of triggers recovered by model-inspection defenses with respect to various attacks (Note: as the trigger position is randomly chosen in TB, its MJS is un-defined).</p><formula xml:id="formula_6">MP NEO STRIP AR FP 2.4×10 1 7.7×10 0 1.8×10 −1 1.7×10 4 2.1×10 3 NC TABOR ABS NI DI 1.8×10 3 4.2×10 3 1.9×10 3 4.6×10 1 4.1×10 2</formula><p>Table <ref type="table" target="#tab_2">13</ref>. Running time of various defenses (second).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Attack -Trigger</head><p>We first explore improving the trigger definition by answering the following questions.</p><p>RQ 1 : Is it necessary to use large triggers? -It is found in § 5.2 that attack efficacy seems insensitive to trigger size. We now consider the extreme case that the trigger is defined as a single pixel and evaluate the efficacy of different attacks (constrained by CAD below 5%), with results show in Table <ref type="table" target="#tab_3">14</ref>. Note that the trigger definition is inapplicable to ESB, due to its requirement for trigger size. Table <ref type="table" target="#tab_3">14</ref>. ASR and TMC of single-pixel triggers (α = 0.0, CAD ≤ 5%).</p><p>Note that single-pixel adversarial attacks have been explored in the literature <ref type="bibr" target="#b53">[54]</ref>; however, its study in the context of backdoor attacks is fairly limited. While it is mentioned in blind backdoor attacks <ref type="bibr" target="#b4">[5]</ref>, the discussion is limited to the specific attack and does not explore the global pattern of neural backdoors. Interestingly, with single-pixel triggers, most attacks attain ASR comparable with the cases of larger triggers (cf. Figure <ref type="figure" target="#fig_1">3</ref>). This implies the existence of universal, single-pixel perturbation <ref type="bibr" target="#b41">[42]</ref> with respect to trojan models (but not clean models!), highlighting the mutual-reinforcement effects between trigger inputs and trojan models <ref type="bibr" target="#b42">[43]</ref>.</p><p>Remark 8 -There often exists universal, single-pixel perturbation with respect to trojan models (but not clean models).</p><p>RQ 2 : Is it necessary to use regular-shaped triggers? -The triggers in the existing attacks are mostly regularshaped (e.g., square), which seems a common design choice. We explore the impact of trigger shape on attack efficacy. We fix |m| = 9 but select the positions of |m| pixels independently and randomly. Except for LB and IMC which already attain extremely high ASR under the regular-trigger setting, all the other attacks achieve higher ASR under the random-trigger setting. For instance, the ASR of BN increases by 25.2%. This may be explained by that lifting the spatial constraint on the trigger entails a larger optimization space for the attacks. RQ 3 : Is the "neuron-separation" guidance effective? -A common search strategy for trigger patterns is using the neuron-separation guidance: searching for triggers that activate neurons rarely used by clean inputs <ref type="bibr" target="#b37">[38]</ref>. Here, we validate this guidance by measuring the NSR ( § 4.2) of benign and trojan models before and after FP, as shown in Table <ref type="table" target="#tab_16">16</ref>. Across all the cases, compared with its benign counterpart, the trojan model tends to have higher NSR, while finetuning reduces NSR significantly. More effective attacks (cf. Figure <ref type="figure">2</ref>) tend to have higher NSR (e.g., IMC). We thus conclude that the neuron-separation heuristic is in general valid. Remark 10 -The separation between the neurons activated by clean and trigger inputs is an indicator of attack effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Attack -Optimization</head><p>We now examine the optimization strategies used by the existing attacks and explore potential improvements.</p><p>RQ 4 : Is it necessary to start from benign models? -To forge a trojan model, a common strategy is to retrain a benign, pre-trained model. Here, we challenge this practice by evaluating whether re-training a benign model leads to more effective attacks than training a trojan model from scratch. Training from scratch ASR 76.9 98.9 81.2 100.0 100.0 CAD -0.7 -0.6 -0.7 -0.8 -0.9 Table <ref type="table" target="#tab_0">17</ref>. ASR and CAD of trojan models by training from scratch and re-training from benign models.</p><p>Table <ref type="table" target="#tab_0">17</ref> compares the ASR of trojan models generated using the two strategies. Except for LB and IMC achieving similar ASR in both settings, the other attacks observe marginal improvement if they are trained from scratch. For instance, the ASR of TNN improves by 7.4%. One possible explanation is as follows. Let f and f * represent the benign and trojan models, respectively. In the parameter space, re-training constrains the search for f * within in f 's vicinity, while training from scratch searches for f * in the vicinity of a randomly initialized configuration, which may lead to better starting points.  We first compare the attack performance on three DNN models, VGG, ResNet, and DenseNet, with results shown in Figure <ref type="figure" target="#fig_5">7</ref>. First, different model architectures manifest varying attack vulnerabilities, ranked as ResNet &gt; DenseNet &gt; VGG. This may be explained as follows. Compared with traditional convolutional networks (e.g., VGG), the unique constructs of ResNet (i.e., residual block) and DenseNet (i.e., dense connection) enable more effective feature extraction, but also allow more effective propagation of trigger patterns. Second, among all the attacks, LB, IMC, and ESB seem insensitive to model architectures, which may be attributed to the optimization strategies of LB and IMC, and the direct modification of DNN architectures by ESB.</p><p>We then consider the skip-connect structures and attempt to improve the gradient backprop in training trojan models. In such networks, gradients propagate through both skip-connects and residual blocks. By setting the weights of gradients from skip-connects or residual blocks, it amplifies the gradient update towards inputs or model parameters <ref type="bibr" target="#b63">[64]</ref>. Specifically, we modify the backprop procedure in IMC by setting a decay coefficient γ = 0.5 for the gradient through skip connections, with ASR improvement over normal training shown in Figure <ref type="figure" target="#fig_13">8</ref>. Observe that by reducing the skip-connection gradients, it marginally improves the ASR of IMC especially for small triggers (e.g., |m| = 2×2). We consider searching for the optimal γ to maximize attack efficacy as our ongoing work. Remark 12 -It is feasible to exploit skip-connect structures to improve attack efficacy marginally.</p><p>RQ 6 : How to mix clean and trigger inputs in training? -To balance attack efficacy and specificity, the adversary often mixes clean and trigger inputs in training trojan models. There are typically three mixing strategies: (i) dataset-level -mixing trigger inputs T t with clean inputs T c directly, (ii) batch-level -adding trigger inputs to each batch of clean inputs during training, and (iii) loss-level -computing and aggregating the average losses of T t and T c . Here, we fix the mixing coefficient λ = 0.01 and compare the effectiveness of different strategies. Table <ref type="table" target="#tab_7">18</ref>. Impact of mixing strategies on attack efficacy (α = 0.0, λ = 0.01).</p><p>We observe in Table <ref type="table" target="#tab_7">18</ref> that across all the cases, the batch-level mixing strategy leads to the highest ASR. This can be explained as follows. With dataset-level mixing, the ratio of trigger inputs in each batch tends to fluctuate significantly due to random shuffling, resulting in inferior training quality. With loss-level mixing, λ = 0.01 results in fairly small gradients of trigger inputs, equivalent to setting an overly small learning rate. In comparison, batchlevel mixing asserts every poisoning instance and its clean version must share the same batch, making the model focus more on the trigger as the classification evidence of target class.</p><p>Here, we provide a potential explanation: the loss-level mixing involves the gradient scale of poisoning data. If the loss is defined as L = L clean +λ•L poison and optimization step as</p><formula xml:id="formula_7">∆ = lr• ∂(L clean +λ•Lpoison) ∂θ</formula><p>, where lr is the learning rate and L clean and L poison are the losses on the clean and poisoning data. Observe that ∆ = lr</p><formula xml:id="formula_8">• ∂L clean ∂θ + lr • λ • ∂Lpoison ∂θ</formula><p>. The real gradient scale is lr • λ rather than lr, which makes the step size smaller than expected. Remark 13 -Batch-level mixing tends to lead to the most effective training of trojan models. Here, we implement 3 variants of BN that use these optimization strategies, respectively. Figure <ref type="figure" target="#fig_9">9</ref> compares their ASR under varying trigger transparency. Observe that the trigger-optimization strategy has a significant impact on ASR, especially under high transparency. For instance, if α = 0.9, the cooptimization strategy improves ASR by over 60% from the non-optimization strategy. Remark 14 -Optimizing the trigger pattern and the trojan model jointly leads to more effective attacks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Defense -Evadability</head><p>RQ 8 : Are the existing defenses evadable? -We now explore whether the existing defenses are potentially evadable by adaptive attacks. We select IMC as the basic attack, due to its flexible optimization framework, and consider MP, AR, STRIP, and ABS as the representative defenses from the categories in Table <ref type="table" target="#tab_2">3</ref>. Specifically, we adapt IMC to each defense.</p><p>Recall that MP uses an auto-encoder to downsample then upsample a given input, during which the trigger pattern tends to be blurred and loses effect. To adapt IMC to MP, we train a surrogate autoencoder h and conduct optimization with inputs reformed by h.</p><p>Recall that AR considers trigger inputs as one type of adversarial inputs and applies adversarial training to improve model robustness against backdoor attacks. To adapt IMC to AR, during training f * , we replace clean accuracy loss with adversarial accuracy loss; thus, the process is a combination of adversarial training and trojan model training, resulting in a robust but trojan model. This way, AR has a limited impact on the embedded backdoor, as the model is already robust.</p><p>Recall that STRIP mixes up given inputs with clean inputs and measures the self-entropy of their predictions. Note that in the mixture, the transparency of the original trigger is doubled; yet, STRIP works as the hightransparency trigger remains effective. To adapt IMC to STRIP, we use trigger inputs with high-transparency triggers together with their ground-truth classes to re-train f * . The re-training reduces the effectiveness of hightransparency triggers while keeping low-transparency triggers effective.</p><p>Recall that ABS identifies triggers by maximizing abnormal activation while preserving normal neuron behavior. To adapt IMC to ABS, we integrate the cost function (Algorithm 2 in <ref type="bibr" target="#b36">[37]</ref>) in the loss function to train f * .</p><p>We compare the efficacy of non-adaptive and adaptive IMC, as shown in Figure <ref type="figure" target="#fig_0">10</ref>. Observe that across all the cases, the adaptive IMC significantly outperforms the nonadaptive one. For instance, under |m| = 6×6, it increases the ASR with respect to MP by 80% and reduces the TPR of STRIP by over 0.85. Also note that a larger trigger size leads to more effective adaptive attacks, as it entails a larger optimization space. Remark 15 -Most existing defenses are potentially evadable by adaptive attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Defense -Interpretability</head><p>RQ 9 : Does interpretability help mitigate backdoor attacks? -The interpretability of DNNs explain how they make predictions for given inputs <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b47">[48]</ref>. Recent studies <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b57">[58]</ref> show that such interpretability helps defend against adversarial attacks. Here, we explore whether it mitigates backdoor attacks. Specifically, for a pair of benign-trojan models and 100 pairs of clean-trigger inputs, we generate the attribution map <ref type="bibr" target="#b47">[48]</ref> of each input with respect to both models and ground and target classes, with an example shown in Figure <ref type="figure" target="#fig_11">11</ref>.</p><p>We measure the difference ( 1 -norm normalized by image size) of attribution maps of clean and trigger inputs. Observe in Table <ref type="table" target="#tab_19">19</ref> that their attribution maps with respect to the target class differ significantly on the trojan model, indicating the possibility of using interpretability to detect the attack. This finding also corroborates recent work on using interpretability to identify possibly tampered regions in images <ref type="bibr" target="#b12">[13]</ref>. However, it may require further study whether the adversary may adapt the attack to deceive such detection <ref type="bibr" target="#b70">[71]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Summary</head><p>Based on the study above, we recommend the following testing strategy for a new neural backdoor attack: (i) attacks that optimize models only (e.g., BN), (ii) attacks that partially optimize triggers (e.g., TNN), (iii) attacks that optimize both models and triggers (e.g., IMC), and (iv) attacks adaptive to the given defense. The increasing level of complexity gives the adversary more flexibility to optimize various settings (e.g., trigger transparency and size) to evade the defense, leading to stronger attacks.</p><p>Looking forward, the study also opens several research directions for future defenses: (i) ensemble defenses that leverage the strengths of individual ones (e.g., input transformation and model sanitization), (ii) defenses that involve human in the loop via interpretability, and (iii) defenses that provide theoretical guarantees based on the invariant properties of various attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Limitations</head><p>First, to date TROJANZOO has integrated 8 attacks and 14 defenses, representing the state of the art of neural backdoor research. Yet, as a highly active research field, a set of concurrent work has proposed new backdoor attacks/defenses <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b66">[67]</ref>, which are not included in the current implementation of TROJANZOO. As examples, <ref type="bibr" target="#b55">[56]</ref> presents a new attack that obscures the representations of benign and trigger inputs; <ref type="bibr" target="#b48">[49]</ref> proposes to leverage interpretability to improve attack effectiveness; while <ref type="bibr" target="#b43">[44]</ref> investigates data augmentation-based defenses. However, thanks to its modular design, TROJANZOO can be readily extended to incorporate new attacks, defenses, and metrics. Moreover, we plan to open-source all the code and data of TROJANZOO and encourage the community to contribute.</p><p>Second, to conduct a unified evaluation, we mainly consider the attack vector of re-using pre-trained trojan models. There are other attack vectors through which backdoor attacks can be launched, including poisoning victims' training data <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b72">[73]</ref> and knowledge distillation <ref type="bibr" target="#b68">[69]</ref>, which entail additional constraints for attacks or defenses. For instance, the poisoning data needs to be evasive to bypass inspection. We consider studying alternative attack vectors as our ongoing work.</p><p>Third, due to space limitations, our evaluation focuses on popular DNN models (e.g., ResNet) and assumes fixed training/test data split. We consider evaluating the impact of model configuration and data split on neural backdoor attacks/defenses as our ongoing work.</p><p>Finally, because of the plethora of work on neural backdoors in the computer vision domain, TROJANZOO focuses on the image classification task, while recent work has also explored neural backdoors in other settings, including natural language processing <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b71">[72]</ref>, reinforcement learning <ref type="bibr" target="#b27">[28]</ref>, and federated learning <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b64">[65]</ref>. We plan to extend TROJANZOO to support such settings in its future releases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We design and implement TROJANZOO, the first platform dedicated to assessing neural backdoor attacks/defenses in a holistic, unified, and practical manner. Leveraging TROJANZOO, we conduct a systematic evaluation of existing attacks/defenses, which demystifies a number of open questions, reveals various design tradeoffs, and sheds light on further improvement. We envision TROJANZOO will serve as a useful benchmark to facilitate neural backdoor research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Implementation Details</head><p>Below we elaborate on the implementation of attacks and defenses in this paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Default Parameter Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Pseudo-linearity of downstream model</head><p>We have shown in § 5 that most attacks seem agnostic to the downstream model. Here, we provide possible explanations. Consider a binary classification setting and a trigger input x with ground-truth class "-" and target class "+". Recall that a backdoor attack essentially shifts x in the feature space by maximizing the quantity of</p><formula xml:id="formula_9">∆ f = E µ + [f (x)] − E µ − [f (x)]<label>(7)</label></formula><p>where µ + and µ − respectively denote the data distribution of the ground-truth positive and negative classes. Now consider the end-to-end system g • f . The likelihood that x is misclassified into "+" is given by: One sufficient condition for the attack to succeed is that ∆ g•f is linearly correlated with ∆ f (i.e., ∆ g•f ∝ ∆ f ). If so, we say that the function represented by g is pseudo-linear. Unfortunately, in practice, most downstream models are fairly simple (e.g., one fully-connected layer), showing pseudo-linearity. Possible reasons include: (i) complex architectures are difficult to train especially when the training data is limited; (ii) they imply much higher computational overhead; (iii) the ground-truth mapping from the feature space to the output space may indeed be pseudo-linear.    Figure <ref type="figure" target="#fig_7">16</ref> illustrate the impact of DNN architecture on the performance of input filtering defenses (NEO, STRIP), which complements Figure <ref type="figure" target="#fig_7">6</ref>.     </p><formula xml:id="formula_10">∆ g•f = E µ + [g • f (x)] − E µ − [g • f (x)]<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Defense</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of neural backdoor attacks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: ASR and TMC with respect to trigger size (α = 0.8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: ASR with respect to trigger transparency (|m| = 3×3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 4 plots the ASR of various attacks as a function of α (|m| = 3×3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Trade-off between attack effectiveness and model evasiveness (|m| = 3 × 3, α = 0.8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Table 7 .</head><label>7</label><figDesc>94.5 (0.99) 100.0 (1.0) 100.0 (1.0) 100.0 (1.0) 100.0 (1.0) C → I 8.4 (0.29) 7.8 (0.29) 8.6 (0.30) 8.2 (0.30) 9.4 (0.32) I → I 90.0 (0.98) 95.2 (0.99) 94.6 (0.98) 97.4 (0.99) 98.4 (1.0) I → C 77.0 (0.84)) 26.9 (0.72) 11.0 (0.38) 10.0 (0.38) 14.3 (0.48) ASR and TMC of transfer attacks across CIFAR10 (C) and ImageNet (I) (|m| = 3×3, α = 0.0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>3 -0.6 -0.3 -0.4 -0.4 -0.3 -0.3 -0.4 -0.5 (±0.2) (±0.3) (±0.1) (±0.1) (±0.3) (±0.1) (±0.1) (±0.1) (±0.20) (±0.0) (±0.0) (±0.0) (±0.0) (±0.1) (±0.0) (±0.0) (±0.1) AR -11.1 -11.1 -10.4 -10.4 -10.4 -10.9 -10.9 -10.5 -11.4 (±4.6) (±3.7) (±4.4) (±2.8) (±3.6) (±5.1) (±3.0) (±3.1) (±3.6)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: TPR of NEO and STRIP under varying trigger definition (left: |m| = 3 × 3, right: |m| = 6 × 6; lower: α = 0.0, upper: α = 0.8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>99) (0.96) (0.96) (0.99) (0.99) (0.97) (0.99)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Remark 9 -</head><label>9</label><figDesc>Lifting spatial constraints on trigger patterns tends to lead to more effective attacks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Remark 11 -</head><label>11</label><figDesc>Training from scratch tends to lead to more effective attacks than benign-model re-training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>RQ 5 :Figure 7 :</head><label>57</label><figDesc>Figure 7: Impact of DNN architecture on attack efficacy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: ASR improvement (and CAD change) by reducing skipconnection gradients (α = 0.9).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>RQ 7 :</head><label>7</label><figDesc>How to optimize the trigger pattern? -An attack involves optimizing both the trigger pattern and the trojan model. The existing attacks use 3 typical strategies: (i) Pre-defined trigger -it fixes the trigger pattern and only optimizes the trojan model. (ii) Partially optimized trigger -it optimizes the trigger pattern in a pre-processing stage and optimizes the trojan model. (iii) Trigger-model cooptimization -it optimizes the trigger pattern and the trojan model jointly during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Impact of trigger optimization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Figure 10: Performance of non-adaptive and adaptive IMC against representative defenses (α = 0.0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: ASR with respect to trigger size (α = 0.3, ImageNet).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 14</head><label>14</label><figDesc>Figure 14 and 15 shows the influence of DNN architecture and trigger definition on the performance of attackagnostic defenses (MP, AR, RS, DU).Figure16illustrate the impact of DNN architecture on the performance of input filtering defenses (NEO, STRIP), which complements Figure6.Figure17and 18 illustrate the impact of DNN architecture and trigger definition on the performance of modelinspection defenses (ABS, NI, TABOR, DI, NC).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 17</head><label>17</label><figDesc>Figure 14 and 15 shows the influence of DNN architecture and trigger definition on the performance of attackagnostic defenses (MP, AR, RS, DU).Figure16illustrate the impact of DNN architecture on the performance of input filtering defenses (NEO, STRIP), which complements Figure6.Figure17and 18 illustrate the impact of DNN architecture and trigger definition on the performance of modelinspection defenses (ABS, NI, TABOR, DI, NC).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Impact of DNN architecture on attack-agnostic defenses (lower: ResNet18, middle: DenseNet121; upper: VGG13).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 15 :Figure 16 :</head><label>1516</label><figDesc>Figure 15: Impact of trigger definition on attack-agnostic defenses (left: |m| = 3 × 3, right: |m| = 6 × 6; lower: α = 0.0, upper: α = 0.8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Impact of DNN architecture on model filtering defenses (lower: ResNet18, middle: DenseNet121; upper: VGG13; note: ESB-ABS pair is inapplicable).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Impact of trigger definition on model filtering defenses (left: |m| = 3 × 3, right: |m| = 6 × 6; lower: α = 0.0, upper: α = 0.8; note: ESB-ABS pair is inapplicable).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Symbols and notations.</figDesc><table><row><cell>A, D</cell><cell>attack, defense</cell></row><row><cell>x, x  *</cell><cell>clean input, trigger input</cell></row><row><cell>x i</cell><cell>i-th dimension of x</cell></row><row><cell>r</cell><cell>trigger</cell></row><row><cell>m</cell><cell>mask (α for each pixel)</cell></row><row><cell>f, f  *</cell><cell>benign model, trojan model</cell></row><row><cell>f feat</cell><cell>upstream feature extractor</cell></row><row><cell>g, g  *</cell><cell>downstream classifier, surrogate classifier</cell></row><row><cell>t</cell><cell>adversary's target class</cell></row><row><cell>T</cell><cell>reference set</cell></row><row><cell>R , F δ</cell><cell>trigger, model feasible sets</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Summary of representative neural backdoor defenses currently implemented in TROJANZOO (A -backdoor attack, xclean input, x * -trigger input, f -benign model, f * -trojan model, t -target class)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>default, we assume ACC of benign models over different datasets.</figDesc><table><row><cell>Dataset</cell><cell># Class</cell><cell># Dimension</cell><cell>Model</cell><cell>ACC</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ResNet18</cell><cell>95.37%</cell></row><row><cell>CIFAR10</cell><cell>10</cell><cell>32×32</cell><cell>DenseNet121</cell><cell>93.84%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>VGG13</cell><cell>92.44%</cell></row><row><cell>CIFAR100</cell><cell>100</cell><cell>32×32</cell><cell></cell><cell>73.97%</cell></row><row><cell>GTSRB ImageNet</cell><cell>43 10</cell><cell>32×32 224×224</cell><cell>ResNet18</cell><cell>98.18% 92.40%</cell></row><row><cell>VGGFace2</cell><cell>20</cell><cell>224×224</cell><cell></cell><cell>90.77%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Impact of data complexity on ASR and TMC.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>ARD and TMC of attack-agnostic defenses against various attacks (±: standard deviation).</figDesc><table><row><cell>Defense</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>Impact of defenses on classification accuracy (−: clean model without attack/defense; ±: standard deviation).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 (</head><label>10</label><figDesc>statistics in § B).</figDesc><table><row><cell>Defense</cell><cell>BN</cell><cell>TNN</cell><cell>RB</cell><cell>Attack TB LB</cell><cell>ESB</cell><cell>ABE</cell><cell>IMC</cell></row><row><cell>STRIP</cell><cell cols="7">0.07 (±0.01) (±0.01) (±0.13) (±0.08) (±0.20) (±0.01) (±0.01) (±0.02) 0.13 0.34 0.27 0.91 0.10 0.07 0.99</cell></row><row><cell>NEO</cell><cell cols="7">0.29 (±0.09) (±0.10) (±0.07) (±0.11) (±0.06) (±0.24) (±0.05) (±0.05) 0.23 0.29 0.36 0.29 0.64 0.28 0.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Remark 5 -The design of input filtering defenses needs to balance the detection accuracy with respect to different attacks.</figDesc><table><row><cell>Defense</cell><cell>BN</cell><cell>TNN</cell><cell>RB</cell><cell>Attack TB LB</cell><cell>ESB</cell><cell>ABE</cell><cell>IMC</cell></row><row><cell>NC</cell><cell cols="7">3.08 (±0.65) (±0.47) (±0.51) (±0.38) (±0.20) (±0.02) (±0.51) (±0.25) 2.69 2.48 2.44 2.12 0.04 2.67 1.66</cell></row><row><cell>DI</cell><cell cols="7">0.54 (±0.06) (±0.04) (±0.04) (±0.03) (±0.04) (±0.00) (±0.10) (±0.03) 0.46 0.39 0.29 0.21 0.01 0.76 0.26</cell></row><row><cell>TABOR</cell><cell cols="7">3.26 (±0.77) (±0.49) (±0.51) (±0.29) (±0.63) (±0.04) (±0.22) (±0.19) 2.49 2.32 2.15 2.01 0.89 2.44 1.89</cell></row><row><cell>NI</cell><cell cols="7">1.28 (±0.21) (±0.11) (±0.06) (±0.34) (±0.87) (±0.10) (±0.05) (±0.13) 0.59 0.78 1.11 0.86 0.71 0.41 0.52</cell></row><row><cell>ABS</cell><cell cols="4">3.02 (±0.81) (±1.33) (±1.27) (±6.59) (±0.25) 4.16 4.10 15.55 2.88</cell><cell></cell><cell cols="2">8.45 (±3.22) (±0.43) 3.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 .</head><label>11</label><figDesc>AIV of clean models and trojan models by various attacks. Given defense D and model f , we measure the anomaly index value (AIV) of all the classes; if f is a trojan model, we use the AIV of the target class to quantify D's TPR of detecting trojan models and target classes; if f is a clean model, we use the largest AIV to quantify D's FPR of misclassifying clean models.</figDesc><table><row><cell>5.3.3. Detection Accuracy vs. Recovery Capability. We</cell></row><row><cell>evaluate model-inspection defenses in terms of their effec-</cell></row><row><cell>tiveness of (i) identifying trojan models and (ii) recovering</cell></row><row><cell>trigger patterns.</cell></row><row><cell>Detection Accuracy -</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 15 compares</head><label>15</label><figDesc>ASR under the settings of regular and random triggers.</figDesc><table><row><cell>Trigger</cell><cell>BN</cell><cell>TNN</cell><cell>RB</cell><cell>LB</cell><cell>IMC</cell></row><row><cell>Regular</cell><cell>72.4</cell><cell>91.5</cell><cell>79.2</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>Random</cell><cell>97.6</cell><cell>98.5</cell><cell>92.7</cell><cell>97.6</cell><cell>94.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 15 .</head><label>15</label><figDesc>Comparison of regular and random triggers.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 16 .</head><label>16</label><figDesc>NSR of benign and trojan models before and after FP.</figDesc><table><row><cell cols="2">Fine-Pruning -</cell><cell>BN TNN RB LB ABE IMC</cell></row><row><cell>Before</cell><cell cols="2">0.03 0.59 0.61 0.65 0.61 0.54 0.64</cell></row><row><cell>After</cell><cell cols="2">0.03 0.20 0.19 0.27 0.37 0.18 0.38</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 19 .</head><label>19</label><figDesc>Distance between the heatmaps of clean and trigger inputs (α = 0.0).</figDesc><table><row><cell>Remark 16 -It seems promising to exploit model inter-</cell></row><row><cell>pretability to enhance defense robustness.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 20 and</head><label>20</label><figDesc>Table 21 summarize the default parameter setting in our empirical evaluation ( § 5).</figDesc><table><row><cell cols="2">Attack Parameter</cell><cell>Setting</cell></row><row><cell></cell><cell>learning rate</cell><cell>0.01</cell></row><row><cell></cell><cell>retrain epoch</cell><cell>50</cell></row><row><cell>Training</cell><cell>optimizer</cell><cell>SGD (nesterov)</cell></row><row><cell></cell><cell>momentum</cell><cell>0.9</cell></row><row><cell></cell><cell>weight decay</cell><cell>2e-4</cell></row><row><cell>BN</cell><cell>toxic data percent</cell><cell>1%</cell></row><row><cell></cell><cell>preprocess layer</cell><cell>penultimate logits</cell></row><row><cell></cell><cell>neuron number</cell><cell>2</cell></row><row><cell></cell><cell>preprocess optimizer</cell><cell>PGD</cell></row><row><cell>TNN</cell><cell>preprocess lr</cell><cell>0.015</cell></row><row><cell></cell><cell>preprocess iter</cell><cell>20</cell></row><row><cell></cell><cell>threshold</cell><cell>5</cell></row><row><cell></cell><cell>target value</cell><cell>10</cell></row><row><cell></cell><cell>candidate number</cell><cell>50</cell></row><row><cell>RB</cell><cell>selection number selection iter</cell><cell>10 5</cell></row><row><cell></cell><cell>inner epoch</cell><cell>5</cell></row><row><cell></cell><cell>preprocess layer</cell><cell>penultimate logits</cell></row><row><cell></cell><cell>preprocess lr</cell><cell>0.1</cell></row><row><cell>LB</cell><cell>preprocess optimizer preprocess iter</cell><cell>Adam (tanh constrained) 100</cell></row><row><cell></cell><cell>samples per class</cell><cell>1000</cell></row><row><cell></cell><cell>MSE loss weight</cell><cell>0.5</cell></row><row><cell></cell><cell>TrojanNet</cell><cell>4-layer MLP</cell></row><row><cell></cell><cell cols="2">hidden neurons per layer 8</cell></row><row><cell>ESB</cell><cell>single layer structure TrojanNet influence</cell><cell>[fc, bn, relu] α =0.7</cell></row><row><cell></cell><cell>amplify rate</cell><cell>100</cell></row><row><cell></cell><cell>temperature</cell><cell>0.1</cell></row><row><cell>ABE</cell><cell cols="2">discriminator loss weight λ =0.1 discriminator lr 1e-3</cell></row><row><cell></cell><cell>trigger optimizer</cell><cell>PGD</cell></row><row><cell>IMC</cell><cell>PGD lr</cell><cell>α =20/255</cell></row><row><cell></cell><cell>PGD iter</cell><cell>20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 20 .</head><label>20</label><figDesc>Attack default parameter setting.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 21 .</head><label>21</label><figDesc>Defense default parameter setting.</figDesc><table><row><cell cols="2">Defense Parameter</cell><cell>Setting</cell></row><row><cell></cell><cell>sample distribution</cell><cell>Gaussian</cell></row><row><cell>RS</cell><cell>sample number</cell><cell>100</cell></row><row><cell></cell><cell>sample std</cell><cell>0.01</cell></row><row><cell>DU</cell><cell>downsample filter downsample ratio</cell><cell>Anti Alias 0.95</cell></row><row><cell>MP</cell><cell>training noise std structure</cell><cell>0.1 [32]</cell></row><row><cell>STRIP</cell><cell>mixing weight sample number</cell><cell>0.5 (equal) 64</cell></row><row><cell></cell><cell>sample number</cell><cell>100</cell></row><row><cell>NEO</cell><cell>Kmeans cluster number</cell><cell>3</cell></row><row><cell></cell><cell>threshold</cell><cell>80</cell></row><row><cell></cell><cell>PGD lr</cell><cell>α =2/255</cell></row><row><cell></cell><cell>perturbation threshold</cell><cell>=8/255</cell></row><row><cell>AR</cell><cell>PGD iter</cell><cell>7</cell></row><row><cell></cell><cell>learning rate</cell><cell>0.01</cell></row><row><cell></cell><cell>epoch</cell><cell>50</cell></row><row><cell>FP</cell><cell>prune ratio</cell><cell>0.95</cell></row><row><cell></cell><cell cols="2">norm regularization weight 1e-3</cell></row><row><cell>NC</cell><cell>remask lr</cell><cell>0.1</cell></row><row><cell></cell><cell>remask epoch per label</cell><cell>10</cell></row><row><cell></cell><cell>sample dataset ratio</cell><cell>0.1</cell></row><row><cell>DI</cell><cell>noise dimension remask lr</cell><cell>100 0.01</cell></row><row><cell></cell><cell>remask epoch per label</cell><cell>20</cell></row><row><cell></cell><cell></cell><cell>λ 1 =1e-6</cell></row><row><cell></cell><cell></cell><cell>λ 2 =1e-5</cell></row><row><cell cols="2">TABOR regularization weight</cell><cell>λ 3 =1e-7 λ 4 =1e-8</cell></row><row><cell></cell><cell></cell><cell>λ 5 =0</cell></row><row><cell></cell><cell></cell><cell>λ 6 =1e-2</cell></row><row><cell></cell><cell></cell><cell>λsp =1e-5</cell></row><row><cell></cell><cell>weighting coefficient</cell><cell>λsm =1e-5</cell></row><row><cell>NI</cell><cell></cell><cell>λpe =1</cell></row><row><cell></cell><cell>threshold</cell><cell>0</cell></row><row><cell></cell><cell>sample ratio</cell><cell>0.1</cell></row><row><cell></cell><cell>sample k</cell><cell>1</cell></row><row><cell></cell><cell>sample number</cell><cell>5</cell></row><row><cell></cell><cell>max trojan size</cell><cell>16</cell></row><row><cell>ABS</cell><cell>remask lr remask iter per neuron</cell><cell>0.1 1000</cell></row><row><cell></cell><cell></cell><cell>0.1 if norm&lt; 16</cell></row><row><cell></cell><cell>remask weight</cell><cell>10 if 16 &lt;norm&lt; 100</cell></row><row><cell></cell><cell></cell><cell>100 if norm&gt; 100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 22</head><label>22</label><figDesc>complements the results in Table5.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 23</head><label>23</label><figDesc>presents more information (F1-score, precision, recall, and accuracy), which complements Table10.BN TNN RB TB LB ESB ABE IMC GTSRB 65.63 71.70 0.94 0.58 98.42 68.41 68.41 97.58 CIFAR100 64.53 89.76 42.77 23.44 97.83 0.98 67.86 98.75 VGGFace2 85.62 97.30 92.31 88.75 98.08 100.00 72.74 98.43</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 22 .</head><label>22</label><figDesc>Impact of data complexity on ASR (|m| = 3 × 3 and α = 0.8 for GTSRB and CIFAR100, |m| = 25×25 and α = 0.0 for VGGFace2). .<ref type="bibr" target="#b44">45</ref> 0.37 0.45 0.34 0.45 0.77 0.43 0.45 Precision 1.00 1.00 1.00 0.35 1.00 0.96 0.90 1.00 Recall 0.29 0.23 0.29 0.36 0.29 0.64 0.28 0.29 Accuracy 0.65 0.62 0.65 0.36 0.65 0.81 0.63 0.65</figDesc><table><row><cell cols="2">Defense Measure BN TNN RB TB</cell><cell>LB ESB ABE IMC</cell></row><row><cell></cell><cell cols="2">F1 Score 0.12 0.21 0.47 0.39 0.91 0.18 0.13 0.95</cell></row><row><cell>STRIP</cell><cell cols="2">Precision 0.41 0.56 0.77 0.73 0.90 0.52 0.43 0.91 Recall 0.07 0.13 0.34 0.27 0.91 0.10 0.07 0.99</cell></row><row><cell></cell><cell cols="2">Accuracy 0.48 0.51 0.62 0.58 0.91 0.50 0.49 0.95</cell></row><row><cell></cell><cell>F1 Score 0</cell></row><row><cell>NEO</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 23 .</head><label>23</label><figDesc>Additional statistics of input filtering.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We thank anonymous reviewers and shepherd for valuable feedback. This work is supported by the National Science Foundation under Grant No. 1951729, 1953813,  and 1953893. Any opinions, findings, and conclusions or recommendations are those of the authors and do not necessarily reflect the views of the National Science Foundation. X. Luo is partly supported by Hong Kong RGC Project (No. PolyU15222320).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Additional Experiments B.1. Attack</head><p>Figure <ref type="figure">12</ref> and 13 complement the results of attack performance evaluation on ImageNet with respect to trigger size and trigger transparency in Section 5.2. Note that Figure <ref type="figure">13</ref> uses α = 0.3, which is more transparent than α = 0.0 used in Table <ref type="table">14</ref>. Therefore, all attacks at 1 × 1 trigger size are not working and their ASR are close to 10%. This is not conflict to the observation in Table <ref type="table">14</ref>.</p><p>The attacks tend to be sensitive to the trigger transparency but insensitive to the trigger size (claimed in Section 4.2.1). All the attacks fail under |m| = 1 × 1 and are excluded from Figure <ref type="figure">3</ref> in Section 4.2.1. Table <ref type="table">14</ref> and Figure <ref type="figure">13</ref> use different settings. Table <ref type="table">14</ref>: α = 0.0 on CIFAR10, Figure <ref type="figure">13</ref>: α = 0.3 on ImageNet, which cause the difference in terms of trigger transparency and data complexity.</p><p>The attacks tend to be sensitive to the trigger transparency but insensitive to the trigger size (claimed in Section 4.2.1). |m| = 1 × 1 is not working for all attacks and are excluded from Figure <ref type="figure">3</ref> in Section 4.2.1. Table <ref type="table">14</ref> and Figure <ref type="figure">13</ref> use different settings. Table <ref type="table">14</ref>: α = 0.0 on CIFAR10, Figure <ref type="figure">13</ref>: α = 0.3 on ImageNet, which cause the difference in terms of trigger transparency and data complexity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ASR (%)</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>Advbox</surname></persName>
		</author>
		<ptr target="https://github.com/advboxes/AdvBox/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<ptr target="https://github.com/tensorflow/cleverhans/" />
	</analytic>
	<monogr>
		<title level="j">CleverHans Adversarial Examples Library</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="https://github.com/Trusted-AI/adversarial-robustness-toolbox/" />
		<title level="m">IBM Adversarial Robustness Toolbox (ART)</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Trojai</surname></persName>
		</author>
		<ptr target="https://trojai.readthedocs.io" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Blind Backdoors in Deep Learning Models</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Bagdasaryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of USENIX Security Symposium (SEC)</title>
				<meeting>USENIX Security Symposium (SEC)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">How To Backdoor Federated Learning</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Bagdasaryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqing</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename><surname>Estrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
				<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Poisoning Adaptive Biometric Systems</title>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Roli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Didaci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Joint IAPR International Workshop on Structural, Syntactic, and Statistical Pattern Recognition (SSPR&amp;SPR)</title>
				<meeting>Joint IAPR International Workshop on Structural, Syntactic, and Statistical Pattern Recognition (SSPR&amp;SPR)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<ptr target="https://github.com/BVLC/caffe/wiki/Model-Zoo" />
	</analytic>
	<monogr>
		<title level="j">BVLC. Model zoo</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName><forename type="first">Qiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Ian Molloy, and Biplav Srivastava. Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering</title>
		<author>
			<persName><forename type="first">Bryant</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilka</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathalie</forename><surname>Baracaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DeepInspect: A Black-box Trojan Detection and Mitigation Framework for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Huili</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jishen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farinaz</forename><surname>Koushanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence</title>
				<meeting>International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning</title>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimberly</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">SentiNet: Detecting Physical Attacks Against Deep Learning Systems</title>
		<author>
			<persName><forename type="first">Edward</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giancarlo</forename><surname>Pellegrino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Certified Adversarial Robustness via Randomized Smoothing</title>
		<author>
			<persName><forename type="first">Elan</forename><surname>Jeremy M Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Machine Learning (ICML)</title>
				<meeting>IEEE Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Meet AISight: The scary CCTV network completely run by AI</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Cooper</surname></persName>
		</author>
		<ptr target="http://www.itproportal.com/" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ima-geNet: A Large-scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dermatologist-level classification of skin cancer with deep neural networks</title>
		<author>
			<persName><forename type="first">Andre</forename><surname>Esteva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brett</forename><surname>Kuprel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><forename type="middle">A</forename><surname>Novoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">M</forename><surname>Swetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><forename type="middle">M</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">542</biblScope>
			<biblScope unit="issue">7639</biblScope>
			<biblScope unit="page" from="115" to="118" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interpretable Explanations of Black Boxes by Meaningful Perturbation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ruth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV</title>
				<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">STRIP: A Defence Against Trojan Attacks on Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiping</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damith</forename><surname>Ranasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Nepal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Computer Security Applications Conference (ACSAC)</title>
				<meeting>Annual Computer Security Applications Conference (ACSAC)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
				<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Garg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">LEMNA: Explaining Deep Learning Based Security Applications</title>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongliang</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Purui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Conference on Computer and Communications (CCS)</title>
				<meeting>ACM Conference on Computer and Communications (CCS)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI Systems</title>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Data Mining (ICDM)</title>
				<meeting>IEEE International Conference on Data Mining (ICDM)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neu-ronInspect: Detecting Backdoors in Neural Networks via Output Explanations</title>
		<author>
			<persName><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustafa</forename><surname>Alzantot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mani</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence (AAAI)</title>
				<meeting>AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Model-Reuse Attacks on Deep Learning Systems</title>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shouling</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiapu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SAC Conference on Computer and Communications (CCS)</title>
				<meeting>ACM SAC Conference on Computer and Communications (CCS)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Panagiota</forename><surname>Kiourti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kacper</forename><surname>Wardega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susmit</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchao</forename><surname>Li</surname></persName>
		</author>
		<title level="m">TrojDRL: Trojan Attacks on Deep Reinforcement Learning Agents. ArXiv e-prints</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weight Poisoning Attacks on Pre-trained Models</title>
		<author>
			<persName><forename type="first">Keita</forename><surname>Kurita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<meeting>Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bypassing Backdoor Detection Algorithms in Deep Learning</title>
		<author>
			<persName><forename type="first">Te</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE European Symposium on Security and Privacy (Euro S&amp;P)</title>
				<meeting>IEEE European Symposium on Security and Privacy (Euro S&amp;P)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Invisible Backdoor Attacks Against Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Shaofeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin Zi Hao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minhui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dali</forename><surname>Kaafar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haojin</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<title level="m">Backdoor Learning: A Survey</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features</title>
		<author>
			<persName><forename type="first">Junyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SAC Conference on Computer and Communications (CCS)</title>
				<meeting>ACM SAC Conference on Computer and Communications (CCS)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">DEEPSEC: A Uniform Platform for Security Analysis of Deep Learning Model</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Symposium on Security and Privacy (S&amp;P)</title>
				<meeting>IEEE Symposium on Security and Privacy (S&amp;P)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Symposium on Research in Attacks, Intrusions and Defenses (RAID)</title>
				<meeting>Symposium on Research in Attacks, Intrusions and Defenses (RAID)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">ABS: Scanning Neural Networks for Back-Doors by Artificial Brain Stimulation</title>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Chuan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanhong</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yousra</forename><surname>Aafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SAC Conference on Computer and Communications (CCS)</title>
				<meeting>ACM SAC Conference on Computer and Communications (CCS)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Trojaning attack on neural networks</title>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yousra</forename><surname>Aafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Chuan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Network and Distributed System Security Symposium (NDSS)</title>
				<meeting>Network and Distributed System Security Symposium (NDSS)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Yunfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
				<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards Deep Learning Models Resistant to Adversarial Attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
				<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Dimitris Tsipras, and Adrian Vladu</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">MagNet: A Two-Pronged Defense Against Adversarial Examples</title>
		<author>
			<persName><forename type="first">Dongyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SAC Conference on Computer and Communications (CCS</title>
				<meeting>ACM SAC Conference on Computer and Communications (CCS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Analysis of Universal Adversarial Perturbations</title>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Seyed-Mohsen Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><surname>Soatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models</title>
		<author>
			<persName><forename type="first">Ren</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shouling</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yevgeniy</forename><surname>Vorobeychik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiapu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SAC Conference on Computer and Communications (CCS)</title>
				<meeting>ACM SAC Conference on Computer and Communications (CCS)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deepsweep: An evaluation framework for mitigating dnn backdoor attacks using data augmentation</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meikang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavani</forename><surname>Thuraisingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Symposium on Information, Computer and Communications Security (AsiaCCS)</title>
				<meeting>ACM Symposium on Information, Computer and Communications Security (AsiaCCS)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Dynamic Backdoor Attacks Against Machine Learning Models</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Salem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Humpty Dumpty: Controlling Word Meanings via Corpus Poisoning</title>
		<author>
			<persName><forename type="first">Roei</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Meri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Symposium on Security and Privacy (S&amp;P)</title>
				<meeting>IEEE Symposium on Security and Privacy (S&amp;P)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hidden Technical Debt in Machine Learning Systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietmar</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinay</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Francois</forename><surname>Crespo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Dennison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
				<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV</title>
				<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers</title>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Severi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Coull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alina</forename><surname>Oprea</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Ronny</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Octavian</forename><surname>Suciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tudor</forename><surname>Dumitras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
				<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
				<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Computer: Benchmarking Machine Learning Algorithms for Traffic Sign Recognition</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Metworks</title>
		<imprint>
			<biblScope unit="page" from="323" to="332" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Man vs</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Certified Defenses for Data Poisoning Attacks</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
				<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">One pixel attack for fooling deep neural networks</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><forename type="middle">Vasconcellos</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kouichi</forename><surname>Sakurai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="828" to="841" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</title>
		<author>
			<persName><forename type="first">Octavian</forename><surname>Suciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Mȃrginean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yigitcan</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Tudor</forename><surname>Dumitras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">¸</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of USENIX Security Symposium (SEC)</title>
				<meeting>USENIX Security Symposium (SEC)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination Detection</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kehuan</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">An Embarrassingly Simple Approach for Trojan Attack in Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Ruixiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengnan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Knowledge Discovery and Data Mining (KDD)</title>
				<meeting>ACM International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Attacks Meet Interpretability: Attribute-Steered Detection of Adversarial Samples</title>
		<author>
			<persName><forename type="first">Guanhong</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
				<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Spectral Signatures in Backdoor Attacks</title>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
				<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Model Agnostic Defence against Backdoor Attacks in Machine Learning</title>
		<author>
			<persName><forename type="first">Sakshi</forename><surname>Udeshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanshan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lionell</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louth</forename><surname>Rawshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudipta</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Researchers Hack Into Driverless Car System</title>
		<author>
			<persName><forename type="first">Allyson</forename><surname>Versprille</surname></persName>
		</author>
		<ptr target="http://www.nationaldefensemagazine.org/" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Viswanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Symposium on Security and Privacy (S&amp;P)</title>
				<meeting>IEEE Symposium on Security and Privacy (S&amp;P)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">RAB: Provable Robustness Against Backdoor Attacks</title>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bojan</forename><surname>Karlas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>ArXiv eprints</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Skip Connections Matter: On the Transferability of Adversarial Examples Generated with ResNets</title>
		<author>
			<persName><forename type="first">Dongxian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
				<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">DBA: Distributed Backdoor Attacks against Federated Learning</title>
		<author>
			<persName><forename type="first">Chulin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
				<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Network and Distributed System Security Symposium (NDSS)</title>
				<meeting>Network and Distributed System Security Symposium (NDSS)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Detecting AI Trojans Using Meta Neural Analysis</title>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huichen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Borisov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><forename type="middle">A</forename><surname>Gunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Symposium on Security and Privacy (S&amp;P)</title>
				<meeting>IEEE Symposium on Security and Privacy (S&amp;P)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Latent Backdoor Attacks on Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Yuanshun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SAC Conference on Computer and Communications (CCS)</title>
				<meeting>ACM SAC Conference on Computer and Communications (CCS)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Disabling Backdoor and Identifying Poison Data by Using Knowledge Distillation in Backdoor Attacks on Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Kota</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeshi</forename><surname>Fujino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Workshop on Artificial Intelligence and Security (AISec)</title>
				<meeting>ACM Workshop on Artificial Intelligence and Security (AISec)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">How Transferable Are Features in Deep Neural Networks?</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
				<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Interpretable Deep Learning under Fire</title>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningfei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shouling</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiapu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of USENIX Security Symposium</title>
				<meeting>USENIX Security Symposium</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Trojaning Language Models for Fun and Profit</title>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Transferable Clean-Label Poisoning Attacks on Deep Neural Nets</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Ronny</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengduo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Machine Learning (ICML)</title>
				<meeting>IEEE Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
