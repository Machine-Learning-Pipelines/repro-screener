<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transfer Fine-Tuning: A BERT Case Study</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-09-03">3 Sep 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuki</forename><surname>Arase</surname></persName>
							<email>arase@ist.osaka-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Osaka University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Artificial Intelligence Research Center (AIRC)</orgName>
								<orgName type="institution">AIST</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junichi</forename><surname>Tsujii</surname></persName>
							<email>j-tsujii@aist.go.jp</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">NaCTeM</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">University of Manchester</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transfer Fine-Tuning: A BERT Case Study</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-09-03">3 Sep 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">821C04AA103FA0EAB552508FC4518C66</idno>
					<idno type="arXiv">arXiv:1909.00931v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-10-27T19:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A semantic equivalence assessment is defined as a task that assesses semantic equivalence in a sentence pair by binary judgment (i.e., paraphrase identification) or grading (i.e., semantic textual similarity measurement). It constitutes a set of tasks crucial for research on natural language understanding. Recently, BERT realized a breakthrough in sentence representation learning <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, which is broadly transferable to various NLP tasks. While BERT's performance improves by increasing its model size, the required computational power is an obstacle preventing practical applications from adopting the technology. Herein, we propose to inject phrasal paraphrase relations into BERT in order to generate suitable representations for semantic equivalence assessment instead of increasing the model size. Experiments on standard natural language understanding tasks confirm that our method effectively improves a smaller BERT model while maintaining the model size. The generated model exhibits superior performance compared to a larger BERT model on semantic equivalence assessment tasks. Furthermore, it achieves larger performance gains on tasks with limited training datasets for fine-tuning, which is a property desirable for transfer learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Paraphrase identification and semantic textual similarity (STS) measurements aim to assess semantic equivalence in sentence pairs. These tasks are central problems in natural language understanding research and its applications. In this paper, these tasks are defined as semantic equivalence assessments.</p><p>Sentence representation learning is the basis of assessing semantic equivalence. Unsupervised learning is becoming the preferred approach because it only requires plain corpora, which are now abundantly available. In this approach, a model is pre-trained to generate generic sentence representations that are broadly transferable to various natural language processing (NLP) tasks. Subsequently, it is fine-tuned to generate specific representations for solving a target task using an annotated corpus. Considering the high costs of annotation, a pre-trained model that efficiently fits the target task with a smaller amount of annotated corpus is desired.</p><p>Recently, Bidirectional Encoder Representations from Transformers (BERT) realized a breakthrough, which dramatically improved sentence representation learning <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>. BERT pre-trains its encoder using language modeling and by discriminating surrounding sentences in a document from random ones. Pre-training in this manner allows distributional relations between sentences to be learned. Intensive efforts are currently being made to pre-train larger models by feeding them enormous corpora for improvement <ref type="bibr" target="#b18">(Radford et al., 2019;</ref><ref type="bibr" target="#b31">Yang et al., 2019)</ref>. For example, a large model of BERT has 340M parameters, which is 3.1 times larger than its smaller alternative. Although such a large model achieves performance gains, the required computational power hinders its application to downstream tasks.</p><p>Given the importance of natural language understanding research, we focus on sentence representation learning for semantic equivalence assessment. Instead of increasing the model size, we propose the injection of semantic relations into a pre-trained model, namely BERT, to improve performance. <ref type="bibr" target="#b15">Phang et al. (2019)</ref> showed that BERT's performance on downstream tasks improves by simply inserting extra training on data-rich supervised tasks. Unlike them, we inject semantic relations of finer granularity using phrasal paraphrase alignments automatically iden-tified by <ref type="bibr" target="#b0">Arase and Tsujii (2017)</ref> to improve semantic equivalent assessment tasks. Specifically, our method learns to discriminate phrasal and sentential paraphrases on top of the representations generated by BERT. This approach explicitly introduces the concept of the phrase to BERT and supervises semantic relations between phrases. Due to studies on sentential paraphrase collection <ref type="bibr" target="#b11">(Lan et al., 2017)</ref> and generation <ref type="bibr" target="#b27">(Wieting and Gimpel, 2018)</ref>, a million-scale paraphrase corpus is ready for use. We empirically show that further training of a pre-trained model on relevant tasks transfers well to downstream tasks of the same kind, which we name as transfer fine-tuning.</p><p>The contributions of our paper are:</p><p>• We empirically demonstrate that transfer finetuning using paraphrasal relations allows a smaller BERT to generate representations suitable for semantic equivalence assessment. The generated model exhibits superior performance to the larger BERT while maintaining the small model size.</p><p>• Our experiments indicate that phrasal paraphrase discrimination contributes to representation learning, which complements simpler sentence-level paraphrase discrimination.</p><p>• Our model exhibits a larger performance gain over the BERT model for a limited amount of fine-tuning data, which is an important property of transfer learning.</p><p>We hope that this study will open up one of the crucial research directions that will make the approach of pre-trained models more practically useful. Our codes, datasets, and the trained models will be made publicly available at our web site.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Sentence representation learning is an active research area due to its importance in various downstream tasks. Early studies employed supervised learning where a sentence representation is learned in an end-to-end manner using an annotated corpus. Among these, the importance of phrase structures in representation learning has been discussed <ref type="bibr" target="#b23">(Tai et al., 2015;</ref><ref type="bibr" target="#b29">Wu et al., 2018)</ref>.</p><p>In this paper, we use structural relations in sentence pairs for sentence representations. Specifically, we employ phrasal paraphrase relations that introduce the notion of a phrase to the model.</p><p>The research focus of sentence representation learning has moved toward unsupervised learning in order to exploit the gigantic corpus. Skip-Thought, which was an early learning attempt, learns to generate surrounding sentences given a sentence in a document <ref type="bibr">(Kiros et al., 2015)</ref>. This can be interpreted as an extension of the distributional hypothesis on sentences. Quick-Thoughts, a successor of Skip-Thought, conducts classification to discriminate surrounding sentences instead of generation <ref type="bibr" target="#b13">(Logeswaran and Lee, 2018)</ref>. GenSen combines these approaches in massive multi-task learning <ref type="bibr" target="#b21">(Subramanian et al., 2018)</ref> based on the premise that learning dependent tasks enriches sentence representations.</p><p>Embeddings from Language Models (ELMo) made a significant step forward <ref type="bibr" target="#b14">(Peters et al., 2018)</ref>. ELMo uses language modeling with bidirectional recurrent neural networks (RNN) to improve word embeddings. ELMo's embedding contributes to the performance of various downstream tasks. OpenAI GPT <ref type="bibr" target="#b17">(Radford et al., 2018)</ref> replaced ELMo's bidirectional RNN for language modeling with the Transformer <ref type="bibr" target="#b24">(Vaswani et al., 2017)</ref> decoder. More recently, BERT combined the approaches of Quick-Thoughts (i.e., a nextsentence prediction approach) and language modeling on top of the deep bidirectional Transformer. BERT broke the records of the previous stateof-the-art methods in eleven different NLP tasks. While BERT's pre-training generates generic representations that are broadly transferable to various NLP tasks, we aim to fit them for semantic equivalence assessment by injecting paraphrasal relations. <ref type="bibr" target="#b12">Liu et al. (2019)</ref> showed that BERT's performance improves when fine-tuning with a multi-task learning setting, which is applicable to our trained model for further improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Phrase Alignment for Paraphrases</head><p>In order to obtain phrasal paraphrases, we used the phrase alignment method proposed in <ref type="bibr" target="#b0">(Arase and Tsujii, 2017)</ref> and apply it to our paraphrase corpora. The alignment method aligns phrasal paraphrases on the parse forests of a sentential paraphrase pair as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>According to the evaluation results reported in <ref type="bibr" target="#b0">(Arase and Tsujii, 2017)</ref>, the precision and recall of alignments are 83.6% and 78.9%, which are 89% and 92% of those of humans, respec- tively. Although alignment errors occur, previous studies show that neural networks are relatively robust against noise in a training corpus and still benefit from extra supervisions as demonstrated in <ref type="bibr" target="#b6">(Edunov et al., 2018;</ref><ref type="bibr" target="#b16">Prabhumoye et al., 2018)</ref>. We collect all the spans of phrases in a sentential paraphrase pair and their alignments as pairs of phrase spans. Because the phrase alignment method allows unaligned phrases, not all of the phrases have aligned counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pre-Training on BERT</head><p>BERT is a bidirectional Transformer that generates a sentence representation by conditioning both the left and right contexts of a sentence. A pre-trained BERT model can be easily finetuned for a wide range of tasks by just adding a fully-connected layer, without any task-specific architectural modifications. BERT achieved stateof-the-art performances for eleven NLP tasks, thereby outperforming the previous state-of-theart methods by a large margin.</p><p>Pre-training in BERT accomplishes two tasks. The first task is masked language modeling, where some words in a sentence are randomly masked and the model then predicts them from the context. This task design allows the representation to fuse both the left and the right context. The second task predicts whether a pair of sentences are consecutive in a document to learn the relation between the sentences. Specifically, as illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>, BERT takes two sentences as input that are concatenated by a special token <ref type="bibr">[SEP]</ref>. 1 The first 1 Throughout the paper, typewriter font represents For sentential paraphrase task: L s (Θ) 10:</p><formula xml:id="formula_0">L(Θ) = L p (Θ) + L s (Θ) 11: Compute gradient: ∇(Θ) 12:</formula><p>Update the model parameters 13: until convergence token of every input is always the special token of <ref type="bibr">[CLS]</ref>. The final hidden state corresponding to this [CLS] token is regarded as an aggregated representation of the input sentence pair. This is used to predict whether the sentence pair is composed of consecutive sentences in a document or not during pre-training.</p><p>BERT has a deep architecture. The BERTbase model has 12 layers of 768 hidden size and 12 self-attention heads. The BERT-large model has 24 layers of 1024 hidden size and 16 selfattention heads. Both BERT-bese and BERT-large models were pre-trained using BookCorpus <ref type="bibr">(Zhu et al., 2015)</ref> and English Wikipedia (in total 3.3B words).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Transfer Fine-Tuning with Paraphrasal Relation Injection</head><p>We inject semantic relations between a sentence pair into a pre-trained BERT model through classification of phrasal and sentential paraphrases. After the training, the model can be fine-tuned in exactly the same manner as with BERT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>Algorithm 4.1 provides an overview of our method. It takes a sentential paraphrase pair s, t as an input, which are referred to as the source and target, respectively, for the sake of clarity. First, a set of phrase alignments A is obtained for s, t  (line 1) as described in Sec. 3.1. Because BERT uses sub-words as a unit instead of words, all the input sentences are tokenized (line 2) by Word-Piece <ref type="bibr" target="#b30">(Wu et al., 2016)</ref>. In addition, t is concatenated to s when being input to the BERT model, where the first token should always be [CLS] and the sentence pair is separated by [SEP] as described in Sec. 3.2. In order to accommodate to these factors, phrase spans in alignments A are adjusted accordingly (line 3).</p><p>Our method learns to discriminate phrasal and sentential paraphrases simultaneously as illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>. Cross-entropy is used as the loss functions for both tasks (line 8, 9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phrasal Paraphrase Classification</head><p>The middle part of Fig. <ref type="figure" target="#fig_2">2</ref> illustrates phrasal paraphrase classification. We first generate phrase embedding for each aligned phrase as follows. The tokenized sentence pair is encoded by the BERT model. For the input sequence of N tokens {w i } i=1,...,N , we obtain the final hidden states {h i } i=1,...,N (i.e., output of the bidirectional Transformer):</p><formula xml:id="formula_1">h i = Transformer(w 1 , . . . , w N ),</formula><p>where h i ∈ R λ and λ is the hidden size. We then combine {h i } i for a phrase pair with an alignment (j, k), (m, n) where 2 ≤ j &lt; k &lt; m &lt; n ≤ N − 1 represent indexes of the beginning and ending of phrases (recall that the first and last tokens are always special tokens in BERT). As a combination function, we apply max-pooling that showed strong performance in <ref type="bibr" target="#b3">(Conneau et al., 2017)</ref> to generate a representation of source and target phrases:</p><formula xml:id="formula_2">h s = max-pooling(h j , . . . , h k ),<label>(1)</label></formula><formula xml:id="formula_3">h t = max-pooling(h m , . . . , h n ).</formula><p>(2)</p><p>The max-pooling(•) function selects the maximum value over each dimension of the hidden units. Then h s and h t are converted to a single vector. To extract relations between h s and h t , three matching methods are used <ref type="bibr" target="#b3">(Conneau et al., 2017)</ref>: (a) concatenating the representations (h s , h t ), (b) taking the element-wise product h s * h t , and (c) finding the absolute element-wise difference |h s − h t |. The final vector of R 4λ is fed into a classifier. 2  Because our method aims to generate representations for semantic equivalence assessment, the classifier should be simple <ref type="bibr" target="#b13">(Logeswaran and Lee, 2018)</ref>. Otherwise, a sophisticated classifier would fit itself with the task instead of the representations. We use a single fully-connected layer culminating in a softmax layer as our classifier.</p><p>Previous studies have calculated interactions between words <ref type="bibr" target="#b7">(He and Lin, 2016)</ref> and phrases <ref type="bibr" target="#b2">(Chen et al., 2017)</ref> using the final hidden states of bidirectional RNN or recursive neural networks when composing a sentence representation. Our approach differs from these by giving explicit supervision of which phrase pairs have semantic interactions (i.e., paraphrases).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negative Example Selection</head><p>In paraphrase identification, non-paraphrases with large lexical differences are easy to discriminate. Discrimination becomes far more difficult when they contain a number of identical or related words. To effectively supervise the model by solving difficult discrimination problems, we designed a three-way classification task: discrimination of paraphrase, random, and in-paraphrase pairs.</p><p>The random examples are generated by pairing s to a random sentence t from the training corpus, and then pairing all phrases in s to randomly chosen phrases in t . The in-paraphrase examples aim to make the discrimination problem difficult, which requires distinguishing true paraphrases and phrases in the paraphrasal sentence pair t. These may provide sub-phrases or ancestor phrases of true paraphrases as difficult negative examples, which tend to retain the same topic and similar wordings. To prepare such examples, for each phrase pair (j, k), (m, n) ∈ A, the target span (m, n) is replaced by a randomly chosen phrase span in t.</p><p>Phrasal paraphrase classification aims to give explicit supervision of semantic relations among phrases in representation learning. It also introduces structures in sentences, which is completely missed in BERT's pre-training. <ref type="bibr" target="#b22">Swayamdipta et al. (2018)</ref> showed that supervision of phrasebased syntax improves the performance of a task relevant to semantics, e.g., semantic role labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentential Paraphrase Classification</head><p>The left side of Fig. <ref type="figure" target="#fig_2">2</ref> illustrates the sentential paraphrase classification. The process is simple; the final hidden state of the [CLS] token, i.e., h 1 , is fed into a classifier to discriminate whether a sentence pair is a paraphrase or a random sentence combination. Note that these random sentence pairs provide random phrases for the phrasal paraphrase classification described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Setting</head><p>We collected paraphrases from various sources as summarized in Table <ref type="table" target="#tab_1">1</ref>, which shows the numbers of sentential and phrasal paraphrase pairs after phrase alignment. 3  (LDC) or authors' websites. The following bullets describe the sources.</p><p>• NIST OpenMT 4 : We randomly paired reference translations of the same source sentence as was done in <ref type="bibr" target="#b0">(Arase and Tsujii, 2017)</ref>.</p><p>• Twitter URL corpus <ref type="bibr" target="#b11">(Lan et al., 2017)</ref>: This corpus was collected from Twitter by linking tweets through shared URLs. We used a threemonth collection of paraphrases. 5</p><p>• Simple Wikipedia <ref type="bibr" target="#b8">(Kauchak, 2013)</ref>: This corpus aligned English Wikipedia and Simple English Wikipedia for text simplification. We used "sentence-aligned, version 2.0." 6</p><p>• Para-NMT <ref type="bibr" target="#b27">(Wieting and Gimpel, 2018)</ref>: This corpus was created by translating the Czech side of a large Czech-English parallel corpus and pairing the translated English and originally target-side English as paraphrases. We used "Para-nmt-5m-processed." 7</p><p>Note that these sentential and phrasal paraphrases are obtained by automatic methods. On the contrary, dataset creation for downstream tasks generally requires expensive human annotation. We employed the pre-trained BERT-base model 8 and conducted paraphrase classification using the collected paraphrase corpora. Adam (Kingma and Ba, 2015) was applied as an optimizer with a learning rate of 5e − 5. A dropout probability was 0.2 for the fully-connected layers in the classifiers. A development set and a test set, each with 50k sentence pairs, were subtracted from the paraphrase corpus. The rest of the corpus was used for training. The training was conducted on four NVIDIA Tesla V100 GPUs with a batch-size of 100. Early stopping was applied to stop training at the second time decrease in the accuracy of the phrasal paraphrase classification, which was measured on the development set. The final test-set accuracies were 98.1% and 99.9% for phrasal and sentential paraphrase classification, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Hypotheses to Verify</head><p>BERT's pre-training learns to generate sentence representations broadly transferable to different NLP tasks. In contrast, our method gives more direct supervision to generate representations suitable for semantic equivalence assessment tasks. We set up the following hypotheses on features of our method, which will be empirically verified through evaluation: H1 Our method contributes to semantic equivalence assessment tasks.</p><p>H2 Our method achieves improvement on downstream tasks that only have small amounts of training datasets for fine-tuning.</p><p>H3 Our method moderately improves tasks if they are relevant to semantic equivalence assessment.</p><p>H4 Our training does not transfer to distant downstream tasks that are independent to semantic equivalence assessment.</p><p>H5 Phrasal and sentential paraphrase classification complementarily benefits sentence representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">GLUE Datasets</head><p>We empirically verified the hypotheses H1 to H5 using the General Language Understanding Evaluation (GLUE) benchmark <ref type="bibr" target="#b25">(Wang et al., 2019)</ref> 9 , which is the standard benchmark and provides collections of datasets for natural language understanding tasks.  evaluation server unless stated otherwise. Accuracies on MRPC and QQP and Spearman correlation on STS-B are omitted due to space limitations. Note that they showed the same trends as F1 and Pearson correlation, respectively, in our experiment. WNLI was excluded because the GLUE web site reports its issues. 10 GLUE tasks can be categorized according to their aims as follows.</p><p>Semantic Equivalence Assessment Tasks (MRPC, STS-B, QQP) These are the primary targets of our method, which are used to verify hypothesis H1. Paraphrase identification assesses equivalence in a sentence pair by binary judgments. Microsoft Paraphrase Corpus (MRPC) <ref type="bibr" target="#b5">(Dolan et al., 2004</ref>) consists of sentence pairs drawn from news articles, while Quora Question Pairs (QQP) 11 consists of question pairs from the community QA website.</p><p>STS assesses semantic equivalence by grading. STS benchmark (STS-B) <ref type="bibr" target="#b1">(Cer et al., 2017)</ref> provides sentence pairs drawn from heterogeneous sources, which are human-annotated with a level of equivalence from 1 to 5.</p><p>NLI Tasks (MNLI-m/mm, RTE, QNLI) We use natural language inference (NLI) tasks to verify hypothesis H3 because they constitute a class of problems relevant to semantic equivalence assessment. NLI tasks are different from semantic equivalence assessment in that they often require logical inference and understanding of commonsense knowledge. The Multi-Genre Natural Language Inference Corpus (MNLI) <ref type="bibr" target="#b28">(Williams et al., 2018)</ref>  NLI task while MNLI-mm is a cross-domain NLI task.</p><p>The Recognizing Textual Entailment (RTE) corpus 12 was created from news and Wikipedia. Question-answering NLI (QNLI) was created from The Stanford Question Answering Dataset <ref type="bibr" target="#b19">(Rajpurkar et al., 2016)</ref> on which all the sentences were drawn from Wikipedia.</p><p>Single-Sentence Tasks (SST, CoLA) We use these tasks to verify hypothesis H4. They aim to estimate features in a single sentence, which has little interaction with semantic equivalence assessment in a sentence pair. The Stanford Sentiment Treebank (SST) <ref type="bibr" target="#b20">(Socher et al., 2013)</ref> task is a binary sentiment classification, while The Corpus of Linguistic Acceptability (CoLA) <ref type="bibr" target="#b26">(Warstadt et al., 2018)</ref> task is a binary classification of grammatical acceptability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Fine-Tuning on Downstream Tasks</head><p>Once trained, our model can be used in exactly the same manner as the pre-trained BERT models. For fine-tuning our models and replicating BERT's results under the same setting, we set the hyperparameter to those recommended in <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>: a batch size of 32, a learning rate of 3e − 5, the number of training epochs to 4, and a dropout probability of 0.1. We fine-tuned all the models on downstream tasks using the script provided in the Pytorch version of BERT. 13 For STS-B, we modified the script slightly to conduct regression instead of classification. All other hyperparameters were set to the default values defined in the BERT's fine-tuning script.</p><p>For fair comparison, we kept the same hyperparameter settings described above across all tasks and models. <ref type="bibr" target="#b15">Phang et al. (2019)</ref> discussed that BERT performances become unstable when a training dataset with fine-tuning is small. In our 12 https://aclweb.org/aclwiki/ Recognizing_Textual_Entailment 13 run classifier.py in https://github.com/ huggingface/pytorch-pretrained-BERT evaluation, performances were stable when setting the same hyper-parameters, but further investigation is our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Effect on Semantic Equivalence Assessment Tasks</head><p>Table <ref type="table" target="#tab_5">3</ref> shows fine-tuning results on GLUE; our model, denoted as Transfer Fine-Tuning, is compared against BERT-base and BERT-large. The first set of columns shows the results of semantic equivalence assessment tasks. Our model outperformed BERT-base on MRPC (+0.9 points) and STS-B (+2.7 points). Furthermore, it outperformed even BERT-large by 0.6 points on MRPC and by 1.4 points on STS-B, despite BERT-large having 3.1 times more parameters than our model. <ref type="bibr" target="#b4">Devlin et al. (2019)</ref> described that the nextsentence prediction task in BERT's pre-training aims to train a model that understands sentence relations. Herein, we argue that such relations are effective at generating representations broadly transferable to various NLP tasks, but are too generic to generate representations for semantic equivalence assessment tasks. Our method allows semantic relations between sentences and phrases that are directly useful for this class of tasks to be learned. These results support hypothesis H1, indicating that our approach is more effective than blindly enlarging the model size. A smaller model size is desirable for practical applications. We have also applied our method on the BERT-large model, but its performance was not much improved to warrant the larger model size. Further investigation regarding pre-trained model sizes is our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Effect of the Amount of Fine-Tuning Datasets</head><p>Our method did not improve upon BERT-base for QQP. We consider this is because a large QQP training set (364k sentence pairs) allows the BERT model to converge to a certain optimum. This also  relates to hypothesis H2.</p><p>To investigate the effect of the sizes of training sets, we fine-tuned our model and BERTbase for semantic equivalence assessment tasks using randomly subsampled training sets. Table <ref type="table" target="#tab_7">4</ref> shows scores on the development sets. 14 The result clearly indicates that our method is more beneficial when a training dataset is limited on a downstream task, which supports hypothesis H2. This property is preferable for a transfer learning scenario that unsupervised sentence representation learning assumes.</p><p>Another factor that may affect the performance is domain mismatch between our paraphrase corpora and QQP corpus. The former was mostly collected from news while the latter was extracted from a social QA forum. In the future, we will investigate the effects of domains by generating multi-domain paraphrase corpora using a method proposed by <ref type="bibr" target="#b27">Wieting and Gimpel (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Effect on NLI Tasks</head><p>The second set of columns in Table <ref type="table" target="#tab_5">3</ref> shows the results on NLI tasks. Our model presents moderate improvements on most NLI tasks, which supports hypothesis H3. We consider this is because the majority of NLI tasks that require inferences in one-direction, contrary to bi-directional entailment relations of paraphrases, are uni-directional.</p><p>Another reason is that our elaborate feature generation for the phrasal paraphrase classifier tightly fits the model for paraphrase identification. This contributes to performance improvements on this 14 We used the development set because the GLUE server allows only two submissions per day. Note that the number of training epochs for fine-tuning is fixed in our experiments, hence, the development set was not used for other purposes.</p><p>task, but sacrifices the model's generality on relevant tasks. We tackle this issue in our follow-up study reported in the Appendix.</p><p>Among NLI tasks, our model largely outperformed BERT-base by 5.0 point on RTE. This may be again due to the property of our method that brings improvement on tasks with a limited training set as RTE has only 2.5k training sentence pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Effect on Single-Sentence Tasks</head><p>The last two columns of Table <ref type="table" target="#tab_5">3</ref> show results on single-sentence tasks; SST and CoLA, which are the most distant tasks from paraphrase classification. Our model presents a slightly lower score on SST compared to BERT-base and performed poorly on CoLA.</p><p>One potential reason for this degradation is that our training takes a sentence pair as input, which may weaken the ability to model a single sentence. Another cause is attributable to similarities between our training and fine-tuning tasks. For SST, sentiment analysis could be adversarial toward paraphrase discrimination tasks. Although paraphrasal sentences tend to have the same sentiments, sentences with the same sentiments do not generally hold paraphrastic relations. For CoLA, semantic relations unlikely contribute to determining grammatical acceptability, as required by CoLA task.</p><p>Together with the results in Sec. 6.3, hypothesis H4 is supported; the effectiveness of our method depends on relevance between paraphrase discrimination and downstream tasks. Our future work will be to examine what characteristics of NLP tasks make our method less effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Ablation Study</head><p>To verify hypothesis H5, we conducted an ablation study that investigates independent effects of sentential and phrasal paraphrase classification. Table <ref type="table" target="#tab_8">5</ref> shows the results; the last three rows show performances when conducting only sentential paraphrase classification, phrasal paraphrase classification, and binary classification of paraphrase and in-paraphrase pairs, respectively. All the models were fine-tuned in the same manner as described in Sec. 5.3.</p><p>First, the results support the hypothesis; sentential and phrasal paraphrase classification complements each other on sentence representation learning. on MRPC, MNLI-m/mm, SST, and CoLA tasks by conducting both sentential and phrasal paraphrase classification simultaneously. Interestingly, these scores are higher than those when sentential and phrasal paraphrase classification are conducted independently. This is reasonable considering the process of fine-tuning. Sentential paraphrase classification directly affects the representation of [CLS], which is the primary tuning factor in fine-tuning for downstream tasks. Alternatively, phrasal paraphrase classification affects representations of phrases, which are the basis for generating the [CLS] representation. Simultaneously conducting both sentential and phrasal paraphrase classification thus creates synergy.</p><p>It is also obvious that the three-way classification of phrasal paraphrases, on which the model discriminates paraphrases, random combinations of phrases from a random pair of sentences, and random combinations of phrases in a paraphrasal sentence pair, is superior to binary classification. This shows that discriminating random combinations of phrases, which is a simpler and easier task, also contributes to representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We empirically demonstrate that sentential and phrasal paraphrase relations help sentence representation learning. While BERT's pre-training aims to generate generic representations transferable to a broad range of NLP tasks, our method generates representations suitable for the class of semantic equivalence assessment tasks. Our method achieves performance gains while maintaining the model size. Furthermore, it exhibits improvement on downstream tasks with limited amounts of training datasets for fine-tuning, which is a property crucial for transfer learning.</p><p>In the future, we plan to investigate the effects of our method on different sizes of BERT models. Additionally, we will apply our model to improve the alignment quality of the phrase alignment model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Transfer Fine-Tuning with Simple Features</head><p>To further investigate effects of transfer finetuning using paraphrase relations on BERT, we designed a model that generates a simplest feature to input into the classifier in Fig. <ref type="figure" target="#fig_2">2</ref>. We assume that this method transmits learning signals to the underlying BERT in a more effective manner. Specifically, we use mean-pooling to generate representations of source and target phrases in Eq. (1) and Eq. (2), respectively. These representations are simply concatenated as a feature representation and then fed into the classifier.</p><p>Table <ref type="table">6</ref> compares this new model (denoted as Simple Transfer Fine-Tuning) to BERT models as well as our model with the elaborate feature generation described in Sec. 4 (denoted as Transfer Fine-Tuning) on semantic equivalent assessment and NLI tasks of GLUE benchmark. Table <ref type="table">7</ref> reports an ablation study. The results and findings are summarized as follows.</p><p>• Our model with simple feature generation (Simple Transfer Fine-Tuning) on BERTbase outperformed BERT on both semantic equivalent assessment and NLI tasks. Furthermore, it performed on-par against BERTlarge on MRPC and outperformed it on STS-B and RTE, despite BERT-large having 3.1 times more parameters than our model.</p><p>• The same trend was confirmed on the model trained on BERT-large, where our model outperformed BERT-large on all the tasks except QNLI.</p><p>• Simple Transfer Fine-Tuning also outperformed our model with elaborate feature generation (Transfer Fine-Tuning) on all semantic equivalent assessment and NLI tasks except MRPC. This result implies that elaborate feature generation tightly fits the model to paraphrase identification while sacrifices its generality to relevant tasks. Further investigation will be our future work.</p><p>• Sentential and phrasal paraphrase classification complements each other on sentence representation learning when using simple feature generation, as also confirmed when using the elaborate feature generation in Table 5. Simple Transfer Fine-Tuning achieved higher on STS-B, RTE, and QNLI tasks than models trained either with only sentential (+sentence) or phrasal paraphrase (+3way-PP [Simple Feature]) classification.</p><p>• Simple feature generation improves the performance of the model trained with only phrasal paraphrase classification; +3way-PP</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Phrasal paraphrases are obtained from (Arase and Tsujii, 2017); arrows indicate phrase alignments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 4. 1</head><label>1</label><figDesc>Paraphrasal Relation InjectionInput: Paraphrase sentence pairs P = { s, t } , a pre-trained BERT model 1: Obtain a set of phrase alignments A as pairs of spans for each s, t ∈ P 2: WordPiece tokenization of P 3: Accommodate phrase spans in A to BERT's token indexing: A = { (j, k), (m, n) } 4: repeat 5: for all mini-batch b t ∈ { P i , A i } do</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our method injects semantic relations to sentence representations through paraphrase discrimination.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>𝒉 𝒔 , 𝒉 𝒕 , |𝒉 𝒔 − 𝒉 𝒕 , |, 𝒉 𝒔 * 𝒉 𝒕</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Phrasal paraphrase classification</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>"paraphrase"</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>fully-connected layer</cell><cell></cell></row><row><cell cols="4">Sentential paraphrase classification</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">"paraphrase"</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">fully-connected layer</cell><cell>𝒉 𝒔</cell><cell cols="3">max-pooling</cell><cell>max-pooling</cell><cell>𝒉 𝒕</cell></row><row><cell></cell><cell></cell><cell></cell><cell>𝑥</cell><cell>𝑥</cell><cell></cell><cell></cell><cell></cell><cell>𝑥</cell><cell>𝑥</cell><cell>𝑥</cell><cell>𝑥</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BERT</cell><cell></cell></row><row><cell>[CLS]</cell><cell>Dreams</cell><cell>will</cell><cell cols="3">come true very soon</cell><cell>[SEP]</cell><cell cols="2">Wishes will be fulfilled in</cell><cell>the</cell><cell>near future</cell><cell>[SEP]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Phrase alignment</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>tokens and labels.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>All the datasets were downloaded from the Linguistic Data Consortium Numbers of sentential and phrasal paraphrases after the phrase alignment process.</figDesc><table><row><cell>Source</cell><cell cols="2">Sentence Phrase</cell></row><row><cell>NIST OpenMT</cell><cell>47k</cell><cell>711k</cell></row><row><cell>Simple Wikipedia</cell><cell>97k</cell><cell>1.4M</cell></row><row><cell>Twitter URL corpus</cell><cell>50k</cell><cell>396k</cell></row><row><cell>Para-NMT</cell><cell>3.9M</cell><cell>26.7M</cell></row><row><cell>Total</cell><cell>4.1M</cell><cell>29.2M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>GLUE tasks and evaluation metrics.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>is a crowd-sourced corpus and covers heterogeneous domains. MNLI-m is an in-domain GLUE test results scored by the GLUE evaluation server. The best scores are represented in bold and scores higher than those of BERT-base are underlined.</figDesc><table><row><cell>Task</cell><cell cols="3">Semantic Equivalence</cell><cell>NLI</cell><cell></cell><cell></cell><cell cols="2">Single-Sent.</cell></row><row><cell>Model</cell><cell cols="8">MRPC STS-B QQP MNLI (m/mm) RTE QNLI SST CoLA</cell></row><row><cell>BERT-base</cell><cell>88.3</cell><cell>84.7</cell><cell>71.2</cell><cell>84.3/83.0</cell><cell>59.8</cell><cell>89.1</cell><cell>93.3</cell><cell>52.7</cell></row><row><cell>BERT-large</cell><cell>88.6</cell><cell>86.0</cell><cell>72.1</cell><cell>86.2/85.5</cell><cell cols="4">65.5 92.7 94.1 55.7</cell></row><row><cell>Transfer Fine-Tuning</cell><cell>89.2</cell><cell>87.4</cell><cell>71.2</cell><cell>83.9/83.1</cell><cell>64.8</cell><cell>89.3</cell><cell>93.1</cell><cell>47.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Development set scores of the BERT-base model and our model (and their differences) that were fine-tuned using subsamples and full-size training sets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Our model achieved its best scores Results of the ablation study where the best scores are represented in bold and scores higher than those of BERT-base are underlined. The last three rows show performances when conducting only sentential paraphrase classification (+sentence), phrasal paraphrase classification (+3way-PP), and binary classification of phrasal paraphrase (+binary-PP), respectively.</figDesc><table><row><cell>Task</cell><cell cols="3">Semantic Equivalence</cell><cell>NLI</cell><cell></cell><cell></cell><cell cols="2">Single-Sent.</cell></row><row><cell>Model</cell><cell cols="8">MRPC STS-B QQP MNLI (m/mm) RTE QNLI SST CoLA</cell></row><row><cell>Transfer Fine-Tuning</cell><cell>89.2</cell><cell>87.4</cell><cell>71.2</cell><cell>83.9/83.1</cell><cell>64.8</cell><cell>89.3</cell><cell>93.1</cell><cell>47.2</cell></row><row><cell>BERT-base</cell><cell>88.3</cell><cell>84.7</cell><cell>71.2</cell><cell>84.3/83.0</cell><cell>59.8</cell><cell cols="3">89.1 93.3 52.7</cell></row><row><cell>+sentence</cell><cell>88.2</cell><cell>87.6</cell><cell>71.1</cell><cell>83.2/82.8</cell><cell cols="3">66.2 90.2 92.4</cell><cell>39.8</cell></row><row><cell>+3way-PP</cell><cell>88.2</cell><cell>85.8</cell><cell>70.9</cell><cell>82.9/81.9</cell><cell>65.8</cell><cell>88.0</cell><cell>91.3</cell><cell>32.6</cell></row><row><cell>+binary-PP</cell><cell>87.7</cell><cell>82.8</cell><cell>70.7</cell><cell>83.7/82.2</cell><cell>61.2</cell><cell>87.6</cell><cell>92.5</cell><cell>42.1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Our follow-up study confirms that a simpler feature generation improves the generality of our model to contribute not only to semantic equivalent assessment but also natural language inference. For details, please refer to the Appendix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The numbers of sentential paraphrase pairs were reduced due to parsing and alignment failures.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://gluebenchmark.com/faq 11 https://data.quora.com/ First-Quora-Dataset-Release-Question-Pairs</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We appreciate the anonymous reviewers for their insightful comments and suggestions to improve the paper. This work was supported by JST, ACT-I, Grant Number JPMJPR16U2, Japan.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Monolingual phrase alignment on parse forests</title>
		<author>
			<persName><forename type="first">Yuki</forename><surname>Arase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun'ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S17-2001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation (SemEval)</title>
				<meeting>the International Workshop on Semantic Evaluation (SemEval)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enhanced lstm for natural language inference</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1152</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1070</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
				<meeting>the Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources</title>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING)</title>
				<meeting>the International Conference on Computational Linguistics (COLING)<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="350" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding back-translation at scale</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pairwise word interaction modeling with deep neural networks for semantic similarity measurement</title>
		<author>
			<persName><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1108</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="937" to="948" />
			<pubPlace>San Diego, California</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving text simplification language modeling using unsimplified text data</title>
		<author>
			<persName><forename type="first">David</forename><surname>Kauchak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1537" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Neural Information Processing Systems (NeurIPS)</title>
				<meeting>Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A continuously growing dataset of sentential paraphrases</title>
		<author>
			<persName><forename type="first">Wuwei</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1126</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1224" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<title level="m">Multi-task deep neural networks for natural language understanding. arXiv</title>
				<imprint>
			<date type="published" when="1901">2019. 1901.11504</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An efficient framework for learning sentence representations</title>
		<author>
			<persName><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
				<meeting>the Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bowman</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Févry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename></persName>
		</author>
		<idno>1811.01088</idno>
	</analytic>
	<monogr>
		<title level="m">Sentence encoders on STILTs: Supplementary training on intermediate labeled-data tasks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Style transfer through back-translation</title>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Shrimai Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="866" to="876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>OpenAI</pubPlace>
		</imprint>
	</monogr>
	<note>Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>OpenAI</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning general purpose distributed sentence representations via large scale multi-task learning</title>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
				<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Syntactic scaffolds for semantic structures</title>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3772" to="3782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCLNLP</title>
				<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Neural Information Processing (NeurIPS)</title>
				<meeting>Conference on Neural Information Processing (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<title level="m">Neural network acceptability judgments. arXiv</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ParaNMT-50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations</title>
		<author>
			<persName><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1101</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1112" to="1122" />
			<pubPlace>New Orleans, Louisiana</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Phrase-level self-attention networks for universal sentence encoding</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3729" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno>1810.04805</idno>
	</analytic>
	<monogr>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
				<editor>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Macduff</forename><surname>Hughes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</editor>
		<meeting><address><addrLine>Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<title level="m">XLNet: Generalized autoregressive pretraining for language understanding. arXiv</title>
				<imprint>
			<date type="published" when="1906">2019. 1906</date>
			<biblScope unit="page">8237</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
				<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
