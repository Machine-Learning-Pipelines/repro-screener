<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Global Optimality and Finite Sample Analysis of Softmax Off-Policy Actor Critic under State Distribution Mismatch</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-24">24 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shangtong</forename><surname>Zhang</surname></persName>
							<email>shangtong@virginia.edu</email>
						</author>
						<author>
							<persName><forename type="first">Romain</forename><surname>Laroche</surname></persName>
							<email>romain.laroche@microsoft.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<addrLine>85 Engineer&apos;s Way</addrLine>
									<postCode>22903</postCode>
									<settlement>Charlottesville</settlement>
									<region>VA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Montreal</orgName>
								<address>
									<addrLine>6795 Rue Marconi, Suite 400</addrLine>
									<postCode>H2S 3J9</postCode>
									<settlement>Montreal</settlement>
									<region>Quebec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Montreal</orgName>
								<address>
									<addrLine>6795 Rue Marconi, Suite 400</addrLine>
									<postCode>H2S 3J9</postCode>
									<settlement>Montreal</settlement>
									<region>Quebec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Global Optimality and Finite Sample Analysis of Softmax Off-Policy Actor Critic under State Distribution Mismatch</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-24">24 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">ED249DB7A7BF80C2A19C163F23E3E59D</idno>
					<idno type="arXiv">arXiv:2111.02997v3[cs.LG]</idno>
					<note type="submission">Submitted 11/21; Revised 10/22;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-10-27T19:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>off-policy learning</term>
					<term>actor-critic</term>
					<term>policy gradient</term>
					<term>density ratio</term>
					<term>distribution mismatch</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we establish the global optimality and convergence rate of an off-policy actor critic algorithm in the tabular setting without using density ratio to correct the discrepancy between the state distribution of the behavior policy and that of the target policy. Our work goes beyond existing works on the optimality of policy gradient methods in that existing works use the exact policy gradient for updating the policy parameters while we use an approximate and stochastic update step. Our update step is not a gradient update because we do not use a density ratio to correct the state distribution, which aligns well with what practitioners do. Our update is approximate because we use a learned critic instead of the true value function. Our update is stochastic because at each step the update is done for only the current state action pair. Moreover, we remove several restrictive assumptions from existing works in our analysis. Central to our work is the finite sample analysis of a generic stochastic approximation algorithm with time-inhomogeneous update operators on time-inhomogeneous Markov chains, based on its uniform contraction properties.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Policy gradient methods <ref type="bibr" target="#b53">(Williams, 1992)</ref>, as well as their actor-critic extensions <ref type="bibr" target="#b45">(Sutton et al., 1999;</ref><ref type="bibr" target="#b23">Konda and Tsitsiklis, 1999)</ref>, are an important class of Reinforcement Learning (RL, Sutton and Barto 2018) algorithms and have enjoyed great empirical success <ref type="bibr">(Silver et al., 2016;</ref><ref type="bibr" target="#b35">Mnih et al., 2016;</ref><ref type="bibr" target="#b48">Vinyals et al., 2019)</ref>, which motivates the importance of the theoretical analysis of policy gradient methods. Policy gradient and actor-critic methods are essentially stochastic gradient ascent algorithms and, therefore, expected to converge to stationary points under mild conditions in on-policy settings, where an agent selects actions according to its current policy <ref type="bibr" target="#b45">(Sutton et al., 1999;</ref><ref type="bibr" target="#b23">Konda and Tsitsiklis, 1999;</ref><ref type="bibr" target="#b24">Kumar et al., 2019;</ref><ref type="bibr" target="#b59">Zhang et al., 2020a;</ref><ref type="bibr" target="#b55">Wu et al., 2020;</ref><ref type="bibr">Xu et al., 2020;</ref><ref type="bibr" target="#b39">Qiu et al., 2021)</ref>. Off-policy learning is a paradigm where an agent learns a policy of interest, referred to as the target policy, but selects actions according to a different policy, referred to as the behavior policy. Compared with on-policy learning, off-policy learning exhibits improved sample efficiency <ref type="bibr" target="#b29">(Lin, 1992;</ref><ref type="bibr" target="#b47">Sutton et al., 2011)</ref> and safety <ref type="bibr" target="#b13">(Dulac-Arnold et al., 2019)</ref>. In off-policy settings, the density ratio, i.e. the ratio between the state distribution of the target policy and that of the behavior policy <ref type="bibr" target="#b17">(Hallak and Mannor, 2017;</ref><ref type="bibr" target="#b15">Gelada and Bellemare, 2019;</ref><ref type="bibr">Liu et al., 2018;</ref><ref type="bibr" target="#b37">Nachum et al., 2019;</ref><ref type="bibr" target="#b60">Zhang et al., 2020b)</ref>, can be used to correct the state distribution mismatch between the behavior policy and the target policy. Consequently, convergence to stationary points of actor-critic methods in off-policy settings with density ratio has also been established <ref type="bibr" target="#b32">(Liu et al., 2019;</ref><ref type="bibr" target="#b61">Zhang et al., 2020c;</ref><ref type="bibr" target="#b18">Huang and Jiang, 2021;</ref><ref type="bibr" target="#b57">Xu et al., 2021)</ref>.</p><p>The seminal work of <ref type="bibr" target="#b0">Agarwal et al. (2020)</ref> goes beyond stationary points by establishing the global optimality of policy gradient methods in the tabular setting. <ref type="bibr" target="#b34">Mei et al. (2020)</ref> further provide some missing convergence rates. Both, however, use the exact policy gradient instead of an approximate and stochastic gradient, i.e., they assume the value function and the state distribution of the current policy are known and query the value function for all states at every iteration. Despite the aforementioned limitation, <ref type="bibr" target="#b0">Agarwal et al. (2020)</ref> still lay the first step towards understanding the global optimality of policy gradient methods. Their success has also been extended to the off-policy setting by <ref type="bibr" target="#b26">Laroche and Tachet (2021)</ref>, who, importantly, consider off-policy actor critic methods without correcting the state distribution mismatch. Consequently, the update step they perform is not a gradient. This aligns better with RL practices: to achieve good performance, practitioners usually do not correct the state distribution mismatch with density ratios for large scale RL experiments <ref type="bibr" target="#b51">(Wang et al., 2017;</ref><ref type="bibr" target="#b14">Espeholt et al., 2018;</ref><ref type="bibr" target="#b48">Vinyals et al., 2019;</ref><ref type="bibr" target="#b41">Schmitt et al., 2020;</ref><ref type="bibr" target="#b58">Zahavy et al., 2020)</ref>. Still, <ref type="bibr" target="#b26">Laroche and Tachet (2021)</ref> use exact and expected update steps, instead of approximate and stochastic update steps.</p><p>In this work, we go beyond <ref type="bibr" target="#b0">Agarwal et al. (2020)</ref>; <ref type="bibr" target="#b26">Laroche and Tachet (2021)</ref> by establishing the global optimality and convergence rate of an off-policy actor critic algorithm with approximate and stochastic update steps. Similarly, we study the off-policy actor critic algorithm in the tabular setting with softmax parameterization of the policy. Like <ref type="bibr" target="#b26">Laroche and Tachet (2021)</ref>, we do not use the density ratio to correct the state distribution mismatch. We, however, use a learned value function (i.e., approximate updates) and perform stochastic updates for both the actor and the critic. Further, we use the KL divergence between a uniformly random policy and the current policy as a regularization with a decaying weight for the actor update. Our off-policy actor critic algorithm, therefore, runs in three timescales: the critic is updated in the fastest timescale; the actor runs in the middle timescale; the weight of regularization decays in the slowest timescale. Besides the advances of using approximate and stochastic update steps,we also remove two restrictive assumptions. The first assumption requires that the initial distribution of the Markov Decision Process (MDP) covers the whole state space, which is crucial to get the desired optimality in <ref type="bibr" target="#b0">Agarwal et al. (2020)</ref>. The second assumption requires that the optimal policy of the MDP is unique, which is crucial to get the nonasymptotic convergence rate of <ref type="bibr" target="#b26">Laroche and Tachet (2021)</ref> for the softmax parameterization. Thanks to the off-policy learning and the decaying KL divergence regularization, we are able to remove those two assumptions in our analysis.</p><p>One important ingredient of our convergence results is the finite sample analysis of a generic stochastic approximation algorithm with time-inhomogeneous update operators on time-inhomogeneous Markov chains (Section 3). Similar to <ref type="bibr">Chen et al. (2021)</ref>, we rely on the use of the generalized Moreau envelope to form a Lyapunov function. Our results, however, extend those of <ref type="bibr">Chen et al. (2021)</ref> from time-homogeneous to time-inhomogeneous Markov chains and from time-homogeneous to time-inhomogeneous update operators. Those extensions make our results immediately applicable to the off-policy actor-critic settings (Section 4) and are made possible by establishing a form of uniform contraction of the timeinhomogeneous update operators. Moreover, we demonstrate that our analysis can also be used for analyzing the soft actor-critic (a.k.a. maximum entropy RL, <ref type="bibr" target="#b36">Nachum et al. 2017;</ref><ref type="bibr" target="#b16">Haarnoja et al. 2018</ref>) under state distribution mismatch (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>In this paper, calligraphic letters denote sets and we use vectors and functions interchangeably when it does not confuse, e.g., let f : S → R be a function; we also use f to denote the vector in R |S| whose s-th element is f (s). All vectors are column. We use • to denote the standard 2 norm and x, y . = x y for the inner product in Euclidean spaces. • p is the standard p norm. For any norm • m , • * m denotes its dual norm. We consider an infinite horizon MDP with a finite state space S, a finite action space A, a reward function r : S × A → [−r max , r max ] for some positive scalar r max , a transition kernel p : S × S × A → [0, 1], a discount factor γ ∈ [0, 1), and an initial distribution p 0 : S → [0, 1]. At time step 0, an initial state S 0 is sampled according to p 0 . At time step t, an agent in state S t takes an action A t ∼ π(•|S t ) according to a policy π : A × S → [0, 1], gets a reward R t+1 . = r(S t , A t ), and proceeds to a successor state S t+1 ∼ p(•|S t , A t ). The return at time step t is the random variable</p><formula xml:id="formula_0">G t . = ∞ i=0 γ i R t+i+1 ,</formula><p>which allows us to define state-and action-value functions v π and q π as</p><formula xml:id="formula_1">v π (s) . = E[G t |S t = s, π, p], q π (s, a) . = E[G t |S t = s, A t = a, π, p].</formula><p>The performance of the policy π is measured by the expected discounted sum of rewards J(π; p 0 ) . = s p 0 (s)v π (s).</p><p>Prediction and control are two fundamental tasks of RL.</p><p>The goal of prediction is to estimate the values v π or q π . Take estimating q π as an example. Let q t ∈ R |S×A| be our estimate for q π at time t. SARSA <ref type="bibr" target="#b40">(Rummery and Niranjan, 1994)</ref> updates {q t } iteratively as δ t . = R t+1 + γq t (S t+1 , A t+1 ) − q t (S t , A t ), q t+1 (s, a) . = q t (s, a) + α t δ t , (s, a) = (S t , A t ) q t (s, a), (s, a) = (S t , A t ) ,</p><p>where δ t is called the temporal difference error <ref type="bibr" target="#b43">(Sutton, 1988)</ref> and {α t } is a sequence of learning rates. It is proved by <ref type="bibr" target="#b4">Bertsekas and Tsitsiklis (1996)</ref> that, under mild conditions, {q t } converges to q π almost surely. So far we have considered on-policy learning, where the policy of interest is the same as the policy used in action selection. In the off-policy learning setting, the goal is still to estimate q π . Action selection is, however, done using a different policy µ (i.e., A t ∼ µ(•|S t )). We refer to π and µ as the target and behavior policy respectively. Off-policy expected SARSA <ref type="bibr" target="#b1">(Asis et al., 2018)</ref> updates {q t } iteratively as</p><formula xml:id="formula_3">δ t .</formula><p>=R t+1 + γ a π(a |S t+1 )q t (S t+1 , a ) − q t (S t , A t ), q t+1 (s, a) . = q t (s, a) + α t δ t , (s, a) = (S t , A t ) q t (s, a), (s, a) = (S t , A t ) ,</p><p>where the target policy π, instead of the behavior policy µ, is used to compute the temporal difference error.</p><p>The goal of control is to find a policy π * such that ∀π, s v π (s) ≤ v π * (s).</p><p>(2)</p><p>One common approach for control is policy gradient. In this paper, we consider a softmax parameterization for the policy π. Letting θ ∈ R |S×A| be the parameters of the policy, We represent it as π(a|s) . = exp(θ s,a ) a exp θ s,a , where θ s,a is the (s, a)-indexed element of θ. Policy gradient methods then update θ iteratively as</p><formula xml:id="formula_4">θ t+1 . = θ t + β t ∇ θ J(π θt ; p 0 ).<label>(3)</label></formula><p>Here {β t } is a sequence of learning rates and π θ emphasizes the dependence of the policy π on its parameter θ. In the rest of the paper, we omit the θ in ∇ θ for simplicity. <ref type="bibr" target="#b0">Agarwal et al. (2020)</ref>; <ref type="bibr" target="#b34">Mei et al. (2020)</ref> prove that when p 0 (s) &gt; 0 holds for all s and {β t } is set properly, the iterates {θ t } generated by (3) satisfy lim t→∞ J(θ t ; p 0 ) = J(π * ; p 0 ), confirming the optimality of policy gradient methods in the tabular setting with exact gradients. <ref type="bibr" target="#b34">Mei et al. (2020)</ref> also establish a convergence rate for the softmax parameterization.</p><p>In practice, we, however, usually do not have access to ∇J(π θt ; p 0 ). Fortunately, the policy gradient theorem <ref type="bibr" target="#b45">(Sutton et al., 1999)</ref> asserts that ∇J(π θ ; p 0 ) . = 1 1 − γ s d π θ ,γ,p 0 (s) a q π θ (s, a)∇π θ (a|s),</p><p>where</p><formula xml:id="formula_5">d π,γ,p 0 . = (1 − γ) ∞ t=0 γ t Pr(S t = s|p 0 , π)</formula><p>is the normalized discounted state occupancy measure. Hence instead of using the gradient update (3), practitioners usually consider the following approximate and stochastic gradient update for the on-policy setting:</p><formula xml:id="formula_6">θ t+1 . = θ t + β t γ t q t (S t , A t )∇ log π θt (A t |S t ),<label>(4)</label></formula><p>where q t is updated according to (1). We refer to ( <ref type="formula" target="#formula_6">4</ref>) and (1) as on-policy actor critic, where the actor refers to π θ and the critic refers to q. Usually α t is much larger than β t , i.e., the critic is updated much faster than the actor and the actor is, therefore, quasi-stationary from the perspective of the critic. Consequently, in the limit, we can expect q t to converge to q π θ t , after which γ t q t (S t , A t )∇ log π θt (A t |S t ) becomes an unbiased estimator of ∇J(π θt ; p 0 ) and the actor update becomes the standard stochastic gradient ascent.</p><p>In the off-policy setting, at time step t, the action selection is done according to some behavior policy µ θt . Here µ θ does not need to have the same parameterization as π θ , e.g., µ θ can be a softmax policy with a different temperature, a mixture of a uniformly random policy and a softmax policy, or a constant policy µ. To account for the difference between π θ and µ θ , one must reweight the actor update (4) as</p><formula xml:id="formula_7">θ t+1 . = θ t + β t t ρ t q t (S t , A t )∇ log π θt (A t |S t ),<label>(5)</label></formula><p>where</p><formula xml:id="formula_8">ρ t . = π θt (A t |S t ) µ θt (A t |S t )</formula><p>is the importance sampling ratio to correct the discrepancy in action selection and</p><formula xml:id="formula_9">t . = d π θ t ,γ,p 0 (s) d t (s) with d t (s) . = Pr(S t = s|µ θ 0 , . . . , µ θt )</formula><p>is the density ratio to correct the discrepancy in state distribution. Thanks to ρ t and t , in the limit, ( <ref type="formula" target="#formula_7">5</ref>) is still a stochastic gradient ascent algorithm following the gradient ∇J(π θt ; p 0 ) if q t converges to q π θ t . Theoretical analysis of variants of (5) includes <ref type="bibr" target="#b32">Liu et al. (2019)</ref>; <ref type="bibr" target="#b61">Zhang et al. (2020c)</ref>; <ref type="bibr" target="#b18">Huang and Jiang (2021)</ref>; <ref type="bibr" target="#b57">Xu et al. (2021)</ref>. Practitioners, however, usually use only ρ t but completely ignore t , yielding variants of</p><formula xml:id="formula_10">θ t+1 . = θ t + β t ρ t q t (S t , A t )∇ log π θt (A t |S t ).<label>(6)</label></formula><p>Clearly, (6) can no longer be regarded as a stochastic gradient ascent algorithm even if q t converges to q π θ t because of the missing term t used to correct the state distribution. Still, variants of (6) enjoy great empirical success <ref type="bibr" target="#b51">(Wang et al., 2017;</ref><ref type="bibr" target="#b14">Espeholt et al., 2018;</ref><ref type="bibr" target="#b48">Vinyals et al., 2019;</ref><ref type="bibr" target="#b41">Schmitt et al., 2020;</ref><ref type="bibr" target="#b58">Zahavy et al., 2020)</ref>. To understand the behavior of ( <ref type="formula" target="#formula_10">6</ref>), <ref type="bibr" target="#b26">Laroche and Tachet (2021)</ref> study the following update rule:</p><formula xml:id="formula_11">θ t+1 . = θ t + β t s d t (s)</formula><p>a q π θ t (s, a)∇π θt (a|s). ( <ref type="formula">7</ref>)</p><p>Different from ( <ref type="formula" target="#formula_10">6</ref>), where the update step is approximate and stochastic, the update in ( <ref type="formula">7</ref>) is exact and expected. <ref type="bibr" target="#b26">Laroche and Tachet (2021)</ref> prove that under mild conditions, the iterates {θ t } generated by ( <ref type="formula">7</ref>) satisfy lim t→∞ J(π θt ; p 0 ) = J(π * ; p 0 ).</p><p>If we further assume the optimal policy π * is unique and inf s,t d t (s) &gt; 0, a nonasymptotic convergence rate of (7) is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Stochastic Approximation with Time-Inhomogeneous Operators on Time-Inhomogeneous Markov Chains</head><p>In this section, we provide finite sample analysis of a generic stochastic approximation algorithm with time-inhomogeneous update operators on time-inhomogeneous Markov chains.</p><p>The results presented in this section are used in the analysis of critics in the rest of this work and may be of independent interest. To motivate this part, consider using off-policy expected SARSA to update the critic in off-policy actor critic. We have</p><formula xml:id="formula_12">δ t . =R t+1 + γ a π θt (a |S t+1 )q t (S t+1 , a ) − q t (S t , A t ), q t+1 (s, a) . = q t (s, a) + α t δ t , (s, a) = (S t , A t ) q t (s, a), (s, a) = (S t , A t ) .</formula><p>Equivalently, we can rewrite the above update in a more compact form as</p><formula xml:id="formula_13">q t+1 = q t + α t (F θt (q t , S t , A t , S t+1 ) − q t ) ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_14">F θ (q, s 0 , a 0 , s 1 )[s, a] . =I (s 0 ,a 0 )=(s,a) δ θ (q, s 0 , a 0 , s 1 ) + q(s, a), δ θ (q, s 0 , a 0 , s 1 ) . =r(s 0 , a 0 ) + γ a 1 π θ (a 1 |s 1 )q(s 1 , a 1 ) − q(s 0 , a 0 ).</formula><p>Here, I statement (•) is the indicator function whose value is 1 if the statement is true, and 0 otherwise. The update (8) motivates us to study a generic stochastic approximation algorithm in the form of</p><formula xml:id="formula_15">w t+1 . = w t + α t (F θt (w t , Y t ) − w t + t ). (<label>9</label></formula><formula xml:id="formula_16">)</formula><p>Here w t ∈ R K are the iterates generated by the stochastic approximation algorithm, {Y t } is a sequence of random variables evolving in a finite space Y, θ t ∈ R L is another sequence of random variables controlling the transition of {Y t }, F θ is a function from R K × Y to R K parameterized by θ, and t ∈ R K is a sequence of random noise. The analysis of critics in this paper only requires t ≡ 0. Nevertheless, we consider a generic noise process { t } for generality.</p><p>The results in this section extend Theorem 2.1 of <ref type="bibr">Chen et al. (2021)</ref> in two aspects. First, the operator F θ changes every time step due to the change of θ, while Chen et al. ( <ref type="formula">2021</ref>) consider a fixed operator F . Second, the random process {Y t } evolves according to time-varying dynamics controlled by {θ t }, while <ref type="bibr">Chen et al. (2021)</ref> assume {Y t } is a Markov chain with fixed dynamics. The introduction of {θ t } makes our results immediately applicable to the analysis of actor-critic algorithms. We now state our assumptions. It is worth reiterating that all the {θ t } below refers to the random sequence used in the update (9). Assumption 3.1 (Time-inhomogeneous Markov chain) There exists a family of parameterized transition matrices</p><formula xml:id="formula_17">Λ P . = P θ ∈ R |Y|×|Y| |θ ∈ R L such that Pr(Y t+1 = y) = P θ t+1 (Y t , y).</formula><p>Assumption 3.2 (Uniform ergodicity) Let ΛP be the closure of Λ P . For any P ∈ ΛP , the chain induced by P is ergodic. We use d θ to denote the invariant distribution of the chain induced by P θ .</p><p>Assumption 3.1 prescribes that the random process {Y t } is a time-inhomogeneous Markov chain. It is worth mentioning that Assumption 3.1 does not prescribe how the transition matrices depend on {θ t }. It does not restrict {θ t } to be deterministic either. An exemplary parameterization we use in the context of off-policy actor critic will be shown later in (13). Assumption 3.2 prescribes the ergodicity of the Markov chains we consider and was also previously used in the analysis of RL algorithms both in the on-policy <ref type="bibr" target="#b33">(Marbach and Tsitsiklis, 2001)</ref> and off-policy settings <ref type="bibr" target="#b62">(Zhang et al., 2021)</ref>. We will show later that Assumption 3.2 is easy to fulfill in our off-policy actor critic setting. Assumption 3.2 implicitly claims that all the matrices in ΛP are stochastic matrices. This is indeed trivial to prove. Pick any P ∞ ∈ ΛP . Since ΛP is the closure of Λ P , there must exist a sequence {P n } such that P n ∈ Λ P and lim n→∞ P n = P ∞ . It is then easy to see that P ∞ (y, y ) ∈ [0, 1] and</p><formula xml:id="formula_18">y P ∞ (y, y ) = y lim n→∞ P n (y, y ) = lim n→∞ y P n (y, y ) = 1.</formula><p>In other words, P ∞ is a stochastic matrix. One important consequence of Assumption 3.2 is uniform mixing.</p><p>Lemma 1 (Uniform ergodicity implies uniform mixing) Let Assumption 3.2 hold. Then, there exist constants C 0 &gt; 0 and τ ∈ (0, 1), independent of θ, such that for any n &gt; 0, sup y,θ y</p><formula xml:id="formula_19">P n θ (y, y ) − d θ (y ) ≤ C 0 τ n . (<label>10</label></formula><formula xml:id="formula_20">)</formula><p>The proof of Lemma 1 is provided in Section A.1. The result in Lemma 1 is referred to as uniform mixing since it demonstrates that for any θ, the chain induced by P θ mixes geometrically fast, with a common rate τ . For a specific θ, the existence of a θ-dependent mixing rate τ θ is a well-known result when the chain is ergodic, see, e.g., Theorem 4.9 of <ref type="bibr" target="#b27">Levin and Peres (2017)</ref>. In Lemma 1, we further conclude to the existence of a θ-independent rate. The ergodicity on the closure ΛP is key to our proof. If we make ergodicity assumption only on Λ P , it might be possible to find a sequence {θ t } such that the corresponding rates {τ θt } converges to 1. We remark that (10) usually appears as a technical assumption directly in many existing works concerning time-inhomogeneous Markov chains, see, e.g., <ref type="bibr" target="#b63">Zou et al. (2019)</ref>; <ref type="bibr" target="#b55">Wu et al. (2020)</ref>. In this paper, we prove that ( <ref type="formula" target="#formula_19">10</ref>) is a consequence of Assumption 3.2, with the help of the extreme value theorem exploiting the compactness of ΛP . We will show in the next section that Assumption 3.2 can easily be fulfilled.</p><formula xml:id="formula_21">Assumption 3.3 (Uniform contraction) For any θ ∈ R L , define F θ : R K → R K as Fθ (w) . = y∈Y d θ (y)F θ (w, y).</formula><p>Then, there exists a constant κ ∈ (0, 1) and a norm • c such that for all θ, w, w ,</p><formula xml:id="formula_22">Fθ (w) − Fθ (w ) c ≤ κ w − w c .</formula><p>We use w * θ to denote the unique fixed point of Fθ .</p><p>The existence and uniqueness of w * θ follows from the Banach fixed point theorem. Assumption 3.3 is another major development beyond <ref type="bibr">Chen et al. (2021)</ref>. The fact that both • c and κ are independent of θ makes it possible to design a Lyapunov function for our timeinhomogeneous Markov chain. We will show later that our critic updates indeed satisfy this uniform contraction assumption.</p><p>Assumption 3.4 (Continuity and boundedness) There exist positive constants L F , L F , L F , U F , U F , U F , L w , U w , L P such that for any w, w , y, y and any time step t, k, almost surely,</p><formula xml:id="formula_23">(i). F θt (w, y) − F θt (w , y) c ≤ L F w − w c (ii). F θt (w, y) − F θ k (w, y) c ≤ L F θ t − θ k c ( w c + U F ) (iii). F θt (0, y) c ≤ U F (iv). Fθt (w) − Fθ k (w) c ≤ L F θ t − θ k c ( w c + U F ) (v). w * θt − w * θ k c ≤ L w θ t − θ k c (vi). sup t w * θt c ≤ U w (vii). |P θt (y, y ) − P θ k (y, y )| ≤ L P θ t − θ k c</formula><p>Assumption 3.5 (Noise) Let F t be the σ-algebra generated by Assumption 3.6 (Two timescales) The learning rate {α t } has the form</p><formula xml:id="formula_24">{(w i , Y i , i , θ i )} 0≤i≤t−1 ∪ {w t , θ t }, we have (i). E [ t | F t ] = 0, ∀t<label>(</label></formula><formula xml:id="formula_25">α t . = α (t + t 0 ) α ,</formula><p>where α ∈ (0.5, 1), α &gt; 0, t 0 &gt; 0 are constants to be tuned. Define another sequence {β t } such that</p><formula xml:id="formula_26">β t . = β (t + t 0 ) β ,</formula><p>where β ∈ ( α , 1], β ∈ (0, α) are constants to be tuned. Then there exists a constant L θ &gt; 0 such that ∀t, almost surely,</p><formula xml:id="formula_27">θ t+1 − θ t c ≤ β t L θ .<label>(11)</label></formula><p>Assumption 3.6 ensures that the iterates {w t } evolve sufficiently faster than the change in the dynamics of the chain (i.e., the change of {θ t }). In the off-policy actor critic setting we consider in next section, {α t } and {β t } are the learning rates for the critic and the actor respectively. Though Assumption 3.6 explicitly prescribes the form of the sequences {α t } and {β t }, those are indeed only one of many possible forms (one could e.g., use different t 0 for {α t } and {β t }), we consider these particular forms to ease presentation. We remark that condition in ( <ref type="formula" target="#formula_27">11</ref>) is also used in <ref type="bibr" target="#b22">Konda (2002)</ref>, which gives the asymptotic convergence analysis of the canonical on-policy actor critic with linear function approximation. We are now ready to state our main results.</p><p>Theorem 2 Let Assumptions 3.1 -3.6 hold. For any</p><formula xml:id="formula_28">w ∈ (0, min {2( β − α ), α }), if t 0 is sufficiently large, then ∀t, E w t − w * θt 2 c = O 1 (t + t 0 ) w .</formula><p>See Section A.2 for the proof of Theorem 2 and the constants hidden by O(•). In particular, we clearly document t 0 's dependencies. One could alternatively set t 0 to 0, then the convergence rate in Theorem 2 applies only for sufficiently large t. When both the Markov chain and the update operator are time-homogeneous, <ref type="bibr">Chen et al. (2021)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Off-Policy Actor Critic with Decaying KL Regularization</head><p>We analyze the optimality of an off-policy actor critic algorithm without correction of the state distribution mismatch (Algorithm 1). Our analysis provides, to some extent, a theoretical justification for the practice of ignoring this correction.</p><p>Algorithm 1: Off-Policy Actor-Critic with Decaying KL Regularization</p><formula xml:id="formula_29">S 0 ∼ p 0 (•) t ← 0 while True do Sample A t ∼ µ θt (•|S t ) Execute A t , get R t+1 , S t+1 δ t ← R t+1 + γ a π θt (a |S t+1 )q t (S t+1 , a ) − q t (S t , A t ) q t+1 (s, a) ← q t (s, a) + α t δ t , (s, a) = (S t , A t ) q t (s, a), otherwise ρ t ← π θ t (At|St) µ θ t (At|St) θ t+1 ← θ t + β t ρ t ∇ θ log π θt (A t |S t )Π(q t (S t , A t )) − λ t ∇ θ KL (U A ||π θt (•|S t )) t ← t + 1 end</formula><p>In Algorithm 1, the target policy π θ is a softmax policy. At each time step t, we sample an action A t according to the behavior policy µ θt . Importantly, though the behavior policy is also solely determined by θ, the parameterization of µ θ can be arbitrarily different from π θ . After obtaining the reward R t+1 and the successor state S t+1 , we update the critic with off-policy expected SARSA, where π θt is used as the target policy for bootstrapping. We then update the actor similarly to (6) without correcting the state distribution mismatch. The update to θ t in Algorithm 1 is different from (6) in two aspects. First, we use a projection Π : R → R in case the critic becomes too large:</p><formula xml:id="formula_30">Π(x) . = x, |x| ≤ C Π C Π x |x| , otherwise , where C Π . = rmax 1−γ .</formula><p>Second, we use the KL divergence between a uniformly randomly distribution U A and the current policy π θt (•|S t ) as regularization, with a decaying weight λ t . The KL divergence is introduced to ensure that the target policy π θ is sufficiently explorative such that there are no bad stationary points (cf. Theorem 5.2 of <ref type="bibr" target="#b0">Agarwal et al. (2020)</ref>). In practice, the entropy of the policy is often used to regularize the policy update <ref type="bibr" target="#b54">(Williams and Peng, 1991;</ref><ref type="bibr" target="#b35">Mnih et al., 2016)</ref>. Here we use the KL divergence instead of the entropy mainly for technical consideration. We refer the reader to Remark 5.2 of <ref type="bibr" target="#b0">Agarwal et al. (2020)</ref> for more discussion about this choice. The decaying weight λ t is introduced to ensure that, in the limit, the target policy π θ can still converge to a deterministic policy, which is a necessary condition for optimality.</p><p>Algorithm 1 runs in three timescales. The critic runs in the fastest timescale such that it can provide accurate signal for the actor update, which runs in the middle timescale. It is then expected that the actor would converge to stationary points whose suboptimality is controlled by λ t , which decays in the slowest timescale. Finally, as λ t diminishes, the suboptimality of the actor decays to 0. To achieve this three timescale setting, we make the following assumptions.</p><p>Assumption 4.1 (Three timescales) The learning rates {α t }, {β t } and the weights of KL regularization {λ t } have the forms</p><formula xml:id="formula_31">α t . = α (t + t 0 ) α , β t . = β (t + t 0 ) β , λ t . = λ (t + t 0 ) λ , where 0.5 &lt; α &lt; β ≤ 1, λ &gt; 0, α &gt; β &gt; 0, λ &gt; 0, t 0 &gt; 0 are constants to be tuned. Assumption 4.2 (Learning rates) 2(1 − β ) &lt; min {2( β − α ), α }, 0 ≤ λ &lt; 1− β 2</formula><p>We remark that Assumptions 4.1 and 4.2 are only one of many possible forms of learning rates and we choose this particular form to ease presentation. To ensure each update to θ t does not change the dynamics of the induced Markov chain too fast, we impose the following assumption on the parameterization of µ θ .</p><p>Assumption 4.3 (Lipschitz continuity) There exists L µ &gt; 0 such that ∀θ, θ , a, s,</p><formula xml:id="formula_32">µ θ (a|s) − µ θ (a|s) ≤ L µ θ − θ .</formula><p>We remark that given the softmax parameterization of π θ , it is well-known (see, e.g., Lemma 1 of Wang and Zou 2020) that π θ is also Lipschitz continuous, i.e., there exists L π &gt; 0 such that ∀θ, θ , a, s</p><formula xml:id="formula_33">π θ (a|s) − π θ (a|s) ≤ L π θ − θ . (<label>12</label></formula><formula xml:id="formula_34">)</formula><p>To ensure sufficient exploration, we impose the following assumption on the behavior policy.</p><p>Assumption 4.4 (Uniform ergodicity) Let Λµ be the closure of µ θ | θ ∈ R |S×A| . For any µ ∈ Λµ , the chain induced by µ is ergodic and µ(a|s) &gt; 0.</p><p>Assumption 4.4 is easy to fulfill in practice. Assuming the chain induced by a uniformly random policy is ergodic, which we believe is a necessary condition for any assumption regarding ergodicity, one possible choice for µ θ is to mix an arbitrary behavior policy µ θ satisfying the Lipschitz continuous requirement with the uniformly random policy, i.e.,</p><formula xml:id="formula_35">µ θ (•|s) . = (1 − )U A + µ θ (•|s) (13)</formula><p>with any ∈ (0, 1). From now on, we use d µ ∈ R |S| to denote the invariant state distribution of the chain induced by a policy µ and also overload d µ ∈ R |S×A| to denote the invariant state action distribution under policy µ. With all assumptions stated, we are ready to present our convergence results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Convergence of the Critic</head><p>In this section, we study the convergence of the critic by invoking Theorem 2 with the update to q t in Algorithm 1 expressed as (8). Assumption 3.3 requires us to study the expected operator Fθ (q) . = s,a,s d µ θ (s)µ θ (a|s)p(s |s, a)F θ (q, s, a, s ).</p><p>Simple algebraic manipulation yields</p><formula xml:id="formula_36">Fθ (q) = D µ θ (r + γP π θ q − q) + q (14) = (I − D µ θ (I − γP π θ ))q + D µ θ r,</formula><p>where D µ θ ∈ R |S×A|×|S×A| is a diagonal matrix with D µ θ ((s, a), (s, a)) . = d µ θ (s)µ θ (a|s) and P π θ ∈ R |S×A|×|S×A| is the state-action pair transition matrix under policy π θ , i.e., P π θ ((s, a), (s , a )) . = p(s |s, a)π θ (a |s ).</p><p>We now verify Assumption 3.3 with Lemma 3.</p><p>Lemma 3 (Uniform contraction) Let Assumption 4.4 hold. Then, there exists an p norm and a constant κ ∈ (0, 1) such that for any θ, q, q ∈ R |S×A| ,</p><formula xml:id="formula_37">Fθ (q) − Fθ (q ) p ≤ κ q − q p .</formula><p>Further, q π θ is the unique fixed point of Fθ .</p><p>The proof of Lemma 3 is provided in Section B.1. Next, we are able to prove the convergence of the critic.</p><p>Proposition 4 (Convergence of the critic) Let Assumptions 4.1, 4.3, and 4.4 hold. For any</p><formula xml:id="formula_38">q ∈ (0, min {2( β − α ), α }),</formula><p>if t 0 is sufficiently large, the iterates {q t } generated by Algorithm 1 satisfy</p><formula xml:id="formula_39">E q t − q π θ t 2 p = O 1 t q .</formula><p>The proof of Proposition 4 is provided in Section B.2. Proposition 4 confirms that the critic is able to track the true value function in the limit, where the dependence between the convergence rate and the mixing parameter of the Markov chains are hidden in O (•). Similar trackability has also been established in <ref type="bibr" target="#b22">Konda (2002)</ref>; <ref type="bibr" target="#b61">Zhang et al. (2020c)</ref>; <ref type="bibr" target="#b55">Wu et al. (2020)</ref>. Those, however, rely on the uniform negative-definiteness of the limiting update matrix. <ref type="bibr" target="#b22">Konda (2002)</ref> proves that the uniform negative-definiteness holds in the on-policy actor critic with linear function approximation <ref type="bibr">(Lemma 4.18 of Konda 2002)</ref> and establishes this trackability asymptotically. <ref type="bibr" target="#b55">Wu et al. (2020)</ref> assume the uniform negativedefiniteness holds (the second half of Assumption 4.1 of <ref type="bibr" target="#b55">Wu et al. 2020)</ref> in the on-policy actor critic with linear function approximation and establish this trackability nonasymptotically. <ref type="bibr" target="#b61">Zhang et al. (2020c)</ref> achieve this uniform negative-definiteness via introducing extra ridge regularization and using full gradients (cf. Gradient TD, <ref type="bibr">Sutton et al. 2009</ref>) instead of semi-gradients (cf. TD, Sutton 1988) for the critic update in the off-policy actor critic with function approximation and achieve this trackability asymptotically. In our off-policy actor critic setting, the limiting update matrix of the critic can be computed as</p><formula xml:id="formula_40">D µ θ (γP π θ − I).</formula><p>To achieve the desired uniform negative-definiteness, we would need to prove that there exists a constant ζ &gt; 0 such that for all x, θ,</p><formula xml:id="formula_41">x D µ θ (γP π θ − I)x ≤ −ξ x 2 .</formula><p>We, however, do not expect the above inequality to hold without making strong assumptions.  <ref type="formula">2021</ref>) have an inner loop for updating the critic and an outer loop for updating the actor. For the critic to be sufficiently accurate, the inner loop has to take sufficiently many steps. <ref type="bibr" target="#b9">Chen et al. (2022)</ref>; <ref type="bibr" target="#b20">Khodadadian et al. (2021)</ref>, therefore, have a flavor of bi-level optimization. Further, as long as the steps of the inner loop is finite, the bias from using a learned critic instead of the true value function will not diminish in the limit. This bias eventually translates into a suboptimality of the policy that will not vanish in the limit. By contrast, Theorem 2 allows us to consider multi-timescales directly without incurring nested loops, which ensures that the bias from the critic diminishes in the limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Convergence of the Actor</head><p>With the critic able to track the true value function, we are now ready to present the optimality of the actor.</p><p>Theorem 5 (Optimality of the actor) Let Assumptions 4.1 -4.4 hold. Fix</p><formula xml:id="formula_42">q ∈ 2(1 − β ), min {2( β − α ), α } .</formula><p>Let t 0 be sufficiently large. For the iterates {θ t } generated by Algorithm 1 and any t &gt; 0, if k is uniformly randomly selected from the set t 2 , t 2 + 1, . . . , t where • is the ceiling function, then</p><formula xml:id="formula_43">J(π θ k ; p 0 ) ≥ J(π * ; p 0 ) − O (λ k ) (<label>15</label></formula><formula xml:id="formula_44">)</formula><p>holds with probability at least</p><formula xml:id="formula_45">1 − O 1 t 1− β −2 λ + log 2 t t β −2 λ + 1 t q −2 λ , (<label>16</label></formula><formula xml:id="formula_46">)</formula><p>where π * can be any optimal policy.</p><p>The proof of Theorem 5 is provided in Section B.3. We remark that the 1 2 in t 2 is purely ad-hoc. We can use any positive constant smaller than 1 and the new rate will be different from the current one in only the constants hidden by O(•). We now optimize the selection of α and β . Let 0 by any positive scalar sufficiently close to 0 and set</p><formula xml:id="formula_47">β = 3 4 + 0 , α = 1 2 + 0 , q = 1 2 − 0 . (<label>17</label></formula><formula xml:id="formula_48">)</formula><p>Then the high probability in ( <ref type="formula" target="#formula_45">16</ref>) becomes</p><formula xml:id="formula_49">1 − O t −( 1 4 − 0 −2 λ)</formula><p>and the suboptimality in (15) remains</p><formula xml:id="formula_50">J(π θ k ; p 0 ) ≥ J(π * ; p 0 ) − O k − λ .</formula><p>It now becomes clear that the selection of</p><formula xml:id="formula_51">λ ∈ (0, 1 8 ) (<label>18</label></formula><formula xml:id="formula_52">)</formula><p>trades off suboptimality and high probability. When λ is large, the suboptimality diminishes quickly but the high probability approaches one slowly and vice versa. To our best knowledge, Theorem 5 is the first to establish the global optimality and convergence rate of a naive off-policy actor critic algorithm without density ratio correction even in the tabular setting. We leave the improvement of the convergence rate for future work. Importantly, Theorem 5 does not make any assumption on the initial distribution p 0 . By contrast, to obtain the asymptotic optimality in <ref type="bibr" target="#b0">Agarwal et al. (2020)</ref> or to obtain the convergence rate in <ref type="bibr" target="#b34">Mei et al. (2020)</ref>, p 0 (s) &gt; 0 is assumed to hold for all states. Both <ref type="bibr" target="#b0">Agarwal et al. (2020)</ref> and <ref type="bibr" target="#b34">Mei et al. (2020)</ref> leave it an open problem whether p 0 (s) &gt; 0 is a necessary condition for optimality. Our results show that at least in the off-policy setting, this is not necessary. The intuition is simple. Let p 0 be another initial distribution such that p 0 (s) &gt; 0 holds for all states. Then we could optimize J(π θ ; p 0 ) instead of J(π θ ; p 0 ) since the optimal policy w.r.t. J(π θ ; p 0 ) must also be optimal w.r.t. J(π θ ; p 0 ). To optimize J(π θ ; p 0 ), we would need samples starting from p 0 , which is impractical in the on-policy setting since the initial distribution of the MDP is p 0 . In the off-policy setting, we can, however, use samples starting from p 0 and make corrections with the density ratio. Since our results show that density ratio correction actually does not matter in the tabular setting we consider, we can then simply ignore the density ratio, yielding Algorithm 1. <ref type="bibr" target="#b0">Agarwal et al. (2020)</ref>; <ref type="bibr" target="#b34">Mei et al. (2020)</ref> refer to the assumption p 0 (s) &gt; 0 as the sufficient exploration assumption. Unfortunately, the initial distribution p 0 is usually considered as part of the problem and thus is not controlled by the user. In our off-policy setting, we instead achieve sufficient exploration by making assumptions on the behavior policy (Assumption 4.4), which demonstrates the flexibility of off-policy learning in terms of exploration. Moreover, to obtain the nonasymptotic convergence rate of the off-policy actor critic with exact update, <ref type="bibr" target="#b26">Laroche and Tachet (2021)</ref> require the optimal policy π * to be unique. By contrast, Theorem 5 does not assume any such uniqueness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Soft Actor Critic</head><p>In this section, we study the convergence of soft actor critic in the framework of maximum entropy RL, which penalizes deterministic policies via adding the entropy of the policy into the reward <ref type="bibr" target="#b54">(Williams and Peng, 1991;</ref><ref type="bibr" target="#b35">Mnih et al., 2016;</ref><ref type="bibr" target="#b36">Nachum et al., 2017;</ref><ref type="bibr" target="#b16">Haarnoja et al., 2018)</ref>. The soft state value function of a policy π is defined as</p><formula xml:id="formula_53">ṽπ,η (s) . =E ∞ i=0 γ i r(S t+i , A t+i ) + ηH (π(•|S t+i )) | S t = s, π =v π (s) + ηE ∞ i=0 γ i H (π(•|S t+i )) | S t = s, π ,</formula><p>where</p><formula xml:id="formula_54">H (π(•|s)) . = − a π(a|s) log π(a|s)</formula><p>is the entropy and η ≥ 0 is the parameter controlling the strength of entropy regularization. Correspondingly, the soft action value function of a policy π is defined as</p><formula xml:id="formula_55">qπ,η (s, a) . = r(s, a) + γ s p(s |s, a)ṽ π,η (s ),<label>(19)</label></formula><p>which satisfies the recursive equation</p><formula xml:id="formula_56">qπ,η (s, a) = r(s, a) + γ s ,a p(s |s, a)π(a |s ) qπ,η (s, a) − η log π(a |s ) .</formula><p>The entropy regularized discounted total rewards is then</p><formula xml:id="formula_57">Jη (π; p 0 ) . = s p 0 (s)ṽ π,η (s) = J(π; p 0 ) + η 1 − γ s d π,γ,p 0 (s)H (π(•|s)) . (<label>20</label></formula><formula xml:id="formula_58">)</formula><p>We still consider the softmax parameterization for the policy π. Similar to the canonical policy gradient theorem, it can be computed <ref type="bibr" target="#b28">(Levine, 2018)</ref> that</p><formula xml:id="formula_59">∇ Jη (π θ ; p 0 ) = 1 1 − γ s d π θ ,γ,p 0 (s) a (q π θ ,η (s, a) − η log π θ (a|s)) ∇π θ (a|s).</formula><p>To get unbiased estimates of ∇ Jη (π θ ; p 0 ), one would need to sample states from d π θ ,γ,p 0 , which is, however, impractical in off-policy settings. Practitioners, instead, directly use states obtained by following the behavior policy (see, e.g., Algorithm 2), yielding a distribution mismatch.</p><p>Algorithm 2: Expected Soft Actor-Critic</p><formula xml:id="formula_60">S 0 ∼ p 0 (•) t ← 0 while True do Sample A t ∼ µ θt (•|S t ) Execute A t , get R t+1 , S t+1 δ t ← R t+1 + γ a π θt (a |S t+1 ) (q t (S t+1 , a ) − λ t log π θt (a |S t+1 )) − q t (S t , A t ) q t+1 (s, a) ← q t (s, a) + α t δ t , (s, a) = (S t , A t ) q t (s, a), otherwise θ t+1 ← θ t + β t a π θt (a|S t )∇ θ log π θt (a|S t ) Π(q t (S t , a)) − λ t log π θt (a|S t ) t ← t + 1 end</formula><p>In Algorithm 2, we still consider the learning rates specified in Assumption 4.1 and use a projection Π in the actor update with</p><formula xml:id="formula_61">C Π . = r max + λ log |A| 1 − γ ,</formula><p>which is the maximum possible soft action value given our selection of λ t since the entropy is always bounded by log |A|. We still consider Assumptions 4.3 and 4.4 for the behavior policy. Importantly, in Algorithm 2, we consider expected actor updates <ref type="bibr" target="#b10">(Ciosek and Whiteson, 2020)</ref> that update the policy for all actions instead of just the executed action A t . This is mainly for technical consideration. If we use stochastic update akin to Algorithm 1, the update to θ t in Algorithm 2 will have the term log π θt (A t |S t ). As {λ t } decreases over time, we would expect that π θt becomes more and more deterministic. Consequently, |log π θt (A t |S t )| tends to go to infinity, imposing additional challenges in verifying (11) unless we ensure {λ t } decays sufficiently fast (e.g., using λ &gt; 1 − β ) such that |λ t log π θt (A t |S t )| remains bounded. By using expected updates instead, we are able to verify (11) without imposing any additional condition on {λ t }. We remark that Algorithm 2 makes expected updates across only actions. At each time step, Algorithm 2 still update the policy only for the current state. Algorithm 2 shares the same spirit of the canonical soft actor critic algorithm (Algorithm 1 in <ref type="bibr" target="#b16">Haarnoja et al. 2018)</ref>. <ref type="bibr" target="#b16">Haarnoja et al. (2018)</ref> derive the canonical soft actor critic algorithm from a soft policy iteration perspective, where the policy evaluation of the soft value function and the policy improvement of the actor are performed alternatively. Importantly, during the soft policy iteration, both the policy evaluation and the policy improvement steps are assumed to be fully executed. By contrast, the soft actor critic algorithm conduct only several gradient steps for both the policy evaluation and the policy improvement. As a consequence, the results concerning the optimality of the soft policy iteration in <ref type="bibr" target="#b16">Haarnoja et al. (2018)</ref> do not apply to soft actor critic. The convergence of soft actor critic with a fixed regularization weight (η) remains an open problem, and convergence with a decaying regularization, to optimality even more so. In this work, we instead derive the soft actor critic algorithm from the policy gradient perspective directly, akin to the canonical actor critic, and establish its convergence. 1</p><p>We first study the convergence of {q t } in Algorithm 2. Different from Algorithm 1, the iterates {q t } now depend on not only θ t but also λ t . In light of this, we consider their concatenation and define</p><formula xml:id="formula_62">ζ t . = λ t θ t , ζ . = η θ .</formula><p>Here ζ is the placeholder for ζ t used for defining functions. The update of {q t } in Algorithm 2 can then be expressed in a compact way as</p><formula xml:id="formula_63">q t+1 = q t + α t (F ζt (q t , S t , A t , S t+1 ) − q t ),</formula><p>where</p><formula xml:id="formula_64">F ζ (q, s 0 , a 0 , s 1 )[s, a] . = δ ζ (q, s 0 , a 0 , s 1 )I (s 0 ,a 0 )=(s,a) + q(s, a), δ ζ (q, s 0 , a 0 , s 1 ) . = r(s 0 , a 0 ) + γ a 1 π θ (a 1 |s 1 ) (q(s 1 , a 1 ) − η log π θ (a 1 |s 1 )) − q(s 0 , a 0 ).</formula><p>We can then establish the convergence of {q t } similarly to Proposition 4.</p><p>Proposition 6 (Convergence of the critic) Let Assumptions 4.1, 4.3, and 4.4 hold. Then there exists an p norm such that for any</p><formula xml:id="formula_65">q ∈ (0, min {2( β − α ), α }),</formula><p>if t 0 is sufficiently large, the iterates {q t } generated by Algorithm 1 satisfy</p><formula xml:id="formula_66">E q t − qπ θ t ,λt 2 p = O 1 t q .</formula><p>The proof of Proposition 6 is provided in Section C.1 and is more convoluted than that of Proposition 4 since we now need to verify the assumptions of Theorem 2 for the concatenated vector ζ t instead of just θ t . With the help of Proposition 6, we now establish the convergence of {θ t }, akin to Theorem 5.</p><p>Theorem 7 (Convergence of the actor) Let Assumptions 4.1, 4.3, and 4.4 hold. Fix any</p><formula xml:id="formula_67">q ∈ 0, min {2( β − α ), α } .</formula><p>Let t 0 be sufficiently large. Fix any 0 &gt; 0 and any state distribution p 0 . For the iterates {θ t } generated by Algorithm 2 and any t &gt; 0, if k is uniformly randomly selected from the set t 2 , t 2 + 1, . . . , t , then</p><formula xml:id="formula_68">∇ Jλ k (π θ k ; p 0 ) 2 ≤ 1 k 0</formula><p>holds with at least probability</p><formula xml:id="formula_69">1 − O 1 t 1− β − 0 + log 2 t t β − 0 + 1 t q − 0 .</formula><p>The proof of Theorem 7 is provided in Section C.2. Theorem 7 confirms the convergence of the actor to stationary points, where the additional 0 trades off the rate at which the gradient vanishes and the rate at which the probability goes to one. This 0 is just to present the results and is not a hyperparameter of Algorithm 2. To our best knowledge, Theorem 7 is the first to establish the convergence of soft actor critic with a decaying entropy regularization weight.</p><p>Based on Theorem 7, the following corollary gives a partial result concerning the optimality of Algorithm 2.</p><p>Corollary 8 (Optimality of the actor) Let Assumptions 4.1, 4.3, and 4.4 hold. Fix any</p><formula xml:id="formula_70">q ∈ 0, min {2( β − α ), α } .</formula><p>Let t 0 be sufficiently large. Let {δ t } be any positive decreasing sequence converging to 0. For the iterates {θ t } generated by Algorithm 2 and any t &gt; 0, if k is uniformly randomly selected from the set t 2 , t 2 + 1, . . . , t , then</p><formula xml:id="formula_71">J(π θ k ; p 0 ) ≥ J(π * ; p 0 ) − O (λ k ) − O δ k λ k (min s,a π θ k (a|s)) 2 holds with at least probability 1 − O t −(1− β ) + t − β log 2 t + t − q δ t ,</formula><p>where π * can be any optimal policy in (2).</p><p>The proof of Corollary 8 is provided in Section C.3. The sequence {δ t } in Corollary 8 trades off the suboptimality and the high probability. For Corollary 8 to be nontrivial (i.e., the suboptimality diminishes and the high probability approaches one), one sufficient condition is that</p><formula xml:id="formula_72">lim t→∞ t −(1− β ) + t − β log 2 t + t − q λ t (min s,a π θt (a|s)) 2 = 0. (<label>21</label></formula><formula xml:id="formula_73">)</formula><p>This requires us to study the decay rate of min s,a π θt (a|s). We conjecture that when λ t decays slower, min s,a π θt (a|s) also decays slower. Consequently, we expect (21) to hold when λ t decays sufficiently slow and the form of the learning rates α t and β t are adjusted correspondingly according to the form of min s,a π θt (a|s)'s decay rate. We leave the investigation of this rate for future work.</p><p>We remark that though Corollary 8 is only a partial result, it still advances the state of the art regarding the optimality of soft policy gradient (policy gradient in the maximum entropy RL framework) methods in <ref type="bibr" target="#b34">Mei et al. (2020)</ref>. Theorem 8 of <ref type="bibr" target="#b34">Mei et al. (2020)</ref> gives a convergence rate of soft policy gradient methods, also with a dependence on the rate at which min s,a π θt (a|s) diminishes. They too leave the investigation of the rate as an open problem. Theorem 8 of <ref type="bibr" target="#b34">Mei et al. (2020)</ref>, however, only considers a bandit setting with the exact soft policy gradient and leaves the general MDP setting for future work. By contrast, Corollary 8 applies to general MDPs with approximate and stochastic update steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>Our Theorem 2 regarding the finite sample analysis of stochastic approximation algorithms follows the line of research of <ref type="bibr" target="#b7">Chen et al. (2020</ref><ref type="bibr">Chen et al. ( , 2021))</ref>. In particular, Chen et al. ( <ref type="formula">2020</ref>) consider ( <ref type="formula" target="#formula_15">9</ref>) with an expected operator (i.e., F θt (w t , Y t ) is replaced by F (w t )). <ref type="bibr">Chen et al. (2021)</ref> extend <ref type="bibr" target="#b7">Chen et al. (2020)</ref> in that the expected operator is replaced by the stochastic operator F (w t , Y t ), though {Y t } here is a Markov chain with fixed dynamics. We further extend Chen et al. ( <ref type="formula">2021</ref>) from time-homogeneous stochastic operator and dynamics to time-inhomogeneous stochastic operator and dynamics. This line of research depends on properties of contraction mappings. There are also ODE-based analysis for stochastic approximation algorithms <ref type="bibr" target="#b3">(Benveniste et al., 1990;</ref><ref type="bibr" target="#b25">Kushner and Yin, 2003;</ref><ref type="bibr" target="#b6">Borkar, 2009)</ref> and we refer the reader to <ref type="bibr" target="#b7">Chen et al. (2020</ref><ref type="bibr">Chen et al. ( , 2021) )</ref> for a more detailed review.</p><p>In this work, we focus on the optimality of naive actor critic algorithms that do not use second order information. With the help of the Fisher information, the optimality of natural actor critic <ref type="bibr" target="#b19">(Kakade, 2001;</ref><ref type="bibr" target="#b38">Peters and Schaal, 2008;</ref><ref type="bibr">Bhatnagar et al., 2009)</ref> is also established in both on-policy settings <ref type="bibr" target="#b0">(Agarwal et al., 2020;</ref><ref type="bibr">Wang et al., 2019;</ref><ref type="bibr" target="#b31">Liu et al., 2020;</ref><ref type="bibr">Khodadadian et al., 2022)</ref> and off-policy settings <ref type="bibr" target="#b20">(Khodadadian et al., 2021;</ref><ref type="bibr" target="#b9">Chen et al., 2022)</ref>. In particular, <ref type="bibr" target="#b0">Agarwal et al. (2020)</ref>; <ref type="bibr">Khodadadian et al. (2022</ref><ref type="bibr" target="#b20">Khodadadian et al. ( , 2021) )</ref> establish the optimality of natural actor critic in the tabular setting. They, however, make synchronous updates to the actor. In other words, they update the policy for all states at each time step. Consequently, the state distribution is not important there. By contrast, the naive actor critic this work considers makes asynchronous updates to the actor. In other words, at each time step, we only update the policy for the current state. This asynchronous update is more practical in large scale experiments. Moreover, <ref type="bibr" target="#b57">Xu et al. (2021)</ref> establish the convergence to stationary points of an off-policy actor critic with density ratio correction and a fixed sampling distribution. To study the optimality of the stationary points, <ref type="bibr" target="#b57">Xu et al. (2021)</ref> also make some assumptions about the Fisher information. In this work, we do not use any second order information. How this work achieves optimality (i.e., vanilla actor critic with decaying KL regularization) is fundamentally different from natural actor critic. <ref type="bibr" target="#b31">Liu et al. (2020)</ref> improve the results of <ref type="bibr" target="#b0">Agarwal et al. (2020)</ref> regarding the optimality of policy gradient methods from exact gradient to stochastic and approximate gradient. <ref type="bibr" target="#b31">Liu et al. (2020)</ref>, however, work on on-policy settings and require nested loops. By contrast, we work on off-policy settings and consider three-timescale updates. <ref type="bibr" target="#b12">Degris et al. (2012)</ref> also study the convergence of an off-policy actor critic without using density ratio to correct the state distribution mismatch. As noted in the Errata of <ref type="bibr" target="#b12">Degris et al. (2012)</ref>, their results also exclusively apply to tabular settings. Additionally, <ref type="bibr" target="#b12">Degris et al. (2012)</ref> establish asymptotic convergence to only some locally asymptotically stable points of an ODE without any convergence rate. And the optimality of those locally asymptotically stable points remains unclear. Further, <ref type="bibr" target="#b12">Degris et al. (2012)</ref> assume the transitions are identically and independently sampled. By contrast, our transitions are obtained by following a time-inhomogeneous behavior policy.</p><p>In this paper, we focus on the tabular setting as a starting point for this line of research. When linear function approximation is used for the critic, compatible features <ref type="bibr" target="#b45">(Sutton et al., 1999;</ref><ref type="bibr" target="#b22">Konda, 2002;</ref><ref type="bibr" target="#b61">Zhang et al., 2020c</ref>) can be used to eliminate the bias resulting from the limit of the representation capacity. With the help of compatible features, <ref type="bibr" target="#b31">Liu et al. (2020)</ref> show the optimality of their on-policy actor critic and <ref type="bibr" target="#b57">Xu et al. (2021)</ref> show the optimality of their off-policy actor critic. We leave the study of linear function approximation in our settings with compatible features for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head><p>In this section, we provide some empirical results in complement to our theoretical analysis. The implementation is made publicly available to facilitate future research. 2 In particular, we are interested in the following three questions: (i). Can the claimed convergence and optimality of Algorithm 1 in Theorem 5 be observed in computational experiments?</p><p>(ii). Can the claimed convergence of Algorithm 2 in Theorem 7 and its conjectured optimality from (21) and Corollary 8 be observed in computational experiments? (iii). How is the KL-based regularization (cf. Algorithm 1) qualitatively different from the entropy-based regularization (cf. Algorithm 2)?</p><p>Figure <ref type="figure">1</ref>: The chain domain from <ref type="bibr" target="#b26">Laroche and Tachet (2021)</ref> with γ = 0.99.</p><p>We use the chain domain from Laroche and Tachet (2021) as our testbed. As described in Figure <ref type="figure">1</ref>, there are N non-terminal states in the chain and the agent is always initialized   at state s 1 . There are two actions available in each state. The solid action leads the agent from s i to s i+1 and yields a reward of 0 for all i &lt; N . At s N , the solid action instead leads to the terminal state and yields a reward of 1. The dotted action always leads to the terminal state directly and yields a reward 0.8 × γ N −1 . Trivially, the optimal policy is to always choose the solid action, which will yield an episodic return of γ N −1 . As noted by <ref type="bibr" target="#b26">Laroche and Tachet (2021)</ref>, the challenge of this chain domain is to overcome the immediate rewards pushing the agent towards suboptimal policies. We remark that though this chain has a finite horizon, we can indeed reformalize it into an infinite-horizon chain with transition-dependent discounting. We refer the reader to <ref type="bibr" target="#b52">White (2017)</ref> for more details about this technique and we believe our theoretical results can be easily extended to transition-dependent discounting.</p><formula xml:id="formula_74">J(π; p0) N = 6 λ = 2 −5 λ = 2 −4 λ = 2 −3 λ = 2 −1 λ = 2 1 0 2 ×</formula><formula xml:id="formula_75">J(π; p0) N = 6 λ = 2 −5 λ = 2 −4 λ = 2 −3 λ = 2 −1 λ = 2 1 0 2 ×</formula><p>We run Algorithms 1 and 2 in the chain domain. According to (13), we use the behavior policy</p><formula xml:id="formula_76">µ θ (•|s) . =0.1 × 1 2 + 0.9 × exp(0.1 × θ s,• ) exp(0.1 × θ s,solid ) + exp(0.1 × θ s,dotted )</formula><p>.</p><p>According to (17) and Assumption 4.1, we set {α t , β t , λ t } as</p><formula xml:id="formula_77">α t . = 100 + 1 (t + 10 5 ) 0.5+0.001 , β t . = 100 (t + 10 5 ) 0.75+0.001 , λ t . = 0.025 (t + 10 5 ) λ ,</formula><p>where we test a range of λ from 2 −5 , 2 −4 , 2 −3 , 2 −1 , 2 3 . We run both Algorithms 1 and 2 for 2 × 10 6 steps and evaluate the target policy every 2 × 10 3 steps, where we execute it for 10 episodes and take the mean episodic return. The evaluation performance is reported in Figures <ref type="figure" target="#fig_3">2 and 3</ref> respectively. Curves are averaged over 30 independent runs with shaded regions indicating standard errors. The black dotted lines are the performance of the optimal policy.</p><p>As suggested by Figure <ref type="figure" target="#fig_2">2</ref> with N ∈ {6, 7}, when λ ∈ 2 −1 , 2 , the target policy found by Algorithm 1 is indeed very close to the optimal policy at the end of training, which gives an affirmative answer to the question (i). It is important to note that neither λ = 2 −1 nor λ = 2 is recommended by ( <ref type="formula" target="#formula_51">18</ref>). This is expected as Assumption 4.1 is only sufficient and the convergence rate in Theorem 5 can possibly be significantly improved. Further, with the increase of N , the suboptimality of the target policy at the end of training also increases. This is expected as increasing N makes the problem more challenging. We, however, remark that though with N ∈ {8, 9}, the target policy is not close to the optimal policy at the end of training, all curves are monotonically improving as time progresses. Similarly, the results in Figure <ref type="figure" target="#fig_3">3</ref> give an affirmative answer to the question (ii). Comparing Figures <ref type="figure" target="#fig_3">2 and 3</ref>, it is easy to see that Algorithm 1 is much more sensitive to λ than Algorithm 2. As shown by Figure <ref type="figure" target="#fig_2">2</ref>, the selection of λ significantly affects the rate that the suboptimality diminishes in Algorithm 1. By contrast, Figure <ref type="figure" target="#fig_3">3</ref> suggests that the rate that the suboptimality diminishes is barely affected by λ in Algorithm 2. This comparison gives an intuitive answer the question (iii). This difference is because the KL regularization is much more aggressive than the entropy regularization. To be more specific, the entropy of the policy is always bounded but the KL divergence used here can be unbounded when the policy becomes deterministic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this paper, we demonstrate the optimality of the off-policy actor critic algorithm even without using a density ratio to correct the state distribution mismatch. This result is significant in two aspects. First, it advances the understanding of the optimality of policy gradient methods in the tabular setting from <ref type="bibr" target="#b0">Agarwal et al. (2020)</ref>; <ref type="bibr" target="#b34">Mei et al. (2020)</ref>; <ref type="bibr" target="#b26">Laroche and Tachet (2021)</ref>. Second, it provides, to certain extent, a theoretical justification for the practice of ignoring state distribution mismatch in large scale RL experiments <ref type="bibr" target="#b51">(Wang et al., 2017;</ref><ref type="bibr" target="#b14">Espeholt et al., 2018;</ref><ref type="bibr" target="#b48">Vinyals et al., 2019;</ref><ref type="bibr" target="#b41">Schmitt et al., 2020;</ref><ref type="bibr" target="#b58">Zahavy et al., 2020)</ref>. One important ingredient of our results is the finite sample analysis of a generic stochastic approximation algorithm with time-inhomogeneous update operators on timeinhomogeneous Markov chains, which we believe can be used to analyze more RL algorithms and has interest beyond RL.</p><p>Part of this work was done during SZ's internship at Microsoft Research Montreal and SZ's DPhil at the University of Oxford. SZ is also funded by the Engineering and Physical Sciences Research Council (EPSRC) during his DPhil.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proofs of Section 3</head><p>A.1 Proof of Lemma 1 Lemma 9 (Uniform ergodicity implies uniform mixing) Let Assumption 3.2 hold. Then, there exist constants C 0 &gt; 0 and τ ∈ (0, 1), independent of θ, such that for any n &gt; 0, sup y,θ y</p><formula xml:id="formula_78">P n θ (y, y ) − d θ (y ) ≤ C 0 τ n . (<label>10</label></formula><formula xml:id="formula_79">)</formula><p>Proof Theorem 4.9 of Levin and Peres (2017) confirms the geometric mixing for a single ergodic chain. Here we adapt its proof to show the uniform mixing.</p><p>For any P ∈ ΛP , define the indicator matrix I P ∈ {0, 1} |Y|×|Y| such that</p><formula xml:id="formula_80">I P [y, y ] = 1, P [y, y ] &gt; 0 0, P [y, y ] = 0 .</formula><p>Consider the stochastic matrix M P defined as</p><formula xml:id="formula_81">M P (i, j) = I P (i, j) j I P (i, j )</formula><p>.</p><p>Since the chain induced by P is ergodic, it is easy to see the chain induced by M P is also ergodic. This is because (1) a finite chain is ergodic if and only if it is irreducible and aperiodic;</p><p>(2) the connectivity of the chain induced by M P is the same as that by P ; and (3) irreducibility and aperiodicity depend only on connectivity, not on the specific probability of each transition. The proof of Proposition 1.7 of Levin and Peres (2017) then asserts that there exists a constant t P &gt; 0 such that for all t ≥ t P , M t P (i, j) &gt; 0 holds for any i, j. Hence P t (i, j) &gt; 0 also holds because M P and P share the same connectivity, so do their powers. Formally, it can be proved via induction that</p><formula xml:id="formula_82">∀t ≥ 1, i, j, M t P (i, j) &gt; 0 =⇒ P t (i, j) &gt; 0. (<label>22</label></formula><formula xml:id="formula_83">)</formula><p>First, (22) obviously holds for t = 1. Suppose (22) holds for t ≤ k. If</p><formula xml:id="formula_84">M k+1 P (i, j) = l M k P (i, l)M P (l, j) &gt; 0,</formula><p>there must exist at least one l such that M k P (i, l)M P (l, j) &gt; 0, i.e., M k P (i, l) &gt; 0, M P (l, j) &gt; 0.</p><p>Using the induction hypothesis yields P k (i, l) &gt; 0, P (l, j) &gt; 0, from which P k+1 (i, j) &gt; 0 follows easily. This completes the induction. Since Y is finite, the set I P |P ∈ ΛP is also finite (at most 2 |Y|×|Y| elements), and so are the sets M P |P ∈ ΛP and t P |P ∈ ΛP . Let</p><formula xml:id="formula_85">t * . = max P ∈ ΛP {t P },</formula><p>we then have for any P ∈ ΛP , P t * (i, j) &gt; 0 always holds. Importantly, t * is independent of P . Then the extreme value theorem implies that</p><formula xml:id="formula_86">δ . = inf P ∈ ΛP ,i,j P t * (i, j) &gt; 0.</formula><p>Let d P be the invariant distribution of the chain induced by P and take any δ such that 0 &lt; δ &lt; δ, then</p><formula xml:id="formula_87">P t * (i, j) &gt; δ ≥ δ d P (j)</formula><p>holds for any P ∈ ΛP , i, j.</p><p>For any P ∈ ΛP , let Π be a matrix, each row of which is d P , and define</p><formula xml:id="formula_88">ζ . = 1 − δ .</formula><p>We now verify that the matrix</p><formula xml:id="formula_89">Q . = P t * + ζΠ − Π ζ</formula><p>is a stochastic matrix. First, its row sums are 1:</p><formula xml:id="formula_90">(Q1)(i) . = 1 + ζ − 1 ζ = 1.</formula><p>Second, its elements are nonnegative:</p><formula xml:id="formula_91">Q(i, j) = P t * (i, j) + ζd P (j) − d P (j) ζ ≥ δ d P (j) + ζd P (j) − d P (j) ζ = 0.</formula><p>Rearranging terms yields</p><formula xml:id="formula_92">P t * = (1 − ζ)Π + ζQ. (<label>23</label></formula><formula xml:id="formula_93">)</formula><p>We now use induction to show that for any k ≥ 1,</p><formula xml:id="formula_94">P t * k = (1 − ζ k )Π + ζ k Q k . (<label>24</label></formula><formula xml:id="formula_95">)</formula><p>For k = 1, we know (24) holds from (23). Suppose (24) holds for k = n, then</p><formula xml:id="formula_96">P t * (n+1) = P t * n P t * = ((1 − ζ n )Π + ζ n Q n ) P t * = (1 − ζ n )ΠP t * + ζ n Q n ((1 − ζ)Π + ζQ) = (1 − ζ n )ΠP t * + (1 − ζ)ζ n Q n Π + ζ n+1 Q n+1 = (1 − ζ n )Π + (1 − ζ)ζ n Q n Π + ζ n+1 Q n+1 (Property of invariant distribution) = (1 − ζ n )Π + (1 − ζ)ζ n Π + ζ n+1 Q n+1 (QΠ = Q1d P = 1d P = Π for any stochastic matrix Q) = (1 − ζ n+1 )Π + ζ n+1 Q n+1 ,</formula><p>which completes the induction. Consequently, for any l ∈ {0, 1, . . . , t * − 1}, multiplying by P l both sides of (24) yields</p><formula xml:id="formula_97">P t * k+l = (1 − ζ k )ΠP l + ζ k Q k P l = (1 − ζ k )Π + ζ k Q k P l .</formula><p>Rearranging terms yields</p><formula xml:id="formula_98">P t * k+l − Π = ζ k (Q k P l − Π),</formula><p>implying for any i,</p><formula xml:id="formula_99">j P t * k+l (i, j) − d P (j) = ζ k j (Q k P l )(i, j) − d P (j) ≤ 2ζ k (Boundedness of total variation) = 2ζ − l t * ζ 1 t * t * k+l ≤ 2ζ − t * −1 t * ζ 1 t * t * k+l . Let C 0 . = 2ζ − t * −1 t * , τ . = ζ 1 t * .</formula><p>It is easy to see C 0 &gt; 0, τ ∈ (0, 1) and both C 0 and τ are independent of P . Consequently, for any n ≥ t * , we have</p><formula xml:id="formula_100">j |P n (i, j) − d P (j)| ≤ C 0 τ n .</formula><p>By the boundedness of total variation, for n ∈ {0, 1, . . . , t * − 1}, we have</p><formula xml:id="formula_101">j |P n (i, j) − d P (j)| ≤ 2 ≤ 2 τ t * τ n . Setting C 0 . = max C 0 , 2 τ t *</formula><p>completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Theorem 2</head><p>Theorem 2 Let Assumptions 3.1 -3.6 hold. For any</p><formula xml:id="formula_102">w ∈ (0, min {2( β − α ), α }),</formula><p>if t 0 is sufficiently large, then ∀t,</p><formula xml:id="formula_103">E w t − w * θt 2 c = O 1 (t + t 0 ) w .</formula><p>Proof Since the theorem is a generalization of the results in Chen et al. ( <ref type="formula">2021</ref>), we follow their framework to complete the proof. In our setting, the dynamics of the Markov chain changes every time step according to a secondary random sequence {θ t }. Consequently, we have many new error terms which are not controlled by <ref type="bibr">Chen et al. (2021)</ref> and that we handle using techniques from <ref type="bibr" target="#b63">Zou et al. (2019)</ref>.</p><p>Following <ref type="bibr">Chen et al. (2021)</ref>, we use a Lyapunov method for the proof with the generalized Moreau envelope of 1 2 • 2 c as the Lyapunov function. In particular, we consider the Lyapunov function</p><formula xml:id="formula_104">M (w) . = inf u∈R K 1 2 u 2 c + 1 2ξ w − u 2 s ,</formula><p>where ξ &gt; 0 is a constant to be tuned, • c is the norm w.r.t. which Fθ is contractive (cf. Assumption 3.3), and • s is an arbitrary norm such that 1 2 • 2 s is L-smooth (Lemma 45). It can, e.g., be an p norm with p ≥ 2 (Example 5.11 of <ref type="bibr" target="#b2">Beck (2017)</ref>). Due to the equivalence between norms, there exist positive constants l cs and u cs such that l cs w s ≤ w c ≤ u cs w s holds for any w. The following lemma proved by <ref type="bibr">Chen et al. (2021)</ref>  (ii). There exists a norm</p><formula xml:id="formula_105">• m such that M (w) = 1 2 w 2 m .</formula><p>(iii). Define</p><formula xml:id="formula_106">l cm = (1 + ξl 2 cs ) u cm = (1 + ξu 2 cs ),</formula><p>then ∀w,</p><formula xml:id="formula_107">l cm w m ≤ w c ≤ u cm w m .</formula><p>Lemma 10 (i) and Lemma 45 imply that for any x, x ,</p><formula xml:id="formula_108">M (x ) ≤ M (x) + ∇M (x), x − x + L 2ξ x − x 2 s .</formula><p>Using x = w t+1 −w * θ t+1 and x = w t −w * θt in the above inequality and the update equation ( <ref type="formula" target="#formula_15">9</ref>):</p><formula xml:id="formula_109">w t+1 . = w t + α t (F θt (w t , Y t ) − w t + t ), Lemma 10 (ii) yields 1 2 w t+1 − w * θ t+1 2 m (25) ≤ 1 2 w t − w * θt 2 m + ∇M (w t − w * θt ), w t+1 − w t + w * θt − w * θ t+1 + L 2ξ w t+1 − w t + w * θt − w * θ t+1 2 s = 1 2 w t − w * θt 2 m + ∇M (w t − w * θt ), w * θt − w * θ t+1 T 1 + α t ∇M (w t − w * θt ), Fθt (w t ) − w t T 2 + α t ∇M (w t − w * θt ), F θt (w t , Y t ) − Fθt (w t ) T 3 + α t ∇M (w t − w * θt ), t T 4 + α 2 t L ξ F θt (w t , Y t ) − w t + t 2 s T 5 + L ξ w * θt − w * θ t+1 2 s T 6</formula><p>.</p><p>We now bound T 1 -T 6 one by one. T 1 and T 6 are errors resulting from changing dynamics and are not controlled in <ref type="bibr">Chen et al. (2021)</ref>. T 2 , T 4 , and T 5 can be bounded similarly to <ref type="bibr">Chen et al. (2021)</ref>. To bound T 3 , we further decompose it as</p><formula xml:id="formula_110">T 3 = ∇M (w t − w * θt ), F θt (w t , Y t ) − Fθt (w t ) = ∇M (w t − w * θt ) − ∇M (w t−τα t − w * θ t−τα t ), F θt (w t , Y t ) − Fθt (w t ) T 31 + ∇M (w t−τα t − w * θ t−τα t ), F θt (w t , Y t ) − F θt (w t−τα t , Y t ) + Fθt (w t−τα t ) − Fθt (w t ) T 32 + ∇M (w t−τα t − w * θ t−τα t ), F θt (w t−τα t , Y t ) − Fθt (w t−τα t ) T 33</formula><p>, where</p><formula xml:id="formula_111">τ αt . = min {n ≥ 0 | C 0 τ n ≤ α t },<label>(26)</label></formula><p>and C 0 and τ are defined in Lemma 1. τ αt denotes the number of steps the chain needs to mix to an accuracy of α t . T 31 and T 32 can be bounded similarly to <ref type="bibr">Chen et al. (2021)</ref>. The bound for T 33 is however significantly different. We decompose T 33 as Ỹt−τα t +2 → . . .</p><formula xml:id="formula_112">T 33 = ∇M (w t−τα t − w * θ t−τα t ), F θt (w t−τα t , Y t ) − Fθt (w t−τα t ) = ∇M (w t−τα t − w * θ t−τα t ), F θ t−</formula><formula xml:id="formula_113">{Y t } : • • • → Y t−τα t −1 → P θ t−τα t Y t−τα t → P θ t−τα t +1 Y t−τα t +1 → P θ t−τα t +2 Y t−τα t +2 → . . . .</formula><p>We are now ready to present bounds for each of the above terms. To begin, we define some shorthand and study their properties:</p><formula xml:id="formula_114">α t 1 ,t 2 . = t 2 t=t 1 α t , β t 1 ,t 2 . = t 2 t=t 1 β t A . = U + L F + 1, B . = U F + U , C . = AU w + B + A + A(1 + U F + U F ). (<label>27</label></formula><formula xml:id="formula_115">)</formula><p>Lemma 11 For sufficiently large t 0 ,</p><formula xml:id="formula_116">τ αt = O(log(t + t 0 )), α t−τα t ,t−1 = O log(t + t 0 ) (t + t 0 ) α , β t−τα t ,t−1 = O log(t + t 0 ) (t + t 0 ) α , α t α t−τα t ,t−1 β t = O log(t + t 0 ) (t + t 0 ) 2 α− β .</formula><p>The proof of Lemma 11 is provided in Section E.11. Lemma 11 asserts that we can select a t 0 sufficiently large such that</p><formula xml:id="formula_117">α t−τα t ,t−1 ≤ 1 4A</formula><p>holds for all t. This condition is crucial for Lemma 47, which plays an important role in the following bounds.</p><p>Lemma 12 (Bound of T 1 )</p><formula xml:id="formula_118">T 1 ≤ L w L θ β t l cm w t − w * θt m .</formula><p>The proof of Lemma 12 is provided in Section E.1.</p><p>Lemma 13 (Bound of T 2 )</p><formula xml:id="formula_119">T 2 ≤ −(1 − κ u cm l cm ) w t − w * θt 2 m .</formula><p>The proof of Lemma 13 is provided in Section E.2.</p><p>Lemma 14 (Bound of T 31 )</p><formula xml:id="formula_120">T 31 ≤ 8L(L w L θ + 1)α t−τα t ,t−1 ξl 2 cs u 2 cm A 2 w t − w * θt 2 m + C 2 .</formula><p>The proof of Lemma 14 is provided in Section E.3.</p><p>Lemma 15 (Bound of T 32 )</p><formula xml:id="formula_121">T 32 ≤ 32Lα t−τα t ,t−1 (1 + L w L θ β t−τα t ,t−1 ) ξl 2 cs u 2 cm A 2 w t − w * θt 2 m + C 2 .</formula><p>The proof of Lemma 15 is provided in Section E.4.</p><p>Lemma 16 (Bound of T 331 )</p><formula xml:id="formula_122">E [T 331 ] ≤ 8Lα t (1 + L w L θ β t−τα t ,t−1 ) Aξl 2 cs u 2 cm A 2 E w t − w * θt 2 m + C 2 .</formula><p>The proof of Lemma 16 is provided in Section E.5.</p><p>Lemma 17 (Bound of T 332 )</p><formula xml:id="formula_123">E [T 332 ] ≤ 8|Y|L P L θ t−1 j=t−τα t β t−τα t ,j L(1 + L w L θ β t−τα t ,t−1 ) Aξl 2 cs u 2 cm A 2 E w t − w * θt 2 m + C 2 .</formula><p>The proof of Lemma 17 is provided in Section E.6.</p><p>Lemma 18 (Bound of T 333 )</p><formula xml:id="formula_124">T 333 ≤ 8LL F L θ β t−τα t ,t−1 (1 + L w L θ β t−τα t ,t−1 ) A 2 ξl 2 cs u 2 cm A 2 w t − w * θt 2 m + C 2 .</formula><p>The proof of Lemma 18 is provided in Section E.7.</p><p>Lemma 19 (Bound of T 334 )</p><formula xml:id="formula_125">T 334 ≤ 8LL F L θ β t−τα t ,t−1 (1 + L w L θ β t−τα t ,t−1 ) A 2 ξl 2 cs u 2 cm A 2 w t − w * θt 2 m + C 2 .</formula><p>The proof of Lemma 19 is provided in Section E.8.</p><p>Lemma 20 (Bound of T 4 )</p><formula xml:id="formula_126">E [T 4 ] = 0.</formula><p>The proof of Lemma 20 is provided in Section E.9.</p><p>Lemma 21 (Bound of T 5 )</p><formula xml:id="formula_127">T 5 ≤ 2L ξl 2 cs A 2 u 2 cm w t − w * θt 2 m + C 2 .</formula><p>The proof of Lemma 21 is provided in Section E.10.</p><p>Lemma 22 (Bound of T 6 )</p><formula xml:id="formula_128">T 6 = L ξ w * θt − w * θ t+1 2 s ≤ LL 2 w L 2 θ β 2 t ξl 2 cs .</formula><p>Lemma 22 follows immediately from Assumptions 3.4 and 3.6. We now assemble the bounds in Lemmas 12 -22 back into (25). By the definition of u cm and l cm in Lemma 10, we have lim ξ→0 u cm l cm = 1.</p><p>Since κ &lt; 1, we can select a sufficiently small ξ &gt; 0 such that</p><formula xml:id="formula_129">ψ 1 . = 2 9 (1 − κ u cm l cm )</formula><p>satisfies ψ 1 ∈ (0, 1), implying</p><formula xml:id="formula_130">T 2 ≤ − 9 2 ψ 1 w t − w * θt 2 m , α t T 2 ≤ − 9 2 α t ψ 1 w t − w * θt 2 m . (<label>28</label></formula><formula xml:id="formula_131">)</formula><p>Let ψ 2 be a positive constant to be tuned. For T 1 , suppose ψ 2 is large enough, then we have</p><formula xml:id="formula_132">T 1 ≤ 1 2 β t ψ 2 w t − w * θt m .<label>(29)</label></formula><p>For T 31 , Lemmas 11 and 14 assert that we can select sufficiently large t 0 and ψ 2 such that</p><formula xml:id="formula_133">T 31 ≤ 1 2 ψ 1 w t − w * θt 2 m + 1 2 α t−τα t ,t−1 ψ 2 , α t T 31 ≤ 1 2 α t ψ 1 w t − w * θt 2 m + 1 2 α t α t−τα t ,t−1 ψ 2 . (<label>30</label></formula><formula xml:id="formula_134">)</formula><p>For T 32 , Lemma 11 implies that for t 0 large enough</p><formula xml:id="formula_135">β t−τα t ,t−1 ≤ 1.</formula><p>Hence, Lemma 15 guarantees that we can select sufficiently large t 0 and ψ 2 such that</p><formula xml:id="formula_136">T 32 ≤ 1 2 ψ 1 w t − w * θt 2 m + 1 2 α t−τα t ,t−1 ψ 2 , α t T 32 ≤ 1 2 α t ψ 1 w t − w * θt 2 m + 1 2 α t α t−τα t ,t−1 ψ 2 . (<label>31</label></formula><formula xml:id="formula_137">)</formula><p>For T 331 , similarly, we can select sufficiently large t 0 and ψ 2 such that</p><formula xml:id="formula_138">E [T 331 ] ≤ 1 2 ψ 1 E w t − w * θt 2 m + 1 2 α t ψ 2 ≤ 1 2 ψ 1 E w t − w * θt 2 m + 1 2 α t−τα t ,t−1 ψ 2 , α t E [T 331 ] ≤ 1 2 α t ψ 1 E w t − w * θt 2 m + 1 2 α t α t−τα t ,t−1 ψ 2 . (<label>32</label></formula><formula xml:id="formula_139">)</formula><p>For T 332 , we have Since the RHS of the above inequality approaches 0 when t 0 is sufficiently large, we can select sufficiently large t 0 such that</p><formula xml:id="formula_140">t−1 j=t−τα t β t−τα t ,j ≤ α t−τα t ,t−1 .</formula><p>Then it is easy to see for sufficiently large t 0 and ψ 2 ,</p><formula xml:id="formula_141">E [T 332 ] ≤ 1 2 ψ 1 E w t − w * θt 2 m + 1 2 α t−τα t ,t−1 ψ 2 , α t E [T 332 ] ≤ 1 2 α t ψ 1 E w t − w * θt 2 m + 1 2 α t α t−τα t ,t−1 ψ 2 . (<label>33</label></formula><formula xml:id="formula_142">)</formula><p>Similarly, for sufficiently large t 0 and ψ 2 ,</p><formula xml:id="formula_143">α t T 333 ≤ 1 2 α t ψ 1 w t − w * θt 2 m + 1 2 α t α t−τα t ,t−1 ψ 2 ,<label>(34)</label></formula><formula xml:id="formula_144">α t T 334 ≤ 1 2 α t ψ 1 w t − w * θt 2 m + 1 2 α t α t−τα t ,t−1 ψ 2 . (<label>35</label></formula><formula xml:id="formula_145">)</formula><p>For T 5 , it is easy to see for sufficiently large t 0 and ψ 2 ,</p><formula xml:id="formula_146">α 2 t T 5 ≤ 1 2 α t ψ 1 w t − w * θt 2 m + 1 2 α 2 t ψ 2 (36) ≤ 1 2 α t ψ 1 w t − w * θt 2 m + 1 2 α t α t−τα t ,t−1 ψ 2 .</formula><p>For T 6 , since β t &lt; α t , we can similarly select sufficiently large t 0 and ψ 2 such that</p><formula xml:id="formula_147">T 6 ≤ 1 2 α t α t−τα t ,t−1 ψ 2 .<label>(37)</label></formula><p>Putting ( <ref type="formula" target="#formula_130">28</ref>), ( <ref type="formula" target="#formula_132">29</ref>), ( <ref type="formula" target="#formula_133">30</ref>), ( <ref type="formula" target="#formula_136">31</ref>), ( <ref type="formula" target="#formula_138">32</ref>), ( <ref type="formula" target="#formula_141">33</ref>), ( <ref type="formula" target="#formula_143">34</ref>), ( <ref type="formula" target="#formula_144">35</ref>), (36), and (37) back to (25) yields</p><formula xml:id="formula_148">E w t+1 − w * θ t+1 2 m (38) ≤(1 − ψ 1 α t )E w t − w * θt 2 m + β t ψ 2 E w t − w * θt m + 8α t α t−τα t ,t−1 ψ 2 ≤(1 − ψ 1 α t )E w t − w * θt 2 m + β t ψ 2 E w t − w * θt 2 m + 8α t α t−τα t ,t−1 ψ 2 (Jensen's inequality).</formula><p>(38) applies only for t such that t − τ αt ≥ 0. According to Lemma 11, we can select a sufficiently large t 0 such that for all t ≥ t 0 , we have t − τ αt ≥ 0. We now bound</p><formula xml:id="formula_149">E w t − w * θt 2</formula><p>m for both t ≤ t 0 and t ≥ t 0 .</p><p>Lemma 23 There exists a constant C t 0 ,w 0 such that for all t ≤ t 0 ,</p><formula xml:id="formula_150">E w t − w * θt 2 m ≤ C t 0 ,w 0 .</formula><p>The proof of Lemma 23 is provided in Section E.12. We now proceed to the case of t ≥ t 0 . When t 0 is sufficiently large, Lemma 11 asserts that there exists a constant ψ 3 such that</p><formula xml:id="formula_151">8α t α t−τα t ,t−1 ψ 2 ≤ ψ 3 log(t + t 0 ) (t + t 0 ) 2 α .</formula><p>Then using</p><formula xml:id="formula_152">z t . = E w t − w * θt 2 m</formula><p>as a shorthand, we get from (38) that</p><formula xml:id="formula_153">z 2 t+1 ≤(1 − αψ 1 (t + t 0 ) α )z 2 t + βψ 2 z t (t + t 0 ) β + ψ 3 log(t + t 0 ) (t + t 0 ) 2 α .</formula><p>We now use an induction to show that ∀t ≥ t 0 ,</p><formula xml:id="formula_154">z t ≤ C 0 (t + t 0 ) ,<label>(39)</label></formula><p>where C 0 &gt; 1 and ∈ (0, 1) are constants to be tuned. Since Lemma 23 asserts that z t 0 ≤ C t 0 ,w 0 , we can select</p><formula xml:id="formula_155">C 0 ≥ C t 0 ,w 0 (2t 0 )</formula><p>such that (39) holds for t = t 0 . Now assume that (39) holds for t = n, then for t = n + 1, we have</p><formula xml:id="formula_156">z 2 n+1 ≤(1 − αψ 1 (n + t 0 ) α )z 2 n + βψ 2 z n (n + t 0 ) β + ψ 3 log(n + t 0 ) (n + t 0 ) 2 α (i) ≤(1 − αψ 1 (n + t 0 ) α ) C 2 0 (n + t 0 ) 2 + βψ 2 C 0 (n + t 0 ) β + + ψ 3 log(n + t 0 ) (n + t 0 ) 2 α = C 2 0 (n + t 0 ) 2 − αψ 1 C 2 0 (n + t 0 ) α+2 + βψ 2 C 0 (n + t 0 ) β + + ψ 3 log(n + t 0 ) (n + t 0 ) 2 α (ii) ≤ C 2 0 (n + 1 + t 0 ) 2 + 2C 2 0 (n + t 0 ) 2 +1 − αψ 1 C 2 0 (n + t 0 ) α+2 + βψ 2 C 0 (n + t 0 ) β + + ψ 3 log(n + t 0 ) (n + t 0 ) 2 α ≤ C 2 0 (n + 1 + t 0 ) 2 + 2 (n + t 0 ) 2 +1 − αψ 1 (n + t 0 ) α+2 + βψ 2 (n + t 0 ) β + + ψ 3 log(n + t 0 ) (n + t 0 ) 2 α z n C 2 0 .</formula><p>Here (i) results from the inductive hypothesis and (ii) results from the fact that</p><formula xml:id="formula_157">x −2 ≤ (x + 1) −2 + 2 x 2 +1 .</formula><p>To see the above inequality, consider</p><formula xml:id="formula_158">f (x) = x −2 ,</formula><p>which is convex on (0, +∞), implying</p><formula xml:id="formula_159">f (x) − f (x + 1) ≤ f (x) (x − (x + 1)) .</formula><p>To complete the induction, it is sufficient to ensure that ∀n,</p><formula xml:id="formula_160">z n ≤ 0.</formula><p>One way to achieve this is to select such that</p><formula xml:id="formula_161">     α + 2 &lt; 2 + 1 α + 2 &lt; β + α + 2 &lt; 2 α ⇐⇒      α &lt; 1 &lt; β − α &lt; α 2</formula><p>and pick t 0 sufficiently large (depending on the chosen ).</p><p>With the induction completed, (39) implies that ∀t ≥ t 0 ,</p><formula xml:id="formula_162">E w t − w * θt 2 m ≤ C 2 0 (t + t 0 ) 2 .<label>(40)</label></formula><p>Combining ( <ref type="formula" target="#formula_162">40</ref>) and Lemma 23, we conclude that for any w ∈ (0, min {2( β − α ), α }), if t 0 is sufficiently large, then ∀t,</p><formula xml:id="formula_163">E w t − w * θt 2 c = O 1 (t + t 0 ) w ,</formula><p>which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proofs of Section 4</head><p>B.1 Proof of Lemma 3 Lemma 24 (Uniform contraction) Let Assumption 4.4 hold. Then, there exists an p norm and a constant κ ∈ (0, 1) such that for any θ, q, q ∈ R |S×A| , Fθ (q) − Fθ (q ) p ≤ κ q − q p . Further, q π θ is the unique fixed point of Fθ .</p><p>Proof Assumption 4.4 implies that for any µ ∈ Λµ , we have</p><formula xml:id="formula_164">d µ (s, a) &gt; 0.</formula><p>Then by the continuity of invariant distribution (Lemma 48) and the extreme value theorem, we have</p><formula xml:id="formula_165">d µ,min . = inf µ∈ Λµ,s,a d µ (s, a) &gt; 0. Let A θ . = I − D µ θ (I − γP π θ ),<label>(41)</label></formula><p>then Fθ (q) − Fθ (q ) = A θ (q − q ).</p><p>The matrix A θ has the following properties (i). Each element of A θ is always nonnegative (ii). The column sum of A θ is always smaller than 2 (iii). The row sum of A θ is always smaller than κ 0 . = 1 − (1 − γ)d µ,min and greater than 0.</p><p>To see (i), for any diagonal entry, we have</p><formula xml:id="formula_166">A θ (i, i) = 1 − d µ θ (i) + γd µ θ (i)P π θ (i, i) ≥ 0;</formula><p>for any off-diagonal entry, we have</p><formula xml:id="formula_167">A θ (i, j) = γ(D µ θ P π θ )(i, j) ≥ 0.</formula><p>To see (ii), we have</p><formula xml:id="formula_168">1 A θ = 1 − d µ θ + γd µ θ P π θ .</formula><p>Then (ii) follows immediately from the fact that d µ θ P π θ is a valid probability distribution.</p><p>To see (iii), we have</p><formula xml:id="formula_169">A θ 1 = 1 − d µ θ + γd µ θ = 1 − (1 − γ)d µ θ .</formula><p>Then for each i, (A θ 1) (i) &gt; 0 and</p><formula xml:id="formula_170">(A θ 1) (i) = 1 − (1 − γ)d µ θ (i) ≤ 1 − (1 − γ)d µ,min = κ 0 &lt; 1.</formula><p>With those three properties, for any p norm with p &gt; 1, we have</p><formula xml:id="formula_171">A θ x p p = i j A θ (i, j)x j p = i k A θ (i, k) p j A θ (i, j) k A θ (i, k) x j p (Row sum of A θ is strictly positive) ≤ i k A θ (i, k) p j A θ (i, j) k A θ (i, k) |x j | p (Jensen's inequality and convexity of |•| p ) = i k A θ (i, k) p−1 j A θ (i, j) |x j | p ≤ i κ p−1 0 j A θ (i, j) |x j | p (Row sum of A θ is smaller than κ 0 ) =κ p−1 0 j |x j | p i A θ (i, j) ≤2κ p−1 0 j |x j | p , implying A θ x p ≤ (2κ p−1 0 ) 1 p x p .</formula><p>Since κ 0 &lt; 1, for sufficiently large p, we have</p><formula xml:id="formula_172">2κ p−1 0 &lt; 1, implying κ . = (2κ p−1 0 ) 1 p &lt; 1.</formula><p>Consequently, Fθ (q) − Fθ (q ) p = A θ (q − q ) p ≤ κ q − q p , i.e., Fθ is a κ-contraction w.r.t. • p for all θ. Further, Fθ (q) = q, ⇐⇒ D µ θ (r + γP π θ q − q) = 0, ⇐⇒ r + γP π θ q − q = 0, ⇐⇒ q = q π θ , which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Proof of Proposition 4</head><p>Proposition 25 (Convergence of the critic) Let Assumptions 4.1, 4.3, and 4.4 hold. For any</p><formula xml:id="formula_173">q ∈ (0, min {2( β − α ), α }),</formula><p>if t 0 is sufficiently large, the iterates {q t } generated by Algorithm 1 satisfy</p><formula xml:id="formula_174">E q t − q π θ t 2 p = O 1 t q .</formula><p>Proof As previously described, the iterates {q t } in Algorithm 1 evolve according to (8). We, therefore, proceed by verifying Assumptions 3.1 -3.6 in order to invoke Theorem 2.</p><p>To start with, define</p><formula xml:id="formula_175">Y . = (s, a, s ) | s ∈ S, a ∈ A, s ∈ S, p(s |s, a) &gt; 0 ,<label>(42)</label></formula><formula xml:id="formula_176">Y t . = (S t , A t , S t+1 ), P θ ((s 1 , a 1 , s 1 ), (s 2 , a 2 , s 2 )) . = 0 s 1 = s 2 µ θ (a 2 |s 2 )p(s 2 |s 2 , a 2 ) s 1 = s 2 .</formula><p>According to the action selection rule for A t specified in Algorithm 1, we have Pr(Y t+1 = y) = P θ t+1 (Y t , y), Assumption 3.1 is then fulfilled.</p><p>Assumption 3.2 is immediately implied by Assumption 4.4. In particular, for any θ, the invariant distribution of the chain induced by P θ is d µ θ (s)µ θ (a|s)p(s |s, a).</p><p>Assumption 3.3 is verified by Lemma 3. We now verify Assumption 3.4. In particular, the norm • c in Section 3 is now realized as the p norm specified by Lemma 3. We will repeatedly use the equivalence between • ∞ ,</p><p>• , and • p , i.e., there exist positive constants l ∞,p , u ∞,p , l 2,p , u 2,p such that ∀x,</p><formula xml:id="formula_177">l ∞,p x ∞ ≤ x p ≤ u ∞,p x ∞ l 2,p x ≤ x p ≤ u 2,p x .</formula><p>To verify Assumption 3.4 (i), for any y = (s 0 , a 0 , s 1 ), we have,</p><formula xml:id="formula_178">F θ (q, y) − F θ (q , y) (s, a) = q(s, a) − q (s, a), (s, a) = (s 0 , a 0 ) γ a 1 π θ (a 1 |s 1 ) (q(s 1 , a 1 ) − q (s 1 , a 1 )) , (s, a) = (s 0 , a 0 ) . Hence F θ (q, y) − F θ (q , y) ∞ ≤ q − q ∞ , implying F θ (q, y) − F θ (q , y) p ≤ u ∞,p l ∞,p q − q p .</formula><p>Assumption 3.4 (i) is then fulfilled.</p><p>To verify Assumption 3.4 (ii), for any y = (s 0 , a 0 , s 1 ), we have (F θ (q, y) − F θ (q, y)) (s, a) = 0, (s, a) = (s 0 , a 0 ) γ a 1 (π θ (a 1 |s 1 ) − π θ (a 1 |s 1 )) q(s 1 , a 1 ), (s, a) = (s 0 , a 0 ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hence</head><p>F θ (q, y) − F θ (q, y) ∞ ≤ γ|A|L π θ − θ q ∞ (using ( <ref type="formula" target="#formula_33">12</ref>)), implying</p><formula xml:id="formula_179">F θ (q, y) − F θ (q, y) p ≤ u ∞,p γ|A|L π l ∞,p l 2,p θ − θ p q p .</formula><p>Assumption 3.4 (ii) is then fulfilled.</p><p>To verify Assumption 3.4 (iii), for any y = (s 0 , a 0 , s 1 ), we have (F θ (0, y)) (s, a) = 0, (s, a) = (s 0 , a 0 ) r(s 0 , a 0 ), (s, a) = (s 0 , a 0 ) .</p><p>Then</p><formula xml:id="formula_180">F θ (0, y) p ≤ u ∞,p F θ (0, y) ∞ ≤ u ∞,p r max .</formula><p>Assumption 3.4 (iii) is the fulfilled.</p><p>To verify Assumption 3.4 (iv), we have</p><formula xml:id="formula_181">Fθ (q) − Fθ (q) = D µ θ − D µ θ r + γ D µ θ P π θ − D µ θ P π θ q − D µ θ − D µ θ q.</formula><p>Since D µ θ is Lipschitz continuous in θ (Lemma 48) and D µ θ is bounded from above, and P π θ is Lipschitz continuous in θ (see ( <ref type="formula" target="#formula_33">12</ref>)) and P π θ is bounded from the above, Lemma 44 confirms the Lipschitz continuity of Fθ , which completes the verification of Assumption 3.4 (iv).</p><p>To verify Assumption 3.4 (v), recall that Lemma 3 asserts that the fixed point of Fθ is q π θ . We have</p><formula xml:id="formula_182">q π θ − q π θ = (I − γP π θ ) −1 − (I − γP π θ ) −1 r. (<label>43</label></formula><formula xml:id="formula_183">)</formula><p>Using Lemma 49 yields</p><formula xml:id="formula_184">q π θ − q π θ p ≤ (I − γP π θ ) −1 p γP π θ − γP π θ p (I − γP π θ ) −1 p r p .</formula><p>Notice that (1) for any policy π, (I − γP π ) −1 is always well-defined; (2) (I − γP π ) −1 is continuous in π (this can be seen by writing the inverse explicitly with the adjugate matrix);</p><p>(3) the space of all policies is compact, by the extreme value theorem we conclude that sup</p><formula xml:id="formula_185">θ (I − γP π θ ) −1 p &lt; ∞,</formula><p>which together with (12) completes the verification of Assumption 3.4 (v). Assumption 3.4 (vi) follows immediately from the fact that</p><formula xml:id="formula_186">|q π θ (s, a)| ≤ r max 1 − γ .</formula><p>Assumption 3.4 (vii) follows immediately from Assumption 4.3. Assumption 3.5 is automatically fulfilled since in our setting we have t ≡ 0. Assumption 3.6 is identical to Assumption 4.1 except for (11). According to the updates of {θ t } in Algorithm 1, we have</p><formula xml:id="formula_187">θ t+1 − θ t =β t ρ t ∇ θ log π θt (A t |S t )Π(q t (S t , A t )) − λ t ∇ θ KL (U A ||π θt (•|S t )) ≤β t ρ t ∇ θ log π θt (A t |S t ) r max 1 − γ + λ t ∇ θ KL (U A ||π θt (•|S t )) .</formula><p>Assumption 4.4 and the extreme value theorem ensures that inf θ,s,a µ θ (a|s) &gt; 0.</p><p>Hence</p><formula xml:id="formula_188">ρ max . = sup θ,s,a π θ (a|s) µ θ (a|s) &lt; ∞, implying ∀t, ρ t &lt; ∞.</formula><p>Assumption 4.1 ensures</p><formula xml:id="formula_189">λ t ≤ λ.</formula><p>Lemma 50 ensures the boundedness of ∇ θ log π θt (A t |S t ) and ∇ θ KL (U A ||π θt (•|S t )) , from which it is easy to see that there exists a constant L θ such that</p><formula xml:id="formula_190">θ t+1 − θ t p ≤ β t L θ ,<label>(44)</label></formula><p>completing the verification of Assumption 3.6. With Assumptions 3.1 -3.6 satisfied, invoking Theorem 2 completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Proof of Theorem 5</head><p>Theorem 5 (Optimality of the actor) Let Assumptions 4.1 -4.4 hold. Fix</p><formula xml:id="formula_191">q ∈ 2(1 − β ), min {2( β − α ), α } .</formula><p>Let t 0 be sufficiently large. For the iterates {θ t } generated by Algorithm 1 and any t &gt; 0, if k is uniformly randomly selected from the set t 2 , t 2 + 1, . . . , t where • is the ceiling function, then</p><formula xml:id="formula_192">J(π θ k ; p 0 ) ≥ J(π * ; p 0 ) − O (λ k ) (<label>15</label></formula><formula xml:id="formula_193">)</formula><p>holds with probability at least</p><formula xml:id="formula_194">1 − O 1 t 1− β −2 λ + log 2 t t β −2 λ + 1 t q −2 λ , (<label>16</label></formula><formula xml:id="formula_195">)</formula><p>where π * can be any optimal policy.</p><p>Proof Sketch We start with a proof sketch and then proceed to the full proof. We first define a KL regularized objective</p><formula xml:id="formula_196">J η (π; p 0 ) . = J(π; p 0 ) − ηE s∼U S [KL (U A ||π(•|s))] ,</formula><p>where U X denotes the uniform distribution on the set X . Key to our proof is the following lemma:</p><p>Lemma 26 (Theorem 5.2 of Agarwal et al. ( <ref type="formula">2020</ref>)) For any state distribution d and d , if</p><formula xml:id="formula_197">∇J η (π θ ; d ) ≤ η 2|S × A| , then J(π θ ; d) ≥ J(π * ; d) − 2η 1 − γ max s d π * ,γ,d (s) d (s) ,</formula><p>where π * can be any optimal policy in (2).</p><p>The above lemma establishes the suboptimality of the stationary points of the KL regularized objective. If we can find those stationary points and decay the weight of the KL regularization (η) properly, optimality is then expected. There are, however, two caveats. First, for the above lemma to be nontrivial, we have to ensure ∀s, d (s) &gt; 0. Consequently, we cannot simply set d = d = p 0 because we do not make any assumption about p 0 . Instead, we consider an artificial state distribution p 0 such that ∀s, p 0 (s) &gt; 0 and set d = p 0 , d = p 0 . The second caveat is the following. To use the above lemma, we now have to optimize J η (π θ ; p 0 ) to find its stationary points. This objective involves state distributions p 0 and U S . We, however, only have access to samples from</p><formula xml:id="formula_198">d t (s) . = Pr S t = s|p 0 , µ θ 0 , . . . , µ θ t−1 .</formula><p>We, therefore, would need to reweight them using</p><formula xml:id="formula_199">d π θ ,γ,p 0 (s) d t (s) and U S (s) d t (s) .</formula><p>Obviously we do not know those quantities but fortunately we can bound them. As a consequence, the reweightings can be properly accounted for even without knowing them exactly (see in particular Lemma 27). With those two caveats addressed, we are now ready to present the full proof.</p><p>Proof This proof borrows ideas from <ref type="bibr" target="#b55">Wu et al. (2020)</ref> but is much more convoluted since we have the additional decaying KL regularization and our algorithm is off-policy without using density ratio for correcting the state distribution mismatch. Define the KL regularized objective</p><formula xml:id="formula_200">J η (π; p 0 ) . = J(π; p 0 ) − ηE s∼U S [KL (U A ||π(•|s))] ,<label>(45)</label></formula><p>where U X denotes the uniform distribution on the set X . Let p 0 be an arbitrary distribution on S such that p 0 (s) &gt; 0 holds for all s ∈ S. In the rest of this proof, we use as shorthand</p><formula xml:id="formula_201">J(θ) . = J(π θ ; p 0 ), (<label>46</label></formula><formula xml:id="formula_202">)</formula><formula xml:id="formula_203">J η (θ) . = J η (π θ ; p 0 ), d π,γ (s) . = d π,γ,p 0 (s),</formula><p>i.e., we work on the initial distribution p 0 (instead of p 0 ) by default. Note that the sampling is still done with respect to p 0 , p 0 is simply an auxiliary distribution used for the proof.</p><p>Similarly, the KL regularized objective is built with a uniform distribution that does not correspond to what the algorithm implements. This too is a proof artefact. Both mismatches are accounted for, in particular in Lemma 27.</p><p>According to Lemma 7 of <ref type="bibr" target="#b34">Mei et al. (2020)</ref>, J(θ) is L J -smoothness for some positive constant L J w.r.t • . Consequently, the Hessian of J(θ) is bounded from above by L J . From Lemma 50, it is easy to see the Hessian of E s∼U S [KL (U A ||π θ (•|s))] is also bounded from above by some positive constant L KL . Consequently, the Hessian of J η (θ) is bounded from above by L J + ηL KL , i.e., J η (θ) is (L J + ηL KL )-smooth. With η = λ t , Lemma 45 then implies</p><formula xml:id="formula_204">J λt (θ t+1 ) ≥J λt (θ t ) + ∇J λt (θ t ), θ t+1 − θ t − (L J + λ t L KL ) θ t+1 − θ t 2 (47) ≥J λt (θ t ) + ∇J λt (θ t ), θ t+1 − θ t M 1 −L J θ t+1 − θ t 2 M 2 , where L J . = L J + λL KL . Using (44) to bound M 2 yields M 2 ≤ 1 l 2,p β 2 t L 2 θ .</formula><p>To bound M 1 , let</p><formula xml:id="formula_205">Y t . = (S t , A t ). (<label>48</label></formula><formula xml:id="formula_206">)</formula><p>Here different from ( <ref type="formula" target="#formula_175">42</ref>), we redefine Y t to consider only state action pairs to ease presentation. For y = (s, a), we define</p><formula xml:id="formula_207">Λ(θ, y, η) . = π θ (a|s) µ θ (a|s) ∇ log π θ (a|s)q π θ (s, a) + η |A| ā ∇ log π θ (ā|s) (49) Λ(θ, η) . = s,a d µ θ (s)µ θ (a|s)Λ(θ, y, η).</formula><p>Then we have</p><formula xml:id="formula_208">M 1 = ∇J λt (θ t ), θ t+1 − θ t =β t ∇J λt (θ t ), ρ t ∇ log π θt (A t |S t )Π(q t (S t , A t )) − λ t ∇KL (U A ||π θt (•|S t )) =β t ∇J λt (θ t ), ρ t ∇ log π θt (A t |S t )Π(q t (S t , A t )) + λ t |A| a ∇ log π θt (a|S t ) =β t ∇J λt (θ t ), ρ t ∇ log π θt (A t |S t )q π θ t (S t , A t ) + λ t |A| a ∇ log π θt (a|S t ) + β t ∇J λt (θ t ), ρ t ∇ log π θt (A t |S t ) Π (q t (S t , A t )) − q π θ t (S t , A t ) =β t ∇J λt (θ t ), Λ(θ t , λ t ) M 11 +β t ∇J λt (θ t ), Λ(θ t , Y t , λ t ) − Λ(θ t , λ t ) M 12 + β t ∇J λt (θ t ), ρ t ∇ log π θt (A t |S t ) Π (q t (S t , A t )) − q π θ t (S t , A t ) M 13</formula><p>.</p><formula xml:id="formula_209">To bound M 12 , define Λ (θ, y, η) . = ∇J η (θ), Λ(θ, y, η) − Λ(θ, η) .<label>(50)</label></formula><p>Assumption 4.4 and Lemma 1 assert that there exist constants C 0 &gt; 0 and τ ∈ (0, 1), independent of θ, such that for any n &gt; 0, sup s,a,θ s ,a</p><formula xml:id="formula_210">P n µ θ ((s, a), (s , a )) − d µ θ (s )µ θ (a |s ) ≤ C 0 τ n ,</formula><p>which allows us to define</p><formula xml:id="formula_211">τ βt . = min    n | sup s,a,θ s ,a P n µ θ ((s, a), (s , a )) − d µ θ (s )µ θ (a |s ) ≤ β t    . (<label>51</label></formula><formula xml:id="formula_212">)</formula><p>We then decompose M 12 as</p><formula xml:id="formula_213">M 12 =Λ (θ t , Y t , λ t ) = Λ (θ t , Y t , λ t ) − Λ (θ t−τ β t , Y t , λ t ) M 121 + Λ (θ t−τ β t , Y t , λ t ) − Λ (θ t−τ β t , Ỹt , λ t ) M 122 + Λ (θ t−τ β t , Ỹt , λ t ) M 123</formula><p>.</p><p>Here Ỹt is an auxiliary chain akin to <ref type="bibr" target="#b63">Zou et al. (2019)</ref> and the one used in the proof of Theorem 2 in A.2 (for β t instead of α t ). Before time t − τ βt − 1, Ỹt is exactly the same as {Y t }. After time t − τ βt − 1, Ỹt evolves according to the fixed behavior policy µ θ t−τ β t while {Y t } evolves according to the changing behavior policy µ θ t−τ β t , µ θ t−τ β t +1 , . . . .</p><formula xml:id="formula_214">Ỹt : • • • → Y t−τ β t −1 → µ θ t−τ β t Y t−τ β t → µ θ t−τ β t Ỹt−τ β t +1 → µ θ t−τ β t Ỹt−τ β t +2 → . . . (<label>52</label></formula><formula xml:id="formula_215">)</formula><formula xml:id="formula_216">{Y t } : • • • → Y t−τ β t −1 → µ θ t−τ β t Y t−τ β t → µ θ t−τ β t +1 Y t−τ β t +1 → µ θ t−τ β t +2 Y t−τ β t +2 → . . . .</formula><p>Let us proceed to bounding each term defined above:</p><p>Lemma 27 (Bound of M 11 ) There exists a constant χ 11 &gt; 0 such that,</p><formula xml:id="formula_217">M 11 ≥ χ 11 ∇J λt (θ t ) 2 .</formula><p>The proof of Lemma 27 is provided in Section E.13.</p><p>Lemma 28 (Bound of M 121 ) There exist constants L * Λ &gt; 0 such that</p><formula xml:id="formula_218">M 121 ≤ L * Λ L θ l 2,p β t−τ β t ,t−1 .</formula><p>The proof of Lemma 28 is provided in Section E.14</p><p>Lemma 29 (Bound of M 122 ) There exists a constant U * Λ &gt; 0 such that</p><formula xml:id="formula_219">E [M 122 ] ≤ U * Λ |S||A|L µ L θ t−1 j=t−τ β t β t−τ β t ,j .</formula><p>The proof of Lemma 29 is provided in Section E.15</p><p>Lemma 30 (Bound of M 123 )</p><formula xml:id="formula_220">E [M 123 ] ≤ U * Λ β t .</formula><p>The proof of Lemma 30 is provided in Section E.16.</p><p>Lemma 31 (Bound of M 13 ) There exists a constant ρ max &gt; 0 such that</p><formula xml:id="formula_221">E [M 13 ] ≤2ρ max |S × A| E q t − q π θ t 2 ∞ E ∇J λt (θ t ) 2 .</formula><p>The proof of Lemma 31 is provided in Section E.17.</p><p>We now assemble the bounds of M 11 , M 121 , M 122 , M 123 , M 12 and M 2 back to (47). Similar to Lemma 11, it is easy to see for sufficiently large t 0 ,</p><formula xml:id="formula_222">τ βt = O (log(t + t 0 )) , β t−τ β t ,t−1 = O log(t + t 0 ) (t + t 0 ) β , t−1 j=t−τ β t β t−τ β t ,j = O log 2 (t + t 0 ) (t + t 0 ) β .</formula><p>Hence if t 0 is sufficiently large, there exist positive constants χ 12 , χ 13 , χ 2 such that</p><formula xml:id="formula_223">E [M 121 + M 122 + M 123 ] ≥ −χ 12 log 2 (t + t 0 ) (t + t 0 ) β , E [M 13 ] ≥ −χ 13 E q t − q π θ t 2 p E ∇J λt (θ t ) 2 , E [M 2 ] ≤ β t χ 2 1 (t + t 0 ) β ,</formula><p>where the p norm is defined by Proposition 4. Then, from (47), we get</p><formula xml:id="formula_224">E [J λt (θ t+1 )] ≥E [J λt (θ t )] + β t χ 11 E ∇J λt (θ t ) 2 (53) − β t χ 12 log 2 (t + t 0 ) (t + t 0 ) β − β t χ 13 E q t − q π θ t 2 p E ∇J λt (θ t ) 2 − β t χ 2 1 (t + t 0 ) β .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rearranging terms yields</head><formula xml:id="formula_225">E ∇J λt (θ t ) 2 ≤ 1 χ 11 1 β t (E [J λt (θ t+1 )] − E [J λt (θ t )]) + χ 13 χ 11 E q t − q π θ t 2 p E ∇J λt (θ t ) 2 + χ 12 + χ 2 χ 11 log 2 (t + t 0 ) (t + t 0 ) β . Defining χ 3 . = 1 χ 11 , χ 4 . = χ 13 χ 11 , χ 5 . = χ 12 + χ 2 χ 11</formula><p>and telescoping the above inequality from t 2 to t yields</p><formula xml:id="formula_226">t k= t 2 E ∇J λ k (θ k ) 2 ≤χ 3 t k= t 2 1 β k (E [J λ k (θ k+1 )] − E [J λ k (θ k )]) (54) + χ 4 t k= t 2 q k − q π θ k 2 p E ∇J λ k (θ k ) 2 + χ 5 t k= t 2 log 2 (k + t 0 ) (k + t 0 ) β .</formula><p>We now bound the right terms of the above inequality.</p><p>Lemma 32 There exists a constant U J,λ such that for all t,</p><formula xml:id="formula_227">|E [J λt (θ t )]| ≤ U J,λ , |E [J λt (θ t+1 )]| ≤ U J,λ .</formula><p>The proof of Lemma 32 is provided in Section E.18.</p><p>Lemma 33</p><formula xml:id="formula_228">E   t k= t 2 1 β k (J λ k (θ k+1 ) − J λ k (θ k ))   ≤ 2U J,λ β (t + t 0 ) β</formula><p>The proof of Lemma 33 is provided in Section E.19. Using Lemma 33, the Cauchy-Schwarz inequality, and</p><formula xml:id="formula_229">t k= t 2 log 2 (k + t 0 ) (k + t 0 ) β ≤ log 2 (t + t 0 ) t x= t 2 −1 1 (x + t 0 ) β dx ≤ log 2 (t + t 0 ) 1 − β (t + t 0 ) 1− β</formula><p>to bound the RHS of (54) yields</p><formula xml:id="formula_230">t k= t 2 E ∇J λ k (θ k ) 2 ≤ 2χ 3 U J,λ β (t + t 0 ) β + χ 5 log 2 (t + t 0 ) 1 − β (t + t 0 ) 1− β + χ 4 t k= t 2 E q k − q π θ k 2 p t k= t 2 E ∇J λ k (θ k ) 2 . Multiplying 1 t− t 2 +1 in both sides yields t k= t 2 E ∇J λ k (θ k ) 2 t − t 2 + 1 zt ≤ 2χ 3 U J,λ β (t + t 0 ) β t − t 2 + 1 + χ 5 log 2 (t + t 0 ) 1 − β (t + t 0 ) 1− β t − t 2 + 1 + χ 4 t k= t 2 E q k − q π θ k 2 p t − t 2 + 1 et t k= t 2 E ∇J λ k (θ k ) 2 t − t 2 + 1</formula><p>.</p><p>It is then easy to see that there exist positive constants E 1 , E 2 , E 3 , E 4 such that</p><formula xml:id="formula_231">z t ≤ E 1 t 1− β + E 2 log 2 t t β + 2E 3 √ e t √ z t =⇒ ( √ z t − E 3 √ e t ) 2 ≤ E 1 t 1− β + E 2 log 2 t t β + E 2 3 e t =⇒ √ z t − E 3 √ e t ≤ E 1 t 1− β + E 2 log 2 t t β + E 2 3 e t ≤ E 1 t 1− β + E 2 log 2 t t β + E 3 √ e t =⇒ z t ≤ 2E 1 t 1− β + 2E 2 log 2 t t β + 8E 2 3 e t .</formula><p>Proposition 4 implies that there exists a constant E 5 &gt; 0 such that</p><formula xml:id="formula_232">e t = t k= t 2 E 5 k q t − t 2 + 1 ≤ E 5 t 1− q (1 − q )(t − t 2 + 1)</formula><p>.</p><p>It is then easy to see</p><formula xml:id="formula_233">e t = O 1 t q , implying t k= t 2 E ∇J λ k (θ k ) 2 t − t 2 + 1 = O 1 t 1− β + log 2 t t β + 1 t q . (<label>55</label></formula><formula xml:id="formula_234">)</formula><p>The above inequality establishes the convergence to stationary points, with which we now study the optimality of the sequence {θ t }. We rely on the following lemma.</p><p>Lemma 34 (Theorem 5.2 of Agarwal et al. ( <ref type="formula">2020</ref>)) For any state distribution d and d , if</p><formula xml:id="formula_235">∇J η (θ; d ) ≤ η 2|S × A| , then J(θ; d) ≥ J(π * ; d) − 2η 1 − γ max s d π * ,γ,d (s) d (s) ,</formula><p>where π * can be any optimal policy in (2).</p><p>Obviously, for Lemma to be nontrivial, we have to ensure d (s) &gt; 0. Fix any t &gt; 0. Then select a k uniformly randomly from t 2 , t 2 + 1, . . . , t − 1, t . Now the random variable ∇J λ k (θ k ) has randomness from both the random selection of k and the learning of θ k . Using Markov's inequality yields</p><formula xml:id="formula_236">Pr ∇J λ k (θ k ) ≤ λ t 2|S × A| = Pr ∇J λ k (θ k ) 2 ≤ λ 2 t 4|S × A| 2 ≥1 − 4|S × A| 2 λ 2 t E ∇J λ k (θ k ) 2 =1 − 4|S × A| 2 λ 2 t E E ∇J λ k (θ k ) 2 | k =1 − 4|S × A| 2 λ 2 t t i= t 2 E ∇J λ k (θ k ) 2 | k = i t − t 2 + 1 ≥1 − 1 λ 2 t O 1 t 1− β + log 2 t t β + 1 t q (Using (55)) ≥1 − C t ,</formula><p>where</p><formula xml:id="formula_237">C t . = O 1 t 1− β −2 λ + log 2 t t β −2 λ + 1 t q −2 λ .</formula><p>Since λ k ≥ λ t , we have</p><formula xml:id="formula_238">∇J λ k (θ k ) ≤ λ t 2|S × A| =⇒ ∇J λ k (θ k ) ≤ λ k 2|S × A| . Consequently, Pr ∇J λ k (θ k ) ≤ λ k 2|S × A| ≥ Pr ∇J λ k (θ k ) ≤ λ t 2|S × A| ≥ 1 − C t .</formula><p>Let d = p 0 , d = p 0 , η = λ k in Lemma 34 and recall (46), we get</p><formula xml:id="formula_239">J(θ k ; p 0 ) ≥ J(π * ; p 0 ) − 2 λ k 1 − γ max s d π * ,γ,p 0 (s) p 0 (s) .</formula><p>holds with at least probability</p><formula xml:id="formula_240">1 − C t ,</formula><p>which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proofs of Section 5</head><p>C.1 Proof of Proposition 6</p><p>Proposition 35 (Convergence of the critic) Let Assumptions 4.1, 4.3, and 4.4 hold. Then there exists an p norm such that for any</p><formula xml:id="formula_241">q ∈ (0, min {2( β − α ), α }),</formula><p>if t 0 is sufficiently large, the iterates {q t } generated by Algorithm 1 satisfy</p><formula xml:id="formula_242">E q t − qπ θ t ,λt 2 p = O 1 t q .</formula><p>Proof The proof is similar to the proof of Proposition 4. To start with, define</p><formula xml:id="formula_243">Y . = (s, a, s ) | s ∈ S, a ∈ A, s ∈ S, p(s |s, a) &gt; 0 , Y t . = (S t , A t , S t+1 ), P ζ ((s 1 , a 1 , s 1 ), (s 2 , a 2 , s 2 )) . = 0 s 1 = s 2 µ θ (a 2 |s 2 )p(s 2 |s 2 , a 2 ) s 1 = s 2 .</formula><p>According to the action selection rule for A t specified in Algorithm 2, we have</p><formula xml:id="formula_244">Pr(Y t+1 = y) = P ζ t+1 (Y t , y), Assumption 3.1 is then fulfilled.</formula><p>Assumption 3.2 is immediately implied by Assumption 4.4. In particular, for any ζ, the invariant distribution of the chain induced by P ζ is d µ θ (s)µ θ (a|s)p(s |s, a).</p><p>To verify Assumption 3.3, first notice that</p><formula xml:id="formula_245">Fζ (q) = s,a,s d µ θ (s)µ θ (a|s)p(s |s, a)F ζ (q, s, a, s ) = D µ θ (r + γP π θ (q − η log π θ ) − q) + q = (I − D µ θ (I − γP π θ ))q + D µ θ (r − ηγP π θ log π θ ),</formula><p>where π θ denotes a vector in R |S×A| whose (s, a)-indexed element is π θ (a|s) and log π θ is the elementwise logarithm of π θ . Then, we have</p><formula xml:id="formula_246">Fζ (q) − Fζ (q ) = A θ (q − q ),</formula><p>where A θ is defined as in (41):</p><formula xml:id="formula_247">A θ . = I − D µ θ (I − γP π θ ).</formula><p>According to the proof of Lemma 3, there exist a κ ∈ (0, 1) and an p norm such that ∀x,</p><formula xml:id="formula_248">A θ x ≤ κ x p , implying Fζ (q) − Fζ (q ) p ≤ κ q − q p .</formula><p>Further, Fζ (q) = q ⇐⇒ r + γP π θ (q − η log π θ ) − q = 0 ⇐⇒ q = qπ θ ,η (Lemma 1 of <ref type="bibr" target="#b16">Haarnoja et al. (2018)</ref>), which completes the verification of Assumption 3.3. We now verify Assumption 3.4. In particular, the norm • c in Section 3 is now realized as the p norm above.</p><p>To verify Assumption 3.4 (i), for any y = (s 0 , a 0 , s 1 ), we have</p><formula xml:id="formula_249">F ζ (q, y) − F ζ (q , y) (s, a) = q(s, a) − q (s, a), (s, a) = (s 0 , a 0 ) γ a 1 π θ (a 1 |s 1 ) (q(s 1 , a 1 ) − q (s 1 , a 1 )) , (s, a) = (s 0 , a 0 ) . Hence F ζ (q, y) − F ζ (q , y) ∞ ≤ q − q ∞ , implying F ζ (q, y) − F ζ (q , y) p ≤ u ∞,p l ∞,p q − q p .</formula><p>Assumption 3.4 (i) is then fulfilled.</p><p>To verify Assumption 3.4 (ii), for any y = (s 0 , a 0 , s 1 ), we have</p><formula xml:id="formula_250">(F ζt (q, y) − F ζ k (q, y)) (s, a) = 0, (s, a) = (s 0 , a 0 ) γ a 1 (π θt (a 1 |s 1 ) − π θ k (a 1 |s 1 )) q(s 1 , a 1 ) + λ t H (π θt (•|s 1 )) − λ k H (π θ k (•|s 1 )) , (s, a) = (s 0 , a 0 ) . Since |λ t H (π θt (•|s 1 )) − λ k H (π θ k (•|s 1 ))| ≤|λ t − λ k |H (π θt (•|s 1 )) + λ k |H (π θt (•|s 1 )) − H (π θ k (•|s 1 ))| ≤|λ t − λ k | log |A| + λ k log |A| + e −1 θ t − θ k (Lemma 50) ≤ log |A| + λ log |A| + λe −1 ζ t − ζ k ,</formula><p>we have</p><formula xml:id="formula_251">F ζ k (q, y) − F ζt (q, y) ∞ ≤γ|A|L π θ t − θ k q ∞ + log |A| + λ log |A| + λe −1 ζ t − ζ k ≤γ|A|L π ζ t − ζ k q ∞ + log |A| + λ log |A| + λe −1 ζ t − ζ k ≤ γ|A|L π l 2,p l ∞,p ζ t − ζ k p q p + (log |A| + λ log |A| + λe −1 ) l 2,p ζ t − ζ k p</formula><p>Assumption 3.4 (ii) is then fulfilled.</p><p>To verify Assumption 3.4 (iii), for any y = (s 0 , a 0 , s 1 ), we have (F ζt (0, y)) (s, a) = 0, (s, a) = (s 0 , a 0 ) r(s 0 , a 0 ) + γλ t H (π θt (•|s 1 )) , (s, a) = (s 0 , a 0 ) .</p><p>Then</p><formula xml:id="formula_252">F ζt (0, y) p ≤ u ∞,p F θ (0, y) ∞ ≤ u ∞,p (r max + γλ log |A|) .</formula><p>Assumption 3.4 (iii) is then fulfilled.</p><p>To verify Assumption 3.4 (iv), we have</p><formula xml:id="formula_253">Fζt (q) − Fζ k (q) = Fθt (q) − Fθ k (q) − γλ t D µ θ t P π θ t log π θt + γλ k D µ θ k P π θ k log π θ k ,</formula><p>where Fθ is defined in ( <ref type="formula">14</ref>). In the proof of Proposition 4, we already show that there exist constants C 1 and C 2 such that</p><formula xml:id="formula_254">Fθt (q) − Fθ k (q) p ≤ C 1 θ t − θ k p q p + C 2 ≤ C 1 ζ t − ζ k p q p + C 2 .</formula><p>We now bound the remaining parts</p><formula xml:id="formula_255">−γλ t D µ θ t P π θ t log π θt +γλ k D µ θ k P π θ k log π θ k . First, notice that (P π θ log π θ ) (s, a) = s p(s |s, a) a π θ (a |s ) log π θ (a |s ) = − s p(s |s, a)H π θ (•|s ) .</formula><p>It is easy to see H (π θ (•|s )) is Lipschitz continuous in θ (Lemma 50) and is bounded by log |A|. We, therefore, conclude that P π θ log π θ is Lipschitz continuous in θ and is bounded from above. Since D µ θ is also Lipschitz continuous in θ (Lemma 48) and is bounded from above, Lemma 44 asserts that there exists constants C 3 and C 4 such that</p><formula xml:id="formula_256">D µ θ P π θ log π θ ≤ C 3 , D µ θ P π θ log π θ − D µ θ P π θ log π θ ≤ C 4 θ − θ , implying λ k D µ θ k P π θ k log π θ k − λ t D µ θ t P π θ t log π θt ≤ λ k − λ t D µ θ k P π θ k log π θ k + λ t D µ θ k P π θ k log π θ k − D µ θ t P π θ t log π θt ≤C 3 λ k − λ t + λC 4 θ t − θ k ≤(C 3 + λC 4 ) ζ t − ζ k ,</formula><p>which completes the verification of Assumption 3.4 (iv).</p><p>To verify Assumptions 3.4 (v), it suffices to show that</p><formula xml:id="formula_257">qπ θ t ,λt − qπ θ k ,λ k p ≤ C 5 ζ t − ζ k p</formula><p>holds for some positive constant C 5 . According to (19), it suffices to show that for some positive constant C 6 ,</p><formula xml:id="formula_258">ṽπ θ t ,λt − ṽπ θ k ,λ k p ≤ C 6 ζ t − ζ k p .</formula><p>Recall by definition</p><formula xml:id="formula_259">ṽπ θ t ,λt (s) =v π θ t (s) + λ t E π θ t ∞ i=0 γ i H (π θt (•|S t+i )) | S t = s H θ t (s) .<label>(56)</label></formula><p>Clearly,</p><formula xml:id="formula_260">|H θ (s)| ≤ log |A| 1 − γ .</formula><p>We now show that H θ (s) is Lipschitz continuous in θ. Let p 0,s denote the distribution on S such that all its mass concentrates on the state s, i.e., p 0,s (s) = 1. We can then express</p><formula xml:id="formula_261">H θ (s) as H θ (s) = 1 1 − γ s d π θ ,γ,p 0,s (s)H (π θ (•|s)) .</formula><p>It is easy to see that</p><formula xml:id="formula_262">d π θ ,γ,p 0,s (s ) =(1 − γ) ∞ t=0 γ t Pr S t = s |S 0 ∼ p 0,s =(1 − γ)p 0,s (s ) + (1 − γ) ∞ t=1 γ t Pr S t = s |S 0 ∼ p 0,s =(1 − γ)p 0,s (s ) + (1 − γ) ∞ t=0 γ t+1 Pr S t+1 = s |S 0 ∼ p 0,s =(1 − γ)p 0,s (s ) + γ(1 − γ) ∞ t=0 γ t s Pr S t = s |S 0 ∼ p 0,s Pr S t+1 = s |S t = s =(1 − γ)p 0,s (s ) + γ s Pr S t+1 = s |S t = s d π θ ,γ,p 0,s<label>(s ).</label></formula><p>In a matrix form, we have</p><formula xml:id="formula_263">d π θ ,γ,p 0,s =(1 − γ)p 0,s + γP π θ d π θ ,γ,p 0,s =⇒ d π θ ,γ,p 0,s =(1 − γ)(I − γP π θ ) −1 p 0,s ,</formula><p>where we have abused the notation a bit to use P π θ to also denote the state transition matrix under the policy π θ . Similar to (43), we conclude that d π θ ,γ,p 0,s is Lipschitz continuous in θ.</p><p>Lemma 50 confirms that H (π θ (•|s)) is Lipschitz continuous in θ. Hence Lemma 44 asserts that H θ (s) is Lipschitz continuous in θ, i.e., there exists a positive constant such that</p><formula xml:id="formula_264">|H θ (s) − H θ (s)| ≤ C 7 θ − θ .<label>(57)</label></formula><p>Similar to (43), we can also show that there exists a constant C 8 such that</p><formula xml:id="formula_265">v π θ (s) − v π θ (s) ≤ C 8 θ − θ .<label>(58)</label></formula><p>We, therefore, have</p><formula xml:id="formula_266">ṽπ θ t ,λt (s) − ṽπ θ k ,λ k (s) ≤ v π θ t (s) − v π θ k (s) + |λ t − λ k ||H θt (s)| + λ k |H θt (s) − H θ k (s)| ≤C 8 θ t − θ k + |λ t − λ k | log |A| 1 − γ + λC 7 θ t − θ k ≤ C 8 + log |A| 1 − γ + λC 7 ζ t − ζ k ,</formula><p>which completes the verification of Assumption 3.4 (v).</p><p>For Assumption 3.4 (vi), first notice that the soft action value function qπ,η can be regarded as the normal action value function q π w.r.t. to the reward</p><formula xml:id="formula_267">r(s, a) + η s p(s |s, a)H π(•|s ) Then it is easy to see qπ θ t ,λ t (s, a) ≤ U J . = r max + λ log |A| 1 − γ ,<label>(59)</label></formula><p>which completes the verification of Assumption 3.4 (vi).</p><p>For Assumption 3.4 (vii), we have</p><formula xml:id="formula_268">P ζt (y, y ) − P ζ k (y, y ) ≤ C 9 θ t − θ k ≤ C 9 ζ t − ζ k ,</formula><p>where the existence of the positive constant C 9 is ensured by Assumption 4.3.</p><p>Assumption 3.5 is automatically fulfilled since in our setting we have t ≡ 0.</p><p>Assumption 3.6 is automatically implied by Assumption 4.1 except for (11). According to the updates of {θ t } in Algorithm 2, we have <ref type="formula" target="#formula_267">59</ref>) and Lemma 50)</p><formula xml:id="formula_269">θ t+1 − θ t =β t a π θt (a|S t )∇ θ log π θt (a|S t ) Π q t (S t , a) − λ t log θt (a|S t ) =β t a ∇π θt (a|S t ) Π q t (S t , a) − λ t log π θt (a|S t ) ≤β t a ∇π θt (a|S t )Π q t (S t , a) + β t λ t a ∇π θt (a|S t ) log π θt (a|S t ) ≤β t 2U J + β t λ t a ∇π θt (a|S t ) log π θt (a|S t ) ((</formula><formula xml:id="formula_270">=β t 2U J + β t λ t a ∇π θt (a|S t ) log π θt (a|S t ) + π θt (a|S t )∇ log π θt (a|S t ) =β t 2U J + β t λ t ∇H (π θt (•|S t )) ≤β t 2U J + β t λ t log |A| + e −1 (Lemma 50) ≤β t 2U J + λ log |A| + λe −1 L θ .<label>(60)</label></formula><p>Further,</p><formula xml:id="formula_271">|λ t+1 − λ t | =λ 1 (t + t 0 ) λ − 1 (t + t 0 + 1) λ =λ (t + t 0 + 1) λ − (t + t 0 ) λ (t + t 0 ) λ (t + t 0 + 1) λ =λ (t + t 0 + 1) λ (t + t 0 ) 1− λ − (t + t 0 ) (t + t 0 )(t + t 0 + 1) λ ≤λ (t + t 0 + 1) λ (t + t 0 + 1) 1− λ − (t + t 0 ) (t + t 0 )(t + t 0 ) λ = λ (t + t 0 ) 1+ λ =β t λ(t + t 0 ) β β(t + t 0 ) 1+ λ ≤β t λ β .</formula><p>We, therefore, conclude that that there exists a constant L ζ such that</p><formula xml:id="formula_272">ζ t+1 − ζ t p ≤ β t L ζ ,</formula><p>which completes the verification of Assumption 3.6. With Assumptions 3.1 -3.6 satisfied, invoking Theorem 2 completes the proof.</p><p>C.2 Proof of Theorem 7</p><p>Theorem 7 (Convergence of the actor) Let Assumptions 4.1, 4.3, and 4.4 hold. Fix any</p><formula xml:id="formula_273">q ∈ 0, min {2( β − α ), α } .</formula><p>Let t 0 be sufficiently large. Fix any 0 &gt; 0 and any state distribution p 0 . For the iterates {θ t } generated by Algorithm 2 and any t &gt; 0, if k is uniformly randomly selected from the set t 2 , t 2 + 1, . . . , t , then</p><formula xml:id="formula_274">∇ Jλ k (π θ k ; p 0 ) 2 ≤ 1 k 0</formula><p>holds with at least probability</p><formula xml:id="formula_275">1 − O 1 t 1− β − 0 + log 2 t t β − 0 + 1 t q − 0 .</formula><p>Proof In this proof, we use as shorthand</p><formula xml:id="formula_276">J(θ) . = J(π θ ; p 0 ), Jη (θ) . = Jη (π θ ; p 0 ), d π,γ (s) . = d π,γ,p 0 (s),</formula><p>i.e., we work on the initial distribution p 0 (instead of p 0 ). Recall the entropy regularized discounted total rewards is defined as</p><formula xml:id="formula_277">Jη (π θ ; p 0 ) = J(π θ ; p 0 ) + η 1 1 − γ s d π θ ,γ,p 0 (s)H (π θ (•|s)) H(π θ )</formula><p>.</p><p>According to Lemma 7 of <ref type="bibr" target="#b34">Mei et al. (2020)</ref>, J(θ) is L J -smooth for some positive constant L J w.r.t • . According to Lemma 14 of <ref type="bibr" target="#b34">Mei et al. (2020)</ref>,</p><formula xml:id="formula_278">H(π θ ) is L H -smooth for some positive constant L H w.r.t. • . Hence Jη (θ) is (L J +ηL H )-smooth. With η = λ t , Lemma 45 then implies Jλt (θ t+1 ) ≥ Jλt (θ t ) + ∇ Jλt (θ t ), θ t+1 − θ t − (L J + λ t L H ) θ t+1 − θ t 2 ≥ Jλt (θ t ) + ∇ Jλt (θ t ), θ t+1 − θ t M1 − L J θ t+1 − θ t 2 M2</formula><p>, where L</p><formula xml:id="formula_279">J . = L J + λL H . Using (60) to bound M2 yields M2 ≤ 1 l 2,p β 2 t L 2 θ .</formula><p>To bound M1 , we reuse the Y t and Ỹt defined in ( <ref type="formula" target="#formula_205">48</ref>) and ( <ref type="formula" target="#formula_214">52</ref>). For any s, we define Λ 1 (θ, s, η) . = a π θ (s, a)∇ log π θ (a|s) (q π θ ,η (s, a) − η log π θ (a|s)) , .</p><p>To bound M12 , define</p><formula xml:id="formula_281">Λ 1 (θ, s, η) . = ∇ Jη (θ), Λ 1 (θ, s, η) − Λ1 (θ, η) .<label>(62)</label></formula><p>We then decompose M12 as</p><formula xml:id="formula_282">M12 = Λ 1 (θ t , S t , λ t ) = Λ 1 (θ t , S t , λ t ) − Λ 1 (θ t−τ β t , S t , λ t ) M121 + Λ 1 (θ t−τ β t , S t , λ t ) − Λ 1 (θ t−τ β t , St , λ t ) M122 + Λ 1 (θ t−τ β t , St , λ t ) M123 ,</formula><p>where we recall that St is defined as part of Ỹt in (52). Let us proceed to bounding each term defined above.</p><p>Lemma 36 (Bound of M11 ) There exists a constant χ 11 &gt; 0 such that,</p><formula xml:id="formula_283">M11 ≥ χ 11 ∇ Jλt (θ t ) 2 .</formula><p>The proof of Lemma 36 is provided in Section E.20.</p><p>Lemma 37 (Bound of M121 ) There exist constants</p><formula xml:id="formula_284">L * Λ 1 &gt; 0 such that M121 ≤ L * Λ 1 L θ β t−τ β t ,t−1 ,</formula><p>where L θ is defined in (60).</p><p>The proof of Lemma 37 is provided in Section E.21</p><p>Lemma 38 (Bound of M122 ) There exists a constant U *</p><formula xml:id="formula_285">Λ 1 &gt; 0 such that E M122 ≤ U * Λ |S||A|L µ L θ t−1 j=t−τ β t β t−τ β t ,j .</formula><p>The proof of Lemma 38 is identical to the proof of Lemma 29 in Section E.15 up to change of notations and is thus omitted.</p><p>Lemma 39 (Bound of M123 )</p><formula xml:id="formula_286">E M123 ≤ U * Λ 1 β t .</formula><p>The proof of Lemma 39 is identical to the proof of Lemma 30 in Section E.16 up to change of notations and is thus omitted.</p><p>Lemma 40 (Bound of M13 ) The exists a constant χ 13 &gt; 0 such that</p><formula xml:id="formula_287">E M13 ≤χ 13 E q t − qπ θ t ,λ t 2 ∞ E ∇ Jλt (θ t ) 2 .</formula><p>The proof of Lemma 40 is identical to the proof Lemma 31 in Section E.17 up to change of notations and is thus omitted. Now using exactly the same routine as the proof of Theorem 5 in Section B.3, we obtain that there exists some positive constants χ 3 , χ 4 and χ 5 such that</p><formula xml:id="formula_288">t k= t 2 E ∇ Jλ k (θ k ) 2 ≤χ 3 t k= t 2 1 β k E Jλ k (θ k+1 ) − E Jλ k (θ k ) + χ 4 t k= t 2 E q k − qπ θ k ,λ k 2 p E ∇ Jλ k (θ k ) 2 + χ 5 t k= t 2 log 2 (k + t 0 ) (k + t 0 ) β ,</formula><p>where the p norm is defined in Proposition 6. To continue mimicing the proof of Theorem 5, we need to establish counterparts of Lemmas 32 and 33 to bound the first summation in the of the above inequality. The counterpart of Lemma 32 is trivial since by the definition of Jη (θ) we have ∀t, θ Jλt (θ) ≤ U J , where U J is defined in (59). This simplification is because that H (π(•|s)) is always bounded by log |A| but KL (U A ||π(•|s)) can be unbounded. Then we have Lemma 41</p><formula xml:id="formula_289">t k= t 2 1 β k Jλ k (θ k+1 ) − Jλ k (θ k ) ≤ 3λβ log |A| 1 − γ + 2U J β (t + t 0 ) β</formula><p>The proof of Lemma 41 is provided in Section E.22. Using the same routine as the proof of Theorem 5 yields</p><formula xml:id="formula_290">t k= t 2 E ∇ Jλ k (θ k ) 2 t − t 2 + 1 = O 1 t 1− β + log 2 t t β + 1 t q Ct .<label>(63)</label></formula><p>We now analyze the above equality from a probabilistic perspective. Consider a positive nonincreasing sequence {δ t } to be tuned. Fix any t &gt; 0. Then select a k uniformly randomly from t 2 , t 2 + 1, . . . , t − 1, t . Now the random variable ∇ Jλ k (θ k ) has randomness from both the random selection of k and the learning of θ k . Using Markov's inequality yields</p><formula xml:id="formula_291">Pr ∇ Jλ k (θ k ) 2 ≤ δ t ≥1 − 1 δ t E ∇ Jλ k (θ k ) 2 =1 − 1 δ t E E ∇ Jλ k (θ k ) 2 | k =1 − 1 δ t t i= t 2 E ∇ Jλ k (θ k ) 2 | k = i t − t 2 + 1 ≥1 − 1 δ t Ct . (Using (63)) Since δ k ≥ δ t , we have ∇ Jλ k (θ k ) 2 ≤ δ t =⇒ ∇ Jλ k (θ k ) 2 ≤ δ k .</formula><p>Consequently, </p><formula xml:id="formula_292">Pr ∇ Jλ k (θ k ) 2 ≤ δ k ≥ Pr ∇ Jλ k (θ k ) 2 ≤ δ t ≥ 1 − 1 δ t Ct .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Letting</head><formula xml:id="formula_293">J(π θ k ; p 0 ) ≥ J(π * ; p 0 ) − O (λ k ) − O δ k λ k (min s,a π θ k (a|s)) 2</formula><p>holds with at least probability</p><formula xml:id="formula_294">1 − O t −(1− β ) + t − β log 2 t + t − q δ t ,</formula><p>where π * can be any optimal policy in (2).</p><p>Proof Fix any state distribution p 0 satisfying ∀s, p 0 (s) &gt; 0. Then, from the proof of Theorem 7 in Section C.2, we conclude that</p><formula xml:id="formula_295">∇ Jλ k (π θ k ; p 0 ) 2 ≤ δ k<label>(64)</label></formula><p>holds with probability at least</p><formula xml:id="formula_296">1 − Ct δ t .</formula><p>With the convergence to stationary points established in (64), we now use the following lemma from <ref type="bibr" target="#b34">Mei et al. (2020)</ref> to study the optimality. Let π * ,η be the optimal policy w.r. </p><p>Putting ( <ref type="formula">66</ref>) and (67) back to (65) yields</p><formula xml:id="formula_298">J(π θ k ; p 0 ) ≥ J(π * ; p 0 ) − 2λ k log |A| 1 − γ − (1 − γ)|S|δ k 2λ k (min s p 0 (s) min s,a π θ k (a|s)) 2 ,</formula><p>which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Technical Lemmas</head><p>Lemma 44 Let f 1 (x), f 2 (x) be two Lipschitz continuous functions with Lipschitz constants</p><formula xml:id="formula_299">L 1 , L 2 . Assume f 1 (x) ≤ U 1 , f 2 (x) ≤ U 2 , then L 1 U 2 + L 2 U 1 is a Lipschitz constant of f (x) . = f 1 (x)f 2 (x). Proof f 1 (x)f 2 (x) − f 1 (y)f 2 (y) ≤ f 1 (x) f 2 (x) − f 2 (y) + f 2 (y) f 1 (x) − f 1 (y) ≤(U 1 L 2 + U 2 L 1 ) x − y .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 45</head><p>The following statements about a differentiable function f (x) are equivalent:</p><formula xml:id="formula_300">(i). f (x) is L-smooth w.r.t. a norm • s . (ii). ∇f (x) − ∇f (y) * s ≤ L x − y s . (iii). |f (y) − f (x) − ∇f (x), y − x | ≤ L 2 x − y 2 s .</formula><p>See e.g. Definition 5.1 and Lemma 5.7 of <ref type="bibr" target="#b2">Beck (2017)</ref>.</p><p>Lemma 46 For any x, x ,</p><formula xml:id="formula_301">∇M (x), x ≤ x m x m , ∇M (x), x ≥ x 2 m .</formula><p>Proof The proof is taken from Section A.2 of Chen et al. ( <ref type="formula">2020</ref>) and we include it here for completeness. Since M (x) = 1 2 x 2 m , by Theorem 3.47 of <ref type="bibr" target="#b2">Beck (2017)</ref>,</p><formula xml:id="formula_302">∇M (x) = x m v x ,</formula><p>where v x is a subgradient of x m at x. Consequently,</p><formula xml:id="formula_303">∇M (x), x = x m v x , x ≤ x m v x * m x m ≤ x m x m ,</formula><p>where the first inequality results from Holder's inequality and the last inequality results from the fact that v x * m ≤ 1 (Lemma A.1 of <ref type="bibr" target="#b7">Chen et al. (2020)</ref>). Further, notice that x m is convex, we thus have</p><formula xml:id="formula_304">x m ≤ 0 m + v x , x − 0 , implying ∇M (x), x = x m v x , x ≥ x 2 m .</formula><p>which together with (71) yields that for any t ∈ (t 1 , t 2 − 1]</p><formula xml:id="formula_305">w t+1 − w t c ≤ α t (A w t c + B) ≤ α t (A (1 + 2Aα t 1 ,t−1 ) w t 1 c + 2ABα t 1 ,t−1 + B) ≤ 2α t (A w t 1 c + B) (Using α t 1 ,t−1 ≤ 1 4A ) .</formula><p>Consequently, for any t ∈ (t 1 , t 2 ], we have</p><formula xml:id="formula_306">w t − w t 1 c ≤ t−1 j=t 1 w j+1 − w j c ≤ t−1 j=t 1 2α j (A w t 1 c + B) = 2α t 1 ,t−1 (A w t 1 c + B) ≤ 2α t 1 ,t 2 −1 (A w t 1 c + B),</formula><p>which completes the proof of (68). For (69), we have</p><formula xml:id="formula_307">w t 2 − w t 1 c ≤2α t 1 ,t 2 −1 (A w t 1 c + B) ≤2α t 1 ,t 2 −1 (A w t 1 − w t 2 c + A w t 2 c + B) ≤ 1 2 w t 1 − w t 2 c + 2α t 1 ,t 2 −1 (A w t 2 c + B), implying w t 2 − w t 1 c ≤ 4α t 1 ,t 2 −1 (A w t 2 c + B).</formula><p>Consequently, for any t ∈ [t 1 , t 2 ],</p><formula xml:id="formula_308">w t − w t 1 c ≤2α t 1 ,t 2 −1 (A w t 1 c + B) ≤2α t 1 ,t 2 −1 (A w t 1 − w t 2 c + A w t 2 c + B) ≤2α t 1 ,t 2 −1 (A4α t 1 ,t 2 −1 (A w t 2 c + B) + A w t 2 c + B) ≤4α t 1 ,t 2 −1 (A w t 2 c + B) (Using α t 1 ,t 2 −1 ≤ 1 4A ) ,</formula><p>which completes the proof of ( <ref type="formula" target="#formula_309">69</ref>). ( <ref type="formula">68</ref>) implies</p><formula xml:id="formula_309">w t − w t 1 ≤ w t 1 c + B A ,<label>(69)</label></formula><p>implies</p><formula xml:id="formula_310">w t − w t 1 ≤ w t 2 c + B A ,</formula><p>then (70) follows immediately, which completes the proof.</p><p>Lemma 48 Assumptions 4.3 and 4.4 hold. Then there exists a constant L µ such that ∀θ, θ , a, s,</p><formula xml:id="formula_311">d µ θ (s, a) − d µ θ (s, a) ≤ L µ θ − θ .</formula><p>Proof See, e.g., Lemma 9 of <ref type="bibr" target="#b62">Zhang et al. (2021)</ref>.</p><p>Lemma 49 For any • , we have</p><formula xml:id="formula_312">X −1 − Y −1 ≤ X −1 X − Y Y −1 . Proof X −1 − Y −1 = X −1 Y Y −1 − X −1 XY −1 ≤ X −1 X − Y Y −1 .</formula><p>Lemma 50 With softmax parameterization, dπ θ (a|s) dθ s ,a = I s=s π θ (a|s) </p><formula xml:id="formula_313">I a=a − π θ (a |s) ,<label>(72)</label></formula><formula xml:id="formula_314">d log π θ (a|s) dθ s ,a = I s=s I a=a − π θ (a |s) ,<label>(73) dKL</label></formula><formula xml:id="formula_315">(U A ||π θ (•|s)) dθ s ,a = I s=s (π θ (a |s) − 1 |A| ),<label>(74)</label></formula><formula xml:id="formula_316">∇H (π θ (•|s)) ≤ log |A| + e −1 ,<label>(77)</label></formula><p>a dπ θ (a|s) dθ s ,a (q π θ ,η (s, a) − η log π θ (a|s)) = I s=s π θ (a |s) Ãdv π θ ,η (s, a)</p><formula xml:id="formula_317">d Jη (π θ ; p 0 ) dθ s,a = 1 1 − γ d π θ ,γ,p 0 (s)π θ (a|s) Ãdv π θ ,η (s, a),<label>(78)</label></formula><p>where</p><formula xml:id="formula_319">Ãdv π θ ,η (s, a) . = qπ θ ,η (s, a ) − η log π θ (a |s) − ṽπ θ ,η (s), Adv π θ ,η (s, a) . = q π θ (s, a) − v π θ (s).</formula><p>Further  By setting γ = 0 and putting all the mass of ρ (initial distribution) in s in Lemma 14 of <ref type="bibr" target="#b34">Mei et al. (2020)</ref>, we obtain that H (π θ (•|s)) is (4 + 8 log |A|)-smooth. For (78), we have</p><formula xml:id="formula_320">a dπ θ (a|s) dθ s ,a (q π θ ,η (s, a) − η log π θ (a|s)) = a I s=s π θ (a|s) I a=a − π θ (a |s) (q π θ ,η (s, a) − η log π θ (a|s)) =I s=s π θ (a |s) qπ θ ,η (s, a ) − η log π θ (a |s) − a π θ (a|s)π θ (a |s) (q π θ ,η (s, a) − η log π θ (a|s)) =I s=s π θ (a |s) qπ θ ,η (s, a ) − η log π θ (a |s) − ṽπ θ ,η (s) .</formula><p>Since ( <ref type="formula" target="#formula_318">79</ref>) is identical to Lemma 10 of <ref type="bibr" target="#b34">Mei et al. (2020)</ref>, we have completed the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Proof of Auxiliary Lemmas</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Proof of Lemma 12</head><p>Lemma 51 (Bound of T 1 )</p><formula xml:id="formula_321">T 1 ≤ L w L θ β t l cm w t − w * θt m . Proof T 1 = ∇M (w t − w * θt ), w * θt − w * θ t+1 ≤ w t − w * θt m w * θt − w * θ t+1 m (Lemma 46) ≤ w t − w * θt m</formula><p>L w L θ β t l cm (Assumptions 3.4, 3.6 and Lemma 10) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Proof of Lemma 13</head><p>Lemma 52 (Bound of T 2 )</p><formula xml:id="formula_322">T 2 ≤ −(1 − κ u cm l cm ) w t − w * θt 2 m . Proof T 2 = ∇M (w t − w * θt ), Fθt (w t ) − w t = ∇M (w t − w * θt ), Fθt (w t ) − Fθt (w * θt ) − ∇M (w t − w * θt ), w t − w * θt (w * θt is the fixed point).</formula><p>To bound the first inner product, we have</p><formula xml:id="formula_323">∇M (w t − w * θt ), Fθt (w t ) − Fθt (w * θt ) ≤ w t − w * θt m Fθt (w t ) − Fθt (w * θt ) m (Lemma 46) ≤ w t − w * θt m 1 l cm κ w t − w * θt c ≤ u cm κ l cm w t − w * θt 2 m</formula><p>For the second inner product, Lemma 46 implies that</p><formula xml:id="formula_324">∇M (w t − w * θt ), w t − w * θt ≥ w t − w * θt 2 m .</formula><p>Putting the bounds for the two inner products together completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Proof of Lemma 14</head><p>Lemma 53 (Bound of T 31 )</p><formula xml:id="formula_325">T 31 ≤ 8L(L w L θ + 1)α t−τα t ,t−1 ξl 2 cs u 2 cm A 2 w t − w * θt 2 m + C 2 . Proof T 31 = ∇M (w t − w * θt ) − ∇M (w t−τα t − w * θ t−τα t ), F θt (w t , Y t ) − Fθt (w t ) ≤ ∇M (w t − w * θt ) − ∇M (w t−τα t − w * θ t−τα t ) * s F θt (w t , Y t ) − Fθt (w t ) s .</formula><p>To bound the first term,</p><formula xml:id="formula_326">∇M (w t − w * θt ) − ∇M (w t−τα t − w * θ t−τα t ) * s ≤ L ξ w t − w t−τα t + w * θ t−τα t − w * θt s (Lemmas 10 and 45) ≤ L ξ w t − w t−τα t s + L ξ w * θt − w * θ t−τα t s ≤ L ξl cs w t − w t−τα t c + L ξl cs L w L θ β t−τα t ,t−1 ≤ 4Lα t−τα t ,t−1 ξl cs (A w t c + B) + L ξl cs L w L θ β t−τα t ,t−1 (Lemma 47) ≤ 4Lα t−τα t ,t−1 ξl cs (A w t − w * θt c + A w * θt c + B) + L ξl cs L w L θ β t−τα t ,t−1 ≤ 4Lα t−τα t ,t−1 ξl cs (A w t − w * θt c + A w * θt c + B) + 4L ξl cs (L w L θ + 1)α t−τα t ,t−1 ≤ 4L(L w L θ + 1)α t−τα t ,t−1 ξl cs (A w t − w * θt c + AU w + B + 1).</formula><p>To bound the second term,</p><formula xml:id="formula_327">F θt (w t , Y t ) − Fθt (w t ) s ≤ 1 l cs F θt (w t , Y t ) − Fθt (w t ) c ≤ 1 l cs F θt (w t , Y t ) c + Fθt (w t ) − Fθt (w * θt ) c + w * θt c ≤ 1 l cs U F + L F w t c + w t − w * θt c + w * θt c (Lemma 61) 1 l cs U F + L F w t − w * θt c + L F w * θt c + w t − w * θt c + w * θt c ≤ 1 l cs A w t − w * θt c + A w * θt c + B .</formula><p>Combining the two inequalities together yields</p><formula xml:id="formula_328">∇M (w t − w * θt ) − ∇M (w t−τα t − w * θt ), F θt (w t , Y t ) − Fθt (w t ) ≤ 4L(L w L θ + 1)α t−τα t ,t−1 ξl 2 cs (A w t − w * θt c + C) 2 ≤ 8L(L w L θ + 1)α t−τα t ,t−1 ξl 2 cs (A 2 u 2 cm w t − w * θt 2 m + C 2 ),</formula><p>which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Proof of Lemma 15</head><p>Lemma 54 (Bound of T 32 )</p><formula xml:id="formula_329">T 32 ≤ 32Lα t−τα t ,t−1 (1 + L w L θ β t−τα t ,t−1 ) ξl 2 cs u 2 cm A 2 w t − w * θt 2 m + C 2 . Proof T 32 = ∇M (w t−τα t − w * θ t−τα t ), F θt (w t , Y t ) − F θt (w t−τα t , Y t ) + Fθt (w t−τα t ) − Fθt (w t ) ≤ ∇M (w t−τα t − w * θ t−τα t ) * s F θt (w t , Y t ) − F θt (w t−τα t , Y t ) + Fθt (w t−τα t ) − Fθt (w t ) s ≤ 1 l cs ∇M (w t−τα t − w * θ t−τα t ) * s F θt (w t , Y t ) − F θt (w t−τα t , Y t ) + Fθt (w t−τα t ) − Fθt (w t ) c</formula><p>For the first term,</p><formula xml:id="formula_330">∇M (w t−τα t − w * θ t−τα t ) s (80) = ∇M (w t−τα t − w * θ t−τα t ) − ∇M (w * θt − w * θt ) * s (Using ∇M (0) = 0, see the proof of Lemma 46) ≤ L ξ (w t−τα t − w * θ t−τα t ) − (w * θt − w * θt ) s (Lemmas 10 and 45) ≤ L ξ w t−τα t − w * θt s + L ξ w * θt − w * θ t−τα t s ≤ L ξl cs w t−τα t − w * θt c + L ξl cs L w L θ β t−τα t ,t−1 ≤ L ξl cs w t−τα t − w t c + w t − w * θt c + L ξl cs L w L θ β t−τα t ,t−1 ≤ L ξl cs w t c + B A + w t − w * θt c + L ξl cs L w L θ β t−τα t ,t−1 (Lemma 47) ≤ L(1 + L w L θ β t−τα t ,t−1 ) ξl cs w * θt c + w t − w * θt c + B A + w t − w * θt c + 1 ≤ 2L(1 + L w L θ β t−τα t ,t−1 ) ξl cs U w + B A + w t − w * θt c + 1 .</formula><p>For the second term,</p><formula xml:id="formula_331">F θt (w t , Y t ) − F θt (w t−τα t , Y t ) + Fθt (w t−τα t ) − Fθt (w t ) c ≤ F θt (w t , Y t ) − F θt (w t−τα t , Y t ) c + Fθt (w t−τα t ) − Fθt (w t ) c ≤L F w t−τα t − w t c + y d θt (y) F θt (w t−τα t , y) − F θt (w t , y) c ≤2L F w t−τα t − w t c ≤2A w t−τα t − w t c ≤8Aα t−τα t ,t−1 (A w t c + B) (Lemma 47) ≤8Aα t−τα t ,t−1 (A w t − w * θt c + A w * θt c + B). Combining the two inequalities together yields ∇M (w t−τα t − w * θt ), F θt (w t , Y t ) − F θt (w t−τα t , Y t ) + Fθt (w t−τα t ) − Fθt (w t ) ≤ 16Lα t−τα t ,t−1 (1 + L w L θ β t−τα t ,t−1 ) ξl 2 cs (A w t − w * θt c + AU w + B + A) 2 ≤ 32Lα t−τα t ,t−1 (1 + L w L θ β t−τα t ,t−1 ) ξl 2 cs (u 2 cm A 2 w t − * θt 2 m + C 2 )</formula><p>which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5 Proof of Lemma 16</head><p>Lemma 55 (Bound of T 331 ) </p><formula xml:id="formula_332">E [T 331 ] ≤ 8Lα t (1 + L w L θ β t−τα t ,t−1 ) Aξl 2 cs u 2 cm A 2 E t − w * θt 2 m + C 2 . Proof E [T 331 ]<label>(81</label></formula><formula xml:id="formula_333">≤α t (U F + L F t−τα t − w t c + L F w t c ) ≤α t (B + A( w t c + B A ) + A w t c ) (Lemma 47) ≤α t (2B + (A + 1) w t c ) ≤2α t (B + A w t c ) ≤2α t (B + A w t − w * θt c + A w * θt c ).</formula><p>Using the above inequality and ( <ref type="formula">80</ref>) yields</p><formula xml:id="formula_334">E [T 331 ] ≤E 4Lα t (1 + L w L θ β t−τα t ,t−1 ) Aξl 2 cs AU w + B + A w t − w * θt c + A 2 ≤E 8Lα t (1 + L w L θ β t−τα t ,t−1 ) Aξl 2 cs A 2 u 2 cm w t − w * θt 2 m + C 2 ,</formula><p>which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.6 Proof of Lemma 17</head><p>Lemma 56 (Bound of T 332 )</p><formula xml:id="formula_335">E [T 332 ] ≤ 8|Y|L P L θ t−1 j=t−τα t β t−τα t ,j L(1 + L w L θ β t−τα t ,t−1 ) Aξl 2 cs u 2 cm A 2 E w t − w * θt 2 m + C 2 . Proof E [T 332 ] =E ∇M (w t−τα t − w * θ t−τα t ), F θ t−τα t (w t−τα t , Y t ) − F θ t−τα t (w t−τα t , Ỹt ) ≤ 1 l cs E ∇M (w t−τα t − w * θ t−τα t ) * s E F θ t−τα t (w t−τα t , Y t ) − F θ t−τα t (w t−τα t , Ỹt ) | w t−τα t θ t−τα t Y t−τα t s (Similar to (81)) ≤E 2L(1 + L w L θ β t−τα t ,t−1 ) ξl 2 cs w * θt c + B A + w t − w * θt c + 1 × 2|Y|L P L θ t−1 j=t−τα t β t−τα t ,j (B + A w t − w * θt c + A w * θt c )</formula><p>(Using (80) and Lemma 62)</p><formula xml:id="formula_336">≤ 8|Y|L P L θ t−1 j=t−τα t β t−τα t ,j L(1 + L w L θ β t−τα t ,t−1 ) Aξl 2 cs u 2 cm A 2 E w t − w * θt 2 m + C 2 ,</formula><p>which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.7 Proof of Lemma 18</head><p>Lemma 57 (Bound of T 333 ) </p><formula xml:id="formula_337">T 333 ≤ 8LL F L θ β t−τα t ,t−1 (1 + L w L θ β t−τα t ,t−1 ) A 2 ξl 2 cs u 2 cm A 2 w t − w * θt 2 m + C 2 . Proof T 333 = ∇M (w t−τα t − w * θ t−τα t ), F θt (w t−τα t , Y t ) − F θ t−τα t (w t−τα t , Y t ) ≤ ∇M (w t−τα t w * θ t−τα t ) * s F θt (w t−τα t , Y t ) − F θ t−τα t (w t−τα t , Y t ) s ≤ 2L(1 + L w L θ β t−τα t ,t−1 ) ξl 2 cs w * θt c + B A + w t − w * θt c + 1 × L F L θ β t−τα t ,</formula><formula xml:id="formula_338">T 333 ≤ 8LL F L θ β t−τα t ,t−1 (1 + L w L θ β t−τα t ,t−1 ) A 2 ξl 2 cs u 2 cm A 2 w t − w * θt 2 m + (AU x + A + B + AU F ) 2 ,</formula><p>which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.8 Proof of Lemma 19</head><p>Lemma 58 (Bound of T 334 )</p><formula xml:id="formula_339">T 334 ≤ 8LL F L θ β t−τα t ,t−1 (1 + L w L θ β t−τα t ,t−1 ) A 2 ξl 2 cs u 2 cm A 2 w t − w * θt 2 m + C 2 . Proof T 334 = ∇M (w t−τα t − w * θ t−τα t</formula><p>), Fθ  <ref type="formula">80</ref>) and Assumption 3.4) .</p><formula xml:id="formula_340">≤ 2L(1 + L w L θ β t−τα t ,t−1 ) ξl 2 cs w * θt c + B A + w t − w * θt c + 1 × L F L θ β t−τα t ,t−1 w t−τα t c + U F (Using (</formula><p>Using (83) completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.9 Proof of Lemma 20</head><p>Lemma 59 (Bound of T 4 ) Lemma 60 (Bound of T 5 )  Lemma 62 We now study the Lipschitz continuity of Λ (θ, y, η) defined in (50). Since J η (θ) is L J + ηL KL smooth, Lemma 45 implies that L J + ηL KL is a Lipschitz constant of ∇J η (θ). From Lemma 50, it is easy to see the upper bound of ∇J η (θ) is also a continuous function of η. Consequently, Lemma 44 implies there exist continuous functions L Λ (η) and U Λ (η) such that for all y, Λ (θ, y, η) − Λ (θ , y, η) ≤ L Λ (η) θ − θ , </p><formula xml:id="formula_341">E [T 4 ] = 0. Proof E [T 4 ] =E ∇M (w t − w * θt ), t =E E ∇M (w t − w * θt ), t | F t (Tower law of expectation) =E ∇M (w t − w * θt ), E [ t | F t ] (<label>Conditional</label></formula><formula xml:id="formula_342">T 5 ≤ 2L ξl 2 cs A 2 u 2 cm w t − w * θt 2 m + C 2 . Proof T 5 = L ξ F θt (w t , Y t ) − w t + t 2 s ≤ L</formula><formula xml:id="formula_343">E F θ t−</formula><formula xml:id="formula_345">E q t − q π θ t 2 ∞ ≤2ρ max E q t − q π θ t 2 ∞ s,a   E dJ λt (θ t ) dθ s,a 2 × 1   ≤2ρ max |S × A| E q t − q π θ t 2 ∞ E ∇J λt (θ t ) 2 ,</formula><p>(Cauchy-Schwarz inequality)</p><p>which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.18 Proof of Lemma 32</head><p>Lemma 70 There exists a constant U J,λ such that for all t, Telescoping the above inequality yields</p><formula xml:id="formula_346">|E [J λt (θ t )]| ≤ U J,</formula><formula xml:id="formula_347">E [J λt (θ t )] ≥ E [J λ 0 (θ 0 )] − t k=0 z k ≥ E [J λ 0 (θ 0 )] − ∞ k=0 z k .</formula><p>Since β &gt; 0.5, we have</p><formula xml:id="formula_348">∞ t=0 β t log 2 (t + t 0 ) (t + t 0 ) β = ∞ t=0 β log 2 (t + t 0 ) (t + t 0 ) 2 β &lt; ∞, ∞ t=0 β t 1 (t + t 0 ) β = ∞ t=0 β (t + t 0 ) 2 β &lt; ∞. Since q 2(1 − β ), we have ∞ t=0 β t t − q 2 &lt; t=0 β t β + q 2 &lt; ∞.</formula><p>We, therefore, conclude that ∞ t=0 z t &lt; ∞, implying E J λ t+1 (θ t+1 ) is bounded from the below by some constant. By ( <ref type="formula" target="#formula_200">45</ref>),</p><formula xml:id="formula_349">E J λ t+1 (θ t+1 ) ≤ r max 1 − γ ,</formula><p>we, therefore, conclude that E J λ t+1 (θ t+1 ) is bounded by some constant. Similarly, we have </p><formula xml:id="formula_350">E [J λt (θ t+1 )] ≥E J λ t−</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>ii). There exist positive constants U , U such that ∀t, t c ≤ U w t c + U Assumptions 3.4 and 3.5 are natural extensions of the counterparts in Chen et al. (2021) from time-homogeneous to time-inhomogeneous Markov chains and from time-homogeneous to time-inhomogeneous operators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Evaluation performance against training steps of Algorithm 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Evaluation performance against training steps of Algorithm 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>describes some properties of M . Lemma 10 (Proposition A.1 of Chen et al. (2021)) (i). M (w) is convex, and L ξ -smooth w.r.t. • s .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>a</head><label></label><figDesc>dπ θ (a|s) dθ s ,a q π θ (s, a) = I s=s π θ (a |s)Adv π θ (s, d π θ ,γ,p 0 (s)π θ (a|s)Adv π θ (s, a), (76)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>II</head><label></label><figDesc>a=a − π θ (a |s) = a 0 − π θ (a |s) + 1 = 1 − |A|π θ (a |s), we have dKL (U||π θ (•|s))dθ s ,a = I s=s (π(a |s) s=s π θ (a|s) I a=a − π θ (a |s) q π θ (s, a)=I s=s π θ (a |s)q π θ (s, a ) + a π θ (a|s) 0 − π θ (a |s) q π θ (s, a) =I s=s π θ (a |s)q π θ (s, a ) − π θ (a |s)v π θ (s) .For (76), see, e.g., Lemma C.1 of Agarwal et al. (2020). For (77), we have dH (π θ (•|s)) dθ s ,a = − I s=s a dπ θ (a|s) dθ s ,a log π θ (a|s) + 0 = − I s=s a π θ (a|s) I a=a − π θ (a |s) log π θ (a|s) = − I s=s π θ (a |s)H (π θ (•|s)) + π θ (a |s) log π θ (a |s) , implying ∇H (π θ (•|s)) ≤ log |A| + e −1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Lemma 61 For any time step t, almost surely,F θt (w, y) c ≤ U F + L F w c Proof Assumption 3.4 implies that F θt (w, y) c − F θt (0, y) c ≤ F θt (0, y) − F θt (w, y) c ≤ L F w − 0 c ,which completes the proof.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Here Ỹt is an auxiliary chain inspired from<ref type="bibr" target="#b63">Zou et al. (2019)</ref>. Before time t − τ αt − 1, Ỹt is exactly the same as {Y t }. After time t − τ αt − 1, Ỹt evolves according to the fixed kernel P θ t−τα t while Y t evolves according the changing kernel P θ t−τα t , P θ k−τα t +1 , . . . .</figDesc><table><row><cell></cell><cell></cell><cell>.</cell></row><row><cell></cell><cell>T 334</cell><cell></cell></row><row><cell>Ỹt : • • • → Y t−τα t −1 → P θ t−τα t</cell><cell>Y t−τα t → P θ t−τα t</cell><cell>Ỹt−τα t +1 →</cell></row></table><note>τα t (w t−τα t , Ỹt ) − Fθ t−τα t (w t−τα t ) T 331 + ∇M (w t−τα t − w * θ t−τα t ), F θ t−τα t (w t−τα t , Y t ) − F θ t−τα t (w t−τα t , Ỹt ) T 332 + ∇M (w t−τα t − w * θ t−τα t ), F θt (w t−τα t , Y t ) − F θ t−τα t (w t−τα t , Y t ) T 333 + ∇M (w t−τα t − w * θ t−τα t ), Fθ t−τα t (w t−τα t ) − Fθt (w t−τα t ) P θ t−τα t</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>θt (a|S t )∇ log π θt (a|S t ) Π(q t (S t , a)) − λ t log π θt (a|S t ) =β t ∇ Jλt (θ t ), Λ1 (θ t , λ t ) (a|S t ) qπ θ t ,λt (S t , a) − λ t log π θt (a|S t ) − Λ1 (θ t , λ t )</figDesc><table><row><cell>Λ1 (θ, η)</cell><cell>. =</cell><cell>d µ θ (s)Λ(θ, s, η).</cell></row><row><cell></cell><cell>s</cell><cell></cell></row><row><cell>Then we have</cell><cell></cell><cell></cell></row><row><cell>M1</cell><cell></cell><cell></cell></row><row><cell cols="2">= ∇ Jλt (θ t ), θ t+1 − θ t</cell><cell></cell></row><row><cell>=β t ∇ Jλt (θ t ),</cell><cell></cell><cell></cell></row></table><note>a π M11 + β t ∇ Jλt (θ t ), a ∇π θt M12 + β t ∇ Jλt (θ t ), a ∇π θt (a|S t ) Π(q t (S t , a)) − qπ θ t ,λt (S t , a) M13</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>∈ 0, min {2( β − α ), α } .Let t 0 be sufficiently large. Let {δ t } be any positive decreasing sequence converging to 0. For the iterates {θ t } generated by Algorithm 2 and any t &gt; 0, if k is uniformly randomly selected from the set</figDesc><table><row><cell>δ t</cell><cell>. =</cell><cell>1 t 0</cell></row><row><cell>then completes the proof.</cell><cell></cell><cell></cell></row><row><cell>C.3 Proof of Corollary 8</cell><cell></cell><cell></cell></row><row><cell cols="3">Corollary 42 (Optimality of the actor) Let Assumptions 4.1, 4.3, and 4.4 hold. Fix any</cell></row><row><cell cols="3">t 2 , t 2 + 1, . . . , t , then</cell></row></table><note>q</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Obviously, for Lemma 43 to be nontrivial, we have to ensure ∀s, d (s) &gt; 0.Letting d = p 0 , d = p 0 in Lemma 43 and using (64) yield thatJλ k (π θ k ; p 0 ) ≥ Jλ k (π * ,λ k ; p 0 ) − |S| ∇ Jλ k (π θ k ; p 0 )</figDesc><table><row><cell>the soft value function, i.e., ∀π, s, then we have Lemma 43 (Lemma 15 of Mei et al. (2020)) For any state distribution d and d , ṽπ,η (s) ≤ ṽπ * ,η,η (s), 2λ k min s p 0 (s) (min s,a π θ k (a|s)) 2 max s d π  * ,λ k ,γ,p 0 (s) d π θ k ,γ,p 0 (s) ≥ Jλ k (π  * ,λ k ; p 0 ) − |S|δ k 2λ k min s p 0 (s) (min s,a π θ k (a|s)) 2 1 mins p 0 (s) 1−γ holds with probability at least 1 − Ct δ t . According to Proposition 2 of Dai et al. (2018), we have max s ṽπ * ,η,η (s) − v π *  (s) ≤ η log |A| 1 − γ , implying Jη (π  * ,η ; p 0 ) − J(π  *  ; p 0 ) ≤ η log |A| 1 − γ , i.e., Jη (π  * ,η ; p 0 ) ≥ J(π  *  ; p 0 ) − η log |A| 1 − γ . From (20), it is easy to see Jη (π 2 Jη (π; p 0 ) ≤ J(π; p 0 ) + η log |A| 1 − γ .</cell><cell>t. (65) (66)</cell></row></table><note>θ ; d) ≥ Jη (π * ,η ; d) − |S| ∇ Jη (π θ ; d ) 2 2η min s d (s) (min s,a π θ (a|s)) 2 max s d π * ,η,γ,d (s) d π θ ,γ,d (s).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>, for any s, H (π θ (•|s)) is (4 + 8 log |A|)-smooth.</figDesc><table><row><cell>For (74), we have</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">dKL (U A ||π θ (•|s)) dθ s ,a</cell><cell cols="2">= −</cell><cell>I s=s |A| a</cell><cell>d log π θ (a|s) dθ s ,a</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">= −</cell><cell>I s=s |A| a</cell><cell>I</cell></row><row><cell cols="5">Proof (72) is well-known. For (73), we have</cell></row><row><cell>d log π θ (a|s) dθ s ,a</cell><cell>=</cell><cell cols="2">1 π θ (a|s)</cell><cell>dπ θ (a|s) dθ s ,a</cell></row></table><note>= I s=s I a=a − π θ (a |s) . a=a − π θ (a |s) .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>We now bound the inner expectation.E F θ t−τα t (w t−τα t , Ỹt ) − Fθ t−τα t (w t−τα t ) |</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>)</cell></row><row><cell cols="5">=E ∇M (w t−τα t − w  *  θ t−τα t</cell><cell cols="6">), F θ t−τα t</cell><cell>(w t−τα t , Ỹt ) − Fθ t−τα t</cell><cell>(w t−τα t )</cell></row><row><cell cols="8">=E E ∇M (w t−τα t − w  *  θ t−τα t</cell><cell cols="3">), F θ t−τα t</cell><cell>(w t−τα t , Ỹt ) − Fθ t−τα t</cell><cell>(w t−τα t ) |</cell><cell>θ t−τα t w t−τα t Y t−τα t</cell></row><row><cell cols="6">=E ∇M (w t−τα t − w  *  θ t−τα t</cell><cell cols="5">), E F θ t−τα t</cell><cell>(w t−τα t , Ỹt ) − Fθ t−τα t</cell><cell>(w t−τα t ) |</cell><cell>θ t−τα t w t−τα t Y t−τα t</cell></row><row><cell cols="5">≤E ∇M (w t−τα t − w  *  θ t−τα</cell><cell cols="2">)</cell><cell cols="2">*  s</cell><cell cols="2">E F θ t−τα t</cell><cell>(w t−τα t , Ỹt ) − Fθ t−τα t</cell><cell>(w t−τα t ) |</cell><cell>θ t−τα t w t−τα t Y t−τα t</cell><cell>s</cell></row><row><cell>≤</cell><cell>1 l cs</cell><cell cols="6">E ∇M (w t−τα t − w  *  θ t−τα t</cell><cell>)</cell><cell>*  s</cell><cell>E F θ t−τα t</cell><cell>(w t−τα t , Ỹt ) − Fθ t−τα t</cell><cell>(w t−τα t ) |</cell><cell>θ t−τα t w t−τα t Y t−τα t</cell><cell>c</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>θ t−τα t w t−τα t Y t−τα t</cell><cell>c</cell></row><row><cell></cell><cell></cell><cell>=</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>c y</cell><cell>Pr Ỹt = y |</cell><cell>θ t−τα t w t−τα t Y t−τα t</cell><cell>− d θ t−τα t</cell><cell>(y)</cell></row><row><cell></cell><cell></cell><cell>≤ max y</cell><cell>F θ t−τα t</cell><cell cols="7">(w t−τα t , y)</cell></row></table><note>y Pr Ỹt = y | θ t−τα t w t−τα t Y t−τα t − d θ t−τα t (y) F θ t−τα t (w t−τα t , y) c ≤ max y F θ t−τα t (w t−τα t , y) c α t (Definition of τ αt ) (82) ≤α t (U F + L F w t−τα t c ) (Lemma 61)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>U</head><label></label><figDesc>ξl 2 cs F θt (w t , Y t ) − w t + t F θt (w t , Y t ) c + w t c + t c ) 2 F + (L F + 1) w t c + U w t c + U</figDesc><table><row><cell>2</cell></row><row><cell>c</cell></row><row><cell>≤ ( ≤ L 2 ξl cs L ξl 2 cs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>τα t (w t−τα t , Y t ) − F θ t−τα t (w t−τα t , Ỹt ) |E.12 Proof of Lemma 23Lemma 64 There exists a constant C t 0 ,w 0 such that for all t ≤ t 0 ,E w t − w * θt 2 m ≤ C t 0 ,w 0 .Proof According to (9), we havew t+1 c ≤ w t c + α t ( F θt (w t , Y t ) c + w t c + t c ) ≤ w t c + α t U F + L F w t c + w t c + U w t c + U(Lemma 61 and Assumption 3.5).Consequently, it is easy to see that there exists a constant C t 0 ,w 0 such that for all t ≤ t 0 ,E w t − w * θt 2 m ≤ C t 0 ,w 0 . E.13 Proof of Lemma 27 Lemma 65 (Bound of M 11 ) There exists a constant χ 11 &gt; 0 such that, M 11 ≥ χ 11 ∇J λt (θ t ) 2 . µ θ t (s)π θt (a|s)∇ log π θt (a|s)q π θ t (s, a) + λ t |A| d µ θ t (s)∇ log π θt (a|s) ∇J λt (θ t ) µ θ t (s )π θt (a |s )Adv π θ t (s , a ) + λ t |A| d µ θ t (s )(1 − |A|π θt (a |s )) dJ λt (θ t ) dθ s ,a µ θ t (s)π θt (a|s)Adv π θ t (s, a) + λ t d µ θ t (s)(</figDesc><table><row><cell cols="2">Proof</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">M 11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>=</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>s,a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>=</cell><cell cols="2">s ,a s,a</cell><cell cols="2">d µ θ t (s)</cell><cell cols="2">dπ θt (a|s) dθ s ,a</cell><cell>q π θ t (s, a) +</cell><cell>λ t |A|</cell><cell>d µ θ t (s)</cell><cell>d log π θt (a|s) dθ s ,a</cell><cell>dJ λt (θ t ) dθ s ,a</cell></row><row><cell>=</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>s ,a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(Lemma 50)</cell></row><row><cell>=</cell><cell>s,a  ×    </cell><cell cols="2">1 1 − γ</cell><cell cols="2">≤2|Y|L P L θ d M 111</cell><cell cols="2">t−1 j=t−τα t</cell><cell>+</cell><cell>λ t |S|</cell><cell>1 |A| ( 1 |A| M 112 − π θt (a|s)) w t−τα t  θ t−τα t Y t−τα t    − π θt (a|s)) </cell></row></table><note>β t−τα t ,j (B + A w t − w * θt c + A w * θt c ) d d d π θ t ,γ (s)π θt (a|s)Adv π θ t (s, a)Λ(θ, y, η) ≤ UΛ(η).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>M 121 = Λ (θ t , Y t , λ t ) − Λ (θ t−τ β t , Y t , λ t ) ≤ L Λ (λ t ) θ t − θ t−τ β t ≤ L Λ (λ t )L θ l 2,p β t−τ β t ,t−1 (Using (44)) .Since λ t ∈ [0, λ], L Λ (η) is a continuous function and well defined in [0, λ], the extreme value theorem asserts that L Λ (η) obtains its maximum in [0, λ], say, e.g., L * Λ . Then .15 Proof of Lemma 29Lemma 67 (Bound of M 122 ) There exists a constant U * Λ &gt; 0 such thatE [M 122 ] ≤ U * Λ |S||A|L µ L θ M 122 | θ t−τ β t , Y t−τ β t ≤E E M 122 | θ t−τ β t , Y t−τ β t .We now bound the inner expectation. In the rest of the proof, all Pr and E are implicitly conditioned on θ t−τ β t and Y t−τ β t .Since λ t ∈ [0, λ] and the continuous function U Λ (η) obtains its maximum, say, e.g., U * Λ , in the compact set [0, λ], we haveE [M 122 ] ≤ U * Λ |S||A|L µ L θ Λ (θ t−τ β t , Ỹt , λ t ) = E E Λ (θ t−τ β t , Ỹt , λ t ) | θ t−τ β t , Y t−τ β t ≤E E Λ (θ t−τ β t , Ỹt , λ t ) | θ t−τ β t , Y t−τ β t .We now bound the inner expectation. In the rest of the proof, all Pr and E are implicitly conditioned on θ t−τ β t and Y t−τ β t . Since Ỹt = ( St , Ãt ) and s,ad µ θ t−τ β t (s)µ θ t−τ β t (a|s)Λ (θ t−τ β t , (s, a), λ t ) = 0,we have E Λ (θ t−τ β t , Ỹt , λ t ) .17 Proof of Lemma 31Lemma 69 (Bound of M 13 ) There exists a constant ρ max &gt; 0 such thatE [M 13 ] ≤2ρ max |S × A| E q t − q π θ t 2 ∞ E ∇J λt (θ t ) 2. ∇J λt (θ t ), ρ t ∇ log π θt (A t |S t ) Π (q t (S t , A t )) − q π θ t (S t , A t ) θt (A t |S t ) dθ s,a Π (q t (S t , A t )) − q π θ t (S t , A t )</figDesc><table><row><cell>Proof</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(Cauchy-Schwarz inequality)</cell></row><row><cell cols="3">E [M 123 ]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hence Proof Lemma 50 implies that Assumption 4.4 implies that Hence E 13 ] ≤2ρ max s,a E =2ρ max s,a E ≤2ρ max s,a E = E Proof ≤2ρ max s,a E</cell><cell cols="4">t−1 j=t−τ β t = E E t−1 E [M 122 ] d log π θ (a|s) dθ s ,a &lt; 2. ρ max . = sup θ,s,a π θ (a|s) µ θ (a|s) &lt; ∞. dJ λt (θ t ) dθ s,a dJ λt (θ t ) dθ s,a dJ λt (θ t ) dθ s,a dθ s,a dJ λt (θ t ) 2</cell><cell>β t−τ β t ,j . β t−τ β t ,j ,</cell><cell>2</cell><cell>2</cell></row><row><cell>E [M 13 ]</cell><cell></cell><cell></cell><cell></cell><cell>j=t−τ β t</cell><cell></cell></row><row><cell cols="2">which completes the proof. = E ≤ s,a E dJ λt (θ t ) dθ s,a ρ t d log π ≤ s,a E dJ λt (θ t ) dθ s,a</cell><cell>M 121 ≤</cell><cell>L  *  Λ L θ l 2,p</cell><cell>β t−τ β t ,t−1 .</cell><cell></cell><cell>2</cell><cell>,</cell></row></table><note>sup θ Λ (θ, y, η) ≤ U Λ (η). EE [M 122 ] = E Λ (θ t−τ β t , Y t , λ t ) − Λ (θ t−τ β t , Ỹt , λ t ) = y Pr Ỹt = y − Pr(Y t = y) Λ (θ t−τ β t , y, λ t ) ≤ max y Λ (θ t−τ β t , y, λ t ) y Pr Ỹt = y − Pr(Y t = y) ≤U Λ (λ t ) y Pr Ỹt = y − Pr(Y t = y) (Using (85)) ≤U Λ (λ t )|S||A|L µ L θ t−1 j=t−τ β t β t−τ β t ,j(Similar to (84)  with L θ defined in (44)) .E.16 Proof of Lemma 30Lemma 68 (Bound of M 123 )E [M 123 ] ≤ U * Λ β t . = s,a Pr St = s, Ãt = a − d µ θ t−τ β t (s)µ θ t−τ β t (a|s) Λ (θ t−τ β t , (s, a), λ t ) ≤ sup s,a,θ Λ (θ, (s, a), λ t ) s,a Pr St = s, Ãt = a − d µ θ t−τ β t (s)µ θ t−τ β t (a|s) ≤U * Λ β t (Using (51)) ,which completes the proof.E2 E ρ t d log π θt (A t |S t ) dθ s,a 2 Π (q t (S t , A t )) − q π θ t (S t , A t ) 2 E Π (q t (S t , A t )) − q π θ t (S t , A t ) 2 E Π (q t (S t , A t )) − Π q π θ t (S t , A t )2 E q t (S t , A t ) − q π θ t (S t , A t )2(Projection to a convex set is nonexpansive)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>λ , |E [J λt (θ t+1 )]| ≤ U J,λ .Since λ t &lt; λ, we conclude that there exists a constant χ 6 (depending on λ) such that ∀t, θ∇J λt (θ) 2 ≤ χ 6 .Then (53) and Proposition 4 imply that there exists some constant χ 7 &gt; 0 such thatE [J λt (θ t+1 )] ≥E [J λt (θ t )] + β t χ 11 E ∇J λt (θ t ) 2 − β t χ 12 log 2 (t + t 0 ) (t + t 0 ) β + β t χ 7 t − q 2 √ χ 6 + β t χ 2 1 (t + t 0 ) β J λ t+1 (θ t+1 ) ≥E [J λt (θ t )] + β t χ 11 E ∇J λt (θ t ) 2 + E J λ t+1 (θ t+1 ) − E [J λt (θ t+1 )] − z t =E [J λt (θ t )] + β t χ 11 E ∇J λt (θ t ) 2 + (λ t − λ t+1 )E s∼U S KL U A ||π θ t+1 (•|s) − z t ≥E [J λt (θ t )] − z t (Using λ t &gt; λ t+1 ) .</figDesc><table><row><cell cols="4">Proof Lemma 50 implies that</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>dJ λt (θ t ) dθ s,a</cell><cell>≤</cell><cell>1 1 − γ</cell><cell>d π θ ,γ,p 0 (s)π θ (a|s)Adv π θ (s, a) +</cell><cell>λ t |S|</cell><cell>π θ (a|s) −</cell><cell>1 |A|</cell><cell>.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(Using (45))</cell></row></table><note>zt. Hence E</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>1 (θ t ) + β t χ 11 E ∇J λt (θ t ) 2 + E [J λt (θ t )] − E J λ t−1 (θ t ) − z t =E J λ t−1 (θ t ) + β t χ 11 E ∇J λt (θ t ) 2 + (λ t−1 − λ t )E s∼U S [KL (U A ||π θt (•|s))] − z t ≥E J λ t−1 (θ t ) − z t (Using λ t−1 &gt; λ t ) .Hence |E [J λt (θ t+1 )]| is also bounded, which completes the proof. U J,λ (Using β k−1 &gt; β k and Lemma 32) Bound of M11 ) There exists a constant χ 11 &gt; 0 such that, M11 ≥ χ 11 ∇ Jλt (θ t ) µ θ t (s)π θt (a|s)∇ log π θt (a|s) q π θ t ,λt (s, a) − λ t log π θt (a|s)∇ Jλt (θ t ) Assumption 4.4, the continuity of d µ θ w.r.t. θ (Lemma 48), and the extreme value theorem ensures that the above inf is strictly positive, which completes the proof.E.21 Proof of Lemma 37Lemma 73 (Bound of M121 ) There exist constants L * as |q π θ ,η (s, a)|, is bounded with the bound being a continuous function of η. From Lemmas 50 and 45, it is then easy to see both ∇π θ (a|s) and ∇H (π θ (•|s)) are bounded and Lipschitz continuous in θ. With Lemma 44, we, therefore, conclude that there exist continuous functions L Λ 1 (η) and U Λ 1 (η) such that for any s,Λ 1 (s, θ, η) − Λ 1 (s, θ , η) ≤ L Λ 1 (η) θ − θ , sup θ Λ 1 (s, θ, η) ≤ U Λ 1 (η).We now study the Lipschitz continuity of Λ1 (θ, η) defined in (61). Lemma 48 confirms the Lipschitz continuity of d µ θ . Consequently, Lemma 44 implies that there exist continuous functions LΛ 1 (η) and UΛ 1 (η) such thatΛ1 (θ, η) − Λ1 (θ , η) ≤ LΛ 1 (η) θ − θ , supWe now study the Lipschitz continuity of Λ 1 (θ, y, η) defined in (62). Since Jη (θ) is L J + ηL H smooth, Lemma 45 implies that L J + ηL H is a Lipschitz constant of ∇ Jη (θ). From Lemma 50, it is easy to see the upper bound of ∇J η (θ) is also a continuous function of η.Consequently, Lemma 44 implies there exist continuous functions L Λ 1 (η) and U Λ 1 (η) such that for all y, M121 = Λ 1 (θ t , S t , λ t ) − Λ 1 (θ t−τ β t , S t , λ t )≤ L Λ 1 (λ t ) θ t − θ t−τ β t ≤ L Λ 1 (λ t )L θ β t−τ β t ,t−1 (Using (60)) .Since λ t ∈ [0, λ], L Λ 1 (η) is a continuous function and well defined in [0, λ], the extreme value theorem asserts that L Λ 1 (η) obtains its maximum in [0, λ], say, e.g., L *</figDesc><table><row><cell cols="2">Proof</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>M11</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>=</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>s,a</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>=</cell><cell cols="2">s ,a s,a</cell><cell cols="2">d µ θ t (s)</cell><cell>dπ θt (a|s) dθ s ,a</cell><cell>q π θ t ,λt (s, a) − λ t log π θt (a|s)</cell><cell>d Jλt (θ t ) dθ s ,a</cell></row><row><cell></cell><cell>=</cell><cell>s ,a</cell><cell cols="4">d µ θ t (s )π θt (a |s ) Ã dv π θ t (s , a )</cell><cell>d Jλt (θ t ) dθ s ,a</cell><cell>(Lemma 50)</cell></row><row><cell></cell><cell>=</cell><cell>s ,a</cell><cell cols="4">(1 − γ)d µ θ t (s ) d π θ t ,γ (s )</cell><cell>d Jλt (θ t ) dθ s ,a</cell><cell>2</cell><cell>(Lemma 50)</cell></row><row><cell></cell><cell cols="2">≥ inf θ,s</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Λ 1</cell><cell>&gt; 0 such that</cell></row><row><cell>= =</cell><cell cols="4">1 β k U J,λ − β t 2 −1 β k−1 1 + 1 β t (t + t 0 ) β − 2U J,λ U J,λ β t β</cell><cell cols="2">U J,λ + U J,λ +</cell><cell>1 β t β t 1 U J,λ + 2 −1 U J,λ β t 1 2 −1 M121 ≤ L  *  Λ 1</cell><cell>L Λ 1</cell><cell>. Then</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>E [M 121 ] ≤ L  *  Λ 1</cell><cell>L θ β t−τ β t ,t−1 ,</cell></row><row><cell cols="7">E.19 Proof of Lemma 33 which completes the proof.</cell></row><row><cell cols="7">Lemma 71 E.20 Proof of Lemma 36 E.22 Proof of Lemma 41</cell></row><row><cell cols="7">E Lemma 72 (2   t k= t 2 1 β k (J λ   ≤ Lemma 74 2 k= t β k . t 1 Jλ</cell><cell>2U J,λ β</cell><cell>(t + t 0 ) β</cell></row></table><note>k (θ k+1 ) − J λ k (θ k )) d (1 − γ)d µ θ (s) d π θ ,γ (s) ∇ Jλt (θ t ) 2 . θ β t−τ β t ,t−1 ,where L θ is defined in (60).Proof We first study the Lipschitz continuity of Λ 1 (θ, s, η) defined in (61). We haveΛ 1 (θ, s, η) = a as well θ Λ1 (θ, y, η) ≤ UΛ 1 (η). Λ 1 (θ, y, η) − Λ 1 (θ , y, η) ≤ L Λ 1 (η) θ − θ , sup θ Λ 1 (θ, y, η) ≤ U Λ 1 (η). Hence E k (θ k+1 ) − Jλ k (θ k ) ≤ 3λβ log |A| 1 − γ + 2U J β (t + t 0 ) β</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Following existing works, e.g.,<ref type="bibr" target="#b22">Konda (2002)</ref>;<ref type="bibr" target="#b61">Zhang et al. (2020c)</ref>;<ref type="bibr" target="#b55">Wu et al. (2020)</ref>;<ref type="bibr" target="#b57">Xu et al. (2021)</ref>, by convergence of the actor, we mean that the gradients converge to 0.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/ShangtongZhang/DeepRL</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We omit λ = 2 −2 and λ = 1 to improve the readability of the figures. The corresponding curves are similar to λ = 2 −1 and λ = 2.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lemma 47 Given positive integers t 1 &lt; t 2 satisfying</p><p>we have, for any t ∈ [t 1 , t 2 ],</p><p>Proof Notice that</p><p>The rest of the proof is exactly the same as the proof of Lemma A.2 of <ref type="bibr">Chen et al. (2021)</ref> up to changes of notations. We include it for completeness. Rearranging terms of the above inequality yields</p><p>implying that for any t ∈ (t 1 , t 2 ],</p><p>Notice that for any x ∈ [0, 1 2 ], 1 + x ≤ exp(x) ≤ 1 + 2x always hold. Hence</p><p>Consequently, for any t ∈ (t 1 , t 2 ], we have</p><p>Proof In this proof, all Pr and E are implicitly conditioned on w t−τα t , θ t−τα t , Y t−τα t . We use Θ t to denote the set of all possible θ t .</p><p>Applying the above inequality recursively yields</p><p>Consequently,</p><p>which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.11 Proof of Lemma 11</head><p>Lemma 63 For sufficiently large t 0 ,</p><p>Proof By the definition of τ αt in (26), it is easy to see</p><p>where • is the ceiling function. Consequently,</p><p>Assumption 3.6 ensures β t &lt; α t holds for all t. Consequently,</p><p>which completes the proof.</p><p>where</p><p>Assumption 4.4, the continuity of d µ θ w.r.t. θ (Lemma 48), and the extreme value theorem ensures that</p><p>If M 111 M 112 &lt; 0, then</p><p>If M 111 M 112 ≥ 0, then</p><p>then we always have</p><p>which completes the proof.</p><p>where (i) results from the inequality 1 (t − 1) x − 1 t x = t x − (t − 1) x (t − 1) x t x = t x (t − 1) 1−x − (t − 1) (t − 1)t x ≤ t x t 1−x − (t − 1) (t − 1)(t − 1) x = 1 (t − 1) 1+x and (ii) results from the inequality</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optimality and approximation with policy gradient methods in markov decision processes</title>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Learning Theory</title>
				<meeting>the Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-step reinforcement learning: A unifying algorithm</title>
		<author>
			<persName><forename type="first">Kristopher</forename><surname>De Asis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Fernando</forename><surname>Hernandez-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Zacharias</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">First-order methods in optimization</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Beck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>SIAM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Adaptive Algorithms and Stochastic Approximations</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Benveniste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Métivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Priouret</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neuro-Dynamic Programming</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dimitri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">N</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific Belmont</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural actor-critic algorithms</title>
		<author>
			<persName><forename type="first">Shalabh</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Stochastic approximation: a dynamical systems viewpoint</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName><surname>Borkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Finite-sample analysis of contractive stochastic approximation using smooth convex envelopes</title>
		<author>
			<persName><forename type="first">Zaiwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Siva Theja Maguluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shakkottai</surname></persName>
		</author>
		<author>
			<persName><surname>Shanmugam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00874</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A lyapunov theory for finite-sample guarantees of asynchronous q-learning and td-learning variants</title>
		<author>
			<persName><forename type="first">Zaiwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Siva Theja Maguluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shakkottai</surname></persName>
		</author>
		<author>
			<persName><surname>Shanmugam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01567</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Finite-sample analysis of offpolicy natural actor-critic with linear function approximation</title>
		<author>
			<persName><forename type="first">Zaiwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sajad</forename><surname>Khodadadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva Theja</forename><surname>Maguluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Control Systems Letters</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Expected policy gradients for reinforcement learning</title>
		<author>
			<persName><forename type="first">Kamil</forename><surname>Ciosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SBEED: convergent reinforcement learning with nonlinear function approximation</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Linear off-policy actor-critic</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Challenges of real-world reinforcement learning</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Dulac-Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mankowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Hester</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12901</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">IMPALA: scalable distributed deep-rl with importance weighted actorlearner architectures</title>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yotam</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><forename type="middle">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Off-policy deep reinforcement learning by bootstrapping the covariate shift</title>
		<author>
			<persName><forename type="first">Carles</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName><forename type="first">Tuomas</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurick</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Consistent on-line off-policy evaluation</title>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Hallak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On the convergence rate of off-policy policy optimization methods with density-ratio correction</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00993</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A natural policy gradient</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Finite-sample analysis of offpolicy natural actor-critic algorithm</title>
		<author>
			<persName><forename type="first">Zaiwei</forename><surname>Sajad Khodadadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva Theja</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Maguluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Finite sample analysis of two-time-scale natural actor-critic algorithm</title>
		<author>
			<persName><surname>Sajad Khodadadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Thinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Theja Maguluri</surname></persName>
		</author>
		<author>
			<persName><surname>Romberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Actor-Critic Algorithms</title>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">R</forename><surname>Konda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Actor-critic algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vijay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">N</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">On the sample complexity of actorcritic method for reinforcement learning with function approximation</title>
		<author>
			<persName><forename type="first">Harshat</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Ribeiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.08412</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Stochastic approximation and recursive algorithms and applications</title>
		<author>
			<persName><forename type="first">Harold</forename><surname>Kushner</surname></persName>
		</author>
		<author>
			<persName><surname>George Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dr Jekyll and Mr Hyde: the strange case of off-policy policy updates</title>
		<author>
			<persName><forename type="first">Romain</forename><surname>Laroche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Tachet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Markov chains and mixing times</title>
		<author>
			<persName><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><surname>Peres</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>American Mathematical Soc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00909</idno>
		<title level="m">Reinforcement learning and control as probabilistic inference: Tutorial and review</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-improving reactive agents based on reinforcement learning, planning and teaching</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Breaking the curse of horizon: Infinite-horizon off-policy estimation</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An improved analysis of (variancereduced) policy gradient and natural policy gradient methods</title>
		<author>
			<persName><forename type="first">Yanli</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamer</forename><surname>Basar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wotao</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Off-policy policy gradient with state distribution correction</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08473</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Simulation-based optimization of markov reward processes</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Marbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the global convergence rates of softmax policy gradient methods</title>
		<author>
			<persName><forename type="first">Jincheng</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrià</forename><forename type="middle">Puigdomènech</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bridging the gap between value and policy based reinforcement learning</title>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections</title>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinlam</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Natural actor-critic</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On finite-time convergence of actor-critic algorithm</title>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoran</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Information Theory</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">On-line Q-learning using connectionist systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahesan</forename><surname>Rummery</surname></persName>
		</author>
		<author>
			<persName><surname>Niranjan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<pubPlace>UK</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge, Department of Engineering Cambridge</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Off-policy actor-critic with shared experience replay</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedavyas</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Nham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thore Graepel, and Demis Hassabis</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to predict by the methods of temporal differences</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><surname>Barto</surname></persName>
		</author>
		<title level="m">Reinforcement Learning: An Introduction</title>
				<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>2nd Edition</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fast gradient-descent methods for temporaldifference learning with linear function approximation</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><forename type="middle">Reza</forename><surname>Maei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalabh</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wiewiora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Horde: a scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Delp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">M</forename><surname>Pilarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Autonomous Agents and Multiagent Systems</title>
				<meeting>the International Conference on Autonomous Agents and Multiagent Systems</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Dudzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petko</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhyuk</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Kroiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Agapiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">Sasha</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Leblond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Dalibard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Sulsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">L</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>¸aglar Gülçehre</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<title level="m">Koray Kavukcuoglu, Demis Hassabis, Chris Apps</title>
				<editor>
			<persName><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Ring</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dario</forename><surname>Wünsch</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Katrina</forename><surname>Mckinney</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Oliver</forename><surname>Smith</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>and David Silver. Grandmaster level in starcraft II using multi-agent reinforcement learning. Nature</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Neural policy gradient methods: Global optimality and rates of convergence</title>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoran</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
				<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Finite-sample analysis of greedy-gq with linear function approximation under markovian noise</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaofeng</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Uncertainty in Artificial Intelligence</title>
				<meeting>the Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sample efficient actor-critic with experience replay</title>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
				<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unifying task specification in reinforcement learning</title>
		<author>
			<persName><forename type="first">Martha</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Function optimization using connectionist reinforcement learning Connection Science</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><surname>Peng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A finite-time analysis of two timescale actor-critic methods</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weitong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Improving sample complexity bounds for (natural) actor-critic algorithms</title>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbin</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Doubly robust off-policy actor-critic: Convergence and optimality</title>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbin</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11866</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A self-tuning actor-critic algorithm</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Zahavy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhyuk</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hado P Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satinder</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Global convergence of policy gradient methods to (almost) locally optimal policies</title>
		<author>
			<persName><forename type="first">Kaiqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamer</forename><surname>Basar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">GradientDICE: Rethinking generalized offline estimation of stationary values</title>
		<author>
			<persName><forename type="first">Shangtong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Provably convergent two-timescale off-policy actor-critic with function approximation</title>
		<author>
			<persName><forename type="first">Shangtong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshuai</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020c</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Breaking the deadly triad with a target network</title>
		<author>
			<persName><forename type="first">Shangtong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshuai</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Finite-sample analysis for SARSA with linear function approximation</title>
		<author>
			<persName><forename type="first">Shaofeng</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbin</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
