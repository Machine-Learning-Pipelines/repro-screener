<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LGD-GCN: Local and Global Disentangled Graph Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-22">22 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jingwei</forename><surname>Guo</surname></persName>
							<email>jingwei.guo@liverpool.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong-Liverpool University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Liverpool</orgName>
								<address>
									<settlement>Liverpool</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
							<email>kaizhu.huang@xjtlu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong-Liverpool University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinping</forename><surname>Yi</surname></persName>
							<email>xinping.yi@liverpool.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Liverpool</orgName>
								<address>
									<settlement>Liverpool</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
							<email>rui.zhang02@xjtlu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong-Liverpool University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LGD-GCN: Local and Global Disentangled Graph Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-22">22 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">FBC1F5B1F47FC6EAB17982AC308BB6C2</idno>
					<idno type="arXiv">arXiv:2104.11893v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-10-31T05:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Disentangled Graph Convolutional Network (Dis-enGCN) is an encouraging framework to disentangle the latent factors arising in a real-world graph. However, it relies on disentangling information heavily from a local range (i.e., a node and its 1-hop neighbors), while the local information in many cases can be uneven and incomplete, hindering the interpretabiliy power and model performance of DisenGCN. In this paper a , we introduce a novel Local and Global Disentangled Graph Convolutional Network (LGD-GCN) to capture both local and global information for graph disentanglement. LGD-GCN performs a statistical mixture modeling to derive a factor-aware latent continuous space, and then constructs different structures w.r.t. different factors from the revealed space. In this way, the global factor-specific information can be efficiently and selectively encoded via a message passing along these built structures, strengthening the intra-factor consistency. We also propose a novel diversity promoting regularizer employed with the latent space modeling, to encourage interfactor diversity. Evaluations of the proposed LGD-GCN on the synthetic and real-world datasets show a better interpretability and improved performance in node classification over the existing competitive models.</p><p>a This paper is a lighter version of "Learning Disentangled Graph Convolutional Networks Locally and Globally" where the results and analysis have been reworked substantially.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graphs are emerging as an insightful structured data modeling technique for capturing the similarity between data samples and the relationship between entities. To mine the domain-specific knowledge in graph structured data, Graph Convolutional Networks (GCNs) have been proposed to integrate the topological patterns and content features <ref type="bibr" target="#b9">[Kipf and Welling, 2017]</ref>, demonstrating excellent expressive power and growing popularity in various graph learning tasks, such as node classification, link prediction, and recommendation <ref type="bibr" target="#b16">[Wu et al., 2020, Chen and</ref><ref type="bibr" target="#b3">Wong, 2020]</ref>.</p><p>Most state-of-the-art methods, such as <ref type="bibr" target="#b9">[Kipf and Welling, 2017</ref><ref type="bibr" target="#b7">, Hamilton et al., 2017</ref><ref type="bibr" target="#b15">, Velickovic et al., 2018]</ref>, study node representations in a holistic approach, i.e., they interpret the node neighborhood as a whole without considering the within-distinctions. By contrast, a real-world graph typically contains multiple heterogeneous node relations which in many cases are implicitly determined by various latent factors shaping node aspects. For instance, a user in a social network, usually links with different persons for different reasons, such as family, work, and/or hobby, which potentially characterize the user from different perspectives. The existing holistic approaches usually fail to disentangle these latent factors, rendering the learned representations hardly explained and less informative.</p><p>Recently, Disentangled Graph Convolutional Network (Dis-enGCN) <ref type="bibr" target="#b11">[Ma et al., 2019]</ref> offers a promising framework to disentangle the latent factors behind graph data via a neighborhood partition. Despite the novel design, Disen-GCN heavily relies on local node neighborhood, which may bring unexpected issues. First, the information from local ranges can be significantly varied across the entire graph. Solely depending on it, DisenGCN could easily produce latent representations losing consistent meaning of the associated factor. That may weaken the intra-factor correlation between disentangled features and leads to diminished interpretability. Second, the local neighborhood information can be scarce and limited especially considering sparse graphs, prohibiting DisenGCN from generating informative node aspects and yielding favourable performance boost. A detailed discussion can be seen later in Section 2.3.</p><p>To tackle this limitation, in this paper, we propose a novel framework, termed as Local and Global Disentangled Graph Convolutional Network (LGD-GCN), to learn disentangled node representations capturing both local and global graph information. The central idea is that disentangling the latent factors inherent in a graph can benefit from a latent continuous space which uncovers the underlying factor-aware node relations. Specifically, we first leverage the neighborhood routing mechanism to locally disentangle node representations into multiple latent units pertinent to different factors. Then, we propose to guide the disentanglement from a global perspective.</p><p>To this end, our approach performs a mixture statistical modeling over the locally disentangled latent units, to derive a factor-aware latent continuous space. This enables a different component or mode, specific to a latent factor, in a different region of the latent space <ref type="bibr" target="#b5">[Ghahramani and Hinton, 1996]</ref>. After that, we manage to build a different structure by connecting near neighbors in a different region of the revealed latent space. These latent structures disclose the underlying factor-aware relations between nodes. Employing message passing along them can efficiently and selectively encode the global factor-specific information, which enhances intra-factor consistency, i.e., the consistent meaning of disentangled latent units w.r.t. the associated factor. Furthermore, we also design a novel diversity promoting regularizer to encourage inter-factor diversity. Practically, it enforces the disentangled latent units related to different factors to fall into separate clusters in the latent space so as to enhance the disentangled informativeness. In sharp contrast to DisenGCN, Fig. <ref type="figure" target="#fig_0">1</ref> clearly visualizes the benefit of learning disentangled node representations both locally and globally. Our contributions are summarized as below:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CONVENTIONAL GNNS</head><p>Graph neural networks (GNNs) are powerful machine learning models in dealing with graph-structured data, where the input data are modeled as graphs. A graph is denoted as G = (V, E) with V being the set of nodes and E being the set of edges. Given two distinct nodes u, v ∈ V with u = v, we define (u, v) ∈ E if u and v are connected with an edge, and the neighborhood of node u as N u = {v|(u, v) ∈ E}. For attributed graphs, each node u usually has an initial representation h</p><formula xml:id="formula_0">(0) u ∈ R d 0 .</formula><p>GNNs are used to mine the underlying relationship between nodes according to their attributes {h (0) u |∀u ∈ V } for (semi-)supervised learning tasks such as node classification and link prediction.</p><p>In the past years, an increasing number of GNN models have been proposed <ref type="bibr" target="#b16">[Wu et al., 2020]</ref>. Most of them can be generalized by a message passing mechanism <ref type="bibr" target="#b6">[Gilmer et al., 2017]</ref>, where the node attributes are exchanged through the graph edges following a neighborhood aggregation strategy descibed below.</p><formula xml:id="formula_1">h u ← UPDATE(AGGREGATE({h v |∀v ∈ N u }), h u ),</formula><p>where the AGGREGATE operation is to aggregate information from a node neighborhood, and the UPDATE operation is to combine these information to update the node's attributes. Such a strategy works in an iterative way to learn node representations. For graph-level representations, a readout operation, such as a simple mean or sum, can be applied to summarize the overall information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DISENTANGLED NODE REPRESENTATION</head><p>Albeit promising several learning tasks, most GNNs treat the neighborhood as a whole and ignore the inner-differences, learning noninterpretable representations. To address this issue, DisenGCN <ref type="bibr" target="#b11">[Ma et al., 2019]</ref> was proposed by hypothesizing nodes are connected due mainly to different kinds of relationship; and there are M inherent factors determining edge connections and potentially shaping nodes from M aspects.</p><p>DisenGCN aims to disentangle each node representation into multiple latent units w.r.t. different latent factors. In each layer, given a node u and its neighborhood N u , the node representations, {h i |∀i ∈ {u} ∪ N u }, will be first projected onto M subspaces using different channels. In each channel m (m = 1, 2, ..., M), the projected representation of node i is given by</p><formula xml:id="formula_2">z i,m = σ (W T m h i + b m ) σ (W T m h i + b m ) 2 (1)</formula><p>where</p><formula xml:id="formula_3">W m ∈ R d in × d out M and b m ∈ R d out</formula><p>M are learnable parameters, and σ is an activation function. A neighborhood routing mechanism, detailed in <ref type="bibr">[Ma et al., 2019, Algorithm-1]</ref>, then iteratively partitions all the neighbors into different clusters. After that, independent information aggregations are applied over them in different channels, to attain the disentangled latent units for node u, {ẑ u,m ∈ R d out M |∀m = 1, 2, ..., M}. Finally, the disentangled node representation is obtained by concatenation, ĥu = ẑu,1 ⊕ ẑu,2 ⊕ ... ⊕ ẑu,M and ĥu ∈ R d out .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">LIMITATIONS OF DISENGCN</head><p>While DisenGCN reveals certain latent factors, we argue that it tends to produce weakly disentangled representations and yield limited performance boost because of its heavy reliance on local graph information. To validate our argument, we further provide two experimental investigations over the graph synthesized with four latent factors (see details in Section 4.1).</p><p>First, we visualize the disentangled latent units of Disen-GCN using t-SNE in Fig. <ref type="figure" target="#fig_2">1a</ref>. At the micro-level, we can observe the separability between points with different colors in some regions. But, when it comes to the macro-level, all points unexpectedly fall into discrete clusters and mixed together, indicating a weak disentanglement. This is because the disentangled latent units by DisenGCN may preserve some specific micro-meanings of the factor, but losing the consistent macro-meaning (intra-factor consistency). Additionally, DisenGCN only considers disentangling representations in different channels without ensuring the diversity between those w.r.t. different factors (inter-factor diversity). The learned representations thereby are prone to preserve the redundant information, partially explaining Fig. <ref type="figure" target="#fig_2">1a</ref>.</p><p>Second, we further augment the synthetic graph by tuning the p value (it controls the density of the synthetic graph as described in Section 4.1) to generate graphs with multiple average neighborhood sizes. We then apply GCN <ref type="bibr" target="#b9">[Kipf and Welling, 2017]</ref> and DisenGCN to train for multi-label classification, and report the F1 scores in Table <ref type="table" target="#tab_1">1</ref>. From the table, the relative improvements reduce from approximately 5% to 1% as the average neighborhood size decreasing from 40 to 6. The result meets the expectation. DisenGCN may perform well on a dense graph by learning disentangled representations. However, sparsing the input graph (limiting the accessible local information) can negatively affect the performance boost, which reflects the heavy local reliance of DisenGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LOCAL AND GLOBAL DISENTANGLED GCN</head><p>We present an novel method for Graph Convolutional Networks (LGD-GCN) to disentangle node representations both locally and globally, as presented in Fig. <ref type="figure">2</ref>. By hiring the neighborhood routing mechanism <ref type="bibr" target="#b11">[Ma et al., 2019]</ref>, we first attain disentangled latent units preserving local graph information w.r.t. different latent factors. However, these disentangled units are prone to be weakly disentangled without incorporating global information and being properly regularized. In the following, we show how to enhance the disentanglement from a global perspective, via disclosing the underling factor-aware relations between nodes, to learn a better disentangled representations with strengthened intrafactor consistency and promoted inter-factor diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MODELING LATENT CONTINUOUS SPACE</head><p>Extending the hypothesis in DisenGCN from local neighborhood to global graph, we assume that the locally disentangled units for all nodes, {ẑ i,m ∈ R d out M |∀i ∈ V, ∀m = 1, 2, ..., M}, are generated from a gaussian mixture distribution with equal mixture weights, s.t.</p><formula xml:id="formula_4">p(ẑ i,m ) = 1 M M ∑ e=1 N (ẑ i,m ; µ e , Σ e ),<label>(2)</label></formula><p>where µ e ∈ R employ this assumption to learn factor-specific means and covariances to regularize the disentangling of the latent units. Specifically, we maximize the conditional likelihood of the latent units ẑi,m (for each node i and each factor m) w.r.t. the associated factor m. It is equivalent to minimizing the negative log term expressed in Eq. ( <ref type="formula" target="#formula_5">3</ref>) after removing constants.</p><formula xml:id="formula_5">L i,m = (ẑ i,m − µ m ) T Σ −1 m (ẑ i,m − µ m )<label>(3)</label></formula><p>Minimizing the term L i,m is equivalent to minimizing the Mahalanobis Distance <ref type="bibr" target="#b4">[De Maesschalck et al., 2000]</ref> between the disentangled latent unit and its globally inferred center. It derives a latent continuous space where the disentangled latent units are encouraged to be more discriminative with respect to their centers, and to carry the type of global factor-specific information shared by all nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CONSTRUCTING LATENT STRUCTURES</head><p>Although node relations are naturally presented in graph data, we believe that they are not always optimal for disentangled graph learning. Taking a huge and sparse graph as an example. It is difficult for most nodes to absorb sufficient information, coming from a small number of their neighbors (one or two in most cases), to learn disentangled representations w.r.t. latent factors in a larger number. On the other hand, the raw graph may not contain the desired topologies after projecting node features in different channels. As such, disclosing the underlying factor-aware relations between nodes from the disentangled latent space becomes a promising alternative.</p><p>The previously modeled latent space enables a different component or mode, specific to a different latent factor, in a different region. It would be reasonable to apply a proper graph construction algorithm over different regions to obtain latent structures specific to different factors. We expect these built structures uncovering the factor-aware relations between nodes, and selecting a sufficient number of latent neighbors (from the entire graph) for each node in shaping node aspects. Accordingly, the global factor-specific information can be efficiently and selectively encoded, by employing a simple message passing scheme independently along these different structures, to strengthen the intra-factor consistency.</p><p>Here, we list two popular methods for building graphs from data using local neighborhood in latent space:</p><p>1) k-Nearest-Neighbor (kNN): It connects every point to its k th nearest neighbors, given a pairwise distance d(z i , z j ).</p><p>Formally, the adjacency matrix A kNN ∈ {0, 1} N×N is defined as:</p><formula xml:id="formula_6">A knn i, j = 1 d(z i , z j ) ≤ d(z i , z (k) j ) or d(z i , z j ) ≤ d(z<label>(k)</label></formula><p>i , z j ) 0 otherwise where z (k) i and z (k) j denote the k th nearest neighbors of z i and z j , respectively.</p><p>2) Continuous k-Nearest-Neighbor (CkNN) <ref type="bibr" target="#b2">[Berry and Sauer, 2016]</ref>: It is a discrete version of kNN for removing kNN's sensitivity to the density parameter k. Similary, the adjacency matrix A CkNN ∈ {0, 1} N×N is defined as:</p><formula xml:id="formula_7">A cknn i, j = 1 d(z i , z j ) &lt; d(z i , z (k) i )d(z j , z (k) j ) 0 otherwise</formula><p>In this paper, we apply the same message passing function in GCN <ref type="bibr" target="#b9">[Kipf and Welling, 2017]</ref> to aggregate the factorspecific node information along these constructed structures, following Eq. (4).</p><formula xml:id="formula_8">Z(m) = D(m) − 1 2 Ȃ(m) D(m) − 1 2 Ẑ(m)<label>(4)</label></formula><p>Here, Â(m) refers to the built structures w.r.t. latent factor</p><formula xml:id="formula_9">m from {ẑ i,m |∀i ∈ V }, Ȃ(m) = Â(m) + I, D(m) i,i = ∑ j Ȃ(m) i, j , D(m)</formula><p>i, j = 0 in case of i = j, and Ẑ(m) is the feature matrix with each column being ẑi,m for node i in V . Particularly, we adopt the Euclidean distance as the pairwise distance d(, ), and denote this proposed module as LG agg .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">PROMOTING INTER-FACTOR DIVERSITY</head><p>Diversity-promoting learning aims to encourage different components in latent space models to be mutually uncorrelated and different, and has been widely studied <ref type="bibr" target="#b18">[Xie, 2018</ref><ref type="bibr" target="#b17">, Xie et al., 2016]</ref>. In the previous sections, we derived a factor-aware latent continuous space and built structures for encoding factor-specific node information from a global range, to strengthen the intra-factor consistency. However, without being regularized to be different with respect to different latent factors, the disentangled latent units may preserve redundant information of other irrelevant latent factors.</p><p>In this paper, we propose to promote the inter-factor diversity to capture the unique information in disentangled latent units. Particularly, we define the diversity on the conditional likelihoods (given different factors) of each disentangled latent unit in latent space. Inspired by the Determinant Point Process <ref type="bibr" target="#b10">[Kulesza and Taskar, 2012]</ref>, we formulate the disentanglement diversity for each node i as</p><formula xml:id="formula_10">DD i = det( FT i Fi ),<label>(5)</label></formula><p>where Fi =&lt; Li,1 , ..., Li,m , ..., Li,M &gt;, Li,m = L i,m 2 , and L i,m =&lt; N (ẑ i,m ; µ 1 , Σ 1 ), ..., N (ẑ i,m ; µ M , Σ M ) &gt; contains the conditional likelihoods (given M factors) of the disentangled latent unit ẑi,m .</p><p>By the property of Determinant <ref type="bibr" target="#b1">[Bernstein, 2005]</ref>, DD i is equal to the volume spanned by { Li,m |∀m = 1, 2, ..., M}, elegantly providing an intuitive geometric interpretation as shown in Fig. <ref type="figure">2</ref> with M = 3. Maximizing DD i encourages L i,1 , L i,2 , ..., L i,M to be orthogonal to each other, i.e., enforcing the disentangled latent units to fall into separated regions of the statistical latent space; it essentially enhances the disentangled informativeness and promotes the inter-factor diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">NETWORK ARCHITECTURE</head><p>In this section, we detail the general network architecture of the proposed LGD-GCN for performing node-level tasks.</p><p>The pseudocode of a LGD-GCN's layer is presented in Algorithm 1, and it is desirable to stack multiple LGD-GCN's layers to sufficiently exploit the graph data.</p><p>Specifically, we adopt ReLU activation function in Eq. ( <ref type="formula">1</ref>) and append a dropout layer <ref type="bibr" target="#b14">[Srivastava et al., 2014]</ref> in the end of each LGD-GCN's layer which is only enabled in training. We can then have the output of layer l as { h(l)</p><formula xml:id="formula_11">i |∀i ∈ V } = Dropout(F (l) ({ h(l−1) i |∀i ∈ V })), where 1 ≤ l ≤ L, h(0) i = h (0)</formula><p>i , L denotes the number of stacked hidden layers, F (l) refers to LGD-GCN's l th layer. Finally, a fully connected layer is taken to map the learned node representations into another dimension, e.g. a class-level for node classification, expressed as Y L+1) , where W (L+1) ∈ R d out ×C , b (L+1) ∈ R C , and C is the number of class.</p><formula xml:id="formula_12">(L+1) i = W (L+1) T h(L) i + b (</formula><p>In this work, we focus on the task of node classification. To incorporate Eq. (3) and Eq. ( <ref type="formula" target="#formula_10">5</ref>) into the final optimization problem, we leverage them into two regularization terms for each node i, as expressed below.</p><formula xml:id="formula_13">L i space = 1 M M ∑ m=1 L i,m , L i div = − log(DD i )<label>(6)</label></formula><p>For L i,m , we update µ m and Σ m (m = 1, 2, ..., M) in each layer iteratively with the newly computed values after each training epoch by an update rate U r . To adaptively modify the influential power of these two regularization terms in different layers, we apply a layer loss weight, λ (l) = 10 l−L . It makes the influence of the regularization terms grows bigger as the layer goes deeper within a proper range. Then, we can formulate the final loss in Eq. ( <ref type="formula">7</ref>) with coefficients λ space and λ div for trade-off.</p><formula xml:id="formula_14">L total = L cls + L ∑ l=1 λ (l) (λ space L (l) space + λ div L (l) div ) (7)</formula><p>Here, for single-label node classification,</p><formula xml:id="formula_15">L cls = − 1 |V | ∑ V i Y T i log(softmax(Y (L+1) i</formula><p>)), and for multi-label classification,</p><formula xml:id="formula_16">L cls = − 1 |V | ∑ V i Y T i log(sigmoid(Y (L+1) i )) + (1 − Y i ) T log(1 − sigmoid(Y (L+1) i</formula><p>)), given Y i ∈ R C being the ground truth label of node i in one hot encoding. The end-to-end optimization procedures are displayed in the supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>4.1 EXPERIMENTAL SETTING Datasets Cora, Citeseer, and Pubmed are three citation benchmark networks widely used in <ref type="bibr" target="#b9">[Kipf and Welling, 2017</ref><ref type="bibr" target="#b7">, Hamilton et al., 2017</ref><ref type="bibr" target="#b15">, Velickovic et al., 2018]</ref>, where nodes and edges denote documents and undirected citations respectively; each node is assigned with one topic and associated with bags-of-words features. We synthesize graphs with latent factors following <ref type="bibr" target="#b11">[Ma et al., 2019]</ref>. In detail, we first generate m Erdős-Rényi random graphs with 1,000 nodes and 16 classes, where nodes connect each other with probability p if they are in the same class, with probability q otherwise. Then, we merge these generated graphs by summing the adjacency matrix and turning the element-value bigger than zero to one, to obtain the final synthetic graphs </p><formula xml:id="formula_17">m ∈ R d in × dout M , b m ∈ R dout M , µ m ∈ R dout M , Σ m ∈ R dout M × dout M , ∀m = 1, 2, ..., M for i ∈ V do z i,1 , z i,2 , ..., z i,M ← h i by Eq. (1). end for i ∈ V do ẑi,m ← {z i,m } ∪ {z v,m |∀v ∈ N i }, ∀m = 1, 2, .</formula><p>.., M by NRM with T routing iterations. Minimize L i space and L i div by Eq. ( <ref type="formula" target="#formula_13">6</ref>). with m latent factors. We set q to 3e −5 following <ref type="bibr" target="#b11">[Ma et al., 2019]</ref>, and tune p value such that the average neighborhood size is between 39.5 and 40.5. Each node is initialized with the row of the adjacency matrix as the features and has m labels. The data statistics are listed in the supplementary material.</p><formula xml:id="formula_18">end for m = 1, 2, ..., M do Construct structure G (m) with A (m) from {ẑ i,m |∀i ∈ V }. {z i,m |∀i ∈ V } ← {ẑ i,m |∀i ∈ V } by Eq. (4). end Output: {z i,1 ⊕ zi,2 ⊕ • • • ⊕ zi,M |∀i ∈ V }</formula><p>Baseline Models. We compare our model with several methods, including the state-of-the-art, as the baselines: MLP is a multi-layer perception; MoNet <ref type="bibr" target="#b12">[Monti et al., 2017</ref>] is a mixture model CNN generalizing convolutional neural network to non-Euclidean graph data structure; GCN <ref type="bibr" target="#b9">[Kipf and Welling, 2017]</ref> approximates graph Laplacian with Chebyshev expainsion; GAT <ref type="bibr" target="#b15">[Velickovic et al., 2018]</ref> combines the attention mechanism with graph neural networks to aggregate information with selective neighbors; DisenGCN attempts to learn disentangled node representations via a neighborhood routing mechanism.</p><p>Hyper-parameters. We set d out = 64 as the output dimension of each LGD-GCN's hidden layer and T = 7 as the number of routing iterations, to follow GAT <ref type="bibr" target="#b15">[Velickovic et al., 2018]</ref> and DisenGCN respectively. For semi- supervised node classification on real-world datasets, we fix the number of channels M as 4 for simplification. We use dropout ∼ [0, 1], learning rate ∼ [3e−3, 1], weight decay ∼ [5e−5, 0.2], update rate ∼ [0.1, 0.9] for µ k and Σ k , and the number of layers ∼ {1, 2, ..., 10}. For multi-label classification on the synthetic datasets, with a slight difference, we fix dropout as 0.5, learning rate ∼ [5e−4, 5e−3], weight decay ∼ [1e−3, 1e−2], and M ∼ {2, 4, ..., 16}.</p><p>Additionally, the regularization coefficients λ space and λ div as well as the density parameter k are empirically searched from different ranges for different datasets as provided in the supplementary material. Then, we carefully tune the hyper-parameters defined above on the validation set using optuna <ref type="bibr" target="#b0">[Akiba et al., 2019]</ref>. With the best hyper-parameters, we train the model in 1,000 epochs using the early-stopping strategy with a patience of 100 epochs, and report the average performance in 10 runs on the test split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">QUANTITATIVE EVALUATION</head><p>In this section, we evaluate our model quantitatively in tasks of semi-supervised node classification and multi-label node classification.</p><p>Semi-supervised Node Classification. In this task, we follow the experimental protocal established by <ref type="bibr" target="#b9">Kipf and Welling [2017]</ref>, <ref type="bibr" target="#b15">Velickovic et al. [2018]</ref>, and consider both standard split <ref type="bibr" target="#b19">[Yang et al., 2016]</ref> and random split. For random split, we uniformly sample the same number of instances as in the standard split in 10 times.</p><p>The results are listed in Table <ref type="table" target="#tab_2">2</ref> measured in classification accuracy. Since <ref type="bibr" target="#b13">Shchur et al. [2018]</ref> have conducted extensive evaluations in their work, we will quote their reported results for baseline methods. For DisenGCN, we not only collect their results, but also optimize and evaluate the model on the random splits using their source codes. For our model, considering the non-linear complexity of the real-world datasets, we adopt CkNN <ref type="bibr" target="#b2">[Berry and Sauer, 2016]</ref> in the module LG agg .</p><p>From the results, the proposed LGD-GCN consistently outperforms other baselines. Especially, our model is able to improve upon DisenGCN by a margin of 1.2% and 2.6% on Cora in standard and random splits, respectively. This Figure <ref type="figure">3</ref>: Features correlation analysis demonstrates the benefits brought by absorbing rich and diverse global information. More importantly, real graphs are typically highly sparse as observed in Cora, Citeseer and Pubmed whose graphs contain an average neighbor number of 3.9, 2.8 and 4.5 for each node. In this case, our model is more effective in capturing long-range dependencies via the created shortcuts in the built geometric structures, which further explains the performance improvement.</p><p>Multi-label Node Classification. To further demonstrate our model's disentangling ability quantitatively, we apply MLP, GCN, DisenGCN, and our model to train graphs synthesized with various number of latent factors for multi-label node classification. Specifically, we randomly split each synthetic dataset into train/validation/test as 0.6/0.2/0.2, adopt kNN in the module LG agg , measure model performance in Micro-F1 and Macro-F1 scores, and report the results in Table <ref type="table" target="#tab_3">3</ref>. It can be observed that our model consistently outperforms others while varying the number of latent factors, and especially achieves significant performance gains by (micro-f1) 4.6% and (macro-f1) 4.3% upon DisenGCN on the graph synthesized with six latent factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">QUALITATIVE EVALUATION</head><p>The qualitative evaluation focuses on disentanglement performance and informativeness of learned embeddings.</p><p>Visualization of disentangled representations. We give in Fig. <ref type="figure" target="#fig_2">1b</ref> a 2D visualization of the learned representations w.r.t. four latent factors on the synthetic graph. Compared to that of DisenGCN in Fig. <ref type="figure" target="#fig_2">1a</ref>, our model displays a highly disentangled pattern with consistent and diverse latent factors, evidenced by the intra-factor compactness and inter-factor separability; it also indicates the nodes carry the common type of factor-specific global information. Visualization of node embeddings. Fig. <ref type="figure" target="#fig_4">4</ref> provides a intuitive comparison between the learned node embeddings of DisenGCN and our model on Citeseer dataset. It can be observed that the proposed LGD-GCN learns better node embeddings and shows a high inter-class similarity and intraclass difference. This is because our model learns more informative node aspects by absorbing rich factor-specific global information, leading to increasing discriminative power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">PARAMETER AND ABLATION ANALYSIS</head><p>We investigate the sensitivity of hyper-parameters, and perform ablation analysis over the proposed modules on realworld and synthetic datasets.</p><p>Analysis of consistency coefficient λ space . We plot the learning performance of our model w/o L div while varying λ space in Eq. ( <ref type="formula">7</ref>) e.g. from 0 to 5 on Cora in Fig. <ref type="figure">5a</ref>. The accuracy goes up first and drops slowly. Practically, promising performance can be attained on Cora by choosing λ space from [0.1, 1].</p><p>Analysis of diversity coefficient λ div . We then test the effect of λ div in Eq. ( <ref type="formula">7</ref>), and vary it from e.g. 0 to 0.5 on Citeseer. λ div is relatively robust within a certain range e.g. [0, 0.1] for Citeseer in Fig. <ref type="figure" target="#fig_5">6b</ref>. Once out of that range, the results drops to a low point, suggesting overly focusing on diversity is harmful to model performance. Analysis of the number of channels M. We study the influence of the number of channels M on the synthetic graphs generated with eight latent factors. From Fig. <ref type="figure">8</ref>, our model performs the best when the number of channels is around eight, the true number of the latent factors.</p><p>Ablation analysis. We validate the contributions of the proposed modules denoted by L space , L div , and LG agg in node classification. From Table <ref type="table" target="#tab_4">4</ref>, we can see that both modules can independently and jointly improve the accuracy.  A SUPPLEMENTARY MATERIAL</p><p>In the supplementary material, for reproducibility, we provide the dataset information, algorithm and optimization procedure, and hyper-parameters' searching ranges. Finally, we show more visualization results to validate our conclusion in the manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 DATASET STATISTICS</head><p>We list the information of datasets evaluated in the manuscript in Table <ref type="table" target="#tab_5">5</ref> and Table <ref type="table" target="#tab_6">6</ref>. For real-world datasets, we only use 20 labeled nodes per class but with all the rest nodes unlabeled for training, another 500 nodes for validation and early-stopping, and 1,000 nodes from the rest for testing. For synthetic datasets, we specify the parameters including probability p and probability q for generating synthetic graphs with various number of latent factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 ALGORITHM AND OPTIMIZATION</head><p>Algorithm 2 illustrates the optimization procedures in pseudo-codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 ADDITIONAL RESULTS</head><p>To further verify the importance of the module LG agg , we ablate it from our model and visualize the disentangled representations on the synthetic graph with four latent factors in  3e-5 3e-5 3e-5 3e-5 3e-5</p><p>LGD-GCN w/o LG agg </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Visualization of the disentangled latent units w.r.t. four latent factors on a synthetic graph. Points with a different color mean the disentangled latent units (for all nodes) of a different latent factor. In sharp contrast to DisenGCN, our LGD-GCN displays a highly disentangled pattern with strong intra-factor consistency and inter-factor diversity; it indicates high (low) intra-factor (inter-factor) correlations between features</figDesc><graphic coords="2,426.43,63.00,114.21,105.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: Illustrative example of the LGD-GCN layer with M = 3 latent factors. First, the node representations are locally disentangled by leveraging the neighborhood routing mechanism. These disentangled representations are then modeled in a latent continuous space, and promoted with consistent and diverse latent factors globally, from which geometric structures are constructed for further aggregation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 :</head><label>1</label><figDesc>LGD-GCN's LayerInput: {h i ∈ R d in |∀i ∈ V }; M:the number of latent factors; T : the routing iterations' number of the Neighborhood Routing Mechanism (NRM); Parameter: W</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>LGD-GCN (2nd Layer) (d) LGD-GCN NG (2nd Layer)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization of node embedding on Citeseer</figDesc><graphic coords="7,180.14,162.72,102.77,81.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 5: Analysis of parameter λ space</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Analysis of parameter k</figDesc><graphic coords="8,341.15,163.74,164.97,54.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Comparing to that of the original model, we can witness an evident performance drop by the weakened intrafactor compactness. Even worse, the integrated blue set in Fig.1bis broken into two disjoint clusters in Fig.9by turning off the module LG agg , indicating its effectiveness. We also show the visualization of node embedding learned on Cora dataset in Fig.10. Similar to that on Citeseer dataset in Fig.4, our model learns better embeddings, evidenced by intra-class compactness and inter-class separability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Visualization of disentangled representations on a synthetic graph with four latent factors</figDesc><graphic coords="10,437.24,314.41,95.01,88.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,54.64,63.00,485.99,137.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Micro (Top)  and Macro (Bottom) F1 scores (%) on graphs synthesized with four latent factors but different average neighborhood sizes</figDesc><table><row><cell>Methods</cell><cell>40</cell><cell cols="3">Average Neighborhood Sizes 30 20 10</cell><cell>6</cell></row><row><cell>GCN</cell><cell cols="5">79.5±0.8 75.5±0.7 66.1±0.9 47.2±0.6 37.2±0.9</cell></row><row><cell>DisenGCN</cell><cell cols="5">84.1±1.0 79.5±0.7 69.0±1.0 48.8±0.9 38.4±0.8</cell></row><row><cell>Improvements</cell><cell>+4.6%</cell><cell>+4.0%</cell><cell>+2.9%</cell><cell>+1.6%</cell><cell>+1.2%</cell></row><row><cell>GCN</cell><cell cols="5">78.3±0.9 75.0±0.8 65.8±1.0 45.8±0.6 36.7±0.8</cell></row><row><cell>DisenGCN</cell><cell cols="5">82.9±1.1 78.9±0.7 68.3±1.0 47.4±1.0 37.7±0.8</cell></row><row><cell>Improvements</cell><cell>+4.6%</cell><cell>+3.9%</cell><cell>+2.5%</cell><cell>+1.6%</cell><cell>+1.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Semi-supervised classification accuracies (%)</figDesc><table><row><cell>Method</cell><cell>Splits</cell><cell>Cora</cell><cell cols="2">Datasets Citeseer Pubmed</cell></row><row><cell>MLP</cell><cell></cell><cell>51.5±1.0</cell><cell>46.5</cell><cell>71.4</cell></row><row><cell>MoNet</cell><cell></cell><cell cols="3">82.2±0.7 70.0±0.6 77.7±0.6</cell></row><row><cell>GCN</cell><cell>Standard</cell><cell cols="3">81.9±0.8 69.5±0.9 79.0±0.5</cell></row><row><cell>GAT</cell><cell></cell><cell cols="3">82.5±0.5 71.0±0.6 77.0±1.3</cell></row><row><cell>DisenGCN</cell><cell></cell><cell>83.7</cell><cell>73.4</cell><cell>80.5</cell></row><row><cell>LGD-GCN (ours)</cell><cell></cell><cell cols="3">84.9±0.4 74.5±0.8 81.3±0.6</cell></row><row><cell>MLP</cell><cell></cell><cell cols="3">58.2±2.1 59.1±2.3 70.0±2.1</cell></row><row><cell>MoNet</cell><cell></cell><cell cols="3">81.3±1.3 71.2±2.0 78.6±2.3</cell></row><row><cell>GCN GAT</cell><cell>Random</cell><cell cols="3">81.5±1.3 71.9±1.9 77.8±2.9 81.8±1.3 71.4±1.9 78.7±2.3</cell></row><row><cell>DisenGCN</cell><cell></cell><cell cols="3">81.4±1.6 69.5±1.4 79.1±2.3</cell></row><row><cell>LGD-GCN (ours)</cell><cell></cell><cell cols="3">84.0±1.3 72.0±1.3 79.8±2.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc> and Macro-F1 (Bottom) scores (%) on synthetic graphs with different number of latent factors</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Number of Latent Factors</cell><cell></cell></row><row><cell>Method</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell></row><row><cell>MLP</cell><cell cols="5">79.3±0.5 55.5±0.4 37.0±0.8 25.9±0.6 21.2±0.8</cell></row><row><cell>GCN</cell><cell cols="5">74.5±0.8 56.3±0.7 38.2±0.9 28.0±0.7 23.1±0.8</cell></row><row><cell>DisenGCN</cell><cell cols="5">84.1±1.0 60.4±0.9 41.4±1.3 29.4±0.7 24.2±0.8</cell></row><row><cell cols="6">LGD-GCN (ours) 87.2±0.5 65.0±0.5 43.6±0.7 30.2±0.5 26.1±0.5</cell></row><row><cell>MLP</cell><cell cols="5">77.9±0.7 54.8±0.6 36.0±0.8 24.5±0.7 20.1±0.9</cell></row><row><cell>GCN</cell><cell cols="5">78.3±0.9 55.6±0.9 37.2±1.0 26.9±0.5 22.2±0.9</cell></row><row><cell>DisenGCN</cell><cell cols="5">82.9±1.1 59.9±1.0 40.2±1.2 28.1±0.7 23.4±0.7</cell></row><row><cell cols="6">LGD-GCN (ours) 86.1±0.5 64.2±0.6 42.5±0.6 28.8±0.5 25.1±0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation analysis in node classification accuracies.</figDesc><table><row><cell>Components</cell><cell></cell><cell>Cora</cell><cell>Citeseer Pubmed</cell></row><row><cell>-</cell><cell></cell><cell cols="2">81.9±1.1 70.4±1.6 78.9±0.7</cell></row><row><cell>L space</cell><cell></cell><cell cols="2">83.0±0.5 72.4±1.3 79.1±0.8</cell></row><row><cell>L space +L div L space +LG agg</cell><cell>%</cell><cell cols="2">83.6±0.6 72.4±1.1 79.1±0.4 84.4±0.3 74.0±0.7 81.3±0.6</cell></row><row><cell>L space +L div +LG agg</cell><cell></cell><cell cols="2">84.9±0.4 74.5±0.8 81.2±0.7</cell></row><row><cell cols="2">5 CONCLUSION</cell><cell></cell></row><row><cell cols="4">In this paper, we propose a novel framework, termed Lo-</cell></row><row><cell cols="4">cal and Global Disentangled Graph Convolutional Net-</cell></row><row><cell cols="4">work (LGD-GCN), to disentangle node representations with</cell></row><row><cell cols="4">strengthened intra-factor consistency and promoted inter-</cell></row><row><cell cols="4">factor diversity. Extensive experiments demonstrate the im-</cell></row><row><cell cols="4">proved performance in node classification and enhanced</cell></row><row><cell cols="4">interpretability of the proposed LGD-GCN over existing</cell></row><row><cell cols="2">state-of-the-art approaches.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Real-world Dataset Statistics</figDesc><table><row><cell>Dataset</cell><cell cols="3">Cora Citeseer Pubmed</cell></row><row><cell>Nodes</cell><cell>2708</cell><cell>3327</cell><cell>19717</cell></row><row><cell>Avg-Neighbors</cell><cell>3.9</cell><cell>2.8</cell><cell>4.5</cell></row><row><cell>Features</cell><cell>1433</cell><cell>3703</cell><cell>500</cell></row><row><cell>Classes</cell><cell>7</cell><cell>6</cell><cell>3</cell></row><row><cell>Train</cell><cell>140</cell><cell>120</cell><cell>60</cell></row><row><cell>Validation</cell><cell>500</cell><cell>500</cell><cell>500</cell></row><row><cell>Test</cell><cell>1000</cell><cell>1000</cell><cell>1000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Parameters for generating synthetic datasets</figDesc><table><row><cell># Latent Factors</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell></row><row><cell>probablity p</cell><cell cols="5">0.164 0.110 0.082 0.065 0.055</cell></row><row><cell>probability q</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optuna: A next-generation hyperparameter optimization framework</title>
		<author>
			<persName><forename type="first">Takuya</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shotaro</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yanase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Matrix mathematics: Theory, facts, and formulas with application to linear systems theory</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Tyrus</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Sauer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02353</idno>
		<title level="m">Consistent manifold representation for topological data analysis</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Handling information loss of graph neural networks for sessionbased recommendation</title>
		<author>
			<persName><forename type="first">Tianwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond Chi-Wing</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1172" to="1180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The mahalanobis distance. Chemometrics and intelligent laboratory systems</title>
		<author>
			<persName><forename type="first">Delphine</forename><surname>Roy De Maesschalck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Désiré L</forename><surname>Jouan-Rimbaud</surname></persName>
		</author>
		<author>
			<persName><surname>Massart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The em algorithm for mixtures of factor analyzers</title>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>abs/1609.02907</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Determinantal point processes for machine learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="123" to="286" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Disentangled graph convolutional networks</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kun Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="5425" to="5434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno>abs/1811.05868</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph attention networks. ArXiv, abs/1710.10903</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Diversity leads to generalization in neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<idno>abs/1611.03131</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Diversity-promoting and large-scale machine learning for healthcare</title>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>abs/1603.08861</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
