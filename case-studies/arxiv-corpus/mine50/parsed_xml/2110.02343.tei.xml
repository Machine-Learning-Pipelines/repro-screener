<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quantum Semi-Supervised Learning with Quantum Supremacy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-24">24 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Zhou</forename><surname>Shangnan</surname></persName>
							<email>snzhou@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Stanford Institute for Theoretical Physics</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Quantum Semi-Supervised Learning with Quantum Supremacy</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-24">24 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">44031F2AC292362BB7E3FC85D0D5FD98</idno>
					<idno type="arXiv">arXiv:2110.02343v4[quant-ph]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-10-27T19:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Quantum machine learning promises to efficiently solve important problems. There are two persistent challenges in classical machine learning: the lack of labeled data, and the limit of computational power. We propose a novel framework that resolves both issues: quantum semi-supervised learning. Moreover, we provide a protocol in systematically designing quantum machine learning algorithms with quantum supremacy, which can be extended beyond quantum semi-supervised learning. In the meantime, we show that naive quantum matrix product estimation algorithm outperforms the best known classical matrix multiplication algorithm. We showcase two concrete quantum semi-supervised learning algorithms: a quantum self-training algorithm named the propagating nearest-neighbor classifier, and the quantum semi-supervised K-means clustering algorithm. By doing time complexity analysis, we conclude that they indeed possess quantum supremacy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine learning has made many seemingly impossible tasks possible: from visual and speech recognition, effective web search, to study of human genomics <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. However, there are several long-standing bottlenecks in the field of machine learning, which slows down its pace in conquering more fields of science and technology. Two major challenges are the lack of labeled data, and the limit of computational power. In this paper, we propose a framework of quantum semi-supervised learning, which can overcome both difficulties at the same time.</p><p>Semi-supervised learning <ref type="bibr" target="#b2">[3]</ref> combines a small amount of labeled data with a large amount of unlabeled data during training, which tackles the common issue of the lack of labeled data. Quantum computation <ref type="bibr" target="#b3">[4]</ref> redefines the way computers create and manipulate information. Many quantum algorithms <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> have been demonstrated to possess quantum supremacy, which means that they can execute the same task substantially faster than their classical counterparts. Quantum semi-supervised learning combines the advantages of both semi-supervised learning and quantum computation, therefore represents the future of machine learning and quantum physics.</p><p>Since semi-supervised learning handles both labeled and unlabeled data, in the limiting case when we only have labeled or unlabeled data, we get back to supervised or unsupervised learning. Hence, in some way, semi-supervised learning is more generic, and its algorithms can be used in supervised or unsupervised learning with small modifications. Therefore, many discussions in this paper also apply to quantum supervised or unsupervised learning.</p><p>In section 2, we propose a general framework of quantum semi-supervised learning. In section 3, we provide a generic protocol for designing quantum machine learning algorithms with quantum supremacy, which can be extended beyond quantum semi-supervised learning. The recipe can also be used to realize quantum supremacy in deep quantum neutral networks. One feature of our time complexity analysis is that we give a clear separation between memory access time complexity and algorithmic time complexity, so that the algorithmic advantage on quantum computers isn't overshadowed by the exponential speed-up from the fast access of quantum random memories. Moreover, we show that naive quantum matrix product estimation algorithm outperforms the best known classical matrix multiplication algorithm.</p><p>In section 4, we introduce quantum self-training, and point out its source of quantum supremacy. We give a concrete example, which is the quantum propagating nearest-neighbor algorithm. By comparing its time complexity to the classical case, we demonstrate its quantum supremacy. In section 5, we present the quantum semi-supervised K-means clustering algorithm, and prove its quantum supremacy by time complexity analysis. We conclude the paper with outlooks in speeding up more complicated machine learning tasks including deep neural networks, along with an ultimate goal of using quantum-quantum learning to learn large quantum systems efficiently and reliably on a quantum computer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Framework of Quantum Semi-Supervised Learning</head><p>We first review semi-supervised learning, and then propose a general framework of quantum semi-supervised learning. Since we are going to introduce different types of quantum semisupervised learning, we also call it classical or classical-classical semi-supervised learning.</p><p>Semi-supervised learning lies between supervised and unsupervised learning: we have a small set of labeled data {(x (i) , z (i) )} l i=1 , and a large set of unlabeled data {x (j) } l+u j=l+1 . Specifically, there are several different settings, including regression or classification with labeled and unlabeled data, constrained clustering, and dimensionality reduction with labeled instances whose reduced feature representation is given. We focus on the first setting.</p><p>In supervised learning, the training sample is fully labeled, so the goal is always to label the future test data. However, in a semi-supervised setting, the training sample contains unlabeled data. Hence, there are two different goals in semi-supervised learning: inductive semi-supervised learning aims at predicting the labels of future test data, while transductive semi-supervised learning predicts the labels of the unlabeled instances in the training sample. Definition 2.1. Inductive Semi-Supervised Learning. Given a training sample {(x (i) , z (i) )} l i=1 , {x (j) } l+u j=l+1 , inductive semi-supervised learning learns a function f : X → Y so that f is expected to be a good predictor on future data, beyond {x (j) } l+u j=l+1 .</p><p>Definition 2.2. Transductive Semi-Supervised Learning. Given a training sample {(x (i) , z (i) )} l i=1 , {x (j) } l+u j=l+1 , transductive learning trains a function f : X l+u → Y l+u so that f is expected to be a good predictor on the unlabeled data {x (j) } l+u j=l+1 .</p><p>There are different settings of quantum semi-supervised learning. First, classical-quantum semi-supervised learning encodes both labeled and unlabeled classical data in quantum states, and then uses quantum processors to carry out the learning phase. With a stateof-art design of quantum algorithm, classical-quantum semi-supervised learning executes a suitable learning task much faster than its classical-classical counterpart. Later, we will provide a general protocol in designing such algorithms, and show several examples. Definition 2.3. Classical-Quantum Semi-Supervised Learning. Given a training sample {(x (i) , z (i) )} l i=1 , {x (j) } l+u j=l+1 , classical-quantum semi-supervised learning encodes the data into quantum states, and then executes the learning task on a quantum computer. One convenient encoding is to map the training sample into product states: labeled data {|x (i) |z (i) )} l i=1 , unlabeled data {|x (j) } l+u j=l+1 , stored in QRAM data structure.</p><p>Second, quantum-classical semi-supervised learning maps quantum data to a classical data structure, and then the problem turns into a classical-classical semi-supervised learning problem. While this sounds simple, there are many underlying subtleties. Suppose our training sample is {(ρ (i) , σ (i) )} l i=1 , {ρ (j) } l+u j=l+1 , where ρ (i) 's are quantum states, and the labels σ (i) 's can be either classical or quantum. If we know ρ (i) 's and σ (i) 's exactly, then we essentially have classical data, and the mapping is completely trivial. Hence, the nontrivial case is when we have zero or partial knowledge of the training sample, but those quantum states are stored in a quantum memory nicely. We consider this case as the general setup of quantum-classical semi-supervised learning.</p><p>In this scenario, the novelty and challenge lie in the first step: how to efficiently and accurately extract classical information from the quantum data? Different mappings may result in different training performances and computational costs. The information loss when making quantum measurement, as well as the no-cloning theorem, adds additional difficulties to the problem. One simple but resource-consuming approach is to do efficient quantum tomography when many copies of same instances are present. To some extent, we have to learn the quantum states first.</p><p>When certain limitations forbid us to do complete tomography, the problem becomes more interesting. It has more of an unsupervised learning flavor, as our training data, even the labeled ones, doesn't give out concrete knowledge. The way we process the quantum data may have crucial influence on the learning performance. Again, even in the regime of classical machine learning, pre-processing data can have significant impact on the training output. For inductive learning, this can be even trickier. When we have new quantum data coming in, are we going to process the test data the same way as we did for the training set?</p><p>Intuitively, the answer is yes. A thorough discussion will be illuminated in future work. Definition 2.4. Quantum-Classical Semi-Supervised Learning. Given a training sample {(ρ (i) , σ (i) )} l i=1 , {ρ (j) } l+u j=l+1 , where ρ (i) 's and σ (i) 's are partially known or completely unknown, quantum-classical semi-supervised learning extracts classical information from the initial quantum data using certain quantum channel, and then train the resulting classical data on a classical computer. For inductive learning, it uses the same channel to process the test quantum data, and then makes prediction on the corresponding classical data.</p><p>Quantum-classical semi-supervised learning turns a complicated quantum problem to a simpler classical problem, and then solves it automatically on a classical computer, which is better understood at the current stage. The drawback is the loss of fidelity of the original data. To combat this, it is natural to skip the first step, which is the conversion of quantum data to classical data. When doing so, we learn the pattern of quantum data on a quantum computer, which is quantum-quantum learning. Definition 2.5. Quantum-Quantum Semi-Supervised Learning. Given a training sample {(ρ (i) , σ (i) )} l i=1 , {ρ (j) } l+u j=l+1 , where ρ (i) 's and σ (i) 's are partially known or completely unknown, quantum-quantum semi-supervised learning executes the learning task on a quantum computer.</p><p>Intuitively, it is most natural to learn a quantum system on a quantum computer. However, with little classical information in this setting, we need a new set of theories and algorithms <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14]</ref> in the training process, even for things as simple as gradient descent <ref type="bibr" target="#b15">[15]</ref>. Another challenge is that we are still limited by near-term quantum devices. Hence, right now we still need quantum-classical learning to aid our process of learning a quantum system. In the near future, when fault-tolerant quantum computers are in commercial use, the default choice is to use quantum-quantum learning. These settings are not completely distinctive. Running tasks in a hybrid way can improve efficiency in time and space.</p><p>3 Quantum Supremacy of Classical-Quantum Learning</p><p>In this section, we provide a generic protocol in designing classical-quantum learning algorithms that possess quantum supremacy. The idea is to take advantage of the fact that certain data acquisitions and manipulations can be done faster on a quantum computer <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b18">18]</ref>. Theorem 3.1. QRAM data structure <ref type="bibr" target="#b16">[16]</ref>. Let V ∈ R N ×d , there exists a data structure to store the rows of V such that 1. The time to insert, update, or delete a single entry v ij is O log(N d) .</p><p>2. A quantum algorithm with access to the data structure can perform the following unitaries in time O log(N d) .</p><p>(a</p><formula xml:id="formula_0">) |i |0 → |i |v i for i ∈ [N ]. (b) |0 → i∈[N ] |v i ||i .</formula><p>Using a classical RAM data structure, these tasks take O(N d) time to complete. Hence, QRAM data structure provides an exponential speed-up. Many quantum algorithm references combine the memory access time complexity and algorithmic time complexity together when doing time complexity analysis. However, most traditional algorithm analysis takes the memory access time complexity as O(1), because modern computers allow processor caches, memory level parallelism, etc. To put the comparison of quantum and classical algorithms on an equal footing, in this paper, we take memory access time complexity as a constant. It is good to keep in mind that quantum computers are inherently exponentially faster in reading and writing at a memory location. Theorem 3.2. Distance Estimation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">17]</ref>. Given data matrices X ∈ R l×d and Y ∈ R u×d stored in the QRAM data structure, where x (i) is the i-th row of X, and y (j) is the j-th row of Y . Suppose that the following unitaries |i |0 → |i |x (i) , and |j |0 → |j |y (j)  can be performed in time Λ and the norms of the vectors are known. For any ∆ &gt; 0 and ǫ &gt; 0, there exists a quantum algorithm that computes the L 2 distance between two vectors x (i) and y (j) : |i |j |0 → |i |j |d 2 (x (i) , y (j) ) , where |d 2 (x (i) , y (j) ) − d 2 (x (i) , y (j) )| ≤ ǫ with probability at least 1 − 2∆ in time T = Õ |x (i) ||y (j) |Λ log(1/∆)/ǫ . Theorem 3.3. Inner Product Estimation <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17]</ref>. Given data matrices X ∈ R l×d and Y ∈ R u×d stored in the QRAM data structure, where x (i) is the i-th row of X, and y (j) is the j-th row of Y . Suppose that the following unitaries |i |0 → |i |x (i) , and |j |0 → |j |y (j)  can be performed in time Λ and the norms of the vectors are known. For any ∆ &gt; 0 and ǫ &gt; 0, there exists a quantum algorithm that computes the inner product between two vectors x (i) and y (j) : |i |j |0 → |i |j |(x (i) , y (j) ), where |(x (i) , y (j) ) − (x (i) , y (j) )| ≤ ǫ with probability at least 1 − 2∆ in time T = Õ |x (i) ||y (j) |Λ log(1/∆)/ǫ .</p><p>The above results show that for quantum distance and inner product estimations, the algorithmic time complexity is O(1) in terms of the dimensions of vectors, while the same classical calculation takes O(d).</p><p>Since matrix multiplications can be interpreted as calculating many inner products, we propose and prove the following theorem: Theorem 3.4. Matrix Product Estimation. Given data matrices X ∈ R l×d and Y ∈ R u×d stored in the QRAM data structure, where x (i) is the i-th row of X, and y (j) is the j-th row of Y . Suppose that the following unitaries |i |0 → |i |x (i) , and |j |0 → |j |y (j)  can be performed in time Λ and the norms of the vectors are known. For any ∆ &gt; 0 and ǫ &gt; 0, there exists a quantum algorithm that computes the product between two matrices X and Y T : Z = XY T , where</p><formula xml:id="formula_1">|z ij − z ij | ≤ ǫ with probability at least 1 − 2∆ in time T = Õ |x (i) ||y (j) |luΛ log(1/∆)/ǫ .</formula><p>Proof. z ij = (x (i) , y (j) ), which is the inner product of x (i) and y (j) . To calculate matrix Z, we need to estimate lu such inner products. By Theorem 3.3, it takes time</p><formula xml:id="formula_2">T = Õ |x (i) ||y (j) |luΛ log(1/∆)/ǫ to achieve |z ij − z ij | ≤ ǫ with probability at least 1 − 2∆.</formula><p>For naive matrix multiplications between an m×k matrix and an k ×n matrix, quantum estimation takes time O(mn), while classical calculation takes time O(mnk). When dealing with n × n matrices, then the naive quantum matrix multiplication algorithm takes time O(n 2 ). This is dramatic, because the classical matrix multiplication algorithm with best asymptotic complexity runs in O(n 2.3728596 ) time, and the naive classical algorithm runs in O(n 3 ) time.</p><p>Theorem 3.5. HHL Algorithm <ref type="bibr" target="#b18">[18]</ref>. Given a sparse N × N matrix A with condition number κ, and a vector b. Suppose M is a matrix, and x is a vector such that Ax = b. HHL algorithm estimates x † M x in Õ poly(log N, κ) time.</p><p>In contrast, the classical algorithm that estimates x † M x takes time Õ(N √ κ). For the same task, HHL algorithm presents an exponential speed-up with respect to matrix size N .</p><p>Most machine learning algorithms require calculations of distance, inner product, matrix product and inverse. For these algorithms, if we can perform these calculations on a quantum computer, and make sure the quantum state preparation and classical information retrieval are not exponentially costly, we can realize quantum supremacy in all these algorithms. This sheds lights on training deep quantum neural networks faster, considering that many inner product calculations and matrix inverses are carried out in the training process.</p><p>For the remainder of the paper, we showcase general classes of quantum semi-supervised learning algorithms, and give some concrete examples. Moreover, we compare the time complexity of the classical and quantum versions, and demonstrate quantum supremacy in these scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Quantum Self-Training</head><p>Self-training is characterized by the fact that the learning process uses its own predictions to teach itself. It is also called self-teaching or bootstrapping because of this. Self-training assumes that its own predictions, at least the high confidences ones, tend to be correct. For classification tasks with well-separated clusters, this is usually the case.</p><p>The major goal of self-training is to learn an appropriate predictor f . We now show how this is done in quantum self-training, and point out which parts of the general quantum self-training algorithm possess quantum supremacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 4.1. Quantum Self-training</head><p>Input: labeled data {|x (i) |z (i) )} l i=1 , unlabeled data {|x (j) } l+u j=l+1 , stored in QRAM data structure.</p><p>1. Initially, let L = {|x (i) |z (i) )} l i=1 , and U = {|x (j) } l+u j=l+1 . 2. Repeat until convergence: 3. Train f from L using supervised learning. 4. Apply f to the unlabeled instances in U .</p><p>In line 3, we can apply existing quantum supervised learning algorithms <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20]</ref> or derive new quantum supervised learning algorithms based on principles mentioned in section 3, so that quantum supremacy is achieved. In line 4, because QRAM reads and writes data exponentially faster than RAM, we naturally have quantum supremacy.</p><p>We now give a simple and concrete example of quantum self-training, which is the propagating nearest-neighbor algorithm. We first show the classical version and then propose the quantum version. Input: labeled data</p><formula xml:id="formula_3">x (i) , f (x (i) ) l i=1 , unlabeled data {x (j) } l+u j=l+1 , distance function d(). 1. Initially, let L = x (i) , f (x (i) ) l i=1</formula><p>, and U = {x (j) } l+u j=l+1 . 2. Repeat until U is empty: 3. Select x = arg min x∈U min y∈L d(x, y), y = arg min y∈L min x∈U d(x, y). 4. Set f (x) = f (y), where f (y) is the label of y. Break ties randomly. 5. Remove x from U . Add (x, f (x)) to L.</p><p>In each iteration, the classical algorithm selects the unlabeled instance that is closest to any "labeled" instance, i.e., any instance currently in L. These "labeled" instances may from the original labeled set, or were labeled in previous iterations. The selected instance is then assigned the label of its nearest neighbor and inserted into L as if it were truly labeled data. The process repeats until all instances have been added to L. Input: labeled data {|x (i) |z (i) } l i=1 , unlabeled data {|x (j) } l+u j=l+1 , stored in QRAM data structure. Distance function d().</p><p>1. Initially, let L = {|x (i) |z (i) } l i=1 , and U = {|x (j) } l+u j=l+1 . 2. Repeat until U is empty: 3. Step 1: Point Distance Estimation. Perform the map:</p><formula xml:id="formula_4">⊗ |x (i) ∈L |i ⊗ |x (j) ∈U |j |0 → ⊗ |x (i) ∈L |i ⊗ |x (j) ∈U |j |d 2 (x (i) , x (j) ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Step 2: Distance Minimization.</p><p>Find the minimum distance among {d 2 (x (i) , x (j) )} |x (i) ∈L,|x (j) ∈U , record the corresponding register number |i and |j .</p><p>5.</p><p>Step 3: Label Assignment. Set |z (j) = |z (i) to be the label of |z (j) . Remove |x (j) from U , add |x (j) |z (j) to L.</p><p>In the quantum algorithm, we follow the same logic as its classical version, but store and manipulate data on a quantum computer. At each iteration, let l = |L|, u = |U |, the point distance estimation takes time O(lu); the distance minimization takes O(lu), and the label assignment takes O(1), so the combined time complexity is O(lu).</p><p>As a comparison, at each iteration, the corresponding classical algorithm takes time O(lud) to calculate the distance, and then O(lu) for distance minimization, and O(1) for label assignment, with a combined time complexity O(lud).</p><p>The above discussion demonstrates the quantum supremacy of the quantum propagating nearest neighbor classifier. In particular, when the data points are of high dimension, the quantum speed-up becomes significant. By creating state-of-art superpositions, it is possible to reduce the running time of quantum distance estimation even further. Last but not least, we should keep in mind that QRAM processes data exponentially faster than RAM.</p><p>5 Quantum Semi-Supervised K-Means K-means clustering is a simple and popular unsupervised machine learning algorithm. It plays a significant role in cluster analysis. There are different ways of extending it to a semi-supervised setting. We consider the case when some data points are labeled, i.e. we have labeled data {(x (i) , z (i) )} l i=1 , and unlabeled data {x (j) } l+u j=l+1 . One approach to classical semi-supervised K-means clustering is the following algorithm. Algorithm 5.1. Classical Semi-Supervised K-Means.</p><p>Input: labeled data {y (i) = (x (i) , z (i) )} l i=1 , unlabeled data {x (j) } l+u j=l+1 , distance function d(), number of centroids k. The total number of data points is N = l + u, with l &lt;&lt; u.</p><p>1. Initially, for any m ∈ [k], denote the set of labeled data points whose label is m as </p><formula xml:id="formula_5">S m . If S m = ∅, then select centroid c 0 m randomly. If S m = ∅, then c 0 m = 1 |Sm| y (i) ∈Sm x (i) . 2. t = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Step 2: Cluster Assignment. For labeled data, assign their original label; for unlabeled data, find the minimum distance among {d(x (j) , c t m )}, and assign m as the label of x (j) , i.e. z (j) = m. 6. Step 3: Centroid Update. For each m, let</p><formula xml:id="formula_6">c t+1 m = N j=1 1{z (j) = m}x (i) N j=1 1{z (j) = m} .</formula><p>(1)</p><formula xml:id="formula_7">7. t = t + 1.</formula><p>We now propose the quantum semi-supervised K-means algorithm.</p><p>Algorithm 5.2. Quantum Semi-Supervised K-Means. Input: labeled data {y With states |x (j) 's and |c t m 's, perform the map:</p><formula xml:id="formula_8">(i) = |x (i) |z (i) } l i=1 , unlabeled data {|x (j) } l+u j=l+1 ,</formula><formula xml:id="formula_9">1 √ N N j=1 |j ⊗ m∈[k] |m |0 → 1 √ N N j=1 |j ⊗ m∈[k] |m |d 2 (x (j) , c t m ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Step 2: Cluster Assignment. For labeled data, assign their original label; for unlabeled data, find the minimum distance among {d(x (j) , c t m )}, and assign m as the label of |x (j) , i.e. |z (j) = |m . Next, uncompute Step 1 to create the superposition of all points and their labels</p><formula xml:id="formula_10">1 √ N N j=1 |j ⊗ m∈[k] |m |d 2 (x (j) , c t m ) → 1 √ N N j=1</formula><p>|j |z (j) . Hence, the quantum algorithm is faster at every stage of the training, which demonstrates its supremacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose the framework of quantum semi-supervised learning, which resolves two longstanding challenges of machine learning at the same time. Semi-supervised learning tackles issue of the lack of labeled data, and quantum computation provides dramatic speed-ups so that limit of computational power is no longer the issue. We provide a protocol that systematically designs quantum machine learning algorithms with quantum supremacy and showcase examples. The recipe can be extended beyond supervised, unsupervised, and semisupervised learning. In the future, we will demonstrate how we provide quantum speed-ups to deep neural networks <ref type="bibr" target="#b21">[21]</ref>. We also aim at developing more complicated quantum semisupervised learning algorithms, for example, quantum co-training, quantum graph-based training <ref type="bibr" target="#b22">[22]</ref>. Furthermore, we keep an ultimate goal in mind: use quantum-quantum learning to learn large quantum systems efficiently and reliably on a quantum computer <ref type="bibr" target="#b15">[15]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 4 . 2 .</head><label>42</label><figDesc>Classical Propagating Nearest-Neighbor Classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 4 . 3 .</head><label>43</label><figDesc>Quantum Propagating Nearest-Neighbor Classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 .</head><label>3</label><figDesc>Repeat until convergence: 4. Step 1: Centroid Distance Calculation. Calculate d(x (j) , c t m ) for all j ∈ [l + 1, N ], and all m ∈ [k].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>stored in QRAM data structure. Number of centroids k. Distance function d(). The total number of data points is N = l + u, with l &lt;&lt; u. The dimension of each data vector |x (i) is d. 1. Initially, for any m ∈ [k], denote the set of labeled data points whose label is |m as S m . If S m = ∅, then select centroid |c 0 m randomly. If S m = ∅, then |c 0 m = 1 |Sm| y (i) ∈Sm |x (i) . 2. t = 0. 3. Repeat until convergence: 4. Step 1: Centroid Distance Estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>6. Step 3 : 8 .</head><label>38</label><figDesc>Label Measurement. Denote the set of data points whose label is |m as S t m . Measure the label register to obtain a state |χ t m = 1 |S t m | y (j) ∈S t m |j , with probability |S t m | N . 7. Step 4: Centroid Update.Denote the data matrix as V ∈ R N ×d . Perform matrix multiplication to obtain t = t + 1. At each iteration, the quantum distance estimation takes time O(k), while its classical counterpart takes time O(N kd); the quantum cluster assignment takes time O(k), while its classical counterpart takes time O(N k). In the quantum case, the time complexity of label measurement is considered as O(1), and then the quantum matrix multiplication of centroid update takes time O(N ) for each centroid. Hence, steps 3 and 4 take time O(N k) combined. At the same time, the classical centroid update process takes time O(N kd).</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank Xinyu Ren for suggesting a useful reference. Z.S. is supported by the Simons Foundation and the National Science Foundation, under Grant No. 2111998.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improved protein structure prediction using potentials from deep learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongli</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><surname>Žídek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><surname>Bridgland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">577</biblScope>
			<biblScope unit="issue">7792</biblScope>
			<biblScope unit="page" from="706" to="710" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Introduction to semi-supervised learning. Synthesis lectures on artificial intelligence and machine learning</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Isaac</surname></persName>
		</author>
		<author>
			<persName><surname>Chuang</surname></persName>
		</author>
		<title level="m">Quantum Computation and Quantum Information</title>
				<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rapid solution of problems by quantum computation</title>
		<author>
			<persName><forename type="first">David</forename><surname>Deutsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Jozsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society of London. Series A: Mathematical and Physical Sciences</title>
		<imprint>
			<biblScope unit="volume">439</biblScope>
			<biblScope unit="page" from="553" to="558" />
			<date type="published" when="1907">1907. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Algorithms for quantum computation: discrete logarithms and factoring</title>
		<author>
			<persName><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Shor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 35th annual symposium on foundations of computer science</title>
				<meeting>35th annual symposium on foundations of computer science</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="124" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A fast quantum mechanical algorithm for database search</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lov</surname></persName>
		</author>
		<author>
			<persName><surname>Grover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-eighth annual ACM symposium on Theory of computing</title>
				<meeting>the twenty-eighth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="212" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quantum complexity theory</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umesh</forename><surname>Vazirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on computing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1411" to="1473" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the power of quantum computation</title>
		<author>
			<persName><surname>Daniel R Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on computing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1474" to="1483" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Quantum algorithms for supervised and unsupervised machine learning</title>
		<author>
			<persName><forename type="first">Seth</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masoud</forename><surname>Mohseni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Rebentrost</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1307.0411</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Quantum supremacy using a programmable superconducting processor</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Arute</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Babbush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Bardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rupak</forename><surname>Barends</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><surname>Boixo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gsl</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Brandao</surname></persName>
		</author>
		<author>
			<persName><surname>Buell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">574</biblScope>
			<biblScope unit="issue">7779</biblScope>
			<biblScope unit="page" from="505" to="510" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Shangnan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Entropy</forename><surname>Complexity</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10538</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Shangnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11887</idno>
		<title level="m">Quantum cross entropy and maximum likelihood principle</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Quantum data compression and quantum cross entropy</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Shangnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13823</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Quantum cross entropy in quantum machine learning</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Shangnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Preparation</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Architectures for a quantum random access memory</title>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Giovannetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Maccone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review A</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">52310</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">qmeans: A quantum algorithm for unsupervised machine learning</title>
		<author>
			<persName><forename type="first">Iordanis</forename><surname>Kerenidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Luongo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anupam</forename><surname>Prakash</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03584</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Quantum algorithm for linear systems of equations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Aram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinatan</forename><surname>Harrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Hassidim</surname></persName>
		</author>
		<author>
			<persName><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page">150502</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Quantum machine learning</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Biamonte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Wittek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Pancotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Rebentrost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">549</biblScope>
			<biblScope unit="issue">7671</biblScope>
			<biblScope unit="page" from="195" to="202" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Quantum machine learning in feature hilbert spaces</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Schuld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Killoran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">40504</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Quantum supremacy in deep quantum neural networks</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Shangnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Preparation</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Quantum semi-supervised learning with quantum supremacy, ii</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Shangnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Preparation</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
