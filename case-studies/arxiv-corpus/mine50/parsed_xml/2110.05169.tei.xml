<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEARNING A SUBSPACE OF POLICIES FOR ONLINE ADAPTATION IN REINFORCEMENT LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-24">24 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jean-Baptiste</forename><surname>Gaya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CNRS-ISIR</orgName>
								<orgName type="institution" key="instit2">Sorbonne University</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laure</forename><surname>Soulier</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CNRS-ISIR</orgName>
								<orgName type="institution" key="instit2">Sorbonne University</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Now at Ubisoft</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LEARNING A SUBSPACE OF POLICIES FOR ONLINE ADAPTATION IN REINFORCEMENT LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-24">24 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">10948E7ED2E212E650E930E3C6CCF525</idno>
					<idno type="arXiv">arXiv:2110.05169v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-10-27T19:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep Reinforcement Learning (RL) is mainly studied in a setting where the training and the testing environments are similar. But in many practical applications, these environments may differ. For instance, in control systems, the robot(s) on which a policy is learned might differ from the robot(s) on which a policy will run. It can be caused by different internal factors (e.g., calibration issues, system attrition, defective modules) or also by external changes (e.g., weather conditions). There is a need to develop RL methods that generalize well to variations of the training conditions. In this article, we consider the simplest yet hard to tackle generalization setting where the test environment is unknown at train time, forcing the agent to adapt to the system's new dynamics. This online adaptation process can be computationally expensive (e.g., fine-tuning) and cannot rely on meta-RL techniques since there is just a single train environment. To do so, we propose an approach where we learn a subspace of policies within the parameter space. This subspace contains an infinite number of policies that are trained to solve the training environment while having different parameter values. As a consequence, two policies in that subspace process information differently and exhibit different behaviors when facing variations of the train environment. Our experiments 1 carried out over a large variety of benchmarks compare our approach with baselines, including diversity-based methods. In comparison, our approach is simple to tune, does not need any extra component (e.g., discriminator) and learns policies able to gather a high reward on unseen environments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In recent years, Deep Reinforcement Learning (RL) has succeeded at solving complex tasks, from defeating humans in board games <ref type="bibr" target="#b27">(Silver et al., 2017)</ref> to complex control problems <ref type="bibr" target="#b24">(Peng et al., 2017;</ref><ref type="bibr" target="#b25">Schulman et al., 2017)</ref>. It relies on different learning algorithms (e.g., A2C - <ref type="bibr" target="#b20">(Mnih et al., 2016)</ref>, PPO - <ref type="bibr" target="#b25">(Schulman et al., 2017)</ref>). These methods aim at discovering a policy that maximizes the expected (discounted) cumulative reward received by an agent given a particular environment. If existing techniques work quite well in the classical setting, considering that the environment at train time and the environment at test time are similar is unrealistic in many practical applications. As an example, when learning to drive a car, a student learns to drive using a particular car, and under specific weather conditions. But at test time, we expect the driver to be able to generalize to any new car, new roads, and new weather conditions. It is critical to consider the generalization issue where one of the challenges is to learn a policy that generalizes and adapts itself to unseen environments.</p><p>Different techniques have been proposed in the literature (Section 6) to automatically adapt the learned policy to the test environment. In the very large majority of works, the model has access to multiple training environments (meta-RL setting). Therefore, the training algorithm can identify which variations (or invariants) may occur at test time and how to adapt quickly to similar variations. But this setting may still be unrealistic for concrete applications: for instance, it supposes that the student will learn to drive on multiple cars before getting their driving license.</p><p>Published as a conference paper at ICLR 2022 <ref type="bibr">(a)</ref> The figure represents the parameter space. The red (resp. blue) region is the space of good policies over the training (resp. testing) environment. A single learned policy (red point) may be inefficient for the test environment and has to be adapted (e.g., fine-tuning) to become good at test-time (blue point). Instead of learning a single policy, we learn a convex sub-space (the pentagon) delimited by anchor policies (red stars) that aims at capturing a large set of good policies. Then the adaptation is just made by sampling policies in this subspace, keeping the best one (blue star). (b) Qualitative example of k-shot adaptation on a modified Ant environment (20% of observations masked). 5 policies (i.e 5 values of z) are tested on one episode. In this case, for z = 0., the Ant is able to adapt to this new environment. More example of LoP trajectories in Figures <ref type="figure" target="#fig_5">5 and 6</ref>. See https://sites.google. com/view/subspace-of-policies/home for videos of the learned behaviors. In this paper, we address a simpler yet harder to tackle generalization setting in which the learning algorithm is trained over one single environment and has to perform well on test environments; preventing us from using meta-RL approaches. A natural way to attack this setting is to start by learning a single policy using any RL algorithm, and to fine-tune this training policy at test time, over the test environment (See red/blue points in Figure <ref type="figure" target="#fig_0">1a</ref>), but this process may be costly in terms of environment interactions.</p><p>Very recently, the idea of learning a set of diverse yet effective policies <ref type="bibr" target="#b16">(Kumar et al., 2020b;</ref><ref type="bibr" target="#b22">Osa et al., 2021)</ref> has emerged as a way to deal with this adaptation setting. The intuition is that, if instead of learning one single policy, one learns a set of 'diverse' policies, then there is a chance that at least one of these policies will perform well over a new dynamics. The adaptation in that case just consists in selecting the best policy in that set by evaluating each policy over few episodes (K-shot adaptation). But the way this set of policies is built and the notion of diversity proposed in these methods have a few drawbacks: these models increase diversity by using an additional intrinsic reward which encourages the different policies to generate different distributions of states. This objective potentially favors the learning of policies that are sub-optimal at train time. Moreover, these approaches make use of an additional component in the policy architecture (e.g., a discriminator) that may be difficult to tune, particularly considering that, at train time, we do not have access to any test environment and thus cannot rely on validation techniques to tune the extra architecture.</p><p>Inspired by recent research on mode connectivity <ref type="bibr" target="#b0">(Benton et al., 2021;</ref><ref type="bibr" target="#b14">Kuditipudi et al., 2019)</ref> and by <ref type="bibr" target="#b32">(Wortsman et al., 2021)</ref> which aims to learn a subspace of models in the supervised learning setting, we propose to learn a subspace of policies in the parameter space as a solution to the online adaptation in the RL setting (see Figure <ref type="figure" target="#fig_0">1a</ref>). Each particular point in this subspace corresponds to specific parameter values, and thus to a particular policy. This subspace is learned by adapting a classical RL algorithm (PPO and A2C in our case, see Section 3.3) such that an infinite continuum of policies is learned, each policy having different parameters. The policies thus capture and process information differently, and react differently to variations of the training environment (see Figure <ref type="figure" target="#fig_0">1b</ref>). We validate our approach (Section 5) over a large set of reinforcement learning environments and compare it with other existing approaches. These experiments show that our method is competitive, achieves good results and does not require the use of any additional component of hyper-parameters tuning contrarily to baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SETTING</head><p>Reinforcement Learning: Let us define a state space S and an action space A. In the RL setting, one has access to a training Markov Decision Process (MDP) denoted M defined by a transition distribution P (s |s, a) : S × A × S → R+, an initial state distribution P (i) (s) : S → R+ and a reward function r(s, a) : S × A → R.</p><p>A policy is defined as π θ (a|s) : S × A → R+, where θ denotes the parameters of the policy. A trajectory sampled by a policy π θ given a MDP M is denoted τ ∼ π θ (M). The objective of an RL algorithm is to find a policy that maximizes the expected cumulative (discounted) reward:</p><formula xml:id="formula_0">θ * = arg max θ E τ ∼π θ (M) [R(τ )]</formula><p>(1) where R(τ ) is the discounted cumulative reward over trajectory τ .</p><p>Online adaptation: We consider the setting where the policy trained over M will be used over another MDP (denoted M) that shares the same state and action space as M, but with a different dynamics and/or initial state distribution and/or reward function 2 . Importantly, M is unknown at train time, and cannot be used for model selection, making the tuning of hyper-parameters difficult.</p><p>Given a trained model, we consider the K-shot adaptation setting where the test phase is decomposed into two stages: a first phase in which the model adapts itself to the new test environment over K episodes, and a second phase in which the adapted model is used to collect the reward. We thus expect the first phase to be as short as possible (few episodes), corresponding to a fast adaptation to the new environment. Let us consider that a model π θ generates a sequence of trajectories τ1 , τ2 , ...., τ+∞ over M, the performance of such a model, is defined as:</p><formula xml:id="formula_1">P erf (π θ , M, K) = lim T →∞ 1 T T t=1 R(τ K+t )<label>(2)</label></formula><p>which corresponds to the average performance of the policy π θ over M after K episodes used for adapting the policy. Note that we are interested in methods that adapt quickly to new a test environment and we will consider small values of K in our experiments. In the following, for sake of simplicity, K will refer to the number of policies evaluated during adaptation since each policy may be evaluated over more than a single episode when facing stochastic environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LEARNING SUBSPACES OF POLICIES</head><p>Motivation and Idea: To illustrate our idea, let us consider a toy example where the train environment contains states with correlated and redundant features, in such a way that multiple subsets of state features can be used to compute good actions to execute. Traditional RL algorithms will discover one policy π θ * that is optimal w.r.t the environment. This policy will typically use the state features in a particular way to decide the optimal action at each step. If some features become noisy (at test time) while, unluckily, π θ * particularly relies on these noisy features, the performance of the policy will drastically drop. Now, let us consider that, instead of learning just one optimal policy, we also learn a second optimal policy π θ * , but enforcing θ * to be different than θ * . This second policy may tend to make use of various features to compute actions. We thus obtain two policies instead of one, and we have more chances that at least one of these policies is efficient at test time. Identifying which of these two policies is the best for the test environment (i.e., adaptation) can simply be done by evaluating each policy over few episodes, keeping the best one. Our model is built on top of this intuition, extending this example to an infinite set of policies and to variable environment dynamics.</p><p>Inspired by <ref type="bibr" target="#b32">Wortsman et al. (2021)</ref> proposing to learn a subspace of models for supervised learning, we study the approach of learning a subspace of policies in the parameter space, and the use of such a model for online adaptation in reinforcement learning. Studying the structure of the parameter space has seen a recent surge of interest through the mode connectivity concept <ref type="bibr" target="#b0">(Benton et al., 2021;</ref><ref type="bibr" target="#b14">Kuditipudi et al., 2019;</ref><ref type="bibr" target="#b32">Wortsman et al., 2021)</ref> and obtain good results in generalization, but it has never been involved in the RL setting. As motivated in the previous paragraph, we expect that, given a variation of the training environment, having access to a subspace of policies that process information differently instead of a single policy will facilitate the adaptation. As a result, our method is very simple, does not need any extra hyper-parameter tuning and achieves good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SUBSPACES OF POLICIES</head><p>Given Θ the space of all possible parameters, a subspace of policies is a subset Θ ⊂ Θ that defines a set of corresponding policies Π = {π θ } θ∈ Θ.</p><p>Since our objective is to learn such a subspace, we have to rely on a parametric definition of such a subspace and consider Θ as a simplex in Θ. Let us define N anchor parameter values θ1 , .... θN ∈ Θ. We define the Z-space as the set of possible weighted sum of the anchor parameters:</p><formula xml:id="formula_2">Z = z = (z 1 , ...z N ) ∈ [0, 1] N | z i = 1</formula><p>. The subspace we aim to learn is defined by:</p><formula xml:id="formula_3">Θ = { N k=1 z k θk , ∀z ∈ Z}<label>(3)</label></formula><p>In other words, we aim to learn a convex hull of N vertices in Θ. Note that policies in this subspace can be obtained by sampling z ∼ p(z) uniformly over Z.</p><p>The advantages of this approach are: a) the number of parameters of the model can be controlled by choosing the number N of anchor parameters, b) since policies are sharing parameters (instead of learning a set of independent policies), we can expect that the learning will be sample efficient. Such a subspace is illustrated in Figure <ref type="figure" target="#fig_0">1a</ref> through the "pentagon" (i.e., N = 5) in which angles correspond to the anchor parameters and the surface corresponds to all the policies in the built subspace.</p><p>K-shot adaptation: Given a subspace of policies Θ, different methods can be achieved to find the best policy over the test environment. For instance, it could be done by optimizing the distribution p(z) at test time. In this article, we use the same yet effective K-shot adaptation technique than <ref type="bibr" target="#b16">Kumar et al. (2020b)</ref> and <ref type="bibr" target="#b22">Osa et al. (2021)</ref>: we sample K episodes using different policies defined by different values of z that are uniformly spread over Z. In our example, it means that we evaluate policies uniformly distributed within the pentagon to identify a good test policy (blue star). Note that, when the environment is deterministic, only one episode per value of z needs to be executed to find the best policy, which leads to a very fast adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LEARNING ALGORITHM</head><p>Learning a subspace of policies can be done by considering the RL learning problem as maximizing:</p><formula xml:id="formula_4">L( Θ) = θ∈ Θ E τ ∼π θ [R(τ )]dθ<label>(4)</label></formula><p>Considering that Θ is a convex hull as defined in Equation 3, and using the uniform distribution p(z) over Z, the loss function of Equation 4 can be rewritten as:</p><formula xml:id="formula_5">L( θ1 , .... θN ) = E z∼p(z) [E τ ∼π θ [R(τ )]] with θ = N k=1 z k θk<label>(5)</label></formula><p>Maximizing such an objective function over θ1 , .... θN outputs a (uniform) distribution of policies trained to maximize the reward, all these policies sharing common parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Avoiding subspace collapse:</head><p>One possible effect when optimizing L( θ1 , .... θN ) is to reach a solution where all θ k values are similar. In that case, all the policies would have the same parameters value, and will thus all achieve the same performance at test-time. Since we want to encourage the policies to process information differently, and following <ref type="bibr" target="#b32">Wortsman et al. (2021)</ref>, we encourage the anchor policies to have different parameters. This is implemented through the use of a regularization term denoted C( θ1 , .... θN ) that measures how much anchor policies are similar in the parameter space. This auxiliary loss is defined as a pairwise loss between pairs of anchor parameters:</p><formula xml:id="formula_6">C( θ1 , .... θN ) = i =j cosine 2 (θ i , θ j ) (6)</formula><p>The final optimization loss is then:</p><formula xml:id="formula_7">L( θ1 , .... θN ) = E z∼p(z) [E τ ∼π θ [R(τ )]] − β i =j cosine 2 ( θi , θj ) with θ = N k=1 z k θk</formula><p>where β is an hyper-parameter (see Section 5 for a discussion abot the tuning of this term) that weights the auxiliary term.</p><p>Initialize: θ1 , θ2 , φ (Critic), n batch size</p><formula xml:id="formula_8">for k = 0, 1, 2... do Sample z 1 , ..., z n ∼ U [0,1] Define θ zi ← z i θ1 + (1 − z i ) θ2</formula><p>Sample trajectories {τ i } n 1 using {π θz i } Update θ1 and θ2 to maximize:</p><formula xml:id="formula_9">1 n n i=1 L P P O (θ zi ) − β cosine 2 θ1 , θ2</formula><p>Up. φ to minimize: </p><formula xml:id="formula_10">1 n n i=1 L M SE (φ, z i ) end</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">LINE OF POLICIES (LOP)</head><p>In the case of N = 2, the subspace of policies corresponds to a simple segment in the parameter space defined by θ1 and θ2 as extremities. θ1 and θ2 are combined through a scalar value z ∈ [0; 1]:</p><formula xml:id="formula_11">θ = z θ1 + (1 − z) θ2</formula><p>(7) Computationally, learning a line of policies 3 is similar to learning a single policy for which the number of parameters is doubled, making this particular case a good trade-off between expressivity and training speed. It corresponds to the following objective function:</p><formula xml:id="formula_12">L( θ1 , θ2 ) = E z∼U [0;1] E τ ∼π z θ1 +(1−z) θ2 [R(τ )] − cosine 2 ( θ1 , θ2 )<label>(8)</label></formula><p>We provide in Algorithm 7 the adapted version of the clipped PPO algorithm <ref type="bibr" target="#b25">(Schulman et al., 2017)</ref> for learning a subspace of policies. In comparison to the classical approach, the batch of trajectories is first acquired by multiple policies sampled following p(z) (line 2-3). Then the PPO objective is optimized taking into account the policies used when sampling trajectories (line 4). At last, the critic is updated (line 5), taking as an input the z value so that it can make robust estimations of the expected reward for all the policies in the subspace. Adapting off-policy algorithms would be similar. Additional details are provided in appendix. Note that, for environments with discrete actions, we have made the same adaptation based on the A2C algorithm since A2C has less hyperparameters than PPO and is easier to tune, with similar results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We perform experiments in 6 different environments. Implementations based on the SaLinA (Denoyer et al., 2021) library together with train and test environments will be released upon acceptance.</p><p>For each environment, we consider one train environment on which we trained the different methods, and multiple variations of the training environment for evaluation resulting in 50 test environments in total. The details of all the environment configurations and detailed performance are given in Appendix B. Note that the complete experiments correspond to hundred of trained policies, and dozens of thousands of policy evaluations. For simple control environments (i.e., CartPole, Pendulum and AcroBot), we introduce few variations of the physics constant at test-time, for instance by varying the mass of the cart, the length of the pole. For complex control environments (i.e., HalfCheetah and Ant using the BRAX library <ref type="bibr" target="#b8">(Freeman et al., 2021)</ref>, we both use variations of the physics (e.g., gravity), variations of the agent shape (e.g., changing the size of the leg, or of the foot) and sensor alterations. At last, in MiniGrid and ProcGen we perform experiments where the agent is trained in one particular levels, but is evaluated in other levels (single levels on MiniGrid, and set of 10 levels in ProcGen). Note that ProcGen is a pixel-based environment where the architecture of the policy is much more complex than in control environments. Toy experiments on a simple Maze 2d are given in Appendix B.8 to show the nature of the policies learned by the different methods.</p><p>We compare our approach LoP 4 with different state-of-the-art methods: a) The Single approach is just a single policy learned on the train environment, and evaluated on the test ones. b) The DI-AYN+R(reward) method is an extension of DIAYN <ref type="bibr" target="#b6">(Eysenbach et al., 2018)</ref> where a set of discrete policies is learned using a weighted sum between the DIAYN reward and the task reward: R DIAY N +R (s, a) = r(s, a) + β log p(z|s) (9) Critically, this model requires to choose a discriminator architecture to compute log p(z|s) and modifies the train reward by defining an intrinsic reward that may drastically change the behavior of the policies at train time. c) At last, we also compare with the model proposed in <ref type="bibr" target="#b22">(Osa et al., 2021)</ref> denoted Lc (Latent-conditioned) that works only for continuous actions. This model is also based on a continuous z variable sampled uniformly at train time, but only uses an auxiliary loss without changing the reward. This auxiliary loss is defined through the joint learning of a density estimation model log P (z|s, a) where back-propagation is made over the action a. As in DIAYN+R, this model needs to carefully define a good neural network architecture for density estimation. Since Lc cannot be used with environment that have discrete actions, we have adapted DIAYN+R (called DIAYN+R Cont.) using a continuous z variable (instead of a discrete one) and a density estimation model log P (z|s) as in <ref type="bibr" target="#b22">Osa et al. (2021)</ref>. Note that we do not compare to <ref type="bibr" target="#b15">(Kumar et al., 2020a)</ref> for the exact same reason as the one identified in <ref type="bibr" target="#b22">(Osa et al., 2021)</ref>: SMERL assumes that the reward is known over the complete trajectories which results in unnatural adaptation of on-policy RL algorithms like PPO. Moreover, preliminary experiments with SMERL does not demonstrate any advantage against DIAYN+R correctly tuned. We also provide (see Table <ref type="table" target="#tab_2">4</ref> in Appendix B.1) results where K independent policies are learned, the best one being selected over each test environment. This approach obtains lower performance than the proposed baseline and needs K more training samples making it unrealistic in most of the environments.</p><p>As network architectures, we use multi-layer perceptrons (MLP) with ReLU units for both the policy and the critic (detailed neural network architectures are described in Appendix). For DIAYN+R log P (z|s, ...) is also modeled by a MLP with a soft-max output. For Lc and DIAYN+R Cont., log P (z|s, ...) is modeled by a MLP that computes the mean of a Gaussian distribution with a fixed variance. For these baselines, z is concatenated with the environment observation as an input for the policy and the critic models.</p><p>To choose the hyper-parameters of the different methods, let us remind that test environments cannot be used at train time for doing hyper-parameters search and/or model selection which makes this setting particularly difficult. Therefore, we rely on the following procedure: a grid-search over hyper-parameters is made, learning a single policy over the train environment. The best value of the hyper-parameters is then selected as the one that provides the best policy at train time. These hyper-parameters are then used for all the different baselines. Concerning the β value, for LoP, we report test results for β = 1.0 while, for Lc and DIAYN+R, we use the best value of β on test environments. This corresponds to an optimistic evaluation of the baseline performances; aiming at showing that our method is much more efficient since it does not need such a beta-tuning (β = 1.0 giving good performance in the majority of cases). Said otherwise, we compare our model in the less favorable case where baselines have been unrealistically tuned.</p><p>For the adaptation step, each policy is evaluated over 10 episodes for stochastic environments or 1 single episode for deterministic environments. We repeat this procedure over 10 different training seeds, and report the reward over the different test environments together with standard deviation. All detailed results are available in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ANALYSIS</head><p>We report the test performance of the models on different environments in Table <ref type="table" target="#tab_0">1</ref>. In all the environments, the adaptive models perform better than learning a single policy over the train environment which is not surprising. In most of the cases, LoP is able to achieve a better performance than other methods. For instance, on HalfCheetah where we evaluate the different methods over 16 variations of the train environments, LoP achieves an average reward of 10589 while Lc and DIAYN+R obtain respectively 9547 and 9680 (standard deviations are reported in Appendix B). Some examples of the discovered that behaviors in Ant and HalfCheetah 5 for the different methods, and for different values of z are illustrated in Figures <ref type="figure" target="#fig_5">1b, 5 and 6</ref>. This outlines that learning models that are optimal on the train task reward, but with different parameter values, allows us to discover policies react differently to variations of the training environment. It seems to be a better approach than encouraging policies to have a different behaviors (i.e., generating different state distributions) at train time. Same conclusions can be drawn in most of the environments, including MiniGrid where LoP is able to explore large mazes while being trained only on small ones. On the ProcGen environment, where the observation is an image processed through a complex ConvNet architecture (See Appendix B.7), enforcing functional diversity (DIAYN+R) does not allow to learn good policies while the LoP model is able to better generalize to unseen levels. Note that the performance at train time is the same for all the different approaches reported (see the Table <ref type="table" target="#tab_1">3</ref> for instance) but quickly decreases in DIAYN for larger values of β while it stays stable for LoP where the best results are obtained for β = 1.0.</p><p>Interestingly, in CartPole, DIAYN+R performs quite well. Indeed, when analyzing the learned policies, it seems to be a specific case where it is possible to obtain optimal policies that are diverse w.r.t the states they are sampling (by moving the cart more or less on the right/left while maintaining the pole vertical).</p><p>We have also performed experiments where test environments have the same dynamics as the training environment, but with defective sensors (i.e., some features at test time have a null value -see Appendix B.2 Table <ref type="table">7</ref> on the Ant environment). The fact that LoP behaves also well confirms the effectiveness of our approach to different types of variations, including noisy features on which baselines methods were not applied in previous publications.</p><p>Sensitivity to hyper-parameters: One important characteristic of LoP is that it can be used with β = 1.0 and does not need to define any classifier architecture as opposed to DIAYN+R and Lc. Indeed, as shown in Figure <ref type="figure">3</ref> (left), the training performance of DIAYN drastically depends on a good tuning of β. Lc, which is less sensible, needs to use a correct classifier architecture as in DIAYN. LoP is simple to tune since the cosine term is usually easy to satisfy and our approach, at convergence, always reaches a 0 value on this term when β &gt; 0.0. As illustrated in Appendix B.1 and B2, it is also interesting to note than, on the BRAX environments, the number of environment interactions needed to train LoP is similar than the one needed to train a single policy and LoP comes with a very small overhead in comparison to classical methods.</p><p>Online adaptation: One interesting property is the number of policies (and thus of episodes) to test over a new environment to get a good performance. For LoP and Lc, given a trained model, one can evaluate as many policies (i.e., different values of z) as desired. For DIAYN+R, testing more policies also means training more policies which is expensive and less flexible. Table <ref type="table" target="#tab_1">3</ref> (right) provides the reward of the different methods when testing K policies on different HalfCheetah settings: as expected, the performance of DIAYN+R tends to decrease when K is large since the model has difficulties to learn too many diverse policies. For LoP and Lc, spending more episodes to evaluate more policies naturally leads to a better performance: these two models provide a better way to deal with the exploration-exploitation trade-off at test time. Again, please consider that Lc also needs to define an additional neural network architecture to model log P (z|s, a) while LoP does not, making our approach simpler.</p><p>Beyond a Line of Policies: While LoP is based on the learning of N = 2 anchor parameters, it is possible to combine more than two anchor parameters. We study two approaches combining N = 3 anchor parameters (that can be extended to N = 3): a) the first approach is a convex combination of policies (CoP) where z is sampled following a Dirichlet distribution. (b) The second approach is a Bézier combination (BoP) as explained in Appendix A.2. The results are presented in Table <ref type="table" target="#tab_1">3</ref> (right) over multiple HalfCheetah environments. It can be seen that these two strategies are not so efficient.</p><p>LoP is thus a good trade-off between the number of parameters to train and the performance (Note that BoP and CoP need more samples to converge), at least given the particular neural network architectures we have used in this paper. We also performed an in-depth analysis of the evolution of the reward when K is increasing for LoP and CoP in Halfcheetah test environment (Figure <ref type="figure">9</ref> in Annex). While we expected CoP to outperform LoP when K is high, the best reward becomes stable when K=20 for both methods, and in most test environments, CoP is not able to reach the same best reward as LoP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of the learned policies:</head><p>To better understand the nature of the policies discovered by the different approaches, we have made a qualitative study in which we analyze i) the robustness of the methods to corrupted observations, ii) the functional diversity induced by the different models, and iii) the specificity of the different learned policies to particular test environments. First, LoP is more robust to input feature corruption (see Table <ref type="table">B</ref>.2 for the results, and Table <ref type="table" target="#tab_3">5</ref> for the setting in Appendix) and we conjecture that it is because the diversity in the parameter space allows this model to learn policies that does not take into account the same input features equally. We also measure the functional diversity induced by the different models by training a posteriori a classifier that aims at recovering which policy (i.e which value of z) has generated particular trajectories (Exact protocol in Figure <ref type="figure">8</ref> in Appendix, with the training curves). On LoP with K = 5, such a classifier obtains a 82% accuracy at validation time showing that the 5 policies are quite diverse, but less than the DIAYN+R policies where the classifier reaches a 100% accuracy which is logical knowing the auxiliary loss introduced by DIAYN which enforces this type of diversity. It is interesting to note that with the trajectories generated in the test environments with LoP policies, the accuracy of the classifier is reaching 87 %: when LoP is facing new environments, it tends to generate more diverse policies. We think that it is due to the fact that, since the policies have different parameter values, they react differently to states that have not been encountered at train time. At last, Figure <ref type="figure" target="#fig_2">4</ref> (right) (and Figure <ref type="figure">7</ref> in appendix for K=10,20) illustrates which upon K = 5 policies is used for different test environments. It shows that both LoP and DIAYN+R use different policies over different test environments, showing that these methods are able to solve new environments by learning various policies and not a single but robust one. Examples of policies on a simple maze2d are given in Figure <ref type="figure" target="#fig_2">4</ref> (left) and Appendix which illustrate the diversity of the discovered policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Our contribution shares connections with different families of approaches. First of all, it focuses on the problem of online adaptation in Reinforcement Learning which has been studied under different terminologies: Multi-task Reinforcement Learning <ref type="bibr" target="#b31">(Wilson et al., 2007;</ref><ref type="bibr" target="#b29">Teh et al., 2017)</ref>, Transfer Learning <ref type="bibr" target="#b28">(Taylor and Stone, 2009;</ref><ref type="bibr" target="#b17">Lazaric, 2012)</ref> and Meta-Reinforcement Learning <ref type="bibr" target="#b7">(Finn et al., 2017;</ref><ref type="bibr" target="#b10">Hausman et al., 2018;</ref><ref type="bibr" target="#b12">Humplik et al., 2019)</ref>. Many different methods have been proposed, but the best majority considers that the agent is trained over multiple environments such that it can identify variations (or invariant) at train time. For instance, <ref type="bibr" target="#b4">Duan et al. (2016)</ref> assume that the agent can sample multiple episodes over the same environments and methods like <ref type="bibr" target="#b13">(Kamienny et al., 2020;</ref><ref type="bibr" target="#b18">Liu et al., 2021)</ref> consider that the agent has access to a task identifier at train time.</p><p>More recently, diversity-based approaches have been adapted to focus on the setting where only one training environment is available. They share with our model the idea of learning multiple policies instead of a single one. For instance, DIAYN <ref type="bibr" target="#b6">(Eysenbach et al., 2018)</ref> learns a discrete set of policies that can be reused and fine-tuned over new environments. It has been adapted to online adaptation in <ref type="bibr" target="#b15">(Kumar et al., 2020a)</ref> where the authors propose to combine the intrinsic diversity reward together with the training task reward. This trade-off is obtained through a threshold-based method (instead of a simple weighted sum) with good results. But this method suffers from a major drawback identified in <ref type="bibr" target="#b22">(Osa et al., 2021)</ref>: it necessitates to sample complete episodes at each epoch which is painful and not adapted to all the RL learning algorithms. <ref type="bibr" target="#b22">Osa et al. (2021)</ref> also proposed an alternative based on learning a continuous set of policies instead of a discrete one without using any intrinsic reward.</p><p>The method we propose is highly connected to recent researches on mode connectivity with neural networks. Mode connectivity is a set of approaches and analyses that focus on the shape of the parameter space. It has been used as a tool to study generalization in the supervised learning setting <ref type="bibr" target="#b9">(Garipov et al., 2018)</ref>, but also as a way to propose new algorithms in different settings <ref type="bibr" target="#b19">(Mirzadeh et al., 2021)</ref>. Obviously, the work that is the most connected to our approach is the model proposed in <ref type="bibr" target="#b32">(Wortsman et al., 2021</ref>) that provides a way to learn a subspace of models in the supervised learning setting. Our contribution adapts this approach to RL for learning policies in a completely different setting which is online adaptation. At last, our work is sharing similarities with robust RL which aims at discovering policies robust to variations of the training environment <ref type="bibr" target="#b21">(Oikarinen et al., 2020;</ref><ref type="bibr" target="#b33">Zhang et al., 2021)</ref>. The main difference is that robust RL techniques learn policies efficient 'in the worst case' and are not focused on the online adaptation to test environments (the objective is usually to learn a single policy efficient on different variations while we are learning multiple policies, just selecting one at test time).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND PERSPECTIVES</head><p>We investigate the idea of learning a subspace of policies in the reinforcement learning setting, and describe how this approach can be used for online adaptation. While simple, our method allows to obtain policies that are robust to variations of the training environments. Contrarily to other techniques, LoP does not need any particular tuning or definition of additional architectures to handle diversity, which is a critical aspect in the online adaptation setting where hyper-parameters tuning is impossible or at least very difficult. Future work includes the extension of this family of approaches in the continual reinforcement learning setting, the deeper understanding of the the built subspace and the investigation of different auxiliary losses to better control the shape of such a subspace.</p><p>We have made several efforts to ensure that the results provided in the paper are fully reproducible.</p><p>In Appendix, we provide a full list of all hyperparameters and extra information needed to reproduce our experiments. The source code is available on SaLinA repository such that everyone can reproduce the experiments 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A IMPLEMENTATION DETAILS</head><p>In this section, we provide details about the implementations of the baselines and our models. Appendix B provides details and additional results for each environment we used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 LOP-PPO</head><p>We detail the losses used in algorithm 7. First, we recall the clipped-PPO surrogate objective described in <ref type="bibr" target="#b25">Schulman et al. (2017)</ref>. For a given trajectory τ = {(s t , a t , r t )} T 0 collected by the policy π θ old , and denoting ρ t (θ) = π θ (at|st) π θ old , the goal is to maximize:</p><formula xml:id="formula_13">L P P O (θ) := 1 T T t=0 min [ρ t (θ) A π θ old (s t , a t ) , clip (ρ t (θ) , 1 + , 1 − ) A π θold (s t , a t )]<label>(10)</label></formula><p>Where function A π θ old is computed thanks to a value function V φ by using Generalized Advantage Estimation <ref type="bibr" target="#b26">(Schulman et al. (2018)</ref>). This function is simultaneously updated by regression on mean-squared error over the rewards-to-go R t . In our case, this function not only takes s t as an input, but also the value z:</p><formula xml:id="formula_14">L M SE (φ, z) := 1 T T t=0 V φ (s t , z) − R t 2<label>(11)</label></formula><p>In HalfCheetah and Ant experiments, we sampled actions from a reparametrized Gaussian distribution using a squashing function <ref type="bibr" target="#b30">(Ward et al. (2019)</ref>), but we set the standard deviation fixed (so it is an hyper-parameter encouraging exploration, called action std in Tables <ref type="table" target="#tab_4">3 and 6</ref>): the policy network only learns the mean of this distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 BOP AND COP</head><p>The only change between LoP and these models resides in the way we combine the N anchor policies. For CoP, it is just the generalization of LoP for N &gt; 2 (see 3.1). BoP, makes use of a Bezier parametric curve that uses Bernstein polynomials (the anchor parameters being the control points). For N = 3, it is defined by:</p><formula xml:id="formula_15">Θ = (1 − z) 2 θ1 + 2 (1 − z) z θ2 + z 2 θ3 , ∀z ∈ [0, 1]<label>(12)</label></formula><p>Concerning the policies z evaluated at test time, BoP uses the same strategy as LoP by testing values that are uniformly distributed in [0; 1]. For CoP, we opted for sampling K policies using a Dirichlet distribution over [0, 1] 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 DIAYN+R AND LC</head><p>In order to find the best trade-off between maximizing environment rewards and intrinsic rewards in DIAYN+R algorithm, we add the hyper-parameter β :</p><formula xml:id="formula_16">R DIAY N +R (s, a) = r(s, a) + β • log p(z|s)<label>(13)</label></formula><p>As an alternative to <ref type="bibr">DIAYN+R Osa et al. (2021)</ref> proposes an algorithm where the discriminator takes not only observations as an input but also the policy output, updating both discriminator q φ and policy π θ when back propagating the gradient. In this case, it is not necessary to add an intrinsic reward. While <ref type="bibr" target="#b22">Osa et al. (2021)</ref> illustrate their methods with TD3 and SAC, we adapted it to PPO. The surrogate loss is given by:</p><formula xml:id="formula_17">L LC := L P P O + β • log q φ (z | s, π θ (.|s, z))<label>(14)</label></formula><p>B EXPERIMENTS DETAILS AND ADDITIONAL RESULTS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 HALFCHEETAH</head><p>Task originally coming from OpenAI Gym <ref type="bibr" target="#b1">(Brockman et al., 2016)</ref>. Instead of using MuJoCo engine, we decided to use Brax <ref type="bibr" target="#b8">(Freeman et al., 2021)</ref> as it enables the possibility to acquire episodes on GPU. We use the vanilla environment for training.The policy and the critic are encoded by two different multi-layer perceptrons with ReLU activations. The base learning algorithm is PPO.</p><p>Test environments: we operated modifications similar as the ones proposed in <ref type="bibr" target="#b11">(Henderson et al., 2017)</ref>. Morphological variations: we changed the radius and mass of specific body parts (torso, thig, shin, foot). Variations in physics: we changed the gravity and friction coefficients. Table <ref type="table">2</ref> precisely indicates the nature of the changes for each environment. Table <ref type="table">2</ref>: Modified HalfCheetah environments used for testing. Morphological modifications include a variation on the mass and the radius of a specific part of the body (torso, thighs, shins, or feet). We also modified the dynamics (gravity and friction). Environment names are exhaustive: Big refers to a increase of 25% of radius and mass, Small refers to a decrease of 25%. For example, "BigFoot" refers to an HalfCheetah agent where feet have been increased in mass and radius by 25%. For gravity and friction, we also tried an increase/decrease by 50% (respectively tagged "Huge" and "Tiny").    . Figure <ref type="figure">7</ref>: Number of times (y-axis) each policy (x-axis) is chosen by k-shot evaluation over the 16 test environments of the HalfCheetah for each of the 10 seeds (one table per seed). In blue, LoP, in orange, DIAYN+R. Please note that the 10 same LoP models are used for K=5, K=10, K=20 which is not the case for DIAYN+R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Env name Modifications</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BigFeet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-parameter</head><p>. Figure <ref type="figure">8</ref>: We trained small discriminators over a dataset (100,000 environment interactions) of trajectories obtained with the learned policies of LoP and DIAYN+R when K=5. For each environment, for each seed, we trained a single discriminator and averaged the results. While the discriminators trained on DIAYN+R reach 100% accuracy rapidly on both train and test environments, they learn slower for LoP, with a slight advantage for the test environment, validating the fact that the diversity induced by the cosine similarity on the weights is more visible in variations of the environment rather than the environment on which the model has been trained. We evaluated the discriminator on a validation dataset (also 100,000 environment interactions) resulting in 100% accuracy for DIAYN in both train and test environments. For LoP, we obtained 82% accuracy on the training environment, and 87% on the test environments. The discriminator architecture consists in a neural network of two hidden layers of size 16, taking the unprocessed states as an input and outputting the predicted policy used (like in DIAYN).</p><p>Figure <ref type="figure">9</ref>: Evolution of the best reward obtained with respect to K for LoP (N=2) and CoP (N=3) for each Halfcheetah test environment. We ran the K-shot evaluation for each K from K=1 to K=100 using the method described in Appendix A.2: we simply sample K random coefficients using the uniform distribution over [0, 1] for LoP and the Dirichlet distribution over [0, 1] 3 for CoP. Results are averaged over 10 run for each K, and over the 10 models we learned for each method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 ANT</head><p>Task originally coming from OpenAI Gym <ref type="bibr" target="#b1">(Brockman et al. (2016)</ref>). Instead of using MuJoCo engine, we decided to use Brax <ref type="bibr" target="#b8">(Freeman et al. (2021)</ref>) as it enables the possibility to acquire episodes on GPU. We use the vanilla environment for training. The policy and the critic are encoded by two different multi-layer perceptrons with ReLU activations. The base learning algorithm is PPO.</p><p>Test environments: As for HalfCheetah, we operated variations in physics (gravity and friction coefficients). We also designed environments with a percentage of masked features to simulate defective sensors (They are sampled randomly and remain the same for each run).   Table <ref type="table" target="#tab_1">13</ref>: Results over Acrobot, using 10 policies, and 10 episodes per policy at adaptation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 PENDULUM</head><p>We use the openAI gym implementation of Pendulum as a training environment. The 3 test environments are provided by <ref type="bibr" target="#b23">Packer et al. (2018)</ref> where two different factor may vary: the mass and the length of the pendulum. We have considered 5 discrete actions between −1 and +1. We have used A2C as a learning algorithm.    -28.2 ± 13.3 Average -52.7 -28.9 -47.1 -44.0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Environment</head><p>Table <ref type="table" target="#tab_4">16</ref>: Results over Pendulum, using 10 policies, and 10 episodes per policy at adaptation time. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (Left) An illustration of the process of learning a subspace of policies. (Right) Comparison between PPO and our model in a test environment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The adaptation of the PPO Algorithm with the LoP model. The different with the standard PPO algorithm is that: a) trajectories are sampled using multiple policies θ zi b) The policy loss is augmented with the auxiliary loss, and c) The critic takes the values z i as an input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (left) Trajectories generated by K = 10 policies (one rows) on an unseen maze (objective is to go from left to right, see details in Appendix B.8) for DIAYN+R (left column with best β value) and LoP (right column with β = 1.0). It illustrates the diversity obtained with DIAYN+R and LoP. (right) Number of times (y-axis) each policy (x-axis) (over K = 5 tested policies) is chosen over the 16 test environments of the HalfCheetah setting for each of the 10 seeds. Blue is LoP and orange is DIAYN+R. Different policies are used for different test environments showing the interest of learning a subspace of policies. Note that in LoP, the anchor policies are rarely chosen. Results for K = 10 and K = 20 in Appendix (Figure 7).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Feet mass and radius ×1.25 BigFriction Friction coefficient ×1.25 BigGravity Gravity coefficient ×1.25 BigShins Shins mass and radius ×1.25 BigThighs Thighs mass and radius ×1.25 BigTorso Torso mass and radius ×1.25 SmallFeet Feet mass and radius ×0.75 SmallFriction Friction coefficient ×0.75 SmallGravity Gravity coefficient ×0.75 SmallShins Shins mass and radius ×0.75 SmallThighs Thighs mass and radius ×0.75 SmallTorso Torso mass and radius ×0.75 HugeFriction Friction coefficient ×1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Qualitative example of LoP trajectories on HalfCheetah "BigShins" test environment (5shot setting). The best reward is obtained for z = 0.75.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Extreme case: when torso radius and mass are increased by 50%. Only one policy is able to adapt without falling down (z = 0.5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Evolution of the cumulative reward during training on the generic Ant environment for LoP, DIAYN+R and Lc for different values of beta. On can see that DIAYN+R struggles to perform well on the train set for β = 1 and β = 10. Results are averaged over 10 seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :Figure 13 :</head><label>1213</label><figDesc>Figure 12: Trajectories learned by LoP (β = 1.0). Rows are environments, columns are the K = 10 policies test during online adaptation. For each test environment, at least one policy is able to reach the goal</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Average cumulated reward of the different models over multiple testing environments averaged over 10 training seeds (higher is better). For DIAYN and Lc, we report the results and tested 10 policies for LoP, Lc and DIAYN+R using 10 episodes per policy for stochastic environments, and 1 episode per policy on deterministic ones. Performance is evaluated using the deterministic policy. Standard deviation is reported for each single test environment in Appendix B. Performance of the models at train time that shows that for LoP β is not hurting train performance while it is DIAYN+R. Standard error deviation is reported in TableB.2 for each environment. We also report the performace at train time that shows that a too high value of β hurts DIAYN+R performance while is less critical in LoP. (right) Ablation study on the number of policies K used at test time on 3 HalfCheetah environment variations (see Appendix B.1 for further details and additional results) together with the performance of the BoP and CoP variants. Standard deviation is given in appendix, Table4.</figDesc><table><row><cell></cell><cell></cell><cell cols="9">CartPole Acrobot Pendulum Minigrid Brax HalfCheetah Brax Ant ProcGen</cell><cell>toy maze</cell></row><row><cell cols="2">Nb. Test Env.</cell><cell>6</cell><cell>6</cell><cell></cell><cell>3</cell><cell>6</cell><cell>16</cell><cell></cell><cell>15</cell><cell>3</cell><cell>4</cell></row><row><cell cols="2">Type of actions</cell><cell>Discr.</cell><cell cols="2">Discr.</cell><cell>Discr.</cell><cell>Discr.</cell><cell>Cont.</cell><cell></cell><cell>Cont.</cell><cell>Discr.</cell><cell>Discr.</cell></row><row><cell cols="2">Single Policy</cell><cell>143.4</cell><cell cols="2">-99.7</cell><cell>-52.7</cell><cell>0.169</cell><cell>7697</cell><cell></cell><cell>3338</cell><cell>11.09</cell><cell>-83.2</cell></row><row><cell>LoP</cell><cell></cell><cell>149.9</cell><cell cols="2">-93.2</cell><cell>-28.9</cell><cell>0.447</cell><cell>10589</cell><cell></cell><cell>4031</cell><cell>16.38</cell><cell>-33.6</cell></row><row><cell cols="2">DIAYN+R</cell><cell>168.1</cell><cell cols="2">-97.0</cell><cell>-47.1</cell><cell>0.248</cell><cell>9680</cell><cell></cell><cell>3759</cell><cell>11.45</cell><cell>-42.2</cell></row><row><cell cols="2">DIAYN+R L2</cell><cell>156.1</cell><cell cols="2">-93.6</cell><cell>-44.0</cell><cell>0.443</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Lc</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>9547</cell><cell></cell><cell>4020</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SmallFeet TinyFriction BigGravity</cell></row><row><cell></cell><cell>K =</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>Train Perf.</cell><cell></cell><cell>LoP</cell><cell>K=5</cell><cell>8283</cell><cell>10425</cell><cell>10464</cell></row><row><cell>LoP</cell><cell cols="4">β = 0.1 3905 3991 4164 β = 1. 4035 4031 4174 β = 10 3998 4012 4145</cell><cell>7659 7630 7670</cell><cell></cell><cell cols="2">K=10 K=20 DIAYN+R K=5 K=10</cell><cell>8805 8794 7580 7580</cell><cell>10662 10734 9132 9132</cell><cell>10578 10807 8989 8989</cell></row><row><cell>DIAYN</cell><cell cols="4">β = 0.1 3558 3833 3949 β = 1. 3451 3759 2878 β = 10 3356 3400 3109</cell><cell>7739 5388 4430</cell><cell></cell><cell>Lc</cell><cell>K=20 K=5 K=10 K=20</cell><cell>8255 8186 8186 8107</cell><cell>10003 9521 9661 9661</cell><cell>9766 9360 9488 9506</cell></row><row><cell></cell><cell cols="4">β = 0.1 3909 4020 4150</cell><cell>7767</cell><cell></cell><cell>BoP (N=3)</cell><cell>K=5 K=10</cell><cell>6775 6660</cell><cell>7867 7840</cell><cell>7878 8026</cell></row><row><cell>Lc</cell><cell>β = 1.</cell><cell cols="3">3820 3947 4126</cell><cell>7650</cell><cell></cell><cell></cell><cell>K=20</cell><cell>6996</cell><cell>7963</cell><cell>8015</cell></row><row><cell></cell><cell>β = 10</cell><cell cols="3">3870 3945 4108</cell><cell>7710</cell><cell></cell><cell>CoP (N=3)</cell><cell>K=5 K=10</cell><cell>8996 9210</cell><cell>9468 9523</cell><cell>9287 9568</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>K=20</cell><cell>9155</cell><cell>9979</cell><cell>9695</cell></row><row><cell cols="3">Figure 3: (left)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Hyper-parameters for PPO over HalfCheetah</figDesc><table><row><cell>Value</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Mean and standard deviation of cumulative reward achieved on HalfCheetah test sets per</figDesc><table /><note>model (see Table2for environment details). Results are averaged over 10 training seeds (i.e., 10 models are trained with the same hyper-parameters and evaluated on the 16 test environments). K is the number of policies tested at adaptation time, using 1 episode per policy since this environment is deterministic. Ensembling with K = 5 models takes 5 times more iterations to converge and testing values of K &gt; 5 is very costly in terms of GPU consumption.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Table 5 precisely indicates the nature of the changes for each environment Modified Ant environments used for testing.</figDesc><table><row><cell>Env name</cell><cell>Modifications</cell></row><row><cell>BigFriction</cell><cell>Friction coefficient ×1.25</cell></row><row><cell>BigGravity</cell><cell>Gravity coefficient ×1.25</cell></row><row><cell>SmallFriction</cell><cell>Friction coefficient ×0.75</cell></row><row><cell>SmallGravity</cell><cell>Gravity coefficient ×0.75</cell></row><row><cell>HugeFriction</cell><cell>Friction coefficient ×1.5</cell></row><row><cell>HugeGravity</cell><cell>Gravity coefficient ×1.5</cell></row><row><cell>TinyFriction</cell><cell>Friction coefficient ×0.5</cell></row><row><cell>TinyGravity</cell><cell>Gravity coefficient ×0.5</cell></row><row><cell>DefectiveSensor 5%</cell><cell>5% of env obs set to 0</cell></row><row><cell cols="2">DefectiveSensor 10% 10% of env obs set to 0</cell></row><row><cell cols="2">DefectiveSensor 15% 15% of env obs set to 0</cell></row><row><cell cols="2">DefectiveSensor 20% 20% of env obs set to 0</cell></row><row><cell cols="2">DefectiveSensor 25% 25% of env obs set to 0</cell></row><row><cell cols="2">DefectiveSensor 30% 30% of env obs set to 0</cell></row><row><cell cols="2">DefectiveSensor 35% 35% of env obs set to 0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Hyper-parameters for PPO over Ant</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 11 :</head><label>11</label><figDesc>Acrobot train and test environments</figDesc><table><row><cell></cell><cell cols="2">Hyper-parameter</cell><cell>Value</cell><cell></cell></row><row><cell></cell><cell>learning rate:</cell><cell></cell><cell>0.001</cell><cell></cell></row><row><cell></cell><cell cols="2">n acquisition steps per epoch:</cell><cell>8</cell><cell></cell></row><row><cell></cell><cell cols="2">n parallel environments:</cell><cell>32</cell><cell></cell></row><row><cell></cell><cell>critic coefficient:</cell><cell></cell><cell>1.0</cell><cell></cell></row><row><cell></cell><cell cols="2">entropy coefficient:</cell><cell>0.001</cell><cell></cell></row><row><cell></cell><cell>discount factor:</cell><cell></cell><cell>0.99</cell><cell></cell></row><row><cell></cell><cell>gae coefficient:</cell><cell></cell><cell>0.7</cell><cell></cell></row><row><cell></cell><cell>gradient clipping:</cell><cell></cell><cell>2.0</cell><cell></cell></row><row><cell></cell><cell cols="2">n neurons per layer:</cell><cell>16</cell><cell></cell></row><row><cell></cell><cell>n layers:</cell><cell></cell><cell>2</cell><cell></cell></row><row><cell></cell><cell></cell><cell>LoP</cell><cell></cell><cell></cell></row><row><cell></cell><cell>β:</cell><cell></cell><cell>1.0</cell><cell></cell></row><row><cell></cell><cell cols="2">DIAYN</cell><cell></cell><cell></cell></row><row><cell></cell><cell>β:</cell><cell></cell><cell>1.0</cell><cell></cell></row><row><cell></cell><cell cols="2">n neurons per layer discriminator:</cell><cell>16</cell><cell></cell></row><row><cell></cell><cell cols="2">n layers discriminator:</cell><cell>2</cell><cell></cell></row><row><cell></cell><cell cols="2">learning rate discriminator:</cell><cell>0.001</cell><cell></cell></row><row><cell cols="4">Table 12: Hyper-parameters for A2C over Acrobot</cell><cell></cell></row><row><cell></cell><cell>Single</cell><cell>LoP</cell><cell>DIAYN+R</cell><cell>DIAYN+R L 2</cell></row><row><cell>Heavy Acrobot</cell><cell>-108.4 ± 3.2</cell><cell>-105.1 ± 1.0</cell><cell>-108.0 ± 1.8</cell><cell>-108.2 ± 4.4</cell></row><row><cell cols="2">HighInertia Acrobot -108.7 ± 5.6</cell><cell>-99.8 ± 2.8</cell><cell>-106.0 ± 2.7</cell><cell>-106.8 ± 8.9</cell></row><row><cell>Light Acrobot</cell><cell cols="3">-120.7 ± 71.3 -107.2 ± 58.8 -115.2 ± 33.1</cell><cell>-93.1 ± 37.3</cell></row><row><cell>Long Acrobot</cell><cell>-124.3 ± 2.7</cell><cell>-115.8 ± 9.6</cell><cell>-117.3 ± 4.1</cell><cell>-117.5 ± 5.3</cell></row><row><cell>LowInertia Acrobot</cell><cell>-71.3 ± 2.3</cell><cell>-70.7 ± 0.7</cell><cell>-71.2 ± 0.7</cell><cell>-71.3 ± 1.9</cell></row><row><cell>Short Acrobot</cell><cell>-65.1 ± 2.8</cell><cell>-60.7 ± 0.6</cell><cell>-64.2 ± 2.6</cell><cell>-64.7 ± 5.6</cell></row><row><cell>Average</cell><cell>-99.7</cell><cell>-93.2</cell><cell>-97.0</cell><cell>-93.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 14 :</head><label>14</label><figDesc>Pendulum train and test environments</figDesc><table><row><cell>Hyper-parameter</cell><cell></cell><cell></cell><cell>Value</cell></row><row><cell>learning rate:</cell><cell></cell><cell></cell><cell>0.001</cell></row><row><cell cols="2">n acquisition steps per epoch:</cell><cell></cell><cell>8</cell></row><row><cell cols="2">n parallel environments:</cell><cell></cell><cell>32</cell></row><row><cell>critic coefficient:</cell><cell></cell><cell></cell><cell>1.0</cell></row><row><cell>entropy coefficient:</cell><cell></cell><cell></cell><cell>0.001</cell></row><row><cell>discount factor:</cell><cell></cell><cell></cell><cell>0.99</cell></row><row><cell>gae coefficient:</cell><cell></cell><cell></cell><cell>0.7</cell></row><row><cell>gradient clipping:</cell><cell></cell><cell></cell><cell>2.0</cell></row><row><cell>n neurons per layer:</cell><cell></cell><cell></cell><cell>16</cell></row><row><cell>n layers:</cell><cell></cell><cell></cell><cell>2</cell></row><row><cell cols="2">LoP</cell><cell></cell><cell></cell></row><row><cell>β:</cell><cell></cell><cell></cell><cell>1.0</cell></row><row><cell cols="2">DIAYN</cell><cell></cell><cell></cell></row><row><cell>β:</cell><cell></cell><cell></cell><cell>1.0</cell></row><row><cell cols="3">n neurons per layer discriminator:</cell><cell>16</cell></row><row><cell cols="2">n layers discriminator:</cell><cell></cell><cell>2</cell></row><row><cell cols="2">learning rate discriminator:</cell><cell></cell><cell>0.001</cell></row><row><cell cols="5">Table 15: Hyper-parameters for A2C over Pendulum</cell></row><row><cell>Single</cell><cell>LoP</cell><cell cols="2">DIAYN+R</cell><cell>DIAYN+R L 2</cell></row><row><cell>Light</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In the experimental study, one training environment is associated to multiple test environments to analyze the ability to adapt to different variations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Other ways to control the shape of the subspace can be used and we investigate some of them in Section 44  We consider the LoP-A2C and the LoP-PPO models for environments with respectively discrete and continuous actions. LoP-PPO could be also used in the discrete case but requires more hyper-parameter tuning.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Videos available at https://sites.google.com/view/subspace-of-policies/home</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/facebookresearch/salina/tree/main/salina_examples/ rl/subspace_of_policies</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>, 10 models are trained with the same hyper-parameters and evaluated on the 12 test sets). K is the number of policies tested at adaptation time, using 1 episode per policy since this environment is deterministic. For this environment, we split the results per β value as it has been used for beta ablation study (see Figure <ref type="figure">3</ref>)  We performed experiments on pixel-based environment with ProcGen. We used the FruitBot game for training considering 10 levels sampled uniformly at each episode. At test time, we selected 3 different environments, each of them composed of 10 uniformly sampled levels, sampled uniformly, and that were not seen at train time. We used the CNN architecture described in the ProcGen paper (IMPALA architecture <ref type="bibr" target="#b5">(Espeholt et al. (2018)</ref>)). For LoP, the two first blocks are fixed and only the parameters of the last block depend on the value of z. For DIAYN, the z value is provided as a one-hot vector stacked to the observation. As for Brax environment, we used PPO algorithm. To visualize the policies learned by the different methods, we just implemented a simple discrete maze (4 actions = up,down, left, right) where the objective is to go from the top-middle tile to the bottom-middle tile by moving through a corridor (size is 21 × 11). Reward is -1 at each step until goal is reached and the maximum number of steps is −100. The optimal policy in the training environment achieves −16. At test time, we generate walls in the corridor such that the agent has to avoid these walls to reach the goal. The observation space is a 5×5 square around the agent. Policies are learned by using PPO. Illustrations of the trajectories over the train and the 4 test environments are illustrated in <ref type="bibr">Figures 11,</ref><ref type="bibr">12,</ref><ref type="bibr">13</ref>   Table <ref type="table">22</ref>: Results over Maze2d, using K=10, averaged over 5 runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Loss surface simplexes for mode connecting volumes and fast ensembling</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Benton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lotfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v139/benton21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07-24">18-24 July 2021. 2021</date>
			<biblScope unit="page" from="769" to="779" />
		</imprint>
	</monogr>
	<note>139 of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<title level="m">Openai gym</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Minimalistic gridworld environment for openai gym</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chevalier-Boisvert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pal</surname></persName>
		</author>
		<ptr target="https://github.com/maximecb/gym-minigrid" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Salina: Sequential learning of agents</title>
		<author>
			<persName><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De La Fuente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Gaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Kamienny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Thompson</surname></persName>
		</author>
		<ptr target="https://gitHub.com/facebookresearch/salina,2021" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rl$ˆ2$: Fast reinforcement learning via slow reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>abs/1611.02779</idno>
		<ptr target="http://arxiv.org/abs/1611.02779" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Diversity is all you need: Learning skills without a reward function</title>
		<author>
			<persName><forename type="first">B</forename><surname>Eysenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno>abs/1802.06070</idno>
		<ptr target="http://arxiv.org/abs/1802.06070" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<title level="m">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. arXiv e-prints, art</title>
				<imprint>
			<date type="published" when="2017-03">Mar 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Brax -a differentiable physics engine for large scale rigid body simulation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raichuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Girgin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<ptr target="http://github.com/google/brax" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Loss surfaces, mode connectivity, and fast ensembling of dnns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/hash/be3087e74e9100d4bc4c6268cdbe8456-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. 2018. December 3-8, 2018. 2018</date>
			<biblScope unit="page" from="8803" to="8812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning an embedding space for transferable robot skills</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Benchmark environments for multitask learning in continuous domains</title>
		<author>
			<persName><forename type="first">P</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shkurti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dudek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Lifelong Learning: A Reinforcement Learning Approach Workshop</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Humplik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galashov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hasenclever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06424</idno>
		<title level="m">Meta reinforcement learning as task inference. arXiv e-prints, art</title>
				<imprint>
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning adaptive exploration strategies in dynamic environments through informed policy regularization. CoRR, abs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kamienny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pirotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2005.02934" />
		<imprint>
			<date type="published" when="2005">2005.02934, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Explaining landscape connectivity of low-cost solutions for multilayer nets</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kuditipudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/46a4378f835dc8040c8057beb6a2da52-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="14574" to="14583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">One solution is not all you need: Few-shot extrapolation via structured maxent RL</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/5d151d1059a6281335a10732fc49620e-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">December 6-12, 2020, virtual, 2020a</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">One solution is not all you need: Few-shot extrapolation via structured maxent RL. CoRR, abs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.14484" />
		<imprint>
			<date type="published" when="2010">2010.14484, 2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transfer in reinforcement learning: a framework and a survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reinforcement Learning</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="143" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Decoupling exploration and exploitation for meta-reinforcement learning without sacrifices</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v139/liu21s.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07-24">18-24 July 2021. 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="6925" to="6935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Linear mode connectivity in multitask and continual learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Görür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ghasemzadeh</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Fmg_fQYUejf" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
				<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
				<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Robust deep reinforcement learning through adversarial loss. CoRR, abs</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Oikarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Daniel</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2008.01976" />
		<imprint>
			<date type="published" when="1976">2008.01976, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Discovering diverse solutions in deep reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Osa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tangkaratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Assessing generalization in deep reinforcement learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Sim-to-Real Transfer of Robotic Control with Dynamics Randomization</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06537</idno>
		<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Proximal policy optimization algorithms. CoRR, abs/1707.06347</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1707.06347" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">High-dimensional continuous control using generalized advantage estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Mastering chess and shogi by self-play with a general reinforcement learning algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<idno>abs/1712.01815</idno>
		<ptr target="http://arxiv.org/abs/1712.01815" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transfer learning for reinforcement learning domains: A survey</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<idno type="ISSN">1532-4435</idno>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1633" to="1685" />
			<date type="published" when="2009-12">Dec. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Distral: Robust multitask reinforcement learning. CoRR, abs/1707.04175</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1707.04175" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Improving exploration in soft-actor-critic with normalizing flows policies. CoRR, abs</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smofsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Bose</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1906.02771" />
		<imprint>
			<date type="published" when="1906">1906.02771, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-task reinforcement learning: A hierarchical bayesian approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tadepalli</surname></persName>
		</author>
		<idno type="DOI">10.1145/1273496.1273624</idno>
		<ptr target="https://doi.org/10.1145/1273496.1273624" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning, ICML &apos;07</title>
				<meeting>the 24th International Conference on Machine Learning, ICML &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1015" to="1022" />
		</imprint>
	</monogr>
	<note>ISBN 9781595937933</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning neural network subspaces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v139/wortsman21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07-24">18-24 July 2021. 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="11217" to="11227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust reinforcement learning on state observations with learned optimal adversary</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Boning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=sCZbhBvqQaU" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
				<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021. OpenReview.net, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
