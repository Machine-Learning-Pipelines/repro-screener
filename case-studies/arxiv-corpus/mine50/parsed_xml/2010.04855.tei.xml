<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Kernel Methods for Causal Functions: Dose, Heterogeneous, and Incremental Response Curves</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-21">21 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rahul</forename><surname>Singh</surname></persName>
							<email>rahul.singh@mit.edu</email>
						</author>
						<author>
							<persName><forename type="first">Liyuan</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
							<email>arthur.gretton@gmail.com</email>
						</author>
						<title level="a" type="main">Kernel Methods for Causal Functions: Dose, Heterogeneous, and Incremental Response Curves</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-21">21 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">1455A325141029ACA249B44E09DA5620</idno>
					<idno type="arXiv">arXiv:2010.04855v7[econ.EM]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-10-27T19:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose estimators based on kernel ridge regression for nonparametric causal functions such as dose, heterogeneous, and incremental response curves. Treatment and covariates may be discrete or continuous in general spaces. Due to a decomposition property specific to the RKHS, our estimators have simple closed form solutions. We prove uniform consistency with finite sample rates via original analysis of generalized kernel ridge regression. We extend our main results to counterfactual distributions and to causal functions identified by front and back door criteria. We achieve state-of-the-art performance in nonlinear simulations with many covariates, and conduct a policy evaluation of the US Job Corps training program for disadvantaged youths.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction 1.Motivation</head><p>Program evaluation aims to measure the counterfactual relationship between treatment D and outcome Y , which may vary for different subpopulations: if we intervened on treatment, setting D = d, what would be the expected counterfactual outcome Y (d) for individuals with characteristics V = v? When treatment is binary, the causal parameter is a function θ 0 (v) = E{Y (1) − Y (0) | V = v} called the heterogeneous treatment effect; when treatment is continuous, it is a function θ 0 (d, v) = E{Y (d) | V = v} that we call a heterogeneous response curve. Assuming selection on observable covariates (V, X), the causal function θ 0 (d, v) can be recovered by integrating the regression function γ 0 (d, v, x) = E(Y | D = d, V = v, X = x) according to the conditional distribution pr(x | v): θ 0 (d, v) = γ 0 (d, v, x)dpr(x | v) <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b50">51]</ref>, which may be complex when there are many covariates.</p><p>The same is true for other causal functions such as dose and incremental response curves, and even counterfactual distributions, albeit with different regressions and reweightings. Therefore nonparametric estimation of a causal function involves three challenging steps: estimating a nonlinear regression, with possibly many covariates; estimating the distribution for reweighting, which may be conditional; and using the nonparametric distribution to integrate the nonparametric regression. For this reason, flexible estimation of nonparametric causal functions, such as θ 0 (d, v), is often deemed too computationally demanding to be practical for program evaluation.</p><p>Our key insight is that the reproducing kernel Hilbert space (RKHS), a popular nonparametric setting in machine learning, is precisely the class of functions for which the steps of nonparametric causal estimation can be separated. This decomposition follows almost immediately from the definition of the RKHS, and it is a specific strength of our framework; random forests, for example, do not allow such decoupling. Our key insight follows from a more fundamental one. Evaluation of a causal function is generally not a bounded functional over all of L 2 <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b40">41]</ref>. We prove that the evaluation of a causal function is a bounded functional over the RKHS H, which is a subset of L 2 . By the classic Riesz representation theorem of functional analysis, a bounded functional over a Hilbert space admits a decoupled inner product representation within the Hilbert space. We show how to use this representation to separate the steps of nonparametric causal estimation. This insight appears to be original.</p><p>We adapt kernel ridge regression, a classic machine learning algorithm that generalizes splines <ref type="bibr" target="#b71">[72]</ref>, to address the computational challenges of estimating causal functions such as dose, heterogeneous, and incremental response curves. Nonparametric estimation with kernels is quite simple: the nonlinear regression with many covariates can be estimated by simple matrix operations; the conditional distribution can be expressed as a regression problem and estimated by simple matrix operations as well; and the step of integration can be performed by taking the product of the results. The final nonparametric estimator for the causal function has a one line, closed form solution, unlike previous work. This simplicity makes the family of estimators highly practical. The proposed estimators are substantially simpler yet outperform some leading alternatives in nonlinear simulations with many covariates; see Supplement B. As extensions, we generalize our new algorithmic techniques to counterfactual distributions in Supplement C as well as causal functions and distributions identified by front and back door criteria in Supplement D.</p><p>Theoretically, our statistical guarantees rely on smoothness of the causal function and spectral decay of the covariance operator rather than the explicit dimension of treatment and covariates. In economic modelling, many variables may matter for labor market decisions, yet economic theory suggests that the effect of different intensities of job training should be well approximated by smooth functions. The emphasis on smoothness in the causal interpretation of RKHS assumptions generalizes standard Sobolev assumptions, and it differs from the emphasis on sparsity in lasso-type assumptions. Our causal function estimators are uniformly consistent with rates that combine minimax optimal rates for smooth nonparametric regressions. En route to our main results, we prove an improved rate for conditional expectation operators. Our main results are nonasymptotic and imply asymptotic uniform validity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Contribution</head><p>Conceptually, we illustrate how to use RKHS techniques in order to separate the steps of nonparametric causal estimation. In doing so, we provide a template for researchers to develop simple kernel estimators for complex causal estimands. Specifically, we clarify five assumptions under which we derive our various results: (i) identification, from the social scientific problem at hand; (ii) basic regularity conditions on the kernels, which are satisfied by all of the kernels typically used in practice; (iii) basic regularity on the outcome, treatment, and covariates, allowing them to be discrete or continuous variables that take values in general spaces (even texts, images, or graphs); (iv) smoothness of the causal estimand; and (v) spectral decay of the covariance operator. We combine these five assumptions to estimate causal functions, providing insight into the meaning and applicability of RKHS approximation assumptions for causal inference.</p><p>Statistically, we prove uniform consistency: our estimators converge to causal functions in sup norm, which encodes caution about worst case scenarios when informing policy decisions. Our finite sample rates of convergence explicitly account for each source of error at any finite sample size. Our rates do not directly depend on the data dimension, but rather the smoothness of the causal estimand and spectral decay of the covariance operator. The rates may indirectly depend on dimension; see Section 6 for discussion in the context of the Sobolev space, which is a special case of an RKHS. Of independent interest, we provide a technical innovation to justify our main results: relative to previous work, we prove faster rates of convergence in Hilbert-Schmidt norm for conditional expectation operators. We generalize our main results to prove convergence in distribution for counterfactual distributions. The analysis of uniform confidence bands for our causal function estimators is an open question that we pose for future research, since uniform inference for kernel ridge regression remains an open question in statistics.</p><p>Computationally, we demonstrate state-of-the-art performance in nonlinear simulations with many covariates, despite the relative simplicity of our proposal compared to existing machine learning approaches. In order to simplify the causal estimation problem, we assume that underlying conditional expectation functions are elements in an RKHS. We propose a family of global estimators with closed form solutions, avoiding density estimation and sampling even for complex integrals. Throughout, the only hyperparameters are kernel hyperparameters and ridge regression penalties. The former have well established tuning procedures, and the latter are easily tuned using the closed form solution for generalized cross validation (which is asymptotically optimal) or leave-one-out cross validation (which we derive). In practice, the tunings are similar, and the asymptotically optimal choice aligns with our statistical theory.</p><p>Empirically, our kernel ridge regression approach allows for simple yet flexible estimation of nuanced causal estimands. Such estimands provide meaningful insights about the Job Corps, the largest job training program for disadvantaged youth in the US. Our key statistical assumption is that different intensities of job training have smooth effects on counterfactual employment, and those effects are smoothly modified by age. In our program evaluation in Supplement B, we find that the effect of job training on employment substantially varies by class hours and by age; a targeted policy will be more effective. Our program evaluation confirms earlier findings while also uncovering meaningful heterogeneity. We demonstrate how kernel methods for causal functions are a practical addition to the empirical economic toolkit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>We view nonparametric causal functions as reweightings of an underlying regression, synthesizing the g formula <ref type="bibr" target="#b50">[51]</ref> and partial means <ref type="bibr" target="#b41">[42]</ref> frameworks. To express causal functions in this way, we build on canonical identification theorems under the assumption of selection on observables <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b1">2]</ref>. We propose simple, global estimators that combine kernel ridge regressions. Previous works that take a global view include <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31]</ref>, and references therein. A broad literature instead views causal functions as collections of localized treatment effects and proposes local estimators with Nadaraya-Watson smoothing, e.g. <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b10">11]</ref>, and references therein. By taking a global view rather than a local view, we propose simple estimators that can be computed once and evaluated at any value of a continuous treatment, rather than a computationally intensive procedure that must be reimplemented at any treatment value.</p><p>Our work appears to be the first to reduce estimation of dose, heterogeneous, and incremental response curves to kernel ridge regressions. Previous works incorporating the RKHS into nonparametric estimation focus on different causal functions: nonparametric instrumental variable regression <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b58">59]</ref>, and heterogeneous treatment effect conditional on the full vector of covariates <ref type="bibr" target="#b42">[43]</ref>. <ref type="bibr" target="#b42">[43]</ref> propose the R learner to estimate the heterogeneous treatment effect θ 0 (x) = E{Y (1) − Y (0) | X = x}. [43, Section 3] reviews the extensive literature that considers this estimand. The R learner minimizes a loss that contains inverse propensities and different regularization [43, eq. A24], and it does not appear to have a closed form solution. The authors prove oracle mean square error rates. By contrast, we pursue a more general heterogeneous response curve with discrete or continuous treatment, conditional on some interpretable subvector V <ref type="bibr" target="#b0">[1]</ref>:</p><formula xml:id="formula_0">θ 0 (d, v) = E{Y (d) | V = v}.</formula><p>Unlike previous work on nonparametric causal functions in the RKHS, we (i) consider dose, heterogeneous, and incremental response curves; (ii) propose estimators with closed form solutions; and (iii) prove uniform consistency, which is an important norm for policy evaluation.</p><p>We extend the framework from causal functions to counterfactual distributions. Existing work focuses on distributional generalizations of average treatment effect (ATE) or average treatment on the treated (ATT) for binary treatment <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, e.g. θ 0 = pr{Y (1) } − pr{Y (0) }. <ref type="bibr" target="#b39">[40]</ref> propose an RKHS approach for distributional ATE and ATT with binary treatment using inverse propensity scores and an assumption on the smoothness of a ratio of densities, which differs from our approach. Unlike previous work, we (i) allow treatment to be continuous; (ii) avoid inversion of propensity scores and densities; and (iii) study a broad class of counterfactual distributions for the full population, subpopulations, and alternative populations, e.g.</p><formula xml:id="formula_1">θ 0 (d, v) = pr{Y (d) | V = v}.</formula><p>We provide a detailed comparison with kernel methods for binary treatment effects in Section 5. Whereas we study causal functions, these works study causal scalars <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b57">58]</ref>. We clarify the sense in which our causal function estimators generalize known estimators for treatment effects to new estimators for causal functions. Previous work is inherently tied to the L 2 bounded functional perspective. However, evaluation of a causal function is not a bounded functional over all of L 2 <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b40">41]</ref>. Therefore our algorithms extend the conceptual framework of kernel methods for causal inference in a new direction. Our statistical contribution is a new, uniform analysis of response curves that goes beyond pointwise approximation of response curves by local treatment effects. This paper subsumes our previous draft [60, Section 2].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Causal functions</head><p>A causal function summarizes the expected counterfactual outcome Y (d) given a hypothetical intervention on continuous treatment that sets D = d. The causal inference literature studies a rich variety of causal functions with nuanced interpretation, which we define below. Unless otherwise noted, expectations are with respect to the population distribution pr. Likewise we define incremental functions, e.g.</p><formula xml:id="formula_2">θ ∇:AT E 0 (d) = E{∇ d Y (d) } where ∇ d means ∂/∂d.</formula><p>The superscript of each nonparametric causal function corresponds to its familiar parametric analogue. Results for means of potential outcomes immediately imply results for differences thereof. See Supplement C for counterfactual distributions and Supplement D for graphical models.</p><p>The dose response curves θ AT E 0 (d) and θ DS 0 (d, pr) are causal functions for entire populations. The second argument of θ DS 0 (d, pr) gets to the heart of external validity: though our data were drawn from population pr, what would be the dose response curve for a different population pr? For example, a job training study may be conducted in Virginia, yet we may wish to inform policy in Arkansas, a state with different demographics <ref type="bibr" target="#b24">[25]</ref>. Predictive questions of this nature are widely studied in machine learning under the names of transfer learning, distribution shift, and covariate shift <ref type="bibr" target="#b48">[49]</ref>. In θ AT T 0 (d, d ), heterogeneity is indexed by treatment D. Heterogeneity may instead be indexed by some interpretable covariate subvector V , e.g. age, race, or gender <ref type="bibr" target="#b0">[1]</ref>. An analyst may therefore prefer to measure heterogeneous effects for subpopulations characterized by different values of V . For simplicity, we will write covariates as (V, X) for this setting, where X are additional identifying covariates besides the interpretable covariates V . While many works focus on the special case where treatment is binary, our definition of heterogeneous response curve</p><formula xml:id="formula_3">θ CAT E 0 (d, v) = E{Y (d) | V = v} allows for continuous treatment.</formula><p>Lemma 3.1 (Identification of causal functions <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b50">51]</ref>). Under standard assumptions of selection on observables and covariate shift in Supplement A,</p><formula xml:id="formula_4">θ AT E 0 (d) = γ 0 (d, x)dpr(x), θ DS 0 (d, pr) = γ 0 (d, x)d pr(x), θ AT T 0 (d, d ) = γ 0 (d , x)dpr(x | d), and θ CAT E 0 (d, v) = γ 0 (d, v, x)dpr(x | v), where γ 0 (d, x) = E(Y | D = d, X = x) and γ 0 (d, v, x) = E(Y | D = d, V = v, X = x).</formula><p>Likewise we identify incremental functions, e.g. (d, v), we abuse notation by denoting the covariates by (V, X), where V is the subcovariate of interest and selection is with respect to the union (V, X). An analyst requires observations of (Y, D, V, X) drawn from the population pr.</p><formula xml:id="formula_5">θ ∇:AT E 0 (d) = ∇ d γ 0 (d, x)dpr(x) [2].</formula><p>In particular, Lemma 3.1 expresses each causal function as an integral of the regression function γ 0 according to a marginal or conditional distribution. As previewed in Section 1, nonparametric estimation of θ CAT E 0 (d, v) involves three steps: estimating a nonlinear regression γ 0 (d, v, x), which may involve many covariates X; estimating the conditional distribution pr(x | v) for reweighting; and using the latter to integrate the former. In the next section, we propose original estimators that achieve all three steps in a one line, closed form solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">RKHS background</head><p>To present the algorithm, we provide background on the RKHS. The essential property of a function γ in an RKHS H is the eponymous reproducing property: γ(w) = γ, φ(w) H where φ(w) are features, formally defined below, that serve as the basis functions for H. Our key algorithmic insight is to interpret the reproducing property as a way to separate the function γ from the features φ(w). We use this defining property of the RKHS to decouple the three steps of nonparametric causal estimation. After providing RKHS background material, we prove an inner product representation that formalizes the decoupling, then introduce the causal estimators.</p><p>A scalar-valued RKHS H is a Hilbert space with elements that are functions γ : W → R, on which the operator of evaluation is bounded <ref type="bibr" target="#b4">[5]</ref>. Polynomial, spline, and Sobolev spaces are widely used examples of RKHSs. W can be any Polish space, so a value w ∈ W can be discrete or continuous. An RKHS is fully characterized by its feature map, which takes a point w in the original space W and maps it to a feature φ(w) in the RKHS H. The closure of span{φ(w)} w∈W is the RKHS H. In other words, {φ(w)} w∈W can be viewed as the dictionary of basis functions for the RKHS H. The kernel k : W × W → R is the inner product of features φ(w) and φ(w ): k(w, w ) = φ(w), φ(w ) H . A real-valued kernel k is continuous, symmetric, and positive definite. Though we have constructed the kernel from the feature map, the Moore-Aronszajn Theorem states that, for any positive definite kernel k, there exists a unique RKHS H with feature map φ : w → k(w, •). We have already seen that if γ ∈ H, then γ : W → R. With the additional notation of the feature map, we write γ(w) = γ, φ(w) H . If W is separable and φ is continuous, then H is separable and may be infinite dimensional.</p><p>The RKHS is a practical hypothesis space for nonparametric regression. Consider output Y ∈ R, input W ∈ W, and the goal of estimating the conditional expectation function γ 0 (w</p><formula xml:id="formula_6">) = E(Y | W = w). A kernel ridge regression estimator of γ 0 is γ = argmin γ∈H n −1 n i=1 {Y i − γ, φ(W i ) H } 2 + λ γ 2 H .<label>(1)</label></formula><p>λ &gt; 0 is a hyperparameter on the ridge penalty γ 2 H , which imposes smoothness in estimation. The solution to the optimization problem has a well known closed form <ref type="bibr" target="#b32">[33]</ref>, which we exploit and generalize throughout this work:</p><formula xml:id="formula_7">γ(w) = Y (K W W + nλI) −1 K W w .<label>(2)</label></formula><p>The closed form solution involves the kernel matrix K W W ∈ R n×n with (i, j)th entry k(W i , W j ), and the kernel vector K W w ∈ R n with ith entry k(W i , w). To tune the ridge hyperparameter λ, both generalized cross validation and leave-one-out cross validation have closed form solutions, and the former is asymptotically optimal <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>We have seen that the feature map takes a value in the original space w ∈ W and maps it to a feature in the RKHS φ(w) ∈ H. Now we generalize this idea, from the embedding of a value w to the embedding of a distribution q. Just as a value w in the original space is embedded as an element φ(w) in the RKHS, so too the distribution q over the original space can be embedded as an element µ = E q {φ(W )} in the RKHS <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b4">5]</ref>. Boundedness of the kernel implies existence of the mean embedding as well as Bochner integrability, which permits us to exchange the expectation and inner product. Mean embeddings facilitate the evaluation of expectations of RKHS functions: for γ ∈ H, E q {γ(W )} = E q { γ, φ(W ) H } = γ, µ H . The final expression foreshadows how we will use the technique of mean embeddings to decouple the nonparametric regression step from the nonparametric reweighting step in the estimation of causal functions. A natural question is whether the embedding q → E q {φ(W )} is injective, i.e. whether the RKHS element representation is unique. This is called the characteristic property of the kernel k, and it holds for commonly used RKHSs e.g. the exponentiated quadratic kernel <ref type="bibr" target="#b63">[64]</ref>.</p><p>The tensor product RKHS is one way to construct an RKHS for functions with multiple arguments.</p><p>Consider the RKHSs H 1 and H 2 with positive definite kernels k 1 :</p><formula xml:id="formula_8">W 1 × W 1 → R and k 2 : W 2 × W 2 → R, respectively. An element γ 1 ∈ H 1 is a function γ 1 : W 1 → R and an element γ 2 ∈ H 2 is a function γ 2 : W 2 → R. The tensor product RKHS H = H 1 ⊗H 2 is the RKHS with the product kernel k : (W 1 × W 2 ) × (W 1 × W 2 ) → R, {(w 1 , w 2 ), (w 1 , w 2 )} → k 1 (w 1 , w 1 )k 2 (w 2 , w 2 ).</formula><p>Equivalently, the tensor product RKHS H has feature map φ(w 1 ) ⊗ φ(w 2 ) such that φ(w 1 ) ⊗ φ(w 2 ) H = φ(w 1 ) H1 φ(w 2 ) H2 . Formally, tensor product notation means (a ⊗ b)c = a b, c . An element of the tensor product RKHS γ ∈ H is a function γ : W 1 × W 2 → R. We assume that the regression function γ 0 (w</p><formula xml:id="formula_9">1 , w 2 ) = E(Y | W = w 1 , w 2 = w 2</formula><p>) is an element of a tensor product RKHS, i.e. γ 0 ∈ H. As such, the different arguments of γ 0 are decoupled, which we exploit when calculating partial means.</p><p>Finally, we introduce the RKHS L 2 (H 1 , H 2 ) that we employ for conditional expectation operators. Rather than being a space of real-valued functions, it is a space of Hilbert-Schmidt operators from one RKHS to another. If the operator E is an element of L 2 (H 1 , H 2 ), then E :</p><formula xml:id="formula_10">H 1 → H 2 .</formula><p>Formally, it can be shown that L 2 (H 1 , H 2 ) is an RKHS in its own right with an appropriately defined kernel and feature map. L 2 (H 1 , H 2 ) is an example of a vector-valued RKHS; see <ref type="bibr" target="#b38">[39]</ref> for a more general discussion. In the present work, we assume the conditional expectation operator</p><formula xml:id="formula_11">E 0 : γ 1 (•) → E{γ 1 (W 1 ) | W 2 = •} is an element of this RKHS, i.e. E 0 ∈ L 2 (H 1 , H 2 ).</formula><p>We estimate E 0 by a kernel ridge regression in L 2 (H 1 , H 2 ), which coincides with estimating the conditional mean embedding µ w1 (w 2 ) = E{φ(W 1 ) | W 2 = w 2 } via the kernel ridge regression of φ(W 1 ) on φ(W 2 ); see the derivation of Algorithm 4.1 below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Decoupled representation</head><p>Lemma 3.1 makes precise how each causal function is identified as a partial mean of the form γ 0 (d, x)dq for some distribution q. To facilitate estimation, we now assume that γ 0 is an element of an RKHS. In our construction, we define scalar valued RKHSs for treatment D and covariates (V, X), then assume that the regression is an element of the tensor product space. Let k D : D × D → R, k V : V × V → R, and k X : X × X → R be measurable positive definite kernels corresponding to scalar valued RKHSs H D , H V , and H X . Denote the feature maps φ</p><formula xml:id="formula_12">D : D → H D , d → k D (d, •); φ V : V → H V , v → k V (v, •); φ X : X → H X , x → k X (x, •).</formula><p>To lighten notation, we suppress subscripts when arguments are provided.</p><p>For θ AT E 0 , θ DS 0 , and θ AT T 0 , we assume the regression γ 0 is an element of the RKHS H with the kernel k(d, x; d , x ) = k D (d, d )k X (x, x ). We appeal to the fact that the product of positive definite kernels for H D and H X defines a new positive definite kernel for H. The product construction provides a rich composite basis; H has the tensor product feature map φ(d) ⊗ φ(x) and</p><formula xml:id="formula_13">H = H D ⊗ H X . In this RKHS, γ 0 (d, x) = γ 0 , φ(d) ⊗ φ(x) H . Likewise for θ CAT E 0 we assume γ 0 ∈ H = H D ⊗ H V ⊗ H X .</formula><p>We place regularity conditions on this RKHS construction in order to represent causal functions as inner products in H. In anticipation of counterfactual distributions in Supplement C, we also include conditions for an outcome RKHS in parentheses. Assumption 4.1 (RKHS regularity conditions). Assume</p><formula xml:id="formula_14">1. k D , k V , k X (and k Y ) are continuous and bounded. Formally, sup d∈D φ(d) H D ≤ κ d , sup v∈V φ(v) H V ≤ κ v , sup x∈X φ(x) H X ≤ κ x {and sup y∈Y φ(y) H Y ≤ κ y }.</formula><p>2. φ(d), φ(v), φ(x) {and φ(y)} are measurable. </p><formula xml:id="formula_15">3. θ AT T 0 (d, d ) = γ 0 , φ(d ) ⊗ µ x (d) H where µ x (d) = φ(x)dpr(x | d); 4. θ CAT E 0 (d, v) = γ 0 , φ(d) ⊗ φ(v) ⊗ µ x (v) H where µ x (v) = φ(x)dpr(x | v).</formula><p>Likewise for incremental functions, e.g.</p><formula xml:id="formula_16">θ ∇:AT E 0 (d) = γ 0 , ∇ d φ(d) ⊗ µ x H . Sketch. Consider θ CAT E 0 (d, v).</formula><p>Boundedness of the kernel implies Bochner integrability, which allows us to exchange the integral and inner product:</p><formula xml:id="formula_17">γ 0 (d, v, x)dpr(x | v) = γ 0 , φ(d) ⊗ φ(v) ⊗ φ(x) H dpr(x | v) = γ 0 , φ(d) ⊗ µ x (v) H .</formula><p>See Supplement E for the full proof. </p><formula xml:id="formula_18">µ x (v) = φ(x)pr(x | v)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Closed form solution</head><p>The representation in Theorem 4.1 is essential to the algorithm derivation. In particular, the representation cleanly separates the three steps necessary to estimate a causal function: estimating a nonlinear regression, which may involve many covariates; estimating the distribution for reweighting; and using the nonparametric distribution to integrate the nonparametric regression. For example, for</p><formula xml:id="formula_19">θ CAT E 0 (d, v), our estimator is θCAT E (d, v) = γ, φ(d) ⊗ φ(v) ⊗ μx (v) H .</formula><p>The nonlinear regression estimator γ is a standard kernel ridge regression of Y on φ(D) ⊗ φ(V ) ⊗ φ(X); the reweighting distribution estimator μx (v) is a generalized kernel ridge regression of φ(X) on φ(V ); and the latter can be used to integrate the former by simply multiplying the two. This algorithmic insight is a key innovation of the present work, and the reason why our estimators have simple closed form solutions despite complicated causal integrals. Algorithm 4.1 (Estimation of causal functions). Denote the empirical kernel matrices K DD , K V V , K XX ∈ R n×n calculated from observations drawn from population pr. Let Xi (i = 1, ..., ñ) be observations drawn from population pr. Denote by the elementwise product. Causal function estimators have the closed form solutions</p><formula xml:id="formula_20">1. θAT E (d) = n −1 n i=1 Y (K DD K XX + nλI) −1 (K Dd K Xxi ); 2. θDS (d, pr) = ñ−1 ñ i=1 Y (K DD K XX + nλI) −1 (K Dd K X xi ); 3. θAT T (d, d ) = Y (K DD K XX + nλI) −1 [K Dd {K XX (K DD + nλ 1 I) −1 K Dd }]; 4. θCAT E (d, v) = Y (K DD K V V K XX + nλI) −1 [K Dd K V v {K XX (K V V + nλ 2 I) −1 K V v }];</formula><p>where (λ, λ 1 , λ 2 ) are ridge regression penalty hyperparameters. Likewise for incremental functions, e.g. θ∇:AT</p><formula xml:id="formula_21">E (d) = n −1 n i=1 Y (K DD K XX + nλI) −1 (∇ d K Dd K Xxi ) where (∇ d K Dd ) i = ∇ d k(D i , d).</formula><p>Sketch. Consider θ CAT E 0 (d, v). Analogously to (1), the kernel ridge regression estimators of the regression γ 0 and the conditional mean embedding µ x (v) are given by γ = argmin</p><formula xml:id="formula_22">γ∈H n −1 n i=1 {Y i − γ, φ(D i ) ⊗ φ(V i ) ⊗ φ(X i ) H } 2 + λ γ 2 H , Ê = argmin E∈L2(H X ,H V ) n −1 n i=1 {φ(X i ) − E * φ(V i )} 2 + λ 2 E 2 L2(H X ,H V ) ,</formula><p>where μx (v) = Ê * φ(v) and E * is the adjoint of E. Analogously to (2), the closed forms are</p><formula xml:id="formula_23">γ(d, v, •) = Y (K DD K V V K XX + nλI) −1 {K Dd K V v K X(•) }, [μ x (v)](•) = K (•)X (K V V + nλ 2 I) −1 K V v .</formula><p>To arrive at the main result, match the empty arguments (•) of the kernel ridge regressions.</p><p>See Supplement E for the full derivation and a comparison to series estimation. We give theoretical values for (λ, λ 1 , λ 2 ) that optimally balance bias and variance in Theorem 6.1 below. Supplement F gives practical tuning procedures based on generalized and leave-one-out cross validation to empirically balance bias and variance, the former of which is asymptotically optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Comparison to kernel methods for causal scalars</head><p>We now connect our kernel methods for causal functions with related kernel methods for treatment effects. Recall the definition</p><formula xml:id="formula_24">θ AT E 0 (d) = E{Y (d) }.</formula><p>We allow treatment to be continuous, so θ AT E 0 is a causal function called the dose response. In related work, treatment is binary, so θ AT E 0 is a vector of two causal scalars θ AT E 0 (1), θ AT E 0 (0) whose difference is the treatment effect.</p><p>We clarify three points. (i) There is a sense in which our algorithms generalize known estimators for treatment effects to new estimators for causal functions. (ii) A treatment effect is a bounded functional over L 2 with a balancing weight representation, while a response curve is not. Our key insight is that a response curve is a bounded functional over the RKHS H, which is a subset of L 2 . (iii) Our theoretical contribution is a new, uniform analysis of response curves. The analysis goes beyond pointwise approximation of response curves by local treatment effects.</p><p>We begin by reviewing the theory of balancing weights, which are popular in causal inference with binary treatments. For clarity, in this section we emphasize a fixed treatment value by writing d * ∈ D. The following representation is well known. Clearly, the two representations are related by the law of iterated expectations. Moreover, from the closed form of α 0 , we require pr(D = d * | X) &gt; 0 for α 0 to exist. This property keenly relies on the treatment being discrete. Indeed, it is well known that a balancing weight representation does not exist for response curves. Proposition 5.2 (Non-existence for response curves <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b40">41]</ref>). Suppose selection on observables (stated in Supplement A) holds and treatment is continuous.</p><formula xml:id="formula_25">Fix d * ∈ D. Even if the density f (d * | X)</formula><p>is bounded away from zero almost surely, there does not exist a balancing weight α 0 ∈ L 2 such that for all γ ∈ L 2 , γ(d * , x)dpr(x) = γ, α 0 L 2 . In particular, without further restrictions, there does not exist</p><formula xml:id="formula_26">α 0 ∈ L 2 such that θ AT E 0 (d * ) = yα 0 (d, x)dpr(d, x, y) = γ 0 , α 0 L 2 .</formula><p>Whereas a binary treatment effect is a bounded functional over L 2 with a balancing weight representation, a dose response is not a bounded functional over L 2 and does not have a balancing weight representation in the classic sense. From a functional analytic perspective, this discrepancy is the reason why the problems we study are nonparametric whereas previous work on kernel methods for treatment effects are semiparametric. See Supplement G for discussion.</p><p>Our key insight is that the dose response is a bounded functional over the RKHS H, which is a subset of L 2 . This fact follows from three simple observations: (i) the dose response is a partial mean; (ii) in the RKHS, a partial mean can be reformulated as a kind of evaluation; and (iii) the RKHS H is the subset of L 2 for which evaluation is a bounded functional. Through this lens, Theorem 4.1 shows that there can exist a function α0 ∈ H such that θ AT E 0 (d) = γ 0 , α0 H even when there does not exist a function</p><formula xml:id="formula_27">α 0 ∈ L 2 such that θ AT E 0 (d) = γ 0 , α 0 L 2 .</formula><p>What is the relationship between between our kernel methods for causal functions and existing kernel methods for treatment effects? There is a sense in which our dose response estimator, which is the simplest case of our framework, is a relaxation of kernel balancing weight estimators from binary treatment to continuous treatment. We formalize this connection as follows. Corollary 5.1 (Relaxation of balancing weight estimators). Suppose treatment is binary, and take</p><formula xml:id="formula_28">k D (d, d ) = 1(d = d ) to be the treatment kernel. Then θAT E (d) = n −1 n i=1 Y i αi , where αi = α(D i , X i ) and α is a ridge regularized estimator of α 0 ∈ L 2 .</formula><p>See Supplement G for the proof. The balancing weight estimator α minimizes a generalized balancing weight loss with ridge regularization; see <ref type="bibr">[28, eq. 8]</ref>, <ref type="bibr">[24, eq. 1]</ref>, and [58, Definition 3.2] for various formulations. Corollary 5.1 provides intuition for our tensor product RKHS construction. Our product kernel construction ensures that using the binary treatment kernel amounts to subsetting, which recovers previous algorithms. The tensor product RKHS provides a natural way to relax binary treatment to continuous treatment while retaining computational tractability.</p><p>As argued in Proposition 5.2, the balancing weight α 0 ∈ L 2 does not exist for the dose response. Nonetheless, our key insight in Theorem 4.1 is that a function α0 ∈ H does exist to serve a similar purpose. By combining the partial mean perspective with the technique of kernel mean embedding, we demonstrate that our framework easily extends to conditional nonparametric causal functions, e.g. the heterogeneous response curve θ CAT E 0 (d, v), which are substantially more challenging than unconditional nonparametric causal functions, e.g. the dose response θ AT E 0 (d).</p><p>Perhaps the most surprising consequence of our construction is the closed form solution for causal functions. In particular, each closed form solution is a reweighting of the observed outcomes with empirical weights that we characterize even though a population balancing weight in L 2 does not exist. In sum, previous work <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b57">58]</ref> on kernel methods for treatment effects is inherently tied to the L 2 population balancing weight perspective; our algorithms apply the conceptual framework of kernel methods to new classes of causal functions. The following corollary reinterprets Algorithm 4.1 through this lens. Corollary 5.2 (Closed form reweighting even when balancing weight does not exist). Suppose treatment is continuous, with k D that is continuous and bounded. Then θAT</p><formula xml:id="formula_29">E (d) = n −1 n i=1 Y i αAT E i , θDS (d, pr) = n −1 n i=1 Y i αDS i , θAT T (d, d ) = n −1 n i=1 Y i αAT T i</formula><p>, and</p><formula xml:id="formula_30">θCAT E (d, v) = n −1 n i=1 Y i αCAT E i</formula><p>, where the weights have closed form solutions given in Supplement G. Likewise for incremental functions, e.g. θ∇:</p><formula xml:id="formula_31">AT E (d) = n −1 n i=1 Y i α∇:AT E i .</formula><p>Each of our proposed causal function estimators is global. In particular, within Corollary 5.2, the weights (α AT E j , αDS j , αAT T j</p><p>, αCAT E j ) (j = 1, ..., n) depend on all of the observations as refracted through the ridge regularized empirical covariance and the kernel evaluations k(D i , d). This approach departs from a localization approach to causal functions whereby the weight assigned to each observation is determined by Nadaraya-Watson smoothing <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10]</ref>. In the localization approach, the weight is k N W {(D i − d)/h} where k N W is a Nadaraya-Watson kernel and h is a vanishing bandwidth. By contrast, we consider a fixed kernel and vanishing ridge regularization.</p><p>The global perspective has three main advantages. First, our estimators can be computed once and evaluated at any value of a continuous treatment. By contrast, a localized estimator is a computationally intensive procedure that must be reimplemented at any treatment value. Second, our estimators are constructed from function classes with designed-in smoothness properties, which leads to smoother and therefore more plausible response curves. We compare our smooth estimate with a jagged localizing estimate in the program evaluation of Supplement B. Third, we prove uniform consistency of response curves, whereas localizations of previous results would only lead to pointwise consistency. These uniform guarantees are the focus of the next section.</p><p>6 Uniform consistency</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">RKHS background</head><p>In Section 3, we defined the causal functions of interest, and identified them as partial means. In Section 4, we introduced the tensor product RKHS as the function space in which the three steps of nonparametric causal estimation may be decoupled. We then proposed estimators based on kernel ridge regression with closed form solutions. In Section 5, we demonstrated that our estimators generalize known estimators for the binary treatment case. In this section, we prove uniform consistency of the estimators, with finite sample rates that combine minimax optimal rates. To do so, we define our key approximation assumptions, which are standard in RKHS learning theory: smoothness and spectral decay.</p><p>To state our key assumptions, we must introduce a certain eigendecomposition. Recall the example of a generic RKHS H with kernel k : W × W → R consisting of functions γ : W → R. Let ν be any Borel measure on W. We denote by L 2 ν (W) the space of square integrable functions with respect to measure ν. Given the kernel, define the integral operator L :</p><formula xml:id="formula_32">L 2 ν (W) → L 2 ν (W), γ → k(•, w)γ(w)dν(w).</formula><p>If the kernel k is defined on W ⊂ R d and shift invariant, then L is a convolution of k and γ. If k is smooth, then Lγ is a smoothed version of γ. L is a self adjoint, positive, compact operator, so by the spectral theorem we can denote its countable eigenvalues by (η j ) and its countable eigenfunctions, which are equivalence classes, by {(ϕ j ) ν }:</p><formula xml:id="formula_33">Lγ = ∞ j=1 η j (ϕ j ) ν , γ L 2 ν (W) (ϕ j ) ν , (ϕ j ) ν = {f : ν(f = ϕ j ) = 0}.</formula><p>Without loss of generality, η j ≥ η j+1 , and these are also the eigenvalues of the feature covariance operator T = E{φ(W ) ⊗ φ(W )}. For simplicity, we assume (η j ) &gt; 0 in this discussion; see <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">Remark 3]</ref> for the more general case. {(ϕ j ) ν } form an orthonormal basis of L 2 ν (W). By the generalized Mercer's Theorem for Polish spaces [66, Corollary 3.5], we can express the kernel as k(w, w ) = ∞ j=1 η j ϕ j (w)ϕ j (w ), where (w, w ) are in the support of ν, ϕ j is a continuous element in the equivalence class (ϕ j ) ν , and the convergence is absolute and uniform.</p><p>With this notation, we express L 2 ν (W) and the RKHS H in terms of the series {(ϕ j ) ν }. If γ ∈ L 2 ν (W), then γ can be uniquely expressed as γ = ∞ j=1 γ j (ϕ j ) ν and the partial sums</p><formula xml:id="formula_34">J j=1 γ j (ϕ j ) ν converge to γ in L 2 ν (W). Indeed, for γ = ∞ j=1 γ j (ϕ j ) ν and γ = ∞ j=1 γ j (ϕ j ) ν , L 2 ν (W) =    γ = ∞ j=1 γ j (ϕ j ) ν : ∞ j=1 γ 2 j &lt; ∞    , γ, γ L 2 ν (W) = ∞ j=1 γ j γ j .</formula><p>By <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">Theorem 4]</ref>, the RKHS H can be explicitly represented as</p><formula xml:id="formula_35">H =   γ = ∞ j=1 γ j ϕ j : ∞ j=1 γ 2 j η j &lt; ∞   , γ, γ H = ∞ j=1 γ j γ j η j .</formula><p>To interpret this result, recall that (η j ) is a weakly decreasing sequence. The RKHS H is the subset of functions in L 2 ν (W) which are continuous and for which higher order terms in the series {(ϕ j ) ν } have a smaller contribution. The RKHS inner product penalizes higher order coefficients, and the magnitude of the penalty corresponds to how small the eigenvalue is.</p><p>We have seen how to conduct kernel ridge regression with the RKHS H. To analyze the bias from ridge regularization, we place a smoothness assumption called the source condition on the regression function γ 0 (w) = E(Y | W = w) <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. Formally, we place assumptions of the form</p><formula xml:id="formula_36">γ 0 ∈ H c =   f = ∞ j=1 γ j ϕ j : ∞ j=1 γ 2 j η c j &lt; ∞   ⊂ H, c ∈ (1, 2].<label>(3)</label></formula><p>While c = 1 recovers correct specification γ 0 ∈ H, c ∈ (1, 2] is a stronger condition: γ 0 is a particularly smooth element of H, well approximated by the leading terms in the series {(ϕ j ) ν }. Smoothness delivers uniform consistency. A larger value of c corresponds to a smoother target γ 0 and a faster convergence rate for γ. Rates do not further improve for c &gt; 2, which is known as the saturation effect for ridge regularization.</p><p>To analyze the variance of kernel ridge regression, we place a spectral decay assumption called the effective dimension of the basis (ϕ j ) for the RKHS H. To obtain faster convergence rates, we place a direct assumption on the rate at which the eigenvalues (η j ), and hence the importance of the eigenfunctions (ϕ j ), decay: we assume there exists some constant C such that for all j</p><formula xml:id="formula_37">η j ≤ Cj −b , b ≥ 1.<label>(4)</label></formula><p>A bounded kernel, which we have already assumed, implies b = 1 [18, <ref type="bibr">Lemma 10]</ref>. The limit b → ∞ may be interpreted as a finite dimensional RKHS <ref type="bibr" target="#b5">[6]</ref>. For intermediate values of b, the polynomial rate of spectral decay quantifies the effective dimension of the RKHS H in light of the measure ν.</p><p>Intuitively, a higher value of b corresponds to a lower effective dimension and a faster convergence rate for γ.</p><p>For intuition, we relate the source condition and effective dimension to a familiar notion of smoothness in the Sobolev space. The restriction that defines an RKHS generalizes higher order smoothness in a Sobolev space. Indeed, certain Sobolev spaces are RKHSs. Let W ⊂ R p . Denote by H s 2 the Sobolev space with s &gt; p/2 derivatives that are square integrable. This space can be generated by the Matèrn kernel, which converges to the popular exponentiated quadratic kernel as s → ∞. Suppose H = H s 2 is chosen as the RKHS for estimation. Suppose the measure ν supported on W is absolutely continuous with respect to the uniform distribution and bounded away from zero. If γ 0 ∈ H s0 2 , then c = s 0 /s <ref type="bibr" target="#b47">[48]</ref>. Written another way, (H s 2 ) c = H s0 2 . In this sense, c precisely quantifies the additional smoothness of γ 0 relative to H. Moreover, in this Sobolev space, b = 2s/p &gt; 1 <ref type="bibr" target="#b17">[18]</ref>. The effective dimension is increasing in the input dimension p and decreasing in the degree of smoothness s. The minimax optimal rate in Sobolev norm is n −(c−1)/{2(c+1/b)} = n −(s0−s)/(2s0+p) , which is achieved by kernel ridge regression with the rate optimal regularization λ = n −1/(c+1/b) = n −2s/(2s0+p) . Our analysis applies to Sobolev spaces over R p as a special case; our results are much more general, allowing treatment and covariates to be in Polish spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Finite sample rates</head><p>Towards a guarantee of uniform consistency, we place regularity conditions on the original spaces. In anticipation of counterfactual distributions in Supplement C, we also include conditions for the outcome space in parentheses. Assumption 6.1 (Original space regularity conditions). Assume D, V, X (and Y) are Polish spaces. Further assume Y ⊂ R, y 2 dpr(y) &lt; ∞, and a moment condition holds: there exist constants σ, τ such that for all m ≥ 2,</p><formula xml:id="formula_38">|y − γ 0 (D, X)| m dpr(y | D, X) ≤ m!σ 2 τ m−2 /2 almost surely. For θ CAT E 0 , replace X with (V, X).</formula><p>A Polish space is a separable and completely metrizable topological space. Random variables with support in a Polish space may be discrete or continuous and may even be infinite dimensional. Bounded Y implies the moment condition.</p><p>Next, we assume the regression γ 0 is smooth in the sense of <ref type="bibr" target="#b2">(3)</ref>, and H has low effective dimension in the sense of (4). Denote the jth eigenvalue of the convolution operator for H by η j (H). Recall that η j (H) is also the jth eigenvalue of the feature covariance operator. Assumption 6.2 (Smoothness and spectral decay for regression). Assume γ 0 ∈ H c with c ∈ (1, 2], and</p><formula xml:id="formula_39">η j (H) ≤ Cj −b with b ≥ 1.</formula><p>See Supplement H for alternative ways of writing and interpreting Assumption 6.2. We place similar smoothness and spectral decay conditions on the conditional mean embeddings µ x (d) and µ x (v), which are generalized conditional expectation functions. We articulate this assumption abstractly for the conditional mean embedding Formally, define the conditional expectation operator E :</p><formula xml:id="formula_40">H A → H B , f (•) → E{f (A ) | B = •}. By construction, E encodes the same information as µ a (b) since {µ a (b)}(•) = φ(a)dpr(a | b) = {E φ(•)}(b) = {E * φ(b)}(•), a ∈ A , b ∈ B ,</formula><p>where E * is the adjoint of E . We denote the space of Hilbert-Schmidt operators between H A and H B by L 2 (H A , H B ). <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b58">59]</ref> prove that L 2 (H A , H B ) is an RKHS in its own right, for which we can assume smoothness in the sense of (3) and spectral decay in the sense of (4). Assumption 6.3 (Smoothness and spectral decay for mean embedding). Assume the following:</p><formula xml:id="formula_41">E ∈ L 2 (H A , H c B ) with c ∈ (1, 2], and η (H B ) ≤ Cj −b with b ≥ 1.</formula><p>Just as we place approximation assumptions for γ 0 in terms of H, which provides the features onto which we project Y , we place approximation assumptions for E in terms of H B , which provides the features φ(B ) onto which we project φ(A ). Under these conditions, we arrive at our main theoretical guarantee. Theorem 6.1 (Uniform consistency of causal functions). Suppose the conditions of Lemma 3.1 hold, as well as Assumptions 4.1, 6.1, and 6.2. Set (λ, λ 1 , λ 2 ) = {n −1/(c+1/b) , n −1/(c1+1/b1) , n −1/(c2+1/b2) }, which is rate optimal regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Then with high probability θAT</head><formula xml:id="formula_42">E −θ AT E 0 ∞ = O n −(c−1)/{2(c+1/b)} and θDS (•, pr)− θ DS 0 (•, pr) ∞ = O n −(c−1)/{2(c+1/b)} + ñ−1/2 . 2. If in addition Assumption 6.3 holds with A 1 = X and B 1 = D, then with high probability θAT T − θ AT T 0 ∞ = O n −(c−1)/{2(c+1/b)} + n −(c1−1)/{2(c1+1/b1)} .</formula><p>3. If in addition Assumption 6.3 holds with A 2 = X and B 2 = V, then with high probability</p><formula xml:id="formula_43">θCAT E − θ CAT E 0 ∞ = O n −(c−1)/{2(c+1/b)} + n −(c2−1)/{2(c2+1/b2)} .</formula><p>Likewise for incremental functions, e.g. θ∇:</p><formula xml:id="formula_44">AT E − θ ∇:AT E 0 ∞ = O n −(c−1)/{2(c+1/b)} .</formula><p>Explicit constants hidden by the O(•) notation, as well as explicit specializations of Assumption 6. Our conditional mean embedding rate builds on original analysis of conditional expectation operators in Supplement I that is of independent interest. We improve the rate from n −(c −1)/{2(c +1)} [59, Theorem 2] to n −(c −1)/{2(c +1/b )} . Our consideration of Hilbert-Schmidt norm departs from <ref type="bibr" target="#b43">[44]</ref> and <ref type="bibr" target="#b67">[68]</ref>, who study surrogate risk and operator norm, respectively. Our assumptions also depart from <ref type="bibr">[</ref> Overall, rates slower than n −1/4 reflect the challenge of a sup norm guarantee, which is stronger than a mean square error guarantee and encodes caution about worst case scenarios when informing policy decisions. For comparison, the minimax optimal Sobolev norm rate for learning an s 0 -smooth regression, using H s 2 over R p , is n −(c−1)/{2(c+1/b)} = n −(s0−s)/(2s0+p) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Identification</head><p>In seminal work, <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b50">51]</ref> state sufficient conditions under which causal functions, philosophical quantities defined in terms of potential outcomes {Y (d) }, can be measured from empirical quantities such as outcomes Y , treatments D, and covariates (V, X). Colloquially, this collection of sufficient conditions is known as selection on observables. We assume selection on observables in the main text, and Pearl's front and back door criteria in Supplement D. Assumption A.1 (Selection on observables). Assume</p><formula xml:id="formula_45">1. No interference: if D = d then Y = Y (d) .</formula><p>2. Conditional exchangeability:</p><formula xml:id="formula_46">{Y (d) } |= D | X. 3. Overlap: if f (x) &gt; 0 then f (d | x) &gt; 0, where f (x) and f (d | x) are densities.</formula><p>For</p><formula xml:id="formula_47">θ CAT E 0 , replace X with (V, X).</formula><p>No interference is also called the stable unit treatment value assumption. It rules out network effects, also called spillovers. Conditional exchangeability states that conditional on covariates X, treatment assignment is as good as random. Overlap ensures that there is no covariate stratum X = x such that treatment has a restricted support. To handle θ DS 0 , we place a standard assumption in transfer learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Simulations and program evaluation B.1 Simulations</head><p>We demonstrate that our nonparametric causal function estimators outperform some leading alternatives in nonlinear simulations with many covariates, despite the relative simplicity of our proposed approach. For each causal function design and sample size, we implement 100 simulations and calculate mean square error with respect to the true causal function. Figure <ref type="figure" target="#fig_10">1</ref> visualizes results. A lower mean square error is desirable. See Supplement J for a full exposition of the data generating processes and implementation details.</p><p>The dose response curve design <ref type="bibr" target="#b10">[11]</ref> involves learning the causal function</p><formula xml:id="formula_48">θ AT E 0 (d) = 1.2d + d 2 .</formula><p>A single observation consists of the triple (Y, D, X) for outcome, treatment, and high dimensional covariates where Y, D ∈ R and X ∈ R 100 . In addition to our one-line nonparametric estimator (RKHS, white), we implement the estimators of <ref type="bibr" target="#b31">[32]</ref> (DR1, checkered white), <ref type="bibr" target="#b10">[11]</ref> (DR2, lined white), and <ref type="bibr" target="#b55">[56]</ref> (DR-series, gray). DR1 and DR2 are local estimators that involve Nadaraya-Watson smoothing around doubly robust estimating equations. DR-series uses series regression with debiased pseudo outcomes, and we give it the advantage of correct specification as a quadratic function. By the Wilcoxon rank sum test, RKHS significantly outperforms alternatives at sample size 10,000, with p value less than 10 −3 , despite its relative simplicity.</p><p>Though our approach allows for heterogeneous response of a continuous treatment, we implement a design for heterogeneous effect of a binary treatment in order to facilitate comparison with existing methods. The heterogeneous treatment effect design <ref type="bibr" target="#b0">[1]</ref> involves learning the causal functions</p><formula xml:id="formula_49">θ CAT E 0 (0, v) = 0 and θ CAT E 0 (1, v) = v(1 + 2v) 2 (v − 1) 2 .</formula><p>A single observations consists of the tuple (Y, D, V, X) for outcome, treatment, covariate of interest, and other covariates. In this design, Y, D, V ∈ R and X ∈ R 3 . In addition to our one-line nonparametric estimator (RKHS, white), we implement the estimators of <ref type="bibr" target="#b0">[1]</ref> (IPW, lined gray) and <ref type="bibr" target="#b55">[56]</ref> (DR-series, gray). The former involves Nadaraya-Watson smoothing around an inverse propensity estimator, and the latter involves Figure <ref type="figure" target="#fig_10">1</ref>: Nonparametric causal function simulations. We implement the estimators of <ref type="bibr" target="#b0">[1]</ref> (IPW, lined gray), <ref type="bibr" target="#b31">[32]</ref> (DR1, checkered white), <ref type="bibr" target="#b10">[11]</ref> (DR2, lined white), and <ref type="bibr" target="#b55">[56]</ref> (DR-series, gray), in addition to our own (RKHS, white).</p><p>(correctly specified) series regression with a debiased pseudo outcome. The R learner <ref type="bibr" target="#b42">[43]</ref> cannot be implemented since V = X. The simple RKHS approach significantly outperforms alternatives at sample sizes 500 and 1,000 by the Wilcoxon rank sum test, with p values less than 10 −5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Program evaluation: US Job Corps</head><p>To demonstrate how kernel methods for causal functions are a practical addition to the empirical economic toolkit, we conduct a real world program evaluation. Specifically, we estimate dose, heterogeneous, and incremental response curves of the Jobs Corps, the largest job training program for disadvantaged youth in the US. The Job Corps is financed by the US Department of Labor, and it serves about 50,000 participants annually. Participation is free for individuals who meet low income requirements. Access to the program was randomized from November 1994 to February 1996; see <ref type="bibr" target="#b54">[55]</ref> for details. Many studies focus on data from this period to evaluate the effect of job training on employment <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b10">11]</ref>. Though access to the program was randomized, individuals could decide whether to participate and for how many hours. From a causal perspective, we assume selection on observables: conditional on observed covariates, participation was exogenous on the extensive and intensive margins. From a statistical perspective, we assume that different intensities of job training have smooth effects on counterfactual employment, and that those effects are smoothly modified by age, assumptions motivated by labor market theory.</p><p>In this setting, the continuous treatment D ∈ R is total hours spent in academic or vocational classes in the first year after randomization, and the continuous outcome Y ∈ R is the proportion of weeks employed in the second year after randomization. The covariates X ∈ R 40 include age, gender, ethnicity, language competency, education, marital status, household size, household income, previous receipt of social aid, family background, health, and health related behavior at base line. As in <ref type="bibr" target="#b10">[11]</ref>, we focus on the n = 3, 906 observations for which D ≥ 40, i.e. individuals who completed at least one week of training. We implement various causal parameters in Figure <ref type="figure" target="#fig_12">2</ref>: the dose response curve; the incremental response curve; the discrete treatment effects with confidence intervals of <ref type="bibr" target="#b57">[58]</ref>; and the heterogeneous response curve with respect to age. For the discrete effects, we discretize treatment into roughly equiprobable bins: Figure <ref type="figure" target="#fig_12">2</ref>: Effect of job training on employment. We implement our estimators for dose, heterogeneous, and incremental response curves (RKHS, solid). For comparison, we also implement the dose response curve estimator of <ref type="bibr" target="#b10">[11]</ref> (DR2, dashes) as well as the discrete treatment effects of <ref type="bibr" target="#b57">[58]</ref> (DR3, vertical bars).</p><p>(1250, 1500], (1500, 1750], and (1750, 2000] class hours. As far as we know, the heterogeneous response of class hours, a continuous treatment, has not been previously studied in this empirical setting. In Supplement K, we provide implementation details and verify that our results are robust to the choice of sample.</p><p>The dose response curve plateaus and achieves its maximum around d = 500, corresponding to 12.5 weeks of classes. Our global estimate (RKHS, solid) has the same overall shape but is smoother and slightly lower than the collection of local estimates from <ref type="bibr" target="#b10">[11]</ref> (DR2, dashes). The smoothness of our estimator is a consequence of the RKHS assumptions, and we see how it is a virtue for empirical economic research; a smooth dose response curve is more economically plausible in this setting. The first 12.5 weeks of classes confer most of the gain in employment: from 35% employment to more than 47% employment for the average participant. The incremental response curve (RKHS, solid) is the derivative of the dose response curve, and it visualizes where the greatest gain happens. The discrete treatment effects of <ref type="bibr" target="#b57">[58]</ref> (DR3, vertical bars) corroborate our dose response curve, and the 95% confidence intervals contain the dose response curve of <ref type="bibr" target="#b10">[11]</ref> (DR2, dashes) as well as our own (RKHS, solid). Finally, the heterogeneous response curve (RKHS, solid) shows that age plays a substantial role in the effectiveness of the intervention. For the youngest participants, the intervention has a small effect: employment only increases from 28% to at most 36%. For older participants, the intervention has a large effect: employment increases from 40% to 56%. Our policy recommendation is therefore 12-14 weeks of classes targeting individuals 21-23 years old.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Counterfactual distributions C.1 Definition</head><p>In the main text, we study causal functions defined as means of potential outcomes. In this section, we extend the estimators and analyses presented in the main text to counterfactual distributions of potential outcomes. A counterfactual distribution can be encoded by a kernel mean embedding using a new feature map φ(y) for a new scalar valued RKHS H Y . We now allow Y to be a Polish space (Assumption 6.1). Definition C.1 (Counterfactual distributions and embeddings). We define Our strategy is to estimate the embedding of a counterfactual distribution. At that point, the analyst may use the embedding to (i) estimate moments of the counterfactual distribution <ref type="bibr" target="#b29">[30]</ref> or (ii) sample from the counterfactual distribution <ref type="bibr" target="#b72">[73]</ref>. Since we already analyze means in the main text, we focus on (ii) in this supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Identification</head><p>The same identification results apply to counterfactual distributions.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Closed form solution</head><p>To estimate counterfactual distributions, we extend the RKHS construction in Section 4. As before, define scalar valued RKHSs for treatment D and covariates X. Define an additional scalar valued RKHS for outcome Y . Because the regression γ 0 is now a conditional mean embedding, we present a construction involving a conditional expectation operator. Define the conditional expectation operator</p><formula xml:id="formula_50">E 3 : H Y → H D ⊗ H X , f (•) → E{f (Y ) | D = •, X = •}. By construction γ 0 (d, x) = E * 3 {φ(d)⊗φ(x)}.</formula><p>As before, we replace X with (V, X) for θ D:CAT E 0 . We place regularity conditions on this RKHS construction, similar to those in Section 4, to represent counterfactual distributions as evaluations of E * 3 . This representation allows for continuous treatment, unlike the representation in <ref type="bibr">[40, eq. 16, 17, 20]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">θD:AT</head><formula xml:id="formula_51">T 0 (d, d ) = E * 3 {φ(d ) ⊗ µ x (d)} where µ x (d) = φ(x)dpr(x | d). 4. θD:CAT E 0 (d, v) = E * 3 {φ(d) ⊗ φ(v) ⊗ µ x (v)} where µ x (v) = φ(x)dpr(x | v).</formula><p>For θ D:CAT E 0 , we instead assume</p><formula xml:id="formula_52">E 3 ∈ L 2 (H Y , H D ⊗ H V ⊗ H X ).</formula><p>See Supplement E for the proof. The mean embeddings are the same as in Theorem 4.1. They encode the reweighting distributions as elements in the RKHS such that the counterfactual distribution embeddings can be expressed as evaluations of E * 3 . As in Section 4, the abstract representation helps to define estimators with closed form solutions that can be easily computed. In particular, the representation separates the three steps necessary to estimate a counterfactual distribution: estimating a conditional distribution, which may involve many covariates; estimating the distribution for reweighting; and using one distribution to integrate another. For example, for θD:</p><formula xml:id="formula_53">CAT E 0 (d, v), our estimator is θD:CAT E (d, v) = Ê * 3 {φ(d) ⊗ φ(v) ⊗ μx (v)}.</formula><p>Ê3 and μx (v) are generalized kernel ridge regressions, and the latter can be used to integrate the former by simply multiplying the two. This algorithmic insight is a key innovation of the present work, and the reason why our estimators have simple closed form solutions despite complicated causal integration. Algorithm C.1 (Estimation of counterfactual distribution embeddings). Denote the empirical kernel matrices K DD , K XX , K Y Y ∈ R n×n . Let ( Xi ) (i = 1, ..., ñ) be observations drawn from population pr. Denote by the elementwise product. The distribution embedding estimators have the closed form solutions</p><formula xml:id="formula_54">1. { θD:AT E 0 (d)}(y) = n −1 n i=1 K yY (K DD K XX + nλ 3 I) −1 (K Dd K Xxi ); 2. { θD:DS 0 (d)}(y) = ñ−1 ñ i=1 K yY (K DD K XX + nλ 3 I) −1 (K Dd K X xi ); 3. { θD:AT T 0 (d, d )}(y) = K yY (K DD K XX + nλ 3 I) −1 [K Dd {K XX (K DD + nλ 1 I) −1 K Dd }]; 4. { θD:CAT E 0 (d, v)}(y) = K yY (K DD K V V K XX + nλ 3 I) −1 [K Dd K V v {K XX (K V V + nλ 2 I) −1 K V v }];</formula><p>where (λ 1 , λ 2 , λ 3 ) are ridge regression penalty hyperparameters.</p><p>We derive these estimators in Supplement E. We give theoretical values for (λ 1 , λ 2 , λ 3 ) that optimally balance bias and variance in Theorem 6.1 below. Supplement F gives practical tuning procedures, one of which is asymptotically optimal. We avoid the estimation and inversion of propensity scores in <ref type="bibr">[40, eq. 21]</ref>.</p><p>Algorithm C.1 estimates counterfactual distribution embeddings. The ultimate parameters of interest are counterfactual distributions. We present a deterministic procedure that uses the distribution embedding to provide samples ( Ỹj ) from the distribution. In Theorem C.3 below, we prove that these samples converge in distribution to the counterfactual distribution. The procedure is a variant of kernel herding <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b39">40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm C.2 (Estimation of counterfactual distributions). Recall that θD:AT</head><formula xml:id="formula_55">E 0 (d) is a mapping from Y to R. Given θD:AT E 0 (d), calculate 1. Ỹ1 = argmax y∈Y { θD:AT E 0 (d)}(y) ; 2. Ỹj = argmax y∈Y { θD:AT E 0 (d)}(y) − (j + 1) −1 j−1 =1 k Y ( Ỹ , y) for j &gt; 1.</formula><p>Likewise for the other counterfactual distributions, replacing θD:AT E 0 (d) with the other quantities in Algorithm C.1.</p><p>By this procedure, samples from counterfactual distributions are straightforward to compute. With such samples, one may visualize a histogram as an estimator of the counterfactual density of potential outcomes. Alternatively, one may test statistical hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Convergence in distribution</head><p>Towards a guarantee of uniform consistency, we place regularity conditions on the original spaces as in Assumption 6.1. Importantly, we relax the condition that Y ⊂ R; instead, we assume Y is a Polish space. Next, we assume the regression γ 0 is smooth and quantify the spectral decay of its RKHS, parameterized in terms of the conditional expectation operator E 3 . Likewise we assume the conditional mean embeddings µ x (d) and µ x (v) are smooth and quantify their spectral decay. With these assumptions, we arrive at our next main result. Theorem C.2 (Uniform consistency of counterfactual distribution embeddings). Suppose Assumptions A.1, 4.1, 6.1, and 6.3 hold with A 3 = Y and B 3 = D × X (or</p><formula xml:id="formula_56">B 3 = D × V × X for θ D:CAT E 0 ). Set (λ 1 , λ 2 , λ 3 ) = {n −1/(c1+1/b1) , n −1/(c2+1/b2) , n −1/(c3+1/b3</formula><p>) }, which is rate optimal regularization.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Then with high probability</head><formula xml:id="formula_57">sup d∈D θD:AT E (d) − θD:AT E 0 (d) H Y = O n −(c3−1)/{2(c3+1/b3)} .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">If in addition Assumption</head><formula xml:id="formula_58">(d, d ) H Y = O n −(c3−1)/{2(c3+1/b3)} + n −(c1−1)/{2(c1+1/b1)} .</formula><formula xml:id="formula_59">(d, v) H Y = O n −(c3−1)/{2(c3+1/b3)} + n −(c2−1)/{2(c2+1/b2)} .</formula><p>Explicit constants hidden by the O(•) notation are indicated in Supplement H, as well as explicit specializations of Assumption 6.3. Again, these rates approach n −1/4 when (c 1 , c 2 , c 3 ) = 2 and (b 1 , b 2 , b 3 ) → ∞, i.e. when the regressions are smooth and when the effective dimensions are finite. Our assumptions do not include an assumption on the smoothness of an explicit density ratio, which appears in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr">Theorem 11]</ref> and <ref type="bibr" target="#b39">[40,</ref><ref type="bibr">Assumption 3]</ref>. Finally, we state an additional regularity condition under which we can prove that the samples ( Ỹj ) calculated from the distribution embeddings weakly converge to the desired distribution. Assumption C.1 (Additional regularity). Assume 1. Y is locally compact.</p><p>2. H Y ⊂ C 0 , where C 0 is the space of bounded, continuous, real valued functions that vanish at infinity.</p><p>As discussed by <ref type="bibr" target="#b56">[57]</ref>, the combined assumptions that Y is Polish and locally compact impose weak restrictions. In particular, if Y is a Banach space, then to satisfy both conditions it must be finite dimensional. Trivially, Y = R dim(Y ) satisfies both conditions. We arrive at our final result of this section. See Supplement H for the proof. Samples are drawn for given value d. Though our nonparametric consistency result is uniform across treatment values, this convergence in distribution result is for a fixed treatment value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Graphical models</head><p>In the main text, we study causal functions defined in the potential outcomes framework and identified by selection on observables. In this supplement, we study causal functions and counterfactual distributions defined in the directed acyclic graph (DAG) framework and identified by Pearl's front and back door criteria. We derive estimators, then prove uniform consistency and convergence in distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 DAG background</head><p>DAGs provide another popular language for causal inference <ref type="bibr" target="#b46">[47]</ref>. Rather than reasoning about pr{Y (d) }, one reasons about pr{Y | do(D = d)}, where both expressions are concerned with the distribution of outcome Y given intervention D = d. For a specific setting, graphical criteria in terms of the DAG can help verify conditional independence statements in terms of potential outcomes. In this section, we provide results in terms of causal DAGs, analogous to the results in terms of potential outcomes given in the main text. In particular, we focus on the front and back door criteria, which are the fundamental building blocks of DAG-based causal inference.</p><p>Assume the analyst has access to a causal DAG G with vertex set W , partitioned into four disjoint sets W = (Y, D, X, U ). Y is the outcome, D is the set of treatments, X is the set of covariates, and U is the set of unobserved variables. Since counterfactual inquiries involve intervention on the graph G, we require notation for graph modification. Denote by G D the graph obtained by deleting from G all arrows pointing into nodes in D. Denote by G D the graph obtained by deleting from G all arrows emerging from nodes in D. We denote d-separation by |= d . d-separation implies statistical independence. Throughout this section, we make the standard faithfulness assumption: d-connection implies statistical dependence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Identification</head><p>We define causal functions and counterfactual distributions in terms of the do operator on the DAG. For clarity of exposition, we focus on the case where (D, Y ) are nodes rather than sets. In seminal works, <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> states sufficient conditions under which such effects, philosophical quantities defined in terms of interventions on the graph, can be measured from empirical quantities such as outcomes Y , treatments D, and covariates X. We present two sets of sufficient conditions, known as the back door and front door criteria. Assumption D.1 (Back door criterion). Assume 1. No node in X is a descendent of D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">X blocks every path between D and Y that contains an arrow into</head><formula xml:id="formula_60">D: (Y |= d D | X) G D .</formula><p>Intuitively, the analyst requires sufficiently many and sufficiently well placed covariates X in the context of the graph G. Assumption D.1 is satisfied if there is no unobserved confounder U , or if any unobserved confounder U with a back door path into treatment D is blocked by X. Assumption D.2 (Front door criterion). Assume 1. X intercepts all directed paths from D to Y .</p><p>2. There is no unblocked back door path from D to X. Intuitively, these conditions ensure that X serves to block all spurious paths from D to Y ; to leave all directed paths unperturbed; and to create no new spurious paths. As before, define the regression</p><formula xml:id="formula_61">γ 0 (d, x) = E(Y | D = d, X = x).</formula><p>Lemma D.1 (Identification of causal function: DAG <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>). Depending on which criterion holds, the causal parameter θ do 0 (d) has different expressions.</p><p>1. If Assumption D.1 holds then θ do 0 (d) = γ 0 (d, x)dpr(x). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">If Assumption</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Closed form solutions</head><p>We maintain notation from Section 4. Theorem D.1 (Decoupling via kernel mean embedding: DAG). Suppose Assumptions 4.1 and D.2 hold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">If an addition γ</head><formula xml:id="formula_62">0 ∈ H then θ do 0 (d) = γ 0 , µ d ⊗ µ x (d) H ; 2. If in addition E 3 ∈ L 2 (H Y , H D ⊗ H X ) then θdo 0 (d) = E * 3 {µ d ⊗ µ x (d)};</formula><p>where</p><formula xml:id="formula_63">µ d = φ(d)dpr(d) and µ x (d) = φ(x)dpr(x | d).</formula><p>See Supplement E for the proof. </p><formula xml:id="formula_64">(d) = n −1 n i=1 Y (K DD K XX +nλI) −1 [K Ddi {K XX (K DD +nλ 1 I) −1 K Dd }] 2. { θD:F D (d)}(y) = n −1 n i=1 K yY (K DD K XX + nλ 3 I) −1 [K Ddi {K XX (K DD + nλ 1 I) −1 K Dd }]</formula><p>where (λ, λ 1 , λ 3 ) are ridge regression penalty hyperparameters.</p><p>We derive this estimator in Supplement E. We give theoretical values for (λ, λ 1 , λ 3 ) that optimally balance bias and variance in Theorem D.2 below. Supplement F gives practical tuning procedures, one of which is asymptotically optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Uniform consistency and convergence in distribution</head><p>Towards a guarantee of uniform consistency, we place the same assumptions as in Section 4. Theorem D.2 (Uniform consistency of causal functions: DAG). Suppose the conditions of Theorem D.1 hold, as well as Assumptions 6.1 and 6.3 with A 1 = X and B 1 = D. Set (λ, λ 1 , λ 3 ) = {n −1/(c+1/b) , n −1/(c1+1/c1) , n −1/(c3+1/b3) }, which is rate optimal regularization. 1. If in addition Assumption 6.2 holds then with high probability See Supplement H for the proof.</p><formula xml:id="formula_65">θF D − θ do 0 ∞ = O n −(c−1)/{2(c+1/b)} + n −(c1−1)/{2(c1+1/b1)} .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Algorithm derivation</head><p>In this supplement, we derive estimators for (i) causal functions, (ii) counterfactual distributions, and (iii) graphical models. Before we do so, we compare kernel methods to series estimation. For intuition, consider θAT E (d) with linear kernels k(d, d ) = dd and k(x, x ) = x x . Then by singular value decomposition,</p><formula xml:id="formula_66">θAT E (d) = dn −1 n i=1 X i n −1 n i=1 D 2 i X i X i + λI −1 n −1 n i=1 D i X i Y i .</formula><p>This formulation is interpretable as a regularized series estimator with basis function φ(d, x) = dx. However, it requires scalar treatment, finite dimensional covariate, linear ridge regression, and computation O{dim(X) 3 }. By contrast, the formulation in Algorithm 4.1 allows for generic treatment, generic covariate, nonlinear ridge regression, and computation O(n 3 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Causal functions</head><p>Proof of Theorem 4.1. In Assumption 4.1, we impose that the scalar kernels are bounded. This assumption has several implications. First, the feature maps are Bochner integrable <ref type="bibr" target="#b64">[65,</ref><ref type="bibr">Definition A.5.20]</ref>. Bochner integrability permits us to interchange expectation and inner product. Second, the mean embeddings exist. Third, the product kernel is also bounded and hence the tensor product RKHS inherits these favorable properties. By Lemma 3.1 and linearity of expectation,</p><formula xml:id="formula_67">θ AT E 0 (d) = γ 0 (d, x)dpr(x) = γ 0 , φ(d) ⊗ φ(x) H dpr(x) = γ 0 , φ(d) ⊗ φ(x)dpr(x) H = γ 0 , φ(d) ⊗ µ x H .</formula><p>Likewise for θ DS 0 (d, pr). Next,</p><formula xml:id="formula_68">θ AT T 0 (d, d ) = γ 0 (d , x)dpr(x | d) = γ 0 , φ(d ) ⊗ φ(x) H dpr(x | d) = γ 0 , φ(d ) ⊗ φ(x)dpr(x | d) H = γ 0 , φ(d ) ⊗ µ x (d) H .</formula><p>Finally, <ref type="bibr">Lemma 4.34]</ref> guarantees that the derivative feature map ∇ d φ(d) exists, is continuous, and is Bochner integrable since</p><formula xml:id="formula_69">θ CAT E 0 (d, v) = γ 0 (d, v, x)dpr(x | v) = γ 0 , φ(d) ⊗ φ(v) ⊗ φ(x) H dpr(x | v) = γ 0 , φ(d) ⊗ φ(v) ⊗ φ(x)dpr(x | v) H = γ 0 , φ(d) ⊗ φ(v) ⊗ µ x (v) H . [65,</formula><formula xml:id="formula_70">κ d = sup d,d ∈D ∇ d ∇ d k(d, d ) 1/2 &lt; ∞.</formula><p>Therefore the derivations remain valid for incremental functions.</p><p>Proof of Algorithm 4.1. By standard arguments <ref type="bibr" target="#b32">[33]</ref> </p><formula xml:id="formula_71">γ(d, x) = γ, φ(d) ⊗ φ(x) H = Y (K DD K XX + nλI) −1 (K Dd K Xx ).</formula><p>The results for θAT E (d) holds by substitution:</p><formula xml:id="formula_72">μx = n −1 n i=1 φ(x i ), θAT E (d) = γ, φ(d) ⊗ μx H .</formula><p>Likewise for θDS (d, pr).</p><p>The results for θAT T (d, d ) and θCAT E (d, v) use the closed form of the conditional mean embedding from [59, <ref type="bibr">Algorithm 1]</ref>. Specifically,</p><formula xml:id="formula_73">μx (d) = K •X (K DD + nλ 1 I) −1 K Dd , θAT T (d, d ) = γ, φ(d ) ⊗ μx (d) H and μx (v) = K •X (K V V + nλ 2 I) −1 K V v , θCAT E (d, v) = γ, φ(d) ⊗ φ(v) ⊗ μx (v) H . For incremental functions, replace γ(d, x) with ∇ d γ(d, x) = γ, ∇ d φ(d) ⊗ φ(x) H = Y (K DD K XX + nλI) −1 (∇ d K Dd K Xx ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Counterfactual distributions</head><p>Proof of Theorem C.1. Assumption 4.1 implies Bochner integrability, which permits us to interchange expectation and evaluation. Therefore by Lemma 3.1 and linearity of expectation,</p><formula xml:id="formula_74">θD:AT E 0 (d) = γ 0 (d, x)dpr(x) = E * 3 {φ(d) ⊗ φ(x)}dpr(x) = E * 3 {φ(d) ⊗ φ(x)dpr(x)} = E * 3 {φ(d) ⊗ µ x }. Likewise for θD:DS 0 (d, pr). Next, θD:AT T 0 (d) = γ 0 (d , x)dpr(x | d) = E * 3 {φ(d ) ⊗ φ(x)}dpr(x | d) = E * 3 {φ(d ) ⊗ φ(x)dpr(x | d)} = E * 3 {φ(d ) ⊗ µ x (d)}.</formula><p>Finally,</p><formula xml:id="formula_75">θD:CAT E 0 (d) = γ 0 (d, v, x)dpr(x | v) = E * 3 {φ(d) ⊗ φ(v) ⊗ φ(x)}dpr(x | v) = E * 3 {φ(d) ⊗ φ(v) ⊗ φ(x)dpr(x | v)} = E * 3 {φ(d) ⊗ φ(v) ⊗ µ x (v)}. Proof of Algorithm C.1. By [59, Algorithm 1], γ(d, x) = Ê * 3 {φ(d) ⊗ φ(x)} = K •Y (K DD K XX + nλ 3 I) −1 (K Dd K Xx ).</formula><p>The result for θD:AT E follows by substitution:</p><formula xml:id="formula_76">μx = n −1 n i=1 φ(x i ), θD:AT E (d) = Ê * 3 {φ(d) ⊗ μx }.</formula><p>Likewise for θD:DS . Both θD:AT T and θD:CAT E appeal to the closed form for conditional mean embeddings from [59, Algorithm 1]. Specifically,</p><formula xml:id="formula_77">μx (d) = K •X (K DD + nλ 1 I) −1 K Dd , θD:AT T (d, d ) = Ê * 3 {φ(d ) ⊗ μx (d)}; μx (v) = K •X (K V V + nλ 2 I) −1 K V v , θD:CAT E (d, v) = Ê * 3 {φ(d) ⊗ φ(v) ⊗ μx (v)}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Graphical models</head><p>Proof of Theorem D.1. Assumption 4.1 implies Bochner integrability, which permits us to interchange expectation and inner product. Therefore</p><formula xml:id="formula_78">θ do 0 (d) = γ 0 (d , x)dpr(d )dpr(x | d) = γ 0 , φ(d ) ⊗ φ(x) H dpr(d )dpr(x | d) = γ 0 , φ(d )dpr(d ) ⊗ φ(x)dpr(x | d) H = γ 0 , µ d ⊗ µ x (d) H .</formula><p>Similarly, θD:do</p><formula xml:id="formula_79">0 (d) = γ 0 (d , x)dpr(d )dpr(x | d) = E * 3 {φ(d ) ⊗ φ(x)}dpr(d )dpr(x | d) = E * 3 φ(d )dpr(d ) ⊗ φ(x)dpr(x | d) = E * 3 {µ d ⊗ µ x (d)}.</formula><p>Proof of Algorithm D.1. Consider θdo . By standard arguments <ref type="bibr" target="#b32">[33]</ref> </p><formula xml:id="formula_80">γ(d, x) = γ, φ(d) ⊗ φ(x) H = Y (K DD K XX + nλI) −1 (K Dd K Xx ).</formula><p>By [59, Algorithm 1], write the mean embedding and conditional mean embedding as</p><formula xml:id="formula_81">μx = n −1 n i=1 φ(x i ), μx (d) = K •X (K DD + nλ 1 I) −1 K Dd .</formula><p>Substitute these quantities to obtain θdo</p><formula xml:id="formula_82">(d) = γ, μd ⊗ μx (d) H . Next consider θD:do . By [59, Algorithm 1] γ(d, x) = Ê * 3 {φ(d) ⊗ φ(x)} = K •Y (K DD K XX + nλ 3 I) −1 (K Dd K Xx ). Substitution of the mean embeddings gives θD:do (d) = Ê * 3 {μ d ⊗ μx (d)}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Tuning</head><p>In the present work, we propose a family of novel estimators that are combinations of kernel ridge regressions. As such, the same two kinds of hyperparameters that arise in kernel ridge regressions arise in our estimators: ridge regression penalties and kernel hyperparameters. In this section, we describe practical tuning procedures for such hyperparameters. To simplify the discussion, we focus on the regression of Y on W . Recall that the closed form solution of the regression estimator using all observations is</p><formula xml:id="formula_83">f (w) = K wW (K W W + nλI) −1 Y.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Ridge penalty</head><p>It is convenient to tune λ by leave-one-out cross validation (LOOCV) or generalized cross validation (GCV), since the validation losses have closed form solutions. Algorithm F.1 (Ridge penalty tuning by LOOCV). Construct the matrices</p><formula xml:id="formula_84">H λ = I − K W W (K W W nλI) −1 ∈ R n×n , Hλ = diag(H λ ) ∈ R n×n</formula><p>where Hλ has the same diagonal entries as H λ and off diagonal entries of zero. Then set</p><formula xml:id="formula_85">λ * = argmin λ∈Λ n −1 H−1 λ H λ Y 2 2 , Λ ⊂ R.</formula><p>Proof. We prove that n −1 H−1 λ H λ Y 2 2 is the LOOCV loss. By definition, the LOOCV loss is</p><formula xml:id="formula_86">E(λ) = n −1 n i=1 {Y i − f−i (W i )} 2</formula><p>where f−i is the regression estimator using all observations except the ith observation.</p><p>Let Φ be the matrix of features, with ith row φ(W i ) , and let Q = Φ Φ + nλI. By the regression first order condition,</p><formula xml:id="formula_87">f = Q −1 Φ Y, f−i = {Q − φ(W i )φ(W i ) } −1 {Φ Y − φ(W i )Y i }.</formula><p>Recall the Sherman-Morrison formula for rank one updates:</p><formula xml:id="formula_88">(A + uv ) −1 = A −1 − A −1 uv A −1 1 + v A −1 u . Hence {Q − φ(W i )φ(W i ) } −1 = Q −1 + Q −1 φ(W i )φ(W i ) Q −1 1 − φ(W i ) Q −1 φ(W i ) .</formula><p>Let</p><formula xml:id="formula_89">β i = φ(W i ) Q −1 φ(W i ). Then f−i (W i ) = φ(W i ) Q −1 + Q −1 φ(W i )φ(W i ) Q −1 1 − β i {Φ Y − φ(W i )Y i } = φ(W i ) I + Q −1 φ(W i )φ(W i ) 1 − β i { f − Q −1 φ(W i )Y i } = 1 + β i 1 − β i φ(W i ) { f − Q −1 φ(W i )Y i } = 1 + β i 1 − β i { f (W i ) − β i Y i } = 1 1 − β i { f (W i ) − β i Y i },</formula><p>i.e. f−i can be expressed in terms of f . Note that</p><formula xml:id="formula_90">Y i − f−i (W i ) = Y i − 1 1 − β i { f (W i ) − β i Y i } = Y i + 1 1 − β i {β i Y i − f (W i )} = 1 1 − β i {Y i − f (W i )}.</formula><p>Substituting back into the LOOCV loss</p><formula xml:id="formula_91">n −1 n i=1 Y i − f−i (W i ) 2 = n −1 n i=1 {Y i − f (W i )} 1 1 − β i 2 = n −1 H−1 λ {Y − K W W (K W W + nλI) −1 Y } 2 2 = n −1 H−1 λ H λ Y 2 2 , since ( H−1 λ ) ii = 1 ( Hλ ) ii = 1 (H λ ) ii = 1 1 − {K W W (K W W + nλI) −1 } ii and K W W (K W W + nλI) −1 = ΦΦ (ΦΦ + nλI) −1 = Φ(Φ Φ + nλI) −1 Φ = ΦQ −1 Φ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm F.2 (Ridge penalty tuning by GCV). Construct the matrix</head><formula xml:id="formula_92">H λ = I − K W W (K W W + nλI) −1 ∈ R n×n . Then set λ * = argmin λ∈Λ n −1 {tr(H λ )} −1 H λ Y 2 2 , Λ ⊂ R.</formula><p>Proof. We match symbols with the classic derivation of <ref type="bibr" target="#b11">[12]</ref>. Observe that</p><formula xml:id="formula_93">     f (W 1 ) . . . f (W n )      = K W W (K W W + nλI) −1 Y = A λ Y, A λ = K W W (K W W + nλI) −1 . Therefore H λ = I − K W W (K W W + nλI) −1 = I − A λ .</formula><p>GCV can be viewed as a rotation invariant modification of LOOCV. In practice, we find that LOOCV and GCV provide almost identical hyperparameter values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Kernel</head><p>The exponentiated quadratic kernel is the most popular kernel among machine learning researchers:</p><formula xml:id="formula_94">k(w, w ) = exp − 1 2 (w − w ) 2 ι 2 .</formula><p>Importantly, this kernel satisfies the required properties; it is continuous, bounded, and characteristic.</p><p>[50, Section 4.3] characterize the exponentiated quadratic RKHS as an attenuated series of the form</p><formula xml:id="formula_95">H =   f = ∞ j=1 f j ϕ j : ∞ j=1 f 2 j η j &lt; ∞   , f, f H = ∞ j=1 f j f j η j .</formula><p>For simplicity, take W = R and take the measure ν to be the standard Gaussian distribution (more generally, it can be the population distribution pr). Recall that the generalization of Mercer's Theorem permits W to be separable. Then the induced RKHS is characterized by</p><formula xml:id="formula_96">η j = 2ā Ā 1/2 Bj , ϕ j (w) = exp{−(c − ā)w 2 }H j {w(2c) 1/2 }.</formula><p>H j is the jth Hermite polynomial, and the constants (ā, b, c, Ā, B) &gt; 0 are</p><formula xml:id="formula_97">ā = 1 4 , b = 1 2ι 2 , c = (ā 2 + 2ā b) 1/2 , Ā = ā + b + c, B = b Ā &lt; 1.</formula><p>The eigenvalues (η j ) geometrically decay, and the series (ϕ j ) consists of weighted Hermite polynomials. For a function to belong to this RKHS, its coefficients on higher order weighted Hermite polynomials must be small.</p><p>Observe that the exponentiated quadratic kernel has a hyperparameter: the lengthscale ι. A convenient heuristic is to set the lengthscale equal to the median interpoint distance of (W i ) (i = 1, ..., n), where the interpoint distance between observations i and j is W i − W j W . When the input W is multidimensional, we use the kernel obtained as the product of scalar kernels for each input dimension. For example, if</p><formula xml:id="formula_98">W ⊂ R d then k(w, w ) = d j=1 exp − 1 2 (w j − w j ) 2 ι 2 j .</formula><p>Each lengthscale ι j is set according to the median interpoint distance for that input dimension.</p><p>In principle, we could instead use LOOCV or GCV to tune kernel hyperparameters in the same way that we use LOOCV or GCV to tune ridge penalties. However, given our choice of product kernel, this approach becomes impractical in high dimensions. For example, in the dose response curve design, D ∈ R and X ∈ R 100 leading to a total of 101 lengthscales (ι j ). Even with a closed form solution for LOOCV and GCV, searching over this high dimensional grid becomes cumbersome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Balancing weight proof</head><p>In this section, we provide the proofs to relate our algorithm with the balancing weight algorithms in previous work.</p><p>Proof of Proposition 5.1. This result is standard in causal inference textbooks, e.g. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr">Technical Point 3.1]</ref>. We state the proof for clarity:</p><formula xml:id="formula_99">γ(d, x)α 0 (d, x)dpr(d, x) = γ(d, x) 1(d = d * ) pr(D = d * | x) dpr(d | x)dpr(x) = 1 pr(D = d * | x) γ(d, x)1(d = d * )dpr(d | x)dpr(x) = 1 pr(D = d * | x) γ(d * , x)pr(D = d * | x)dpr(x) = γ(d * , x)dpr(x).</formula><p>The variance of α 0 is finite since pr(D = d * | X) is bounded away from zero almost surely. Consider the zero function 0 ∈ L 2 . The definition of continuity of F at 0 is as follows: for all &gt; 0, there exists some δ &gt; 0 such that for all γ ∈ L 2 , γ − 0 L 2 &lt; δ implies |F (γ) − F ( 0)| &lt; . To violate continuity, we must show that there exists some &gt; 0 such that for all δ &gt; 0, there exists a γ ∈ L 2 whereby γ − 0 In summary, we have shown that F is linear but not continuous over L 2 and therefore not bounded over L 2 . Therefore its Riesz representer in L 2 does not exist.</p><p>Proof of Corollary 5.1. We proceed in steps. For clarity, we focus on the formulation of <ref type="bibr" target="#b23">[24]</ref>, who consider estimation θ AT E 0 (0) for binary treatment. We maintain the notation γ 0 (d,</p><formula xml:id="formula_100">x) = E(Y | D = d, X = x).</formula><p>1. Reformulation of <ref type="bibr" target="#b23">[24]</ref>.</p><p>The authors propose the estimator</p><formula xml:id="formula_101">θAT E (0) = n −1 n i=1 1(D i = 0) ŵ(X i )Y i where ŵ(x) is their estimator of 1/pr(d = 0 | x). Define αi = α(D i , X i ) = 1(D i = 0) ŵ(X i ) so that θAT E (0) = n −1 n i=1 αi Y i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Equivalence.</head><p>As noted in [24, <ref type="bibr">Lemma 1]</ref>,</p><formula xml:id="formula_102">θAT E (0) = n −1 n i=1 f (X i )</formula><p>where f (x) is a kernel ridge regression estimator of γ 0 (0, x), which is estimated by subsetting to the untreated observations (i : D i = 0) and then regressing (Y i ) i:Di=0 on (X i ) i:Di=0 .</p><p>3. Reformulation of our proposal.</p><p>In Algorithm 4.1, we propose</p><formula xml:id="formula_103">θAT E (0) = n −1 n i=1 Y (K DD K XX + nλI) −1 (K D0 K Xxi ) = n −1 n i=1 γ(0, X i ),</formula><p>where γ(d, x) is a kernel ridge regression estimator of γ 0 (d, x), which is estimated with all of the observations</p><formula xml:id="formula_104">(i = 1, ..., n). Take k D (d, d ) = 1(d = d ) in the product kernel k(d, x; d , x ) = k D (d, d )k X (x, x ) of the tensor product RKHS H = H D ⊗ H X .</formula><p>Then it is numerically equivalent to set γ(0, x) = f (x), where f (x) is the kernel ridge regression described above.</p><p>4. Collecting results.</p><p>In summary, we have shown</p><formula xml:id="formula_105">n −1 n i=1 αi Y i = θAT E (0) = n −1 n i=1 f (X i ) = n −1 n i=1 γ(0, X i ) = θAT E (0).</formula><p>Proof of Corollary 5.2. We consider each case, appealing to Algorithm 4.1. Let e j ∈ R n be the vector of zeroes whose jth component is one.</p><formula xml:id="formula_106">1. θAT E (d) = n −1 n i=1 Y (K DD K XX + nλI) −1 (K Dd K Xxi ); Write Z = n −1 n i=1 (K DD K XX + nλI) −1 (K Dd K Xxi ). Then θAT E (d) = Y Z = n i=1 Y i Z i = n −1 n i=1 Y i nZ i .</formula><p>Therefore</p><formula xml:id="formula_107">αAT E j = nZ j = ne j Z = e j n i=1 (K DD K XX + nλI) −1 (K Dd K Xxi ). 2. θDS (d, pr) = ñ−1 ñ i=1 Y (K DD K XX + nλI) −1 (K Dd K X xi ); The argument is as above, taking Z = ñ−1 ñ i=1 (K DD K XX + nλI) −1 (K Dd K X xi ). Therefore αDS j = nñ −1 e j ñ i=1 (K DD K XX + nλI) −1 (K Dd K X xi ). 3. θAT T (d, d ) = Y (K DD K XX + nλI) −1 [K Dd {K XX (K DD + nλ 1 I) −1 K Dd }];</formula><p>The argument is as above, taking</p><formula xml:id="formula_108">Z = (K DD K XX + nλI) −1 [K Dd {K XX (K DD + nλ 1 I) −1 K Dd }]. Therefore αAT T j = ne j (K DD K XX + nλI) −1 [K Dd {K XX (K DD + nλ 1 I) −1 K Dd }]. 4. θCAT E (d, v) = Y (K DD K V V K XX + nλI) −1 [K Dd K V v {K XX (K V V + nλ 2 I) −1 K V v }];</formula><p>The argument is as above, taking</p><formula xml:id="formula_109">Z = (K DD K V V K XX + nλI) −1 [K Dd K V v {K XX (K V V + nλ 2 I) −1 K V v }]. Therefore αCAT E j = ne j (K DD K V V K XX +nλI) −1 [K Dd K V v {K XX (K V V +nλ 2 I) −1 K V v }].</formula><p>Likewise for incremental functions, e.g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>α∇:AT</head><formula xml:id="formula_110">E j = e j n i=1 (K DD K XX + nλI) −1 (∇ d K Dd K Xxi ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Uniform consistency and convergence in distribution proof</head><p>In this supplement, we (i) present an equivalent definition of smoothness and relate our key assumptions with previous work; (ii) present technical lemmas for regression, unconditional mean embeddings, and conditional mean embeddings; (iii) appeal to these lemmas to prove uniform consistency of causal functions as well as convergence in distribution for counterfactual distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.1 Assumptions revisited</head><p>Alternative representations of smoothness Lemma H.1 (Alternative representation of smoothness; Remark 2 of <ref type="bibr" target="#b5">[6]</ref>). If the input measure and Mercer measure are the same then there are equivalent formalisms for the source conditions in Assumptions 6.2 and 6.3.</p><p>1. The source condition in Assumption 6.2 holds if and only if the regression γ 0 is a particularly smooth element of H. Formally, define the covariance operator T for H. We assume there exists g ∈ H such that γ 0 = T (c−1)/2 g, c ∈ (1, 2], and g 2 H ≤ ζ. The source condition in Assumption 6.3 holds if and only if the conditional expectation operator E is a particularly smooth element of L 2 (H A , H B ). Formally, define the covariance operator T = E{φ(B ) ⊗ φ(B )} for L 2 (H A , H B ). We assume there exists</p><formula xml:id="formula_111">G ∈ L 2 (H A , H B ) such that E = T (c −1)/2 • G , c ∈ (1, 2], and G 2 L2(H A ,H B ) ≤ ζ .</formula><p>Remark H.1 (Covariance operator). The covariance operator T for the RKHS H depends on the setting.</p><formula xml:id="formula_112">1. θ AT E 0 , θ DS 0 , θ AT T 0 : T = E[{φ(D) ⊗ φ(X)} ⊗ {φ(D) ⊗ φ(X)}]; 2. θ CAT E 0 : T = E[{φ(D) ⊗ φ(V ) ⊗ φ(X)} ⊗ {φ(D) ⊗ φ(V ) ⊗ φ(X)}].</formula><p>[59] prove that T and its powers are well defined under Assumption 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Specific representations of smoothness</head><p>Next, we instantiate the source condition in Assumption 6.3 for the different settings considered in the main text. Assumption H.1 (Smoothness of mean embedding µ x (d)). Assume 1. The conditional expectation operator E 1 is well specified as a Hilbert-Schmidt operator between RKHSs, i.e.</p><formula xml:id="formula_113">E 1 ∈ L 2 (H X , H D ), where E 1 : H X → H D , f (•) → E{f (X) | D = •}.</formula><p>2. The conditional expectation operator is a particularly smooth element of L 2 (H X , H D ).</p><p>Formally, define the covariance operator</p><formula xml:id="formula_114">T 1 = E{φ(D) ⊗ φ(D)} for L 2 (H X , H D ).</formula><p>We assume there exists</p><formula xml:id="formula_115">G 1 ∈ L 2 (H X , H D ) such that E 1 = T (c1−1)/2 1 • G 1 , c 1 ∈ (1, 2], and G 1 2 L2(H X ,H D ) ≤ ζ 1 .</formula><p>Assumption H.2 (Smoothness of mean embedding µ x (v)). Assume 1. The conditional expectation operator E 2 is well specified as a Hilbert-Schmidt operator between RKHSs, i.e.</p><formula xml:id="formula_116">E 2 ∈ L 2 (H X , H V ), where E 2 : H X → H V , f (•) → E{f (X) | V = •}.</formula><p>2. The conditional expectation operator is a particularly smooth element of L 2 (H X , H V ). Formally, define the covariance operator</p><formula xml:id="formula_117">T 2 = E{φ(V ) ⊗ φ(V )} for L 2 (H X , H V ). We assume there exists G 2 ∈ L 2 (H X , H V ) such that E 2 = T (c2−1)/2 2 • G 2 , c 2 ∈ (1, 2], and G 2 2 L2(H X ,H V ) ≤ ζ 2 .</formula><p>Assumption H.3 (Smoothness of conditional expectation operator E 3 ).</p><p>1. The conditional expectation operator E 3 is well specified as a Hilbert-Schmidt operator between RKHSs, i.e.</p><formula xml:id="formula_118">E 3 ∈ L 2 (H Y , H D ⊗ H X ), where E 3 : H Y → H D ⊗ H X , f (•) → E{f (Y ) | D = •, X = •}.</formula><p>2. The conditional expectation operator is a particularly smooth element of L 2 (H Y , H D ⊗H X ).</p><p>Formally, define the covariance operator</p><formula xml:id="formula_119">T 3 = E[{φ(D) ⊗ φ(X)} ⊗ {φ(D) ⊗ φ(X)}] for L 2 (H Y , H D ⊗ H X ).</formula><p>We assume there exists</p><formula xml:id="formula_120">G 3 ∈ L 2 (H Y , H D ⊗ H X ) such that E 3 = T (c3−1)/2 3 • G 3 , c 3 ∈ (1, 2], and G 3 2 L2(H Y ,H D ⊗H X ) ≤ ζ 3 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpreting smoothness for tensor products</head><p>Another way to interpret the smoothness assumption for a tensor product RKHS follows from manipulation of the product kernel. For simplicity, consider the RKHS construction for θ AT E 0 , take k D to be the exponentiated quadratic kernel over D ⊂ R, and take k X to be the exponentiated quadratic kernel over X ⊂ R. Define the vector of differences</p><formula xml:id="formula_121">v = v 1 v 2 = d x − d x = d − d x − x . Then k(d, x; d , x ) = exp − 1 2 v 2 1 ι 2 1 exp − 1 2 v 2 2 ι 2 2 = exp − 1 2 v ι −2 1 0 0 ι −2 2 v .</formula><p>In summary, the product of exponentiated quadratic kernels over scalars is an exponentiated quadratic kernel over vectors. Therefore a tensor product of exponentiated quadratic RKHSs H D and H X begets an exponentiated quadratic RKHS H = H D ⊗ H X , for which the smoothness and spectral decay conditions admit their usual interpretation. The same is true anytime that a product of kernels begets a recognizable kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matching assumptions with previous work</head><p>Finally, we relate our approximation assumptions with previous work. Specifically, we match symbols with <ref type="bibr" target="#b17">[18]</ref>. Remark H.2 (Matching assumptions). Recall our main approximation assumptions.</p><p>1. Source condition c ∈ (1, 2]. <ref type="bibr" target="#b17">[18]</ref> refer to the source condition as SRC parametrized by β.</p><p>Matching symbols, c = β. A larger value of c is a stronger assumption. 3. Embedding property a ∈ (0, 1]. <ref type="bibr" target="#b17">[18]</ref> place an additional assumption EMB parametrized by α ∈ (0, 1]. In our setting of interest, c ≥ 1 and the kernel is bounded. Together, these conditions imply α ≤ 1. Matching symbols, a = α. A larger value of a is a weaker assumption</p><p>In our algorithm derivation, we have already assumed correct specification and bounded kernels, i.e. we have already assumed that c ≥ 1, b ≥ 1, and a ≤ 1. By placing explicit source and effective dimension conditions, we derive rates that adapt to stronger assumptions c &gt; 1 and b &gt; 1.</p><p>It turns out that a further assumption of a &lt; 1 does not improve the rate, so we omit that additional complexity. Observe that c ≥ 1 and b ≥ 1 imply c + 1/b &gt; 1 ≥ a for any value a ∈ (0, 1]. The regime in which the inequality c + 1/b &gt; a holds is the regime in which the rate does not depend on a [18, Theorem 1.ii], so the weakest version of the embedding property is sufficient for our purpose. We pose as a question for future work how to analyze the misspecified case, in which the stronger assumption of a &lt; 1 may play an important role.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2 Lemmas Regression</head><p>For expositional purposes, we summarize classic results for the kernel ridge regression estimator γ for γ 0 (w) = E(Y | W = w). Consider the definitions</p><formula xml:id="formula_122">γ 0 = argmin γ∈H E(γ), E(γ) = E[{Y − γ(W )} 2 ]; γ = argmin γ∈H Ê(γ), Ê(γ) = n −1 n i=1 {Y i − γ(W i )} 2 + λ γ 2 H .</formula><p>Proposition H.1 (Regression rate). Suppose Assumptions 4.1, 6.1, and 6.2 hold. Set λ = n −1/(c+1/b) . Then with probability 1 − δ, for n sufficiently large, we have that</p><formula xml:id="formula_123">γ − γ 0 H ≤ r γ (n, δ, b, c) = C log(4/δ) • n − 1 2 c−1 c+1/b ,</formula><p>where C is a constant independent of n and δ.</p><p>Remark I.1 in the subsequent, technical supplement elaborates on the meaning of the phrase "n sufficiently large".</p><p>Proof. We verify the conditions of [ SRC is the assumption we call the source condition, parametrized by c ∈ (1, 2] in our case. MOM is a Bernstein moment condition satisfied by hypothesis. We study the RKHS norm guarantee, which corresponds to Hilbert scale equal to one. We are in regime (ii) of the theorem, since c + 1/b &gt; 1.</p><p>For the exact finite sample constant, see <ref type="bibr" target="#b17">[18,</ref><ref type="bibr">Theorem 16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unconditional mean embedding</head><p>For expositional purposes, we summarize classic results for the unconditional mean embedding estimator μw for µ w = E{φ(W )}. Lemma H.2 (Bennett inequality; Lemma 2 of <ref type="bibr" target="#b60">[61]</ref>). Let (ξ i ) be i.i.d. random variables drawn from distribution pr taking values in a real separable Hilbert space K. Suppose there exists M that</p><formula xml:id="formula_124">ξ i K ≤ M &lt; ∞ almost surely and σ 2 (ξ i ) = E( ξ i<label>2</label></formula><formula xml:id="formula_125">K</formula><p>). Then for all n ∈ N and for all δ ∈ (0, 1),</p><formula xml:id="formula_126">pr 1 n n i=1 ξ i − E(ξ) K ≤ 2M log(2/δ) n + 2σ 2 (ξ) log(2/δ) n 1/2 ≥ 1 − δ.</formula><p>Proposition H.2 (Mean embedding rate). Suppose Assumptions 4.1 and 6.1 hold. Then with probability 1 − δ,</p><formula xml:id="formula_127">μw − µ w H W ≤ r µ (n, δ) = 4κ w log(2/δ) n 1/2 .</formula><p>Proof. The result follows from Lemma H.2 with </p><formula xml:id="formula_128">ξ i = φ(W i ), since n −1 n i=1 φ(W i ) − E{φ(W )} H W ≤ 2κ w log(2/δ) n + 2κ 2 w log(2/δ) n 1/2 ≤ 4κ w log(2/δ) n 1/2 . [<label>3</label></formula><formula xml:id="formula_129">: with probability 1 − δ, νx − ν x H X ≤ r ν (ñ, δ) = 4κ x log(2/δ)ñ −1/2 . 3. θ F D 0 and θD:F D 0 : with probability 1 − δ, μd − µ d H D ≤ r µ (n, δ) = 4κ d log(2/δ)n −1/2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional expectation operator and conditional mean embedding</head><p>Next, we present original results for the generalized kernel ridge regression estimator Ê of the conditional expectation operator E :</p><formula xml:id="formula_130">H A → H B , f (•) → E{f (A ) | B = •}.</formula><p>We prove these results and compare them with previous work in Supplement I.</p><p>Consider the definitions</p><formula xml:id="formula_131">E = argmin E∈L2(H A ,H B ) E(E), E(E) = E[{φ(A ) − E * φ(B )} 2 ]; Ê = argmin E∈L2(H A ,H B ) Ê(E), Ê(E) = n −1 n i=1 {φ(A i ) − E * φ(B i )} 2 + λ E 2 L2(H A ,H B ) .</formula><p>Proposition H.3 (Conditional mean embedding rate). Suppose Assumptions 4.1, 6.1, and 6.3 hold. Set λ = n −1/(c +1/b ) . Then with probability 1 − δ, for n sufficiently large,</p><formula xml:id="formula_132">Ê − E L2 ≤ r E (δ, n, b , c ) = C log(4/δ) • n −(c −1)/{2(c +1/b )} ,</formula><p>where C is a constant independent of n and δ. Moreover, for all b ∈ B μa</p><formula xml:id="formula_133">(b) − µ a (b) H A ≤ r µ (δ, n, b , c ) = κ b • r E (δ, n, b , c ).</formula><p>Remark I.1 in the subsequent, technical supplement elaborates on the meaning of the phrase "n sufficiently large".</p><p>Proof. We delay the proof of this result to the next supplement due to its technicality. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3 Main results</head><p>Appealing to Propositions H.1, H.2, and H.3, we now prove consistency for (i) causal functions, (ii) counterfactual distributions, and (iii) graphical models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Causal functions</head><p>Proof of Theorem 6.1. We initially consider θ</p><formula xml:id="formula_134">AT E 0 . θAT E (d) − θ AT E 0 (d) = γ, φ(d) ⊗ μx H − γ 0 , φ(d) ⊗ µ x H = γ, φ(d) ⊗ (μ x − µ x ) H + (γ − γ 0 ), φ(d) ⊗ µ x H = (γ − γ 0 ), φ(d) ⊗ (μ x − µ x ) H + γ 0 , φ(d) ⊗ (μ x − µ x ) H + (γ − γ 0 ), φ(d) ⊗ µ x H .</formula><p>Therefore by Propositions H.1 and H.2, with probability 1 − 2δ,</p><formula xml:id="formula_135">| θAT E (d) − θ AT E 0 (d)| ≤ γ − γ 0 H φ(d) H D μx − µ x H X + γ 0 H φ(d) H D μx − µ x H X + γ − γ 0 H φ(d) H D µ x H X ≤ κ d • r γ (n, δ, b, c) • r µ (n, δ) + κ d • γ 0 H • r µ (n, δ) + κ d κ x • r γ (n, δ, b, c) = O n − 1 2 c−1 c+1/b</formula><p>. By the same argument, with probability 1 − 2δ,</p><formula xml:id="formula_136">| θDS (d, pr) − θ DS 0 (d, pr)| ≤ κ d • r γ (n, δ, b, c) • r ν (ñ, δ) + κ d • γ 0 H • r ν (ñ, δ) + κ d κ x • r γ (n, δ, b, c) = O n − 1 2 c−1 c+1/b + ñ− 1 2 . Next, consider θ AT T 0 . θAT T (d, d ) − θ AT T 0 (d, d ) = γ, φ(d ) ⊗ μx (d) H − γ 0 , φ(d ) ⊗ µ x (d) H = γ, φ(d ) ⊗ {μ x (d) − µ x (d)} H + (γ − γ 0 ), φ(d ) ⊗ µ x (d) H = (γ − γ 0 ), φ(d ) ⊗ {μ x (d) − µ x (d)} H + γ 0 , φ(d ) ⊗ {μ x (d) − µ x (d)} H + (γ − γ 0 ), φ(d ) ⊗ µ x (d) H .</formula><p>Therefore by Propositions H.1 and H.3, with probability 1 − 2δ,</p><formula xml:id="formula_137">| θAT T (d, d ) − θ AT T 0 (d, d )| ≤ γ − γ 0 H φ(d ) H D μx (d) − µ x (d) H X + γ 0 H φ(d ) H D μx (d) − µ x (d) H X + γ − γ 0 H φ(d ) H D µ x (d) H X ≤ κ d • r γ (n, δ, b, c) • r AT T µ (n, δ, b 1 , c 1 ) + κ d • γ 0 H • r AT T µ (n, δ, b 1 , c 1 ) + κ d κ x • r γ (n, δ, b, c) = O n − 1 2 c−1 c+1/b + n − 1 2 c 1 −1 c 1 +1/b 1 . Finally, consider θ CAT E 0 . θCAT E (d, v) − θ CAT E 0 (d, v) = γ, φ(d) ⊗ φ(v) ⊗ μx (v) H − γ 0 , φ(d) ⊗ φ(v) ⊗ µ x (v) H = γ, φ(d) ⊗ φ(v) ⊗ {μ x (v) − µ x (v)} H + (γ − γ 0 ), φ(d) ⊗ φ(v) ⊗ µ x (v) H = (γ − γ 0 ), φ(d) ⊗ φ(v) ⊗ {μ x (v) − µ x (v)} H + γ 0 , φ(d) ⊗ φ(v) ⊗ {μ x (v) − µ x (v)} H + (γ − γ 0 ), φ(d) ⊗ φ(v) ⊗ µ x (v) H .</formula><p>Therefore by Propositions H.1 and H.3, with probability 1 − 2δ,   </p><formula xml:id="formula_138">| θCAT E (d, v) − θ CAT E 0 (d, v)| ≤ γ − γ 0 H φ(d) H D φ(v) H V μx (v) − µ x (v) H X + γ 0 H φ(d) H D φ(v) H V μx (v) − µ x (v) H X + γ − γ 0 H φ(d) H D φ(v) H V µ x (v) H X ≤ κ d κ v • r γ (n, δ, b, c) • r CAT E µ (n, δ, b 2 , c 2 ) + κ d κ v • γ 0 H • r CAT E µ (n, δ, b 2 , c 2 ) + κ d κ v κ x • r γ (n, δ, b, c) = O n − 1 2 c−1 c+1/b + n − 1 2 c 2 −1 c 2 +1/b 2</formula><formula xml:id="formula_139">(d) − θD:AT E 0 (d) H Y ≤ κ d • r E (n, δ, b 3 , c 3 ) • r µ (n, δ) + κ d • E 3 L2 • r µ (n, δ) + κ d κ x • r E (n, δ, b 3 , c 3 ) = O n</formula><formula xml:id="formula_140">(d, pr) H Y ≤ κ d • r E (n, δ, b 3 , c 3 ) • r ν (ñ, δ) + κ d • E 3 L2 • r ν (ñ, δ) + κ d κ x • r E (n, δ, b 3 , c 3 ) = O n − 1 2 c 3 −1 c 3 +1/b 3 + ñ− 1 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>By</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Conditional expectation operator proof</head><p>In this supplement, we prove Proposition H.3, improving the rate of <ref type="bibr" target="#b58">[59]</ref> from n −(c−1)/{2(c+1)} to n −(c−1)/{2(c+1/b)} . Our consideration of Hilbert-Schmidt norm departs from <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b67">68]</ref>, who study surrogate risk and operator norm, respectively. Our assumptions also depart from <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b67">68]</ref>. Instead, we directly generalize the assumptions of <ref type="bibr" target="#b17">[18]</ref> from the standard kernel ridge regression to the generalized kernel ridge regression that we use to estimate a conditional mean embedding. Our rate matches the minimax optimal rate shown in contemporaneous work of <ref type="bibr" target="#b34">[35]</ref>, who also study the misspecified case. We focus on the well specified case for reasons described in Section 5, and employ a simpler proof strategy.</p><p>To lighten notation, we suppress the indexing of conditional expectation operators and conditional mean embeddings by . Furthermore, to lighten notation, we abbreviate L 2 = L 2 (H A , H B ). In the simplified notation,  Proof. The result mirrors <ref type="bibr">[18, eq. 44]</ref> and <ref type="bibr">[68, eq. 34]</ref>, strengthening the RKHS norm to Hilbert-Schmidt norm via [59, <ref type="bibr">Proposition 22]</ref>. Proof. We verify the conditions of Lemma H.2. Let</p><formula xml:id="formula_141">ξ i = [{φ(A i ) − µ λ a (B i )} ⊗ φ(B i )](T BB + λI) −1/2 .</formula><p>We proceed in steps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.2 Heterogeneous treatment effect</head><p>A single observations consists of the tuple (Y, D, V, X), where outcome, treatment, and covariate of interest Y, D, V ∈ R and other covariates X ∈ R 3 . A single observation is generated as follows. Draw unobserved noise as j i.i.d.</p><p>∼ U(−1/2, 1/2) (j = 1, ..., 4) and ν ∼ N (0, 1/16). Then set</p><formula xml:id="formula_142">V = 1 , X =    1 + 2V + 2 1 + 2V + 3 (V − 1) 2 + 4    .</formula><p>Draw D ∼ Bernoulli[Λ{(V + X 1 + X 2 + X 3 )/2}] where Λ is the logistic link function. Finally set</p><formula xml:id="formula_143">Y = 0 if D = 0; V X 1 X 2 X 3 + ν if D = 1.</formula><p>[1] also present a simpler version of this design.</p><p>We implement our estimator θCAT E (d, v) (RKHS, white) described in Section 4, with the tuning procedure described in Supplement F. Specifically, we use ridge penalties determined by leave-oneout cross validation. For multivariate functions, we use products of scalar kernels. For the binary treatment D, we use the binary kernel. For continuous variables, we use (product) exponentiated quadratic kernel with lengthscales set by the median heuristic. We implement <ref type="bibr" target="#b0">[1]</ref> (IPW, lined gray) using default settings in the MATLAB code shared by the authors. We implement <ref type="bibr" target="#b55">[56]</ref> (DR-series, gray) using the default settings of the command best_linear_projection in the R package grf. Importantly, we give DR-series the advantage of correct specification of the true heterogeneous treatment effect as the appropriate polynomial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K Application details</head><p>We implement our nonparametric estimators θAT E (d), θ∇:AT E (d), and θCAT E (d, v) described in Section 4 (RKHS, solid). We also implement the nonparametric estimator of <ref type="bibr" target="#b10">[11]</ref> (DR2, dashes) using default settings in Python code shared by the authors. Specifically, we use random forest for prediction, with the suggested hyperparameter values. Finally, we implement the semiparametric estimator of <ref type="bibr" target="#b57">[58]</ref> (DR3, vertical bars) with 95% confidence intervals. Specifically, we reduce the continuous treatment into a discrete treatment that takes nine values corresponding to the roughly equiprobable bins <ref type="bibr" target="#b39">[40,</ref><ref type="bibr">250]</ref>, (250, 500], (500, 750] (750, 1000], (1000, 1250], (1250, 1500], (1500, 1750], and (1750, 2000] class hours. Across estimators, we use the tuning procedure described in Supplement F. Specifically, we use ridge penalties determined by leave-one-out cross validation, and product exponentiated quadratic kernel with lengthscales set by the median heuristic.  We use the dataset published by <ref type="bibr" target="#b25">[26]</ref>. In Supplement B, we focus on the n = 3, 906 observations for which D ≥ 40, i.e. individuals who completed at least one week of training. In this section, we verify that our results are robust to the choice of sample. Specifically, we consider the sample with   We implement our estimators for dose, heterogeneous, and incremental response curves (RKHS, solid). For comparison, we also implement the dose response curve estimator of <ref type="bibr" target="#b10">[11]</ref> (DR2, dashes) as well as the discrete treatment effects of <ref type="bibr" target="#b57">[58]</ref>  Excluding observations for which Y = 0 leads to estimates that have the same shape but higher magnitudes, confirming the robustness of the results we present in Supplement B.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Definition 3 . 1 (</head><label>31</label><figDesc>Causal functions). We define 1. Dose response: θ AT E 0 (d) = E{Y (d) } is the counterfactual mean outcome given intervention D = d for the entire population. 2. Dose response with distribution shift: θ DS 0 (d, pr) = E pr {Y (d) } is the counterfactual mean outcome given intervention D = d for an alternative population with data distribution pr. 3. Conditional response: θ AT T 0 (d, d ) = E{Y (d ) | D = d} is the counterfactual mean outcome given intervention D = d for the subpopulation who actually received treatment D = d. 4. Heterogeneous response: θ CAT E 0 (d, v) = E{Y (d) | V = v} is the counterfactual mean outcome given intervention D = d for the subpopulation with subcovariate value V = v.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and θ DS 0 (d, pr) are dose response curves for entire populations, but causal functions may vary for different subpopulations. Towards the goal of personalized or targeted interventions, an analyst may ask another nuanced counterfactual question: what would have been the effect of treatment D = d for the subpopulation who actually received treatment D = d? When treatment is continuous, we may define the conditional response θ AT T 0 (d, d ) = E{Y (d ) | D = d}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Lemma 3 .</head><label>3</label><figDesc>1 clarifies the data requirements for estimating each causal function. The dose response θ AT E 0 (d) and conditional response θ AT T 0 (d, d ) require observations of outcome Y , treatment D, and covariates X drawn from the population pr. The dose response with distribution shift θ DS 0 (d, pr) additionally requires observations of covariates X drawn from the alternative population pr. For the heterogeneous response θ CAT E 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Proposition 5 . 1 (</head><label>51</label><figDesc>Existence for treatment effects; Point 3.1 of<ref type="bibr" target="#b22">[23]</ref>). Suppose selection on observables (stated in Supplement A) holds and treatment is binary.Fix d * ∈ D. If pr(D = d * | X) is bounded away from zero almost surely, then there exists balancing weight α 0 ∈ L 2 such that for all γ ∈ L 2 , γ(d * , x)dpr(x) = γ, α 0 L 2 . In particular, θ AT E 0 (d * ) = yα 0 (d, x)dpr(d, x, y) = γ 0 , α 0 L 2 and the balancing weight is α 0 (d, x) = 1(d = d * )/pr(D = d * | x). In summary, a treatment effect has two representations: the primal representation of Lemma 3.1 as a partial mean of the regression γ 0 (d, x) = E(Y | D = d, X = x), and the dual representation of Proposition 5.1 as a reweighting of the outcome Y using the balancing weight α 0 (d, x) = 1(d = d * )/pr(D = d * | x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>µ a (b) = φ(a)dpr(a | b) where a ∈ A and b ∈ B . All one has to do is specify A and B to specialize the assumption. For µ x (d), A 1 = X and B 1 = D; for µ x (v), A 2 = X and B 2 = V. For fixed A and B , we parametrize smoothness by c and spectral decay by b .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>3, are indicated in Appendices H and I. These rates approach n −1/4 when (c, c 1 , c 2 ) = 2 and (b, b 1 , b 2 ) → ∞, i.e. when the regressions are smooth and when the effective dimensions are finite. Interestingly, each rate combines minimax optimal rates in RKHS norm: n −(c−1)/{2(c+1/b)} for standard nonparametric regression [18, Theorem 2]; ñ−1/2 for unconditional mean embeddings [69, Theorem 1]; and, in contemporaneous work, n −(c −1)/{2(c +1/b )} for conditional mean embeddings [35, Theorem 3]. Remark 6.1 (Technical innovation).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>59 ,</head><label>59</label><figDesc>Hypothesis 5],<ref type="bibr" target="#b43">[44,</ref> Theorem 4.5], and<ref type="bibr" target="#b67">[68,</ref>  Assumptions 3 and 4]. Instead, Assumption 6.3 directly generalizes<ref type="bibr" target="#b17">[18,</ref> Conditions SRC and EVD]   from RKHS functions to Hilbert-Schmidt operators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Assumption A.2 (Distribution shift). Assume 1. pr(Y, D, X) = pr(Y | D, X) pr(D, X); 2. pr(D, X) is absolutely continuous with respect to pr(D, X). Populations pr and pr differ only in the distribution of treatments and covariates. Moreover, the support of pr contains the support of pr. An immediate consequence is that the regression γ 0 (d, x) = E(Y | D = d, X = x) remains the same across the different populations pr and pr.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Out of sample mean square error (b) Heterogeneous treatment effect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc><ref type="bibr" target="#b39">[40,</ref> 250], (250, 500], (500, 750] (750, 1000], (1000, 1250],</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>1 .</head><label>1</label><figDesc>Counterfactual distribution: θ D:AT E 0 (d) = pr{Y (d) } is the counterfactual distribution of outcomes given intervention D = d for the entire population. 2. Counterfactual distribution with distribution shift: θ D:DS 0 (d, pr) = pr{Y (d) } is the counterfactual distribution of outcomes given intervention D = d for an alternative population with data distribution pr (elaborated in Assumption A.2). 3. Conditional counterfactual distribution: θ D:AT T 0 (d, d ) = pr{Y (d ) | D = d} is the counterfactual distribution of outcomes given intervention D = d for the subpopulation who actually received treatment D = d. 4. Heterogeneous counterfactual distribution: θ D:CAT E 0 (d, v) = pr{Y (d) | V = v} is the counterfactual distribution of outcomes given intervention D = d for the subpopulation with covariate value V = v. Likewise we define counterfactual distribution embeddings, e.g. θD:AT E 0 (d) = E{φ(Y (d) )}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Lemma C. 1 (</head><label>1</label><figDesc>Identification of counterfactual distributions). If Assumption A.1 holds, 1. {θ D:AT E 0 (d)}(y) = pr(y | d, x)dpr(x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>2 .</head><label>2</label><figDesc>If in addition Assumption A.2 holds, then {θ D:DS 0 (d, pr)}(y) = pr(y | d, x)d pr(x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>3 .</head><label>3</label><figDesc>{θ D:AT T 0 (d, d )}(y) = pr(y | d , x)dpr(x | d) [9]. 4. {θ D:CAT E 0 (d, v)}(y) = pr(y | d, v, x)dpr(x | v). Likewise for embeddings of counterfactual distributions. For example, if in addition Assumption 4.1 holds, then θD:AT E 0 (d) = E{φ(Y ) | D = d, X = x}dpr(x). The identification results for embeddings of counterfactual distributions resemble those presented in the main text. Define the generalized regressions γ 0 (d, x) = E{φ(Y ) | D = d, X = x} and γ 0 (d, v, x) = E{φ(Y ) | D = d, V = v, X = x}. Then we can express these results in the familiar form, e.g. θD:AT E 0 (d) = γ 0 (d, x)dpr(x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Theorem C.1 (Decoupling via kernel mean embeddings). Suppose the conditions of Lemma C.1 hold. Further suppose Assumption 4.1 holds and E 3 ∈ L 2 (H Y , H D ⊗ H X ). Then 1. θD:AT E 0 (d) = E * 3 {φ(d) ⊗ µ x } where µ x = φ(x)dpr(x). 2. θD:DS 0 (d, pr) = E * 3 {φ(d) ⊗ ν x } where ν x = φ(x)d pr(x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>A.2 holds, then with high probability sup d∈D θD:DS (d, pr) − θD:DS 0 (d, pr) H Y = O n −(c3−1)/{2(c3+1/b3)} + ñ−1/2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>3 .</head><label>3</label><figDesc>If in addition Assumption 6.3 holds with A 1 = X and B 1 = D, then with high probability sup d,d ∈D θD:AT T (d, d )− θD:AT T 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>4 .</head><label>4</label><figDesc>If in addition Assumption 6.3 holds with A 2 = X and B 2 = V, then with high probability sup d∈D,v∈V θD:CAT E (d, v)− θD:CAT E 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Theorem C. 3 (</head><label>3</label><figDesc>Convergence in distribution of counterfactual distributions). Suppose the conditions of Theorem C.2 hold, as well as Assumption C.1. Suppose samples ( Ỹj ) are calculated for θ D:AT E 0 (d) as described in Algorithm C.2. Then ( Ỹj ) θ D:AT E 0 (d). Likewise for the other counterfactual distributions, replacing θD:AT E 0 (d) with the other quantities in Algorithm C.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Definition D. 1 (</head><label>1</label><figDesc>Causal function and counterfactual distribution: DAG). θ do 0 (d) = E{Y | do(D = d)} is the counterfactual mean outcome given intervention D = d for the entire population. Likewise we define the counterfactual distribution θ D:do 0 (d) = pr{Y | do(D = d)} and counterfactual distribution embedding θD:do 0 (d) = E{φ(Y ) | do(D = d)} as in Supplement C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>3 .</head><label>3</label><figDesc>All back door paths from X to Y are blocked by D. 4. pr(D, X) &gt; 0 almost surely.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>D.2 holds then θ do 0 (d) = γ 0 (d , x)dpr(d )dpr(x | d). If in addition Assumption 4.1 holds then the analogous result holds for counterfactual distribution embeddings using γ 0 (d, x) = E{φ(Y ) | D = d, X = x} instead, as in Supplement C. Comparing Lemma D.1 with Lemma 3.1, we see that if Assumption D.1 holds then our dose response estimator θAT E (d) in Section 4 is also a uniformly consistent estimator of θ do 0 (d). Similarly our counterfactual distribution estimator θD:AT E (d) converges in distribution to θD:do (d). In the remainder of this section, we therefore focus on what happens if Assumption D.2 holds instead. We study the causal function and counterfactual distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>The quantity µ d = φ(d)dpr(d) is the mean embedding of pr(d). The quantity µ x (d) = φ(x)dpr(x | d) is the conditional mean embedding of pr(x | d). This representation helps to derive an estimator with a closed form solution. For θ do 0 (d), our estimator will be θF D (d) = γ, μd ⊗ μx (d) H , where γ is a standard kernel ridge regression, μd is an empirical mean, and μx (d) is an appropriately defined kernel ridge regression. Algorithm D.1 (Estimation of causal functions: DAG). Denote the empirical kernel matrices K DD , K XX , K Y Y ∈ R n×n calculated from observations drawn from population pr. Denote by the elementwise product. The front door criterion estimators have the closed form solutions 1. θF D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>2 .</head><label>2</label><figDesc>If in addition Assumption 6.3 holds with A 3 = Y and B 3 = D×X then with high probability sup d∈D θD:F D (d) θD:do 0 (d) H Y = O n −(c3−1)/{2(c3+1/b3)} + n −(c1−1)/{2(c1+1/b1) . Explicit constants hidden by the O(•) notation are indicated in Supplement H. The rate is at best n −1/4 when (c, c 1 , c 3 ) = 2 and (b, b 1 , b 3 ) → ∞, i.e. when the regressions are smooth and when the effective dimensions are finite. Finally, we present a convergence in distribution result. Theorem D.3 (Convergence in distribution of counterfactual distributions: DAG). Suppose the conditions of Theorem D.2 hold, as well as Assumption C.1. Suppose samples ( Ỹj ) are calculated for θ D:F D 0 (d) as described in Algorithm C.2. Then ( Ỹj ) θ D:do 0 (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>L 2 &lt;</head><label>2</label><figDesc>δ yet |F (γ) − F ( 0)| &gt; . To serve as this counterexample, define the function γ such that γ(d * , x) = 1 for any x, and γ(d, x) = 0 for any x and any d = d * . Observe that, because treatment is continuous, the set of values for which d = d * is a set with measure zero. Therefore γ − 0 2 = 0 yet |F (γ)−F ( 0)| = |1−0| = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>2 .</head><label>2</label><figDesc>Effective dimension b ≥ 1. [18] refer to the effective dimension condition as EVD parametrized by p. Matching symbols, b = 1/p. A larger value of b is a stronger assumption.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Remark H. 4 (0 and θAT T 0 :0 and θCAT E 0 : 3 .</head><label>4003</label><figDesc>Kernel bounds). In various applications, κ a and κ b vary.1. θ AT T κ a = κ x , κ b = κ d ; 2. θ CAT E κ a = κ x , κ b = κ v ; Counterfactual distributions: κ a = κ y , κ b = κ d κ x .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>− 1 2 c 3 −1 c 3 +1/b 3 .</head><label>2333</label><figDesc>Likewise, with probability 1 − 2δ, θD:DS (d, pr) − θD:DS 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>3 . 1 2c 2 − 1 c 2 1 2c 3 −1 c 3 +1/b 3 .− 1 / 2 c 3 −1 c 3 +1/b 3 1 .</head><label>312121333123331</label><figDesc>Proposition H.3, for all d, d ∈ D, with probability 1 − 2δ, θD:AT T (d, d ) − θD:AT T 0(d, d ) H Y ≤ κ d • r E (n, δ, b 3 , c 3 ) • r AT T µ (n, δ, b 1 , c 1 ) + κ d • E 3 L2 • r AT T µ (n, δ, b 1 , c 1 ) + κ d κ x • r E (n, δ, b 3 , c 3 ) By Proposition H.3, for all d ∈ D and v ∈ V with probability 1 − 2δ, θD:CAT E (d, v) − θD:CAT E 0 (d, v) H Y ≤ κ d κ v • r E (n, δ, b 3 , 3 ) • r CAT E µ (n, δ, b 2 , c 2 ) + κ d κ v • E 3 L2 • r CAT E µ (n, δ, b 2 , c 2 ) + κ d κ v x • r E (n, δ, b 3 , c 3 ) = O n − +1/b 2 + n − Proof of Theorem C.3. Fix d. By Theorem C.2 θD:AT E (d) − θD:AT E 0 (d) H Y = O p nThe argument for θD:F D is analogous. By Propositions H.2, H.3, and H.4, for all d ∈ D, with probability 1 − 3δ,θD:F D (d) − θD:F D 0 (d) H Y ≤ r E (n, δ, b 3 , c 3 ){κ x • r µ (n, δ) + κ d • r AT T µ (n, δ, b 1 , c 1 )} + E 3 L2 {κ x • r µ (n, δ) + κ d • r AT T µ (n, δ, b 1 , c 1 )} + κ d κ x • r E (n, δ, b 3 , c 3 )Proof of Theorem D.3. The argument is identical to the proof of Theorem C.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>E 0 1 2 ζ 1/ 2 ,I. 2 VarianceLemma I. 1 (</head><label>01221</label><figDesc>= argmin E∈L2 E(E), E(E) = E[{φ(A) − E * φ(B)} 2 ]; i ) − E * φ(B i )} 2 + λ E 2 L2 .I.1 BiasProposition I.1 (Conditional expectation operator bias; Theorem 6 of<ref type="bibr" target="#b58">[59]</ref>). Suppose Assumptions 4.1, 6.1, and the source condition in 6.3 hold. Then with probability one,E λ − E 0 L2 ≤ λ c−where ζ is defined in Lemma H.1. Helpful bounds). Suppose Assumptions 4.1, 6.1, and 6.3 hold. Let µ λ a (b) = E * λ φ(b). We adopt the language of<ref type="bibr" target="#b5">[6]</ref>.1. The generalized reconstruction error isB(λ) = sup b∈B µ λ a (b) − µ a (b) 2 H A ≤ κ 2 b ζ • λ c−1 .2. The generalized effective dimension isN (λ) = tr{(T + λI) −1 T } ≤ C(π/b){sin(π/b)} −1 λ −1/b .Proof. The first result is a corollary of Proposition I.1. The second result follows from [67, eq. f], appealing to the effective dimension condition in Assumption 6.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Lemma I. 2 ( 2 L2•</head><label>22</label><figDesc>Decomposition of variance). LetT AB = E{φ(A) ⊗ φ(B)} and let E n (•) = n −1 n i=1 (•).The following bound holds:Ê − E λ L2 ≤ { TAB − T AB (T BB + λI) −1 ( TBB + λI)}(T BB + λI) −1/(T BB + λI) 1/2 ( TBB + λI) −1 (T BB + λI) 1/2 op • (T BB + λI) −1/2 op . Moreover, in the first factor, TAB − T AB (T BB + λI) −1 ( TBB + λI) = E n [{φ(A) − µ λ a (B)} ⊗ φ(B)] − E[{φ(A) − µ λ a (B)} ⊗ φ(B)].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Lemma I. 3 ( 2 L2≤ 4</head><label>324</label><figDesc>Bounding the first factor). Suppose Assumptions 4.1 and 6.1 hold. Then with probability 1 − δ/2, the first factor in Lemma I.2 is bounded as{ TAB − T AB (T BB + λI) −1 ( TBB + λI)}(T BB + λI) −1/log(4/δ) κ a κ b nλ 1/2 + κ b B(λ) 1/2 nλ 1/2 + κ a N (λ) 1/2 n 1/2 + B(λ) 1/2 N (λ) 1/2 n 1/2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>1 . 2 L2==</head><label>12</label><figDesc>First moment.Observe thatξ i L2 = [{φ(A i ) − µ λ a (B i )} ⊗ φ(B i )](T BB + λI) −1/(T BB + λI) −1/2 [φ(B i ) ⊗ {φ(A i ) − µ λ a (B i )}] L2 = (T BB + λI) −1/2 φ(B i ) H B • φ(A i ) − µ λ a (B i ) H A . Moreover (T BB + λI) −1/2 φ(B i ) H B ≤ (T BB + λI) −1/2 op φ(B i ) H B ≤ κ b λ 1/2 and φ(A i )−µ λ a (B i ) H A ≤ φ(A i )−µ a (B i ) H A + µ a (B i )−µ λ a (B i ) H A ≤ 2κ a +B(λ) 1/2 . In summary, ξ i L2 ≤ κ b λ 1/2 2κ a + B(λ) 1/2 . tr([{φ(a) − µ λ a (b)} ⊗ φ(b)](T BB + λI) −1 [φ(b) ⊗ {φ(a) − µ λ a (b)}])dpr(a, b) = tr[{φ(a) − µ λ a (b)} φ(b), (T BB + λI) −1 φ(b) H B {φ(a) − µ λ a (b)}, • H A ]dpr(a, b) = tr[ φ(b), (T BB + λI) −1 φ(b) H B {φ(a) − µ λ a (b)}, {φ(a) − µ λ a (b)} H A ]dpr(a, b) ≤ sup a,b φ(a) − µ λ a (b) 2 H A • tr{ φ(b), (T BB + λI) −1 φ(b) H B }dpr(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Class hours for different samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Effect of job training on employment: D ≥ 40 and Y &gt; 0.We implement our estimators for dose, heterogeneous, and incremental response curves (RKHS, solid). For comparison, we also implement the dose response curve estimator of<ref type="bibr" target="#b10">[11]</ref> (DR2, dashes) as well as the discrete treatment effects of<ref type="bibr" target="#b57">[58]</ref> (DR3, vertical bars).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head></head><label></label><figDesc>Figure 4: Effect of job training on employment: D ≥ 40 and Y &gt; 0.We implement our estimators for dose, heterogeneous, and incremental response curves (RKHS, solid). For comparison, we also implement the dose response curve estimator of<ref type="bibr" target="#b10">[11]</ref> (DR2, dashes) as well as the discrete treatment effects of<ref type="bibr" target="#b57">[58]</ref> (DR3, vertical bars).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>3. k X (and k Y ) are characteristic. For incremental functions, further assume D ⊂ R is an open set and ∇ d ∇ d k D (d, d ) exists and is continuous, hence sup d∈D ∇ d φ(d) H ≤ κ d .</figDesc><table><row><cell>1. θ AT E 0</cell><cell>(d) = γ 0 , φ(d) ⊗ µ x H where µ x = φ(x)dpr(x);</cell></row><row><cell cols="2">2. θ DS 0 (d, pr) = γ 0 , φ(d) ⊗ ν x H where ν x = φ(x)d pr(x);</cell></row><row><cell cols="2">Commonly used kernels are continuous and bounded. Measurability is a similarly weak condition.</cell></row><row><cell cols="2">The characteristic property ensures injectivity of the mean embeddings.</cell></row><row><cell cols="2">Theorem 4.1 (Decoupling via kernel mean embeddings). Suppose the conditions of Lemma 3.1,</cell></row><row><cell cols="2">Assumption 4.1, and γ 0 ∈ H hold. Then</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Proof of Proposition 5.2. The result follows from the Riesz representation theorem in L 2 , e.g.<ref type="bibr" target="#b37">[38,</ref>  Theorem 5.3] and<ref type="bibr" target="#b9">[10,</ref> Lemma 2.1]. It is alluded to in e.g.<ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b40">41]</ref>. We state the proof for clarity.Consider the functional F : γ → γ(d * , x)dpr(x) over L 2 . A Riesz representer α 0 ∈ L 2 exists if and only if the functional F is bounded and linear. Clearly the functional F is linear in the sense that, for any scalar c ∈ R, F (cγ) = cF (γ). A linear functional is bounded over L 2 if and only if it is continuous over L 2 [38, Proposition 5.1]. We will show that this functional is not continuous over L 2 .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc><ref type="bibr" target="#b17">18,</ref> Theorem 1.ii]. By Assumption 4.1, the kernel is bounded and measurable. Separability of the original spaces together with boundedness of the kernel imply that H is separable<ref type="bibr" target="#b64">[65,</ref> Lemma 4.33]. By Assumption 6.1, y 2 dpr(y) &lt; ∞. Since Assumption 6.2 implies γ 0 ∈ H, we have that γ 0 ∞ ≤ κ w γ 0 H by Cauchy-Schwarz inequality.Next, we verify the assumptions called EMB, EVD, SRC, and MOM. Boundedness of the kernel implies EMB with a = 1. EVD is the assumption we call effective dimension, parametrized by b ≥ 1.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>, Theorem 15] originally prove this rate by McDiarmid inequality. See [62, Theorem 2] for an argument via Rademacher complexity. See [69, Proposition A.1] for an improved constant and the proof that the rate is minimax optimal.</figDesc><table><row><cell cols="3">Remark H.3 (Kernel bound). In various applications, κ w varies.</cell></row><row><cell cols="2">1. θ AT E 0</cell><cell>and θD:AT E</cell></row><row><cell>2. θ DS 0</cell><cell cols="2">and θD:DS 0</cell></row></table><note>0 : with probability 1−δ, μx −µ x H X ≤ r µ (n, δ) = 4κ x log(2/δ)n −1/2 .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>.</head><label></label><figDesc>For incremental functions, replace φ(d) with ∇ d φ(d) and hence replace φ(d)H D ≤ κ d with ∇ d φ(d) H D ≤ κ d .Proof of Theorem C.2. The argument is analogous to Theorem 6.1. By Propositions H.2 and H.3, for all d ∈ D, with probability 1 − 2δ, θD:AT E</figDesc><table><row><cell>Counterfactual distributions</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The desired result follows from <ref type="bibr" target="#b62">[63]</ref>, as quoted by <ref type="bibr" target="#b56">[57,</ref><ref type="bibr">Theorem 1.1]</ref>. The argument for other counterfactual distributions is identical.</p><p>Graphical models Proposition H.4. If Assumptions 4.1, 6.1, and 6.3 hold with A 1 = X and B 1 = D, then with probability 1 − 2δ,</p><p>where r µ is as defined in Proposition H.2 (with κ w = κ d ) and r AT T µ is as defined in Proposition H.3.</p><p>Proof. By triangle inequality,</p><p>Focusing on the former term, by Proposition H.3,</p><p>Focusing on the latter term, by Proposition H.2,</p><p>Proof of Theorem D.2. To begin, write</p><p>Therefore by Propositions H.1, H.2, H.3, and H.4, with probability 1 − 3δ,</p><p>Focusing on the former factor,</p><p>Focusing on the latter factor,</p><p>In summary,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Concentration.</head><p>Therefore with probability</p><p>Remark I.1 (Sufficiently large n). In the finite sample, we assume a certain inequality holds when bounding the second factor:</p><p>Ultimately, we will choose λ = n −1/(c+1/b) in Proposition H.3. This choice of λ together with the bound on generalized effective dimension N (λ) in Lemma I.1 imply that there exists an n 0 such that for all n ≥ n 0 , (5) holds, as argued by [18, Proof of Theorem 1]. We use the phrase "n sufficiently large" when we appeal to this logic, and we summarize the final bound using O(•) notation. Lemma I.4 (Bounding the second factor). Suppose Assumptions 4.1 and 6.1 hold. Further assume (5) holds. Then probability 1 − δ/2, the second factor in Lemma I.2 is bounded as</p><p>Proof. The result follows from <ref type="bibr">[18, eq. 44b, 47]</ref>. In particular, our assumptions suffice for the properties used in [18, <ref type="bibr">Lemma 17]</ref>  Lemma I.5 (Bounding the third factor). With probability one, the third factor in Lemma I.2 is bounded as</p><p>Proof. The result follows from the definition of operator norm.</p><p>Proposition I.2 (Conditional expectation operator variance). Suppose Assumptions 4.1, 6.1, and 6.3 hold. Further assume (5) holds and λ ≤ 1. Then with probability 1 − δ,</p><p>Proof. We combine the previous lemmas. By Lemmas I.2, I.3, I.4, and I.5, if (5) holds, then with probability 1 − δ</p><p>Next, recall the bounds in Lemma I.1. When λ ≤ 1,</p><p>For brevity, write</p><p>. Therefore when λ ≤ 1 the bound simplifies as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.3 Collecting results</head><p>Proof of Proposition H.3. We combine and simplify Propositions I.1 and I.2. Take λ = n −1/(c+1/b) . For sufficiently large n, (5) holds and λ ≤ 1 as explained in Remark I.1. By triangle inequality, with probability 1 − δ,</p><p>Each term on the RHS simplifies as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Simulation details</head><p>In this appendix, we provide simulation details for (i) the dose response design, and (ii) the heterogeneous treatment effect design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.1 Dose response curve</head><p>A single observation consists of the triple (Y, D, X) for outcome, treatment, and covariates where Y, D ∈ R and X ∈ R 100 . A single observation is generated is as follows. Draw unobserved noise as ν, i.i.d.</p><p>∼ N (0, 1). Define the vector β ∈ R 100 by β j = j −2 . Define the matrix Σ ∈ R 100×100 such that Σ ii = 1 and Σ ij = 1(|i − j| = 1)/2 for i = j. Then draw X ∼ N (0, Σ) and set</p><p>We implement our estimator θAT E (d) (RKHS, white) described in Section 4, with the tuning procedure described in Supplement F. Specifically, we use ridge penalties determined by leave-one-out cross validation, and product exponentiated quadratic kernel with lengthscales set by the median heuristic. We implement <ref type="bibr" target="#b31">[32]</ref> (DR1, checkered white) using the default settings of the command ctseff in the R package npcausal. We implement <ref type="bibr" target="#b10">[11]</ref> (DR2, lined white) using default settings in Python code shared by the authors. Specifically, we use random forest for prediction, with the suggested hyperparameter values. For the Nadaraya-Watson smoothing, we select bandwidth that minimizes out-of-sample mean square error. We implement <ref type="bibr" target="#b55">[56]</ref> (DR-series, gray) by modifying ctseff, as instructed by the authors. Importantly, we give DR-series the advantage of correct specification of the true dose response curve as a quadratic function.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Estimating conditional average treatment effects</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Abrevaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Chin</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">P</forename><surname>Lieli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Business &amp; Economic Statistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="485" to="505" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cross section and panel data estimators for nonseparable models with endogenous regressors</title>
		<author>
			<persName><forename type="first">G</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><surname>Altonji</surname></persName>
		</author>
		<author>
			<persName><surname>Rosa L Matzkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1053" to="1102" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unifying divergence minimization and statistical inference via convex duality</title>
		<author>
			<persName><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computational Learning Theory</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="139" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the equivalence between herding and conditional gradient algorithms</title>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1355" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Reproducing Kernel Hilbert Spaces in Probability and Statistics</title>
		<author>
			<persName><forename type="first">Alain</forename><surname>Berlinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Thomas-Agnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimal rates for the regularized least-squares algorithm</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Caponnetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernesto</forename><forename type="middle">De</forename><surname>Vito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="331" to="368" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Linear inverse problems in structural econometrics estimation based on spectral decomposition and regularization</title>
		<author>
			<persName><forename type="first">Marine</forename><surname>Carrasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Pierre</forename><surname>Florens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Renault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Handbook of Econometrics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="5633" to="5751" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient semiparametric estimation of multi-valued treatment effects under ignorability</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><surname>Cattaneo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Econometrics</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="138" to="154" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inference on counterfactual distributions</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iván</forename><surname>Fernández-Val</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaise</forename><surname>Melly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2205" to="2268" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Debiased machine learning of global and local parameters using regularized Riesz representers</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Newey</surname></persName>
		</author>
		<author>
			<persName><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Econometrics Journal</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Double debiased machine learning nonparametric inference with continuous treatments</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Colangelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying-Ying</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03036</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Smoothing noisy data with spline functions: Estimating the correct degree of smoothing by the method of generalized cross-validation</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerische Mathematik</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="377" to="403" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the mathematical foundations of learning</title>
		<author>
			<persName><forename type="first">Felipe</forename><surname>Cucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Smale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nonparametric instrumental regression</title>
		<author>
			<persName><forename type="first">Serge</forename><surname>Darolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Pierre</forename><surname>Florens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Renault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1541" to="1565" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Targeted data adaptive estimation of the causal doseresponse curve</title>
		<author>
			<persName><forename type="first">Iván</forename><surname>Díaz</surname></persName>
		</author>
		<author>
			<persName><surname>Mark J Van Der Laan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Causal Inference</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="171" to="192" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Estimation of conditional average treatment effects with high-dimensional data</title>
		<author>
			<persName><forename type="first">Qingliang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Chin</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">P</forename><surname>Lieli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Business &amp; Economic Statistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="313" to="327" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient semiparametric estimation of quantile treatment effects</title>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Firpo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="259" to="276" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sobolev norm learning rates for regularized least-squares algorithms</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingo</forename><surname>Steinwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="205" to="206" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Estimating the effects of length of exposure to instruction in a training program: The case of Job Corps</title>
		<author>
			<persName><forename type="first">A</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfonso</forename><surname>Flores</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arturo</forename><surname>Flores-Lagunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Review of Economics and Statistics</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="153" to="171" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kernel Bayes&apos; rule: Bayesian inference with positive definite kernels</title>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3753" to="3783" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Uniformly semiparametric efficient estimation of treatment effects with a continuous treatment</title>
		<author>
			<persName><forename type="first">F</forename><surname>Antonio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Galvao</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">512</biblScope>
			<biblScope unit="page" from="1528" to="1542" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Grünewälder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1184" to="1192" />
		</imprint>
	</monogr>
	<note>Smooth operators</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Miguel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James M</forename><surname>Hernán</surname></persName>
		</author>
		<author>
			<persName><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Causal Inference. CRC</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arian</forename><surname>Hirshberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">R</forename><surname>Maleki</surname></persName>
		</author>
		<author>
			<persName><surname>Zubizarreta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10296</idno>
		<title level="m">Minimax linear estimation of the retargeted mean</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Predicting the efficacy of future training programs using past experiences at other locations</title>
		<author>
			<persName><forename type="first">V Joseph</forename><surname>Hotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Guido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><forename type="middle">H</forename><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName><surname>Mortimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Econometrics</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="241" to="270" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Direct and indirect effects of continuous treatments based on generalized propensity score weighting</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Chin</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Layal</forename><surname>Lettry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Econometrics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Causal inference with general treatment regimes: Generalizing the propensity score</title>
		<author>
			<persName><forename type="first">Kosuke</forename><surname>Imai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David A Van</forename><surname>Dyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">467</biblScope>
			<biblScope unit="page" from="854" to="866" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generalized optimal matching methods for causal inference</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Kallus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="62" to="63" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Policy evaluation and optimization with continuous treatments</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Kallus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1243" to="1251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recovering distributions from Gaussian RKHS embeddings</title>
		<author>
			<persName><forename type="first">Motonobu</forename><surname>Kanagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="457" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Optimal doubly robust estimation of heterogeneous causal effects</title>
		<author>
			<persName><forename type="first">H</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><surname>Kennedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14497</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Nonparametric methods for doubly robust estimation of continuous treatment effects</title>
		<author>
			<persName><forename type="first">H</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongming</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><forename type="middle">S</forename><surname>Mchugh</surname></persName>
		</author>
		<author>
			<persName><surname>Small</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1229</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Some results on Tchebycheffian spline functions</title>
		<author>
			<persName><forename type="first">George</forename><surname>Kimeldorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="95" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Asymptotic optimality of CL and generalized cross-validation in ridge regression with application to spline smoothing. The Annals of Statistics</title>
		<author>
			<persName><forename type="first">Ker-Chau</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="1101" to="1112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Optimal rates for regularized conditional mean embedding learning</title>
		<author>
			<persName><forename type="first">Zhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitri</forename><surname>Meunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mattes</forename><surname>Mollenhauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.01711</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Statistical inference for the mean outcome under a possibly non-unique optimal treatment strategy</title>
		<author>
			<persName><forename type="first">R</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><surname>Luedtke</surname></persName>
		</author>
		<author>
			<persName><surname>Mark J Van Der Laan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">713</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Super-learning of an optimal dynamic treatment rule</title>
		<author>
			<persName><forename type="first">R</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><surname>Luedtke</surname></persName>
		</author>
		<author>
			<persName><surname>Mark J Van Der Laan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Biostatistics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="305" to="332" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Optimization by Vector Space Methods</title>
		<author>
			<persName><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Luenberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On learning vector-valued functions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Micchelli</surname></persName>
		</author>
		<author>
			<persName><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="177" to="204" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Counterfactual mean embeddings</title>
		<author>
			<persName><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Motonobu</forename><surname>Kanagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sorawit</forename><surname>Saengkyongam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanparith</forename><surname>Marukatat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">162</biblScope>
			<biblScope unit="page" from="1" to="71" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The asymptotic variance of semiparametric estimators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><surname>Newey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="page" from="1349" to="1382" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Kernel estimation of partial means and a general variance estimator</title>
		<author>
			<persName><forename type="first">K</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><surname>Newey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometric Theory</title>
		<imprint>
			<biblScope unit="page" from="233" to="253" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Quasi-oracle estimation of heterogeneous treatment effects</title>
		<author>
			<persName><forename type="first">Xinkun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Wager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="299" to="319" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A measure-theoretic approach to kernel conditional mean embeddings</title>
		<author>
			<persName><forename type="first">Junhyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21247" to="21259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Comment: Graphical models, causality and intervention</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="266" to="269" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Causal diagrams for empirical research</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="669" to="688" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><surname>Causality</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes</title>
		<author>
			<persName><forename type="first">Loucas</forename><surname>Pillaud-Vivien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Rudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8114" to="8124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Dataset Shift in Machine Learning</title>
		<author>
			<persName><forename type="first">Joaquin</forename><surname>Quiñonero-Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><surname>Schwaighofer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Carl</forename><forename type="middle">Edward</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><surname>Williams</surname></persName>
		</author>
		<title level="m">Gaussian Processes for Machine Learning</title>
				<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A new approach to causal inference in mortality studies with a sustained exposure period-application to control of the healthy worker survivor effect</title>
		<author>
			<persName><forename type="first">James</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Modelling</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">9-12</biblScope>
			<biblScope unit="page" from="1393" to="1512" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The central role of the propensity score in observational studies for causal effects</title>
		<author>
			<persName><forename type="first">R</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald B</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="55" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">A general imputation methodology for nonparametric regression with censored data</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><surname>Mark J Van Der Laan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>UC Berkeley Division of Biostatistics</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Extending marginal structural models through local, penalized, and additive learning</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><surname>Mark J Van Der Laan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>UC Berkeley Division of Biostatistics</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Does Job Corps work? Impact findings from the national Job Corps study</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schochet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheena</forename><surname>Burghardt</surname></persName>
		</author>
		<author>
			<persName><surname>Mcconnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Economic Review</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1864" to="1886" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Debiased machine learning of conditional average treatment effects and other causal functions</title>
		<author>
			<persName><forename type="first">Vira</forename><surname>Semenova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Chernozhukov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Econometrics Journal</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="264" to="289" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Metrizing weak convergence with maximum mean discrepancies</title>
		<author>
			<persName><forename type="first">Carl-Johann</forename><surname>Simon-Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Barp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lester</forename><surname>Mackey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09268</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11076</idno>
		<title level="m">Debiased kernel methods</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Kernel instrumental variable regression</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Sahani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4595" to="4607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Kernel methods for policy evaluation: Treatment effects, mediation analysis, and off-policy planning</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04855</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning theory estimates via integral operators and their approximations</title>
		<author>
			<persName><forename type="first">Steve</forename><surname>Smale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding-Xuan</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Constructive Approximation</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="172" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A Hilbert space embedding for distributions</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Algorithmic Learning Theory</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="13" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">On the optimal estimation of probability measures in weak and strong topologies</title>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Sriperumbudur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1839" to="1893" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">On the relation between universality, characteristic kernels and RKHS embedding of measures</title>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gert</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="773" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Support Vector Machines</title>
		<author>
			<persName><forename type="first">Ingo</forename><surname>Steinwart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Christmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Mercer&apos;s theorem on general domains: On the interaction between measures, kernels, and RKHSs. Constructive Approximation</title>
		<author>
			<persName><forename type="first">Ingo</forename><surname>Steinwart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clint</forename><surname>Scovel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="363" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Fixing an error in Caponnetto and de Vito</title>
		<author>
			<persName><forename type="first">J</forename><surname>Danica</surname></persName>
		</author>
		<author>
			<persName><surname>Sutherland</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02982</idno>
		<imprint>
			<date type="published" when="2007">2007. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Sobolev norm learning rates for conditional mean embeddings</title>
		<author>
			<persName><forename type="first">Prem</forename><surname>Talwai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shameli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Simchi-Levi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10422" to="10447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Minimax estimation of kernel mean embeddings</title>
		<author>
			<persName><surname>Ilya Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bharath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krikamol</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName><surname>Muandet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3002" to="3048" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Unified cross-validation methodology for selection among estimators and a general cross-validated adaptive epsilon-net estimator: Finite sample oracle inequalities and examples</title>
		<author>
			<persName><forename type="first">Sandrine</forename><surname>Mark J Van Der Laan</surname></persName>
		</author>
		<author>
			<persName><surname>Dudoit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>UC Berkeley Division of Biostatistics</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">On differentiable functionals</title>
		<author>
			<persName><forename type="first">Aad</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName><surname>Vaart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="178" to="204" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Spline Models for Observational Data</title>
		<author>
			<persName><forename type="first">Grace</forename><surname>Wahba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>SIAM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Herding dynamical weights to learn</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1121" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Nonparametric estimation of causal heterogeneity under high-dimensional confounding</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zimmert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lechner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08779</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
