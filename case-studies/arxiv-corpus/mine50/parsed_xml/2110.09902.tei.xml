<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Toward Understanding Convolutional Neural Networks from Volterra Convolution Perspective</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-22">22 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tenghui</forename><surname>Li</surname></persName>
							<email>tenghui.lee@foxmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Guoxu</forename><surname>Zhou</surname></persName>
							<email>gx.zhou@gdut.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Qibin</forename><surname>Zhao</surname></persName>
							<email>qibin.zhao@riken.jp</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Automation at Guangdong</orgName>
								<orgName type="institution">University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Automation at Guangdong</orgName>
								<orgName type="institution">University of Technology</orgName>
								<address>
									<settlement>Guangzhou Yuning</settlement>
									<region>Qiu</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Automation at Guangdong</orgName>
								<orgName type="institution">University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">RIKEN Center for Advanced Intelligence Project and School of Automation at Guangdong</orgName>
								<orgName type="institution">University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Toward Understanding Convolutional Neural Networks from Volterra Convolution Perspective</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-22">22 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">9971234FF1588942C6B32174E4D5BBDC</idno>
					<idno type="arXiv">arXiv:2110.09902v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-10-27T19:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional neural network</term>
					<term>Volterra convolution</term>
					<term>order-n convolution</term>
					<term>unified neural network</term>
					<term>proxy kernel</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We make an attempt to understanding convolutional neural network by exploring the relationship between (deep) convolutional neural networks and Volterra convolutions. We propose a novel approach to explain and study the overall characteristics of neural networks without being disturbed by the horribly complex architectures. Specifically, we attempt to convert the basic structures of a convolutional neural network (CNN) and their combinations to the form of Volterra convolutions. The results show that most of convolutional neural networks can be approximated in the form of Volterra convolution, where the approximated proxy kernels preserve the characteristics of the original network. Analyzing these proxy kernels may give valuable insight about the original network. Base on this setup, we presented methods to approximating the order-zero and order-one proxy kernels, and verified the correctness and effectiveness of our results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks (DNNs) can effectively characterize most complex data as long as training data is large enough and the capable models are well-trained. Nevertheless, a deep network often has a horribly complex structure, the results are hard to interpret in some sense, and the network is likely to be deceptive by adversarial examples. We are eager to search for methods that allow us to analyze the network.</p><p>There is a vast number of excellent researches focusing on theoretically understanding neural networks. Some take the statistical perspective. Deep neural networks can be thought of as being discrete dynamical systems <ref type="bibr" target="#b32">(Weinan, 2017)</ref>. Instead of thinking about features and neurons, one focus on representation of functions, calculus of variation problems, and continuous gradient flow <ref type="bibr">(E et al., 2020)</ref>. Besides, others take a geometric perspective (grids, groups, graphs, geodesics, and gauges), which shows that deep neural networks can be understood in a unified manner as methods that respect the structure and symmetries (invariants and equivalents) of the geometric domains <ref type="bibr" target="#b3">(Bronstein et al., 2021)</ref>. Moreover, we can study how modern deep neural networks transform topologies of data sets <ref type="bibr" target="#b23">(Naitzat et al., 2020)</ref>, or draw the phase diagram for the two-layer ReLU neural network at the infinite-width limit <ref type="bibr" target="#b21">(Luo et al., 2021)</ref>.</p><p>Most of these works focus on certain classes of structures, such as two-layer neural network, multilayer fully connected network, ResNet, pure abstract network, and fully connected network with specific activation functions, i.e., ReLU, sigmoid, tanh, and so on. It seems that it is unlikely to represent and analyze an arbitrarily complex network from a theoretical point of view.</p><p>In this paper, we attempt to build a generic and unified model for analyzing most deep convolutional neural networks rather than thinking about features and layers. The neural network is a universal approximator that is able to approximate any Borel measurable function <ref type="bibr" target="#b6">(Cybenko, 1989;</ref><ref type="bibr" target="#b17">Hornik et al., 1989;</ref><ref type="bibr" target="#b1">Barron, 1993;</ref><ref type="bibr" target="#b35">Zeng et al., 2021)</ref>. The proposed model is expected to be a universal approximator. Additionally, it is supposed to ensure that most networks can be represented by this kind of model. Furthermore, it should be expressed as superposition of submodules, which makes it convenient to analyze.</p><p>The Volterra convolution or Volterra series operator <ref type="bibr">(Volterra, 1932)</ref> owns exactly such features. Briefly, Volterra convolution has the form of</p><formula xml:id="formula_0">y = H 0 + H 1 * x + H 2 * x 2 + • • • = +∞ n=0 H n * x n , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where x is the input signal, H 0 , H 1 , H 2 , • • • are kernels, y is the output signal, and H n * x n is the order-n convolution (All of these will be precisely defined in Section 2). Firstly, it has been proved that any time-invariant continuous nonlinear operator can be approximated by a Volterra series operator and any time invariant operator with fading memory can be approximated (in a strong sense) by a nonlinear moving-average operator, the finite term Volterra series operator <ref type="bibr" target="#b2">(Boyd and Chua, 1985)</ref>. Secondly, a certain class of artificial neural networks (feed-forward network or multilayer perceptron) are equivalent to a finite memory Volterra series <ref type="bibr" target="#b33">(Wray and Green, 1994;</ref><ref type="bibr" target="#b10">Fung et al., 1996)</ref>. In addition to this certain class of artificial neural networks, we show that neural networks, including convolutional neural networks and their numerous variants, can be approximated in the form of Volterra convolution. Thirdly, this is an accumulation of multiple submodules H 1 * x, H 2 * x 2 , • • •, which can be analyzed independently and without being disturbed by the complex network architecture.</p><p>Suppose the well-trained network is f (x) or g(f (x)), we are looking for a Volterra convolution to approximate the network f (x). The functions learned by practical convolutional neural networks (i.e., <ref type="bibr">ReLU-based, sigmoid-based)</ref> often can only be represented by Volterra convolution with infinite series. Nevertheless, if small truncation errors are allowed in practice, we can use finite term Volterra convolution via truncating the infinite counterpart to approximate the functions, which is mainly considered in this paper. Formally, for a function f (x), we are looking for N + 1 proxy kernels H</p><formula xml:id="formula_2">0 , H 1 , • • • , H N such that f (x) ≈ N n=0 H n * x n .</formula><p>(2)</p><p>If a network can be approximated in this form, its kernels H 0 , H 1 , • • • , H N shall preserve characteristics of the original network, and they will probably help us to analyze the stability or robustness or other useful properties of a well-trained network.</p><p>The Volterra convolution looks like a polynomial network (Giles and <ref type="bibr" target="#b11">Maxwell, 1987;</ref><ref type="bibr">Shin and</ref><ref type="bibr">Ghosh, 1995, 2003;</ref><ref type="bibr" target="#b9">Fallahnezhad et al., 2011)</ref>. They are similar in the sense of "polynomial". Nevertheless, they are quite different. A polynomial network learns a function f : R n → R, but Volterra convolution is a map f : R n → R m . Besides, we are not interested in the training this Volterra convolution by raw data. Instead, we will just approximate a well-trained network in the formulation of Volterra convolution.</p><p>In summary, the main contribution of this paper include:</p><p>• We showed that most convolutional neural networks can be represented in the form of Volterra convolutions. This formulation provides a novel perspective on understanding neural networks, and will probably help us to analyze a neural network without being disturbed by its complex architecture.</p><p>• We studied some important properties of this representation, including the effects of perturbations and the rank of combining two order-n convolutions.</p><p>• We showed that a convolutional neural can be well approximated by finite term Volterra convolutions. All we need in this approximation are the proxy kernels. We proposed two methods to infer the proxy kernels. The first is by direct calculation provided that the original network is a white-box to users, whereas the second one is for the case where the original network is a black-box.</p><p>The main structure of this article is listed as follows. In Section 2, we introduce and review the definition and properties of order-n convolution, outer convolution and Volterra convolution. In Section 3, we show that most convolutional neural networks can be approximated in the form of Volterra convolutions, and validate this approximation on simple structures. In Section 4, we proposed a hacking network to approximate the order-zero and order-one proxy kernels.</p><p>Notations: In most situations, vectors are notated in lower case bold face letters x, y, • • •, while matrices and tensors are notated in upper case bold face letters H, G, • • •. We will not deliberately distinguish between them, as this does not affect the generality of our discussion. Elements are notated by regular letters with brackets x(i), X(i, j, k), • • •. The ith-slice of X is noted by X(:, i, :) using the colon notation.</p><p>The vector like notation is x = x 1 , x 2 , • • • , which is nothing but a list of objects and the addition and subtraction are defined as x ± l = x 1 ± l, x 2 ± l, • • • and x ± y = x 1 ± y 1 , x 2 ± y 2 , • • • .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Extension of Convolutions</head><p>In this section, four kinds of convolutions will be introduced in both continuous and discrete time, including the well-known convolution (Equation <ref type="formula">3</ref>), order-n convolution (Definition 2), Volterra convolution (Definition 3), and outer convolution (Definition 4).</p><p>Without loss of generality, all kernels and signals are bounded by a constant M 1 &lt; ∞ and have Lipschitz constant</p><formula xml:id="formula_3">M 2 &lt; ∞, {x ∈ C(R) : |x(t)| ≤ M 1 , |x(s) − x(t)| ≤ M 2 (s − t), for t ≤ s} ,</formula><p>where C(R) : R → R is the space of bounded continuous functions, and all kernels are absolute integrable</p><formula xml:id="formula_4">+∞ −∞ |h(t)|dt &lt; ∞ or t |h(t)| &lt; ∞.</formula><p>The well known one-dimensional convolution (Gonzalez and Woods, 2017) of kernel h and signal x is</p><formula xml:id="formula_5">(h * x)(t) = +∞ −∞ h(τ )x(t − τ )dτ (continuous), (h * x)(t) = τ h(τ )x(t − τ ) (discrete).</formula><p>(3)</p><p>Remark 1 If kernel size equals to signal size and padding is zero, the discrete onedimensional order-one convolution is equivalents to vector inner product with flipped kernel at t = 0,</p><formula xml:id="formula_6">τ h(τ )x(t − τ ) ⇒ τ h(−τ )x(τ ).</formula><p>In signal processing, discrete convolution is notated in minus type, such as τ h(τ )x(t−τ ), and correlation is defined in plus type, like τ h(τ )x(t + τ ). While in convolutional networks, we prefer to notate convolution in plus type. These two operations can be converted from one to the other by flipping kernels and shifting time (see Figure <ref type="figure">1</ref>). Discussing only in minus type does not affect generality of the results.</p><formula xml:id="formula_7">x(t) h(0) h(1) τ h(τ )x(t + τ ) h(2) • • • • • • h(2) h(1) τ h(τ )x(t − τ ) h(0) x(t)</formula><p>Figure <ref type="figure">1</ref>: Differences between addition type and subtraction type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Order-n Convolution</head><p>Nonlinearities of convolutional neural networks come from their nonlinear activation functions. Theoretically, it is possible to embed nonlinearity in convolutional operation by taking order-n convolution.</p><p>For a simple example: a parabola can be described by a two-layer network with activation σ(•), i.e., w 2 σ(w</p><formula xml:id="formula_8">1 x + b 1 ) + b 2 , or it can be expressed in polynomial α 0 + α 1 x + α 2 x 2 + • • •.</formula><p>This type of "polynomial" can be also applied to convolutional operation, i.e.,</p><formula xml:id="formula_9">αx 2 +∞ −∞ H(τ 1 , τ 2 )x(t − τ 1 )x(t − τ 2 )dτ 1 dτ 2 , αxy +∞ −∞ H(τ 1 , τ 2 )x(t − τ 1 )y(t − τ 2 )dτ 1 dτ 2 , αx 3 +∞ −∞ H(τ 1 , τ 2 , τ 3 )x(t − τ 1 )x(t − τ 2 )x(t − τ 3 )dτ 1 dτ 2 dτ 3 .</formula><p>With these basic concepts in mind, formal definition of order-n convolution is presented in Definition 2.</p><p>Definition 2 Order-n convolution <ref type="bibr">(Volterra, 1932;</ref><ref type="bibr" target="#b24">Rugh, 1981)</ref> of kernel H and n signals</p><formula xml:id="formula_10">x 1 , x 2 , • • • , x n ≡ x is H * x 1 , x 2 , • • • , x n ≡ H * x, (<label>4</label></formula><formula xml:id="formula_11">)</formula><p>where x is vector like notation and all x i have the same dimension.</p><formula xml:id="formula_12">If x 1 , x 2 , • • • ,</formula><p>x n are all one-dimensional signals and H is an n-dimensional signal, the continuous order-n convolution for one-dimensional signal is</p><formula xml:id="formula_13">(H * x) (t) = (H * x 1 , x 2 , • • • , x n ) (t) = +∞ −∞ • • • +∞ −∞ H(τ 1 , τ 2 , • • • , τ n ) n i=1 (x i (t − τ i )dτ i ) ≡ +∞ −∞ • • • +∞ −∞ H( τ ) n i=1 (x i (t − τ i )dτ i ) . (5) If x 1 , x 2 , • • • , x n</formula><p>are all m-dimensional signals and H is an nm-dimensional signal, and vector like notations have the form</p><formula xml:id="formula_14">τ 1 = τ 1,1 , τ 1,2 , • • • , τ 1,m ; • • •; τ n = τ n,1 , τ n,2 , • • • , τ n,m and t = t 1 , t 2 , • • • , t m , order-n convolution for m-dimensional signal is (H * x) (t 1 , • • • , t m ) = (H * x 1 , x 2 , • • • , x n ) (t 1 , • • • , t m ) = +∞ −∞ • • • +∞ −∞ H(τ 1,1 , • • • , τ 1,m ; • • • ; τ n,1 , • • • , τ n,m ) n i=1 (x i (t 1 − τ i,1 , • • • , t m − τ i,m )dτ i,1 • • • dτ i,m ) ≡ +∞ −∞ • • • +∞ −∞ H( τ 1 , τ 2 , • • • , τ n ) n i=1 x( t − τ i )d τ i . (6)</formula><p>With the vector like notation, discrete order-n convolution for m-dimensional signal is simplified as</p><formula xml:id="formula_15">(H * x) ( t) ≡ τ 1 ,••• , τn H( τ 1 , τ 2 , • • • , τ n ) n i=1 x i ( t − τ i ). (<label>7</label></formula><formula xml:id="formula_16">)</formula><p>If n = 0, order-zero convolution H * x 0 = H * δ = H, where δ is the Dirac delta,</p><formula xml:id="formula_17">δ(t) = ∞, t = 0 0, t = 0 (continuous), δ(t) = 1, t = 0 0, t = 0 (discrete).</formula><p>If n = 1, this is order-one (first-order) convolution (Equation <ref type="formula">3</ref>), and n = 2, this is order-two (second-order) convolution H * x, y . The order-two convolution does not come from void, as it is an extension of order-one convolution. (Please read Appendix A for more details.)</p><p>For better understanding this notation, few examples are expressed as follows.</p><formula xml:id="formula_18">H * x, y, z, • • •</formula><p>The number of signal equals the order.</p><p>Besides, if all signals are equal</p><formula xml:id="formula_19">x 1 = x 2 = • • • = x n = x, it can be written as H * x n for short. H * x, x, • • • , x = H * x n n equals the order. Dimension of this convolution is determined by its signals. If x j , j = 1, 2, • • • , n are all m-dimensional signals, H * x is called order-n convolution for m-dimensional signal. H * • • • , [x j (i 1 , i 2 , • • • , i m )] , • • • m equals the dimension of signal.</formula><p>If signals are grouped and n 1 + n 2 + • • • + n m equals the order, it can simplify as</p><formula xml:id="formula_20">H *     x 1 , • • • , x 1 n 1 terms , x 2 , • • • , x 2 n 2 terms , • • • , x m , • • • , x m nm terms     = H * x n 1 1 , x n 2 2 , • • • , x nm m .</formula><p>In the following, we will illustrate two observations of H * x n , where x is discrete one-dimensional signal.</p><p>The first observation is that each dimension of H is equal, i.e., H ∈ R   </p><formula xml:id="formula_21">(• • • , τ i , • • • , τ j , • • • ) = Ĥ(• • • , τ j , • • • , τ i , • • • ) for any τ i , τ j at any dimension.</formula><p>The first is obvious, and the second is demonstrated as below.</p><p>Expanding H * x n as</p><formula xml:id="formula_22">• • • + H(• • • , τ i , • • • , τ j , • • • ) • • • x(t − τ i ) • • • x(t − τ j ) • • • + H(• • • , τ j , • • • , τ i , • • • ) • • • x(t − τ j ) • • • x(t − τ i ) • • • + • • • , and take Ĥ(• • • , τ i , • • • , τ j , • • • ) = 1 2 (H(• • • , τ i , • • • , τ j , • • • ) + H(• • • , τ i , • • • , τ j , • • • )), we have H * x n = H * x n .</formula><p>Therefore, without further notice, this kind of kernels are always symmetric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Volterra Convolution</head><p>In this subsection, we sum these convolutions from order-zero to order-n or order-∞. If the order is finite, it is called the finite term Volterra convolution or order-n Volterra convolution, otherwise it is called the infinity term Volterra convolution or Volterra convolution. For instance, the order-two Volterra convolution is sum of order-zero, order-one and order-two convolutions,</p><formula xml:id="formula_23">H 0 * x 0 + H 1 * x 1 + H 2 * x 2 .</formula><p>For simplicity and the fact that a neural network takes only one input (multiple inputs are packed to one tensor), input signals of each order are set to be the same</p><formula xml:id="formula_24">x 1 = x 2 = • • • = x n .</formula><p>If the input signals are one-dimensional, all kernels are symmetric.</p><p>Definition 3 Let x be signal and H n as kernels, Volterra convolution <ref type="bibr">(Volterra, 1932;</ref><ref type="bibr" target="#b24">Rugh, 1981)</ref> is defined as</p><formula xml:id="formula_25">+∞ n=0 H n * x n = +∞ n=0 H n * x, x, • • • n terms . (<label>8</label></formula><formula xml:id="formula_26">)</formula><p>If n = 0, x 0 = δ, i.e., the Dirac delta.</p><p>If x is a one-dimensional signal and each H n is an n-dimensional signal, continuous Volterra convolution for one-dimensional signal is</p><formula xml:id="formula_27">+∞ n=0 H n * x n (t) = +∞ n=0 +∞ −∞ • • • +∞ −∞ H n (τ 1 , • • • , τ n ) n i=1 (x(t − τ i )dτ i ) .<label>(9)</label></formula><p>According to previous discussion, here the kernels H n , n = 1, 2, 3, . . ., are symmetric. If x is an m-dimensional signal and H n is an nm-dimensional signal for each n = 0, 1, • • •, and</p><formula xml:id="formula_28">τ 1 = τ 1,1 , τ 1,2 , • • • , τ 1,m ; • • •; τ n = τ n,1 , τ n,2 , • • • , τ n,m and t = t 1 , t 2 , • • • , t m , continuous Volterra convolution for m-dimensional signal is +∞ n=0 H n * x n (t 1 , • • • , t m ) = +∞ n=0 +∞ −∞ • • • +∞ −∞ H n (τ 1,1 , • • • , τ 1,m ; • • • ; τ n,1 , • • • , τ n,m ) n i=1 (x(t 1 − τ i,1 , • • • , t m − τ i,m )dτ i,1 • • • dτ i,m ) ≡ +∞ n=0 +∞ −∞ • • • +∞ −∞ H n ( τ 1 , τ 2 , • • • , τ n ) n i=1 x( t − τ i )d τ i . (<label>10</label></formula><formula xml:id="formula_29">)</formula><p>With the vector like notation, discrete Volterra convolution for m-dimensional signal is simplified as</p><formula xml:id="formula_30">+∞ n=0 H n * x n ( t) = +∞ n=0 τ 1 ,••• , τn H n ( τ 1 , τ 2 , • • • , τ n ) n i=1 x( t − τ i ).</formula><p>(11)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Outer Convolution</head><p>Stacking two order-one one-dimensional convolutions will produce a one-dimensional convolution with a longer kernel. How do we stack order-n convolutions? In this subsection, an operation, the outer convolution, is introduced to combine these convolutions. Moreover, the rank properties for outer convolutions are described in Appendix D.</p><p>Let G and H 1 , H 2 , • • • , H n ≡ H are the kernels for convolutions of one-dimensional signal. The outer convolution of G and H, denoted by</p><formula xml:id="formula_31">G H 1 , H 2 , • • • , H n ≡ G H,<label>(12)</label></formula><p>is defined as follows.</p><p>Definition 4 (Continuous outer convolution of kernels) Let G be an n-dimensional kernel and each dimension of H i no less than one, then the continuous outer convolution yields an L-dimensional kernel satisfying</p><formula xml:id="formula_32">G H (t 1,1 , t 1,2 , • • • ; t 2,1 , t 2,2 , • • • ; • • • ; t n,1 , t n,2 , • • • ) = +∞ −∞ • • • +∞ −∞ G(τ 1 , τ 2 , • • • , τ n ) n i=1 (H i (t i,1 − τ i , t i,2 − τ i , • • • )dτ i ) ≡ +∞ −∞ • • • +∞ −∞ G( τ ) n i=1 H i ( t i − τ i )dτ i ,<label>(13)</label></formula><p>where L is the sum of the dimensions of</p><formula xml:id="formula_33">H i , τ = τ 1 , τ 2 , • • • , τ n , t 1 = t 1,1 , t 1,2 , • • • , t 2 = t 2,1 , t 2,2 , • • • , • • •, t n = t n,1 , t n,2 ,</formula><p>• • • , using the vector like notations.</p><p>With Equation <ref type="formula" target="#formula_32">13</ref>we can compute the convolution between G H and one-dimensional signals x. Since most signals in this article are one-dimensional, we prefer to express the outer convolution in the form of Equation <ref type="formula" target="#formula_32">13</ref>. Note that Equation 13 allows us to combine multiple convolution layers, and detailed rules will be described in Subsection 2.7. Two of these rules are quick previewed as follows:</p><formula xml:id="formula_34">• G * (H * x) = (G H) * x (Property 7), • G * H 1 * x 1 , H 2 * x 2 = (G H 1 , H 2 ) * x 1 , x 2 (Property 9).</formula><p>More generally, we consider the layers involving the convolution of m-dimensional signals x. To this end, suppose that G is an nm-dimensional kernel, and each dimension of H i is no less than m, continuous outer convolution of G and H i can be expressed as</p><formula xml:id="formula_35">G H     t 1,1,1 , t 1,1,2 , • • • t 1,1,m ; t 1,2,1 , • • • t 1,2,m ; • • • ; t 2,1,1 , t 2,1,2 , • • • t 2,1,m ; t 2,2,1 , • • • t 2,2,m ; • • • ; • • • ; • • • ; t n,1,1 , t n,1,2 , • • • t n,1,m ; t n,2,1 , • • • t n,2,m ; • • • ;     = +∞ −∞ • • • +∞ −∞ G (τ 1,1 , τ 1,2 , • • • , τ 1,m ; τ 2,1 , τ 2,2 , • • • , τ 2,m ; • • • ; • • • ; τ n,1 , τ n,2 , • • • , τ n,m ; ) n i=1   H i   t i,1,1 − τ i,1 , t i,1,2 − τ i,2 , • • • , t i,1,m − τ i,m ; t i,2,1 − τ i,1 , t i,2,2 − τ i,2 , • • • , t i,2,m − τ i,m ; • • •   dτ i,1 dτ i,2 • • • dτ i,m   . ≡ +∞ −∞ • • • +∞ −∞ G( τ 1 , τ 2 , • • • , τ n ) n i=1 H i ( t i,1 − τ i , t i,2 − τ i , • • • )d τ i .<label>(14)</label></formula><p>The outer convolution (Equation <ref type="formula" target="#formula_35">14</ref>) will yield a new kernel whose convolution with mdimentional signals is given by Equation <ref type="formula">6</ref>.</p><p>Using the vector like notation, discrete outer convolution for one-dimensional signal is simplified as</p><formula xml:id="formula_36">G H ( t 1 , t 2 , • • • , t n ) = τ G( τ ) n i=1 H i ( t i − τ i ),<label>(15)</label></formula><p>and discrete outer convolution for m-dimensional signal is simplified as</p><formula xml:id="formula_37">G H ( t 1,1 , t 1,2 , • • • ; t 2,1 , • • • ; • • • ; t n,1 , • • • ) = τ 1 ,••• G( τ 1 , τ 2 , • • • , τ n ) n i=1 H i ( t i,1 − τ i , t i,2 − τ i , • • • ). (<label>16</label></formula><formula xml:id="formula_38">)</formula><p>The shorthand notations of outer convolution are similar to that of order-n convolution. If all signals are equal</p><formula xml:id="formula_39">H 1 = H 2 = • • • = H n , we have G H 1 , H 2 , • • • , H n = G H n . If signals are grouped and n 1 + n 2 + • • • + n m equals</formula><p>the order, the notation can be simplified as</p><formula xml:id="formula_40">G     H 1 , • • • , H 1 n 1 terms , H 2 , • • • , H 2 n 2 terms , • • • , H m , • • • , H m nm terms     = G * H n 1 1 , H n 2 2 , • • • , H nm m .</formula><p>Remark 5 In discrete outer convolution G H, kernels H 1 , • • • , H n are all zero padded on both heads and tails. The padding size of H i is s i − 1, where s i is shape of G at dimension i. For instance, one-dimensional H i is zero padded as</p><formula xml:id="formula_41">  • • • 0 • • • s i −1 H i (0) • • • H i (n) • • • 0 • • • s i −1   .</formula><p>For example, if G has shape (s 1 , s 2 ), H 1 has shape (z 1 ) and H 2 has shape (z 2 , z 3 , z 4 ), outer convolution (with zero padded) G H 1 , H 2 will have shape (s</p><formula xml:id="formula_42">1 + z 1 − 1, s 2 + z 2 − 1, s 2 + z 3 − 1, s 2 + z r ight).</formula><p>Remark 6 G H has the same operation as the "ConvTranspose", which is a deep learning operator 1 .</p><formula xml:id="formula_43">Remark 7 If h 1 , h 2 , • • • , h n</formula><p>are all one-dimensional vectors, and tensor</p><formula xml:id="formula_44">H(t 1 , t 2 , • • • , t n ) = h 1 (t 1 )h 2 (t 2 ) • • • h n (t n ), outer convolution can be transformed to multidimensional convolution G h 1 , h 2 , • • • , h n = G * H.</formula><p>The super diagonal kernel G has only non-zero elements on the diagonal, G(0, 0,</p><formula xml:id="formula_45">• • • , 0), G(1, 1, • • • , 1), • • •,</formula><p>and all other elements are set to zero. More specifically, we define the G = diag(n, g), where g is a vector, and</p><formula xml:id="formula_46">G(τ 1 , τ 2 , • • • , τ n ) = diag(n, g)(τ 1 , τ 2 , • • • , τ n ) = k g(k) n i=1 δ(τ i − k). (<label>17</label></formula><formula xml:id="formula_47">)</formula><p>For better understanding, this diag(•) operator is visualized in Figure <ref type="figure" target="#fig_0">2</ref>. Specially, if G is a super diagonal tensor kernel, its outer convolution has the form of</p><formula xml:id="formula_48">0 (a) one-dimensional g ∈ R 5 0 (b) diag(2, g) ∈ R 5×5 0 (c) diag(3, g) ∈ R 5×5×5</formula><formula xml:id="formula_49">(G H 1 , H 2 , • • • , H n ) ( t 1 , t 2 , • • • , t n ) = τ G( τ ) n i=1 H i ( t i − τ i ) = k g(k) n i=1 δ(τ i − k)H i ( t i − τ i ) = k g(k) n i=1 H i ( t i − k).</formula><p>(18)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Convolution With Stride Grater Than One</head><p>Convolution with stride grater than one is commonly used to replace convolution-pooling structure. If the stride equals one, filters move one point at a time. If the stride equals two, filters jump two points at a time. In addition, convolution with stride s is</p><formula xml:id="formula_50">(h * s x) (t) = τ h(τ )x(st − τ ),</formula><p>where subscript of asterisk * s indicates stride.</p><p>1. https://pytorch.org/docs/stable/generated/torch.nn.functional.conv_transpose1d.html</p><p>This operation can be applied to order-n convolution and outer convolution. With vector like notation, let</p><formula xml:id="formula_51">s t = st 1 , st 2 , • • • , st n . Discrete order-n convolution for m-dimensional signal with stride s is (H * s x) ( t) = τ 1 ,••• , τn H( τ 1 , τ 2 , • • • , τ n ) n i=1 x i (s t − τ i ). (<label>19</label></formula><formula xml:id="formula_52">)</formula><p>Similarly, the discrete outer convolution for m-dimensional signal with stride s is</p><formula xml:id="formula_53">G s H ( t 1,1 , t 1,2 , • • • ; t 2,1 , • • • ; • • • ; t n,1 , • • • ) = τ 1 ,••• G( τ 1 , τ 2 , • • • , τ n ) n i=1 H i (s t i,1 − τ i , s t i,2 − τ i , • • • ). (<label>20</label></formula><formula xml:id="formula_54">)</formula><p>The combination of two convolutions with strides is also equivalent to the outer convolution with strides,</p><formula xml:id="formula_55">G * s (H * z x) = (G z H) * sz x (Property 8 in Subsection 2.7). Remark 8 If G has shape (z 1 , z 2 , • • • , z m ) and H has shape c 1 , c 2 , • • • , c m , the shape of G s H is (c 1 + (z 1 − 1)s, c 2 + (z 2 − 1)s, • • • , c m + (z m − 1)s) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Visualization of Outer Convolution</head><p>For the following cases, three examples are provided to help understand the outer convolution.</p><p>Let G ∈ {1} 8×8 be a 8 × 8 matrix with all elements are one, and h ∈ {1} 8 be a vector of length eight with all elements are one. Figure <ref type="figure" target="#fig_1">3a</ref> demonstrates</p><formula xml:id="formula_56">(G h, h ) (t 1 , t 2 ) = 7,7 τ 1 =0,τ 2 =0 G(τ 1 , τ 2 )h(t 1 − τ 1 )h(t 2 − τ 2 ).</formula><p>Let g ∈ {1} 8 and H ∈ {1} 8×8 . Figure <ref type="figure" target="#fig_1">3b</ref> demonstrates</p><formula xml:id="formula_57">(g H) (t 1 , t 2 ) = 7,7 t 1 =0,t 2 =0 g(τ )H(t 1 − τ, t 2 − τ ).</formula><p>Assume G ∈ {1} 8×8 and H ∈ {1} 8×8 , Figure <ref type="figure" target="#fig_1">3c</ref> demonstrates</p><formula xml:id="formula_58">(G H, H ) (t 1 , t 2 , t 3 , t 4 ) = 7,7 τ 1 =0,τ 2 =0 G(τ 1 , τ 2 )H(t 1 − τ 1 , t 2 − τ 1 )H(t 3 − τ 2 , t 4 − τ 2 ).</formula><p>It is clear that G H, H is a four-dimensional tensor with shape <ref type="bibr">(15,</ref><ref type="bibr">15,</ref><ref type="bibr">15,</ref><ref type="bibr">15)</ref>, and it is flattened to shape <ref type="bibr">(225,</ref><ref type="bibr">225)</ref> and drawn in Figure <ref type="figure" target="#fig_1">3c</ref>. </p><formula xml:id="formula_59">(a) G h, h (b) g H (c) flattened G H, H</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Convolution for Multi-dimensional Signals</head><p>In this subsection, we show that the multidimensional (outer) convolution can be analyzed via one-dimensional (outer) convolution. With this transformation, only (outer) convolution for one-dimensional signals will be studied hereafter, if not specified.</p><p>Definition 9 (Flatten-operator) The flatten-operator is a bijection</p><formula xml:id="formula_60">T (x) = x, such that x(t 1 , t 2 , • • • ) = x(t 1 w 1 + t 2 w 2 + • • • ) and w 1 , w 2 are scalars for locating non-overlap elements. The inverse of T is denoted by T −1 with T −1 (T (x)) = x.</formula><p>Proposition 10 The flatten-operator is homomorphic</p><formula xml:id="formula_61">T (H * x) = T (H) * T ( x), T (G H) = T (G) T ( H). (<label>21</label></formula><formula xml:id="formula_62">)</formula><p>Proof A continuous case of this proposition is proved here, and the discrete one can be obtained in similar way. Recall order-n convolution for m-dimensional signal (Equation <ref type="formula" target="#formula_15">7</ref>), and let flattened index τ i be υ i , flattened index t be ι. We have</p><formula xml:id="formula_63">T (H * x)( t) = T   τ i ,••• , τn H( τ 1 , τ 2 , • • • , τ n ) n i=1 x i ( t − τ i )   = υ 1 ,••• ,υn Ĥ(υ 1 , υ 2 , • • • , υ n ) n i=1 xi (ι − υ i ) = (T (H) * T ( x)) (ι).</formula><p>The flatten-operator of outer convolution for m-dimensional signal is also homomorphic.</p><p>Recall outer convolution for m-dimensional signal (Equation <ref type="formula">11</ref>), and let flattened index t i,j be ι i,j , flattened index τ i be υ i . We have</p><formula xml:id="formula_64">T (G H)( t 1,1 , t 1,2 , • • • ; , t 2,1 , • • • ; • • • ; t n,1 , • • • ) = τ 1 ,••• , τn G( τ 1 , τ 2 , • • • , τ n ) n i=1 H i ( t i,1 − τ i , t i,2 − τ i , • • • ) = υ 1 ,••• ,υn Ĝ(υ 1 , υ 2 , • • • , υ n ) n i=1 Ĥi (ι i,1 − υ 1 , ι i,2 − υ 1 , • • • ) = T (G) T ( H) (ι 1,1 , ι 1,2 , • • • ; ι 2,1 , • • • ; • • • ; ι n,1 , • • • ).</formula><p>With Proposition 10, it could be easily verified</p><formula xml:id="formula_65">H * x = T −1 (T (H) * T ( x)) , G H = T −1 T (G) T ( H) .</formula><p>To help visualize this process, Figure <ref type="figure">4</ref> is an example of flattening a two-dimensional signal to a one-dimensional signal.</p><p>Figure <ref type="figure">4</ref>: Flattening a two-dimensional signal to a one-dimensional signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Combination Properties</head><p>With all definitions above, some useful properties are concluded in this subsection. All proofs are presented in Appendix B.</p><p>Since the multidimensional signals can be analyzed via one-dimensional signals, all signals here are set to be one-dimensional and represented as x or y. Kernels are represented as H and G. α is a scalar.</p><p>The following Property 1, 2 and 3 are about linearity of order-n convolutions. In continuous time, notation G is replaced with</p><formula xml:id="formula_66">+∞ −∞ • • • +∞ −∞ G( t)d t. 1. G * ( x + y) = G * x + G * y; 2. G * ( x + α) = G * x + α G; 3. (G + H) * x = G * x + H * x;</formula><p>Property 4, 5 and 6 are combination properties of order-n convolution. Based on previous discussion, the kernel G here is symmetric. The multinomial coefficient in Property 6 can be obtained from textbook of combinatorial mathematic <ref type="bibr" target="#b4">(Brualdi, 2004)</ref>.</p><formula xml:id="formula_67">4. G * (x + y) 2 = G * x 2 + 2G * x, y + G * y 2 ; 5. G * (x + y) n = n k=0 n k G * x k , y n−k , where n k = n! k!(n − k)! is binomial coefficient; 6. G * (x 1 + x 2 + • • • + x m ) n = n n 1 n 2 •••nm G * x n 1 1 , x n 2 2 , • • • , x nm m , where multinomial coefficient n n 1 n 2 •••nm = n! n 1 !n 2 ! • • • n m ! , and m i=1 n i = m, n i ≥ 0, for all i = 1, 2, • • • , m.</formula><p>Properties below are for combining convolutions. Symbol "#" indicates summation along a specific dimension. For example, #α (G</p><formula xml:id="formula_68">H, α ) = i (G H, α )(:, i) and #α (G H 1 , α, H 2 ) = i (G H 1 , α, H 2 )(:, i, :</formula><p>). In continuous space, they are replaced by</p><formula xml:id="formula_69">+∞ −∞ (G H, α ) (:, t α )dt α and +∞ −∞ (G H 1 , α, H 2 ) (:, t α , :)dt α . 7. G * (H * x) = (G H) * x; 8. G * s (H * z x) = (G z H) * sz x; 9. G * H 1 * x, H 2 * y = (G H 1 , H 2 ) * x, y ; 10. G * H 1 * x 1 , H 2 * x 2 , • • • = (G H 1 , H 2 , • • • ) * x 1 , x 2 , • • • ; 11. G 1 (G 2 G 3 ) = (G 1 G 2 ) G 3 ; 12. G * α, H * x = #α (G α, H ) * x; 13. G * H * x, α = #α (G H, α ) * x; 14. G * H 1 * x, α, H 2 * y = #α (G H 1 , α, H 2 ) * x, y ;</formula><p>Property 15 and 16 focus on convolution with signal that is element-wise power n,</p><formula xml:id="formula_70">([x] n ) (t) = (x(t)) n . 15. h * [x] n = diag(n, h) * x n ; 16. g * [h * x] n = (diag(n, g) h n ) * x n ;</formula><p>In addition to all properties above, some special properties could be obtained via setting special kernels. For example, by setting G as the identity matrix, we have</p><formula xml:id="formula_71">(G H 1 , H 2 ) ( t 1 , t 2 ) = i 1 ,i 2 G(i 1 , i 2 )H 1 ( t 1 − i 1 )H 2 ( t 2 − i 2 ) = H 1 ( t 1 )H 2 ( t 2 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Transformation from Neural Networks to Volterra Convolutions</head><p>Previous section discussed the definition of Volterra convolution and some useful properties of combining two order-n convolutions. In this section, we will go further and try to represent some common convolutional networks in the form of Volterra convolutions.</p><p>Theorem 11 Most convolutional neural networks can be represented in the form of Volterra convolutions.</p><p>Both convolutional networks and Volterra convolutions have the operation of convolution. The convolutional neural network extend this operation by stacking layers and the Volterra convolution extend this by increasing the order. Apart from the convolution, a convolutional neural network is a universal approximator, as it happens, a Volterra convolution is also a universal approximator. Theoretically, if two approximators can approximate the same function, it is possible to approximate one by the other. In light of this, roughly speaking, most convolutional neural network can be approximated in the form of Volterra convolution, and vice versa.</p><p>The proof contains two major parts. The first part is about the small neural network structures, and the second part is about the combination of multiple layers, i.e., the whole network. Since both the small structures and their combinations can be represented in this form, we conclude that most convolutional neural networks build of these structures can also be represented in the form of Volterra convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Conversion of Small Structures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Conv-Act-Conv Structure</head><p>The "conv -act -conv" structure means the stacking of a convolutional layer, an activation layer, and a convolutional layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 12</head><p>The "conv -act -conv" structure can be converted to the form of Volterra convolution.</p><p>Proof Suppose this structure has the form g * σ(h * x), where σ(•) is a nonlinear activation function.</p><p>A polynomial approximation, i.e., Taylor expansion, of this activation function σ(t) at α is</p><formula xml:id="formula_72">σ(t) = σ(α) + σ (α)(t − α) + σ (α) 2! (t − α) 2 + σ (α) 3! (t − α) 3 + • • • .</formula><p>We can assume without loss of generality that α = 0, and g * σ(h * x) becomes</p><formula xml:id="formula_73">g * σ(0) + σ (0)[h * x] + σ (0) 2! [h * x] 2 + σ (0) 3! [h * x] 3 + • • • ,<label>(22)</label></formula><p>where square brackets stand for</p><formula xml:id="formula_74">[h * x] n (t) = τ h(τ )x(t − τ ) n .</formula><p>By linearity of convolution, Equation 22 can be separated by terms. The first term is g. The second term is g * [h * x] 1 = (g h) * x. For the third term and above, recall Property 16, we have the n-th term,</p><formula xml:id="formula_75">g * [h * x] n = (diag(n, g) h n ) * x n . If α = 0, we conclude that g * σ(h * x) = σ(0) g + σ (0)(g h) * x + σ (0) 2! (diag(2, g) h 2 ) * x 2 + σ (0) 3! (diag(3, g) h 3 ) * x 3 + • • • . (<label>23</label></formula><formula xml:id="formula_76">)</formula><p>More generally, if α = 0, the n-th term is</p><formula xml:id="formula_77">g * [h * x − α] n = diag(n, g) * (h * x − α) n (Property 15) = n k=0 n k diag(n, g) * (h * x) k , (−α) n−k (Property 5) = n k=0 n k   #(−α) n−k diag(n, g) h k , (−α) n−k   * x k (Property 13),</formula><p>which implies that the sum from the 0-th term to the ∞-th term is also the form of Volterra convolution. This proof is completed.</p><p>The Taylor expansion of a function often has infinite terms. However, if small truncation errors are allowed in applications, we can truncate the infinite term Taylor expansion to a finite term Taylor expansion. The idea of truncation can also be applied to the Volterra convolution. According to the universal approximation property of Volterra convolution with fading memory <ref type="bibr" target="#b2">(Boyd and Chua, 1985)</ref>, for any given &gt; 0, there always exists n such that</p><formula xml:id="formula_78">f (x) − n i=0 F i * x i 2 &lt; ,</formula><p>where f (•) is a time invariant operation, and F i , i = 0, 1, • • • n are kernels. The fading memory theory means that the outputs are close if two inputs are close in the recent past, but not necessarily close in the remote past <ref type="bibr" target="#b2">(Boyd and Chua, 1985)</ref>. If g * σ(h * x) is time invariant and small truncation errors are allowed, it is reasonable to approximate g * σ(h * x) by a finite term Volterra convolution.</p><p>In the following, we will consider the width of a two-layer network. Suppose a two-layer network has M hidden neurons, and the neurons of distinct channels are independent and identically distributed,</p><formula xml:id="formula_79">y(t) = M −1 i=0 W 2 (i)σ τ W 1 (i, τ )x(t − τ ) .</formula><p>Recall Lemma 12, each channel can be represented in the form of Volterra convolution, and this network can also be approximated by the sum of M Volterra convolutions, the order-n term is</p><formula xml:id="formula_80">n=0 F 0,n * x n + n=0 F 1,n * x n + • • • + n=0 F M −1,n * x n , where F i,n , i = 0, 1, • • • , M − 1; is the proxy kernel of each channel.</formula><p>Based on the previous independent assumptions, these proxy kernels of different channels are also independent and identically. Recall Hoeffding's inequality <ref type="bibr" target="#b30">(Vershynin, 2018)</ref>, we have</p><formula xml:id="formula_81">P 1 M M −1 i=0 F i,n − µ n ≥ η ≤ 2 exp −2M η 2 (b − a) 2 ,</formula><p>where a, b are the minimum and maximum values of all proxy kernels, a</p><formula xml:id="formula_82">≤ F i,n (• • • ) ≤ b.</formula><p>We can observe that for any ∈ (0, 1), 1/M M −1 i=0 F i,n will converge to µ n , with probability at least 1 − as long as</p><formula xml:id="formula_83">M ≥ 1 2η 2 ln 2 (b − a) 2 . (24)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Other Structures</head><p>Some commonly used structures can also be represented in the form of Volterra convolution, including some activations, normalize layers, inception modules, residual connection and pooling layers.</p><p>ReLU activation: The ReLU activation, max(x, 0) <ref type="bibr" target="#b22">(Nair and Hinton, 2010)</ref>, is not differentiable at point 0. This is quite a panic when taking Taylor expansion at that position. Nonetheless, it can be approximated by</p><formula xml:id="formula_84">ReLU(x) = lim α→∞ 1 α ln (1 + e αx ) .</formula><p>Other activations of the ReLU family can be approximated in the same way. Fully connected layers: A fully connected layer is a matrix multiplication with bias. It can be thought as discrete convolution with equal kernel length and signal length.</p><formula xml:id="formula_85">j W (i, j)x(j) + c = j W (i, 0 − j)x(j) + c, where W (i, 0 − j) = W (i, j).</formula><p>Normalization: The normalization layer <ref type="bibr" target="#b18">(Ioffe and Szegedy, 2015;</ref><ref type="bibr" target="#b0">Ba et al., 2016)</ref> scales and shifts the input signal</p><p>x → ax + b.</p><p>This is a linear transformation, and this will not change the generality. Nevertheless, a and b are input related, which implies that the corresponded proxy kernels are also input related.</p><p>In other words, the Volterra convolution is dynamic and will be updated if input differs. We will pause here and left this dynamic Volterra convolution to future work.</p><p>Inception: Main idea of inception module <ref type="bibr">(Szegedy et al., 2015)</ref> is to apply convolution to different sizes of kernels parallelly and then concatenate, which is</p><formula xml:id="formula_86">g * (h 1 * x + h 2 * x + • • • + h n * x) .</formula><p>If we zero pad those kernels to the same size, recalling Property 7, it becomes a convolutional layer, (g (h</p><formula xml:id="formula_87">1 + h 2 + • • • + h n )) * x.</formula><p>Residual connection: A residual connection <ref type="bibr" target="#b14">(He et al., 2016)</ref> proposes f (x) + x, where f (•) is a neural network. If f (•) can be transformed to Volterra convolution, we have</p><formula xml:id="formula_88">f (x) + x = N n=0 H n * x n + x = H 0 + (H 1 * x + δ * x) + N n=2 H n * x n = H 0 + (H 1 + δ) * x + N n=2 H n * x n ,</formula><p>where δ is the Dirac delta and</p><formula xml:id="formula_89">H 1 + δ = H 1 (t) + 1, t = 0 H 1 (t), t = 0 .</formula><p>Pooling: Another family is the pooling layers. They are down sample or up sample operations. Average pooling is convolution with kernel filled by one. Max pooling picks the maximum value in a small region. It is data dependent, the equivalent kernels changes synchronously with input signal, and the proxy kernels are also dynamic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Conversion of Layer Combination</head><p>In the previous subsection, we focus on small structures, and in this section, we combine Volterra convolution layers, showing that the combinations also have the same format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Order-Two-Order-Two Structure</head><p>Before going further, a simple structure is presented in this subsection. The "order-2order-2" structure is stacking two order-two Volterra convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 13</head><p>The "order-2 -order-2" structure can be converted to the form of order-4 Volterra convolution.</p><p>Proof Suppose H i , G j are two groups of kernels, x, y, z are the signals, these two Volterra convolutions are</p><formula xml:id="formula_90">y = 2 i=0 H i * x i , z = 2 j=0 G j * y j .</formula><p>Combining these two convolutions, we have</p><formula xml:id="formula_91">z = 2 j=0 G j * H 0 + H 1 * x 1 + H 2 * x 2 j .</formula><p>The first term is G 0 * y 0 = G 0 . Recall Property 1 and 7, the second term is</p><formula xml:id="formula_92">G 1 * y 1 = H 0 G 1 + G 1 * (H 1 * x) + G 1 * (H 2 * x 2 ) = H 0 G 1 + (G 1 H 1 ) * x + (G 1 H 2 ) * x 2 .</formula><p>Recall Property 4, the last term is</p><formula xml:id="formula_93">G 2 * y 2 = G 2 * H 0 + H 1 * x + H 2 * x 2 2 = G 2 * H 0 , H 0 + G 2 * H 0 , H 1 * x + G 2 * H 1 * x, H 0 + G 2 * H 0 , H 2 * x 2 + G 2 * H 2 * x 2 , H 0 + G 2 * H 1 * x, H 1 * x + G 2 * H 1 * x, H 2 * x 2 + G 2 * H 2 * x 2 , H 1 * x + G 2 * H 2 * x 2 , H 2 * x 2 .</formula><p>And recall Property 7, 9, 12, 13, 14, it becomes</p><formula xml:id="formula_94">G 2 * y 2 = H 2 0 G 2 +   #H 0 G 2 H 0 , H 1   * x +   #H 0 G 2 H 1 , H 0   * x + (G 2 H 1 , H 1 ) * x 2 +   #H 0 G 2 H 0 , H 2   * x 2 +   #H 0 G 2 H 2 , H 0   * x 2 + (G 2 H 2 , H 1 ) * x 3 + (G 2 H 1 , H 2 ) * x 3 + (G 2 H 2 , H 2 ) * x 4 .</formula><p>In summary, we have</p><formula xml:id="formula_95">z = 4 k=0 F k * x k ,</formula><p>where the new kernels are</p><formula xml:id="formula_96">F 0 = G 0 + H 0 G 1 + H 2 0 G 2 F 1 = G 1 H 1 + #H 0 G 2 H 0 , H 1 + #H 0 G 2 H 1 , H 0 F 2 = G 1 H 2 + G 2 H 1 , H 1 + #H 0 G 2 H 0 , H 2 + #H 0 G 2 H 2 , H 0 F 3 = G 2 H 1 , H 2 + G 2 H 2 , H 1 F 4 = G 2 H 2 , H 2 . (<label>25</label></formula><formula xml:id="formula_97">)</formula><p>Notice that it is possible for some i such that F i * x i = 0, but we will also call it the form of order-four Volterra convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Order-n-order-m Structure</head><p>In the following, we generalized "order-2 -order-2" structure (Lemma 13) to "order-norder-m" structure. This structure is to stack order-n Volterra convolution and order-m Volterra convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 14</head><p>The "order-n -order-m" structure with n, m &gt; 0 can be converted to the form of order-nm Volterra convolution.</p><p>Proof Suppose H i , G j are two groups of kernels, x, y, z are signals, these two Volterra convolutions have the form of</p><formula xml:id="formula_98">y = n i=0 H i * x i , z = m j=0 G j * y j .</formula><p>Combining these two Volterra convolutions, we have</p><formula xml:id="formula_99">z = m j=0 G j * n i=0 H i * x i j .</formula><p>Recall Property 6, we have</p><formula xml:id="formula_100">z = m j=0 j j 0 j 1 • • • j n G j * (H 0 * x 0 ) j 0 , (H 1 * x 1 ) j 1 , • • • , (H n * x n ) jn ,</formula><p>where</p><formula xml:id="formula_101">j j 0 j 1 •••jn = j! j 0 !j 1 ! • • • j n !</formula><p>is multinomial coefficient and n k=0 j k = j, j k ≥ 0, for all k = 0, 1, • • • , j.</p><p>Recall Property 10, we have</p><formula xml:id="formula_102">G j * (H 0 * x 0 ) j 0 , (H 1 * x 1 ) j 1 , • • • , (H n * x n ) jn = G j H j 0 0 , H j 1 1 , • • • , H jn n * x 0 , x j 1 , x 2j 2 • • • , x njn = G j H j 0 0 , H j 1 1 , • • • , H jn n * x j 1 +2j 2 +•••+njn .</formula><p>In conclusion, the combination is</p><formula xml:id="formula_103">z = m j=0 j j 0 j 1 • • • j n G j H j 0 0 , H j 1 1 , • • • , H jn n * x j 1 +2j 2 +•••+njn . (<label>26</label></formula><formula xml:id="formula_104">)</formula><p>Clearly, j 1 + 2j 2 + • • • nj n is sequential values from 0 to nm, which implies that this structure can also be converted to the form of order-nm Volterra convolution. This conversion does not convince that all terms are non-zero. It is possible that the nm term is zero, F nm * x nm = 0, where F nm is the proxy kernel. To keep the coherent, we would prefer to call it the form of order-nm Volterra convolution.</p><p>Apart from the order, we also care about the number of terms, G j</p><formula xml:id="formula_105">H j 0 0 , H j 1 1 , • • • , H jn n</formula><p>, that added to a proxy kernel. For a given order o, 0 ≤ p ≤ nm, how many combinations of j</p><formula xml:id="formula_106">0 , j 1 , • • • , j n , n k=0 j k = 0, 1, • • • , m such that j 1 + 2j 2 + • • • + nj n = o?</formula><p>This number can be obtained by counting the terms in Equation <ref type="formula" target="#formula_103">26</ref>. We plot a figure for some combinations of n, m in Figure <ref type="figure">5</ref>. Figure <ref type="figure">5</ref>: The number of terms that added to proxy kernels. The horizontal axis is the order and the vertical axis is the number of terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Multiple Channels and Layers</head><p>In the following, we will show that multichannel and multi-layer structure can also be represented in the form of Volterra convolution. Moreover, we also measure the change of kernel size during this conversion.</p><p>Lemma 15 Multichannel convolution can be represented in the form of Volterra convolution.</p><p>Proof Suppose that a multichannel convolution for signal x and kernel h is</p><formula xml:id="formula_107">y(c, t) = u,τ h(c, u, τ )x(u, t − τ ) = u h(c, u) * x(u).</formula><p>It can be considered as sum of multiple convolutions, which have exactly the same form. Remark 17 With number of stacked layers increasing, the order of converted Volterra convolution increase exponentially.</p><p>Previous lemmas show us the change of orders. The following lemma will measure the change of kernel size. This lemma will be helpful if we want to compute the overall sizes, strides, and paddings of proxy kernels.</p><p>Lemma 18 Suppose that size of kernels are</p><formula xml:id="formula_108">z 1 , z 2 , • • • , z n , strides are s 1 , s 2 , • • • , s n , and paddings are p 1 , p 2 , • • • , p n , size of combined kernel is z 1 +(z 2 −1)s 1 +(z 3 −1)s 1 s 2 +• • •+(z n − 1) n−1 k=1 s k , combined stride is n k=1 s k and combined padding is p 1 + p 2 s 1 + • • • + p n n−1 k=1 s k .</formula><p>Proof Size of convolution is 2 out size = in size + 2 padding − kernel size stride + 1.</p><p>Stacking the first two layers, we have</p><formula xml:id="formula_109">out size = (in size + 2p 1 − z 1 ) /s 1 + 1 + 2p 2 − z 2 s 2 + 1 = in size + 2(p 1 + p 2 s 1 ) − (z 1 + (z 2 − 1)s 1 ) s 1 s 2 + 1.</formula><p>Equivalent size is z 1 + (z 2 − 1)s 1 , stride is s 1 s 2 , and padding is p 1 + p 2 s 1 . Stacking the first three layers, we have</p><formula xml:id="formula_110">out size = in size + 2(p 1 + p 2 s 1 + p 3 s 1 s 2 ) − (z 1 + (z 2 − 1)s 1 + (z 3 − 1)s 1 s 2 ) s 1 s 2 s 3 + 1.</formula><p>Equivalent size is z 1 +(z 2 −1)s 1 +(z 3 −1)s 1 s 2 , stride is s 1 s 2 s 3 , and padding is p 1 +p 2 s 1 +p 3 s 1 s 2 .</p><p>Recursively, this proof is completed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Validation of Lemma 12 and Lemma 13</head><p>In this subsection, we will validate the approximation of two simple structures (truncated version of Lemma 12 and Lemma 13), which are two fundamental structures in our proof and which can show the effectiveness and correctness of our results. The set M is defined for numerical validation in this subsection and below,</p><formula xml:id="formula_111">M = x : x = y y 2 , y(•) ∼ N (0, 1) ,<label>(27)</label></formula><p>where N (0, 1) is standard Gaussian distribution with mean zero and variance one and y 2 is L 2 norm of y. The upper script of these symbols indicates the shape, i.e., H ∈ M 9×9 is a matrix taken from M and the shape is (9, 9).</p><p>Validate Lemma 12: We will check whether the "conv -act -conv" structure can be approximated in the form of Volterra convolution. Take the first four terms into consideration and activation function σ(t) = 1/(1 + e −t ), we have</p><formula xml:id="formula_112">g * σ(h * x) ≈ 1 2 g + 1 4 (g h) * x − (diag(3, g) h 3 ) * x 3 48 + (diag(5, g) h 5 ) * x 5 480 .<label>(28)</label></formula><p>This approximation relies on the Taylor expansion, which is valid only in a small neighbor of zero. Therefore, we need to make sure that h * x is located in such neighbor. We randomly generate x ∈ M 64 , h ∈ M 9 and g ∈ M 5 . Convolution with sigmoid activation is plotted in Figure <ref type="figure">6a</ref> and the approximated order-five Volterra convolution is plotted in Figure <ref type="figure">6b</ref>. </p><formula xml:id="formula_113">(b) 1 2 g + 1 4 (g h) * x + • • • Figure 6: (6a)</formula><p>The output of convolution with activation (left-hand side of Equation <ref type="formula" target="#formula_112">28</ref>); (6b) The output of the approximated Volterra convolution (right-hand side of Equation <ref type="formula" target="#formula_112">28</ref>). The Reconstruct error (L 2 -norm) between (6a) and (6b) is 5.64877e −07 .</p><p>Validate Lemma 13: We will check whether the "order-2 -order-2" structure can be converted to the form of order-four Volterra convolution,</p><formula xml:id="formula_114">2 i=0 G i *   2 j=0 H j * x j   i = 4 k=0 F k * x k ,<label>(29)</label></formula><p>where F k are kernels from Equation <ref type="formula" target="#formula_96">25</ref>.</p><p>We randomly generate H 0 , G 0 ∼ N (0, 1), H 1 , G 1 ∈ M 5 and H 2 , G 2 ∈ M 5×5 , and x ∈ M 64 . The left-hand side of Equation 29 is plotted in Figure <ref type="figure" target="#fig_4">7a</ref> and the right-hand side is plotted in Figure <ref type="figure" target="#fig_4">7b</ref>. It shows that these two are exactly the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Inferring the Proxy Kernels</head><p>All we need to approximate a convolutional neural network to finite term Volterra convolution are the proxy kernels. If the structures and all parameters of a well-trained network is known, i.e., white box, the proxy kernels can be explicitly computed, just as demonstrated in Subsection 3.3. However, in some cases, the structure or parameters are not accessible and only the input output pairs are known, i.e., the network is a black box. This may happen when we want to attack a network using black-box mode. In such cases, the proxy kernels can be approximate by training a hacking network, which will be detailed below.  <ref type="formula" target="#formula_114">29</ref>); (7b) The output of order-four Volterra convolutions (right-hand side of Equation <ref type="formula" target="#formula_114">29</ref>). The Reconstruct error (L 2 -norm) between ( <ref type="formula" target="#formula_15">7a</ref>) and ( <ref type="formula" target="#formula_15">7b</ref>) is 1.79705e −15 .</p><p>Basically, a hacking network is a network that has the structure of finite term Volterra convolution. The number of terms of the hacking network controls the order of Volterra convolution and hence the approximation precision. To train the hacking network, we feed the input-output pairs generated by target network (or its parts) to the hacking network, and minimize the mean square error between the output of the hacking network and that of the target network.</p><p>To validate the effectiveness of hacking network, we build a network and compute its order-zero and order-one proxy kernels manually. Then, we train a hacking network to infer these kernels and compare whether they are the same. To infer the order-zero and order-one terms, we need to build a hacking network with the structure of order-one Volterra convolution, i.e., w * x + b. This structure can be implemented by a single convolutional layer. If the hacking network is well-trained, the order-zero proxy kernel is the bias of the convolutional layer, and the order-one proxy kernel is the weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Two Examples</head><p>As our first example, we consider a simple two-layer network with sigmoid activation. Suppose that a two-layer network is represented as g * σ(h * x). Without loss of generality, we use the same settings as Equation <ref type="formula" target="#formula_112">28</ref>,</p><formula xml:id="formula_115">g * σ(h * x) ≈ 1 2 g + 1 4 (g h) * x − (diag(3, g) h 3 ) * x 3 48 + (diag(5, g) h 5 ) * x 5 480 .</formula><p>It is clear that, the order-zero proxy kernel is 1 2 g, and the order-one proxy kernel is 1 4 (g h).</p><p>We randomly pick two kernels The hacking network is a single layer convolutional network. While training, we randomly generate the input data x ∈ M batch size×1×length and minimize mean square error of the output of the hacking network and the output of g * σ(h * x).</p><p>By straightforward calculation, the order-zero proxy kernel is −4.10236e −01 , and that obtained from the hacking network is −4.10295e −01 , and the inferred order-one proxy kernels respectively obtained by direct calculation and by training the hacking network are compared in Figure <ref type="figure" target="#fig_6">8</ref>. In summary, the hacking network can approximate the true order-zero and order-one proxy kernels very well.  For the second example, we consider a three-layer network with sigmoid activation. Suppose that a three-layer network is represented as f * σ(g * σ(h * x)). Using the same settings as example and approximating this three-layer network into the form of Volterra convolution, we have</p><formula xml:id="formula_116">f * σ(g * σ(h * x)) = 1 2 f + 1 4 f * [g * σ(h * x)] − 1 48 f * [g * σ(h * x)] 3 + 1 480 f * [g * σ(h * x)] 5 = 1 2 f + 1 4 f * (g * σ(h * x)) − 1 48 diag(3, f ) * (g * σ(h * x)) 3 + 1 480 diag(5, f ) * (g * σ(h * x)) 5 . (<label>Property 15)</label></formula><p>Substituting Equation <ref type="formula" target="#formula_112">28</ref>and reordering the terms, we obtain the order-zero proxy kernel as</p><formula xml:id="formula_117">f 1 2 + 1 4 1 2 g − 1 48 1 2 g 3 + 1 480 1 2 g 5 ,</formula><p>and the order-one proxy kernel as 1 4</p><formula xml:id="formula_118">1 4 − 3 48 1 2 g 2 + 5 480 1 2 g 4 (f g h) .<label>(30)</label></formula><p>We randomly pick three kernels The training process of the hacking network is the same as the previous example. The order-zero proxy kernel by direct calculation is −1.83091e −02 , and that by training the hacking network is −1.82950e −02 , and the order-one proxy kernels obtained by these two ways shown in Figure <ref type="figure" target="#fig_8">9</ref>. The figure shows again that the proxy kernels obtained training the hacking network is almost the same as that obtained by direct calculation.  These two examples show that the order-zero and order-one proxy kernels inferred by direct calculation and by training the hacking network are very close to each other. To cover a broader selection of parameters, we analyze the statistics of the reconstruction errors of the proxy kernels inferred from these two methods. The results are illustrated in Figure <ref type="figure" target="#fig_9">10</ref>. It shows that the choice of parameters has a less effect on the reconstruction error and these two methods are comparable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">An Application of the Order-one Proxy Kernels</head><p>Why do we need to infer the proxy kernels?</p><p>We think the proxy kernels shall contain some useful information about the original network. It is possible to carefully design special inputs according to the inferred proxy kernels to change the behavior of the original network. Below we provide a toy example to illustrate this interesting application of order-one proxy kernels. First, we build a hacking network to approximate the order-one proxy kernel of a classifier network trained on the MNIST dataset <ref type="bibr" target="#b20">(Lecun et al., 1998)</ref>. Then, we add visually imperceptible perturbation to the inputs to cheat the classifier to give wrong labels.</p><p>The structure of the classifier network is shown in the left of Table <ref type="table" target="#tab_6">1</ref>. Training images are all scaled to [0, 1]. To obtain a network that is robust to noise, uniform noise U(0, 0.2) and Gaussian noise N (0, 0.2) are respectively added to training images with a probability of 5%. After 512 training episodes, the network achieved 98.280% accuracy on test set.</p><p>The hacking network is illustrated in the right of Table <ref type="table" target="#tab_6">1</ref>. We only approximate the first six layers of the classifier network. This is because approximating the entire classifier network to a linear network will result in high fitting errors, and these errors will make it harder for hacking network to converge. From Table <ref type="table" target="#tab_6">1</ref>, the hacking network has ten output channels. Hence, we ten proxy kernels h 1 , • • • , h 10 , corresponding to the ten output channels.  The training of the hacking network is similar to the procedure discussed in the previous subsection. The training images and their outputs of the classifier network form input-label pairs, and the hacking network is trained with these pairs until convergence. Compared to the outputs of the first six layers of the classifier network, the mean square error of the hacking network is about 3.19016e −02 . In other words, these two networks have the similar behavior.</p><p>Suppose that some neurons in the network are suppressed when feeding input image x. To change the behavior of this network, we need to activate some extra neurons. Specifically, in this application, we try to increase the energy of these extra neurons to activate them.</p><p>One important advantage of the hacking network is its simple structure: only convolutions are involved. Hence, to largely change the outputs, for given order-one proxy kernel h and input signal x, we search for perturbation such that</p><formula xml:id="formula_119">argmax h * (x + ) , s.t. 2 ≤ c, (<label>31</label></formula><formula xml:id="formula_120">)</formula><p>where c is a constant. If h and x + are close to each other in the frequency domain, the energy h * (x + ) 2 should be large. Recall Parseval's Theorem and Convolution Theorem, convolution in time domain equals multiplication in frequency domain and the energy is preserved, can be obtained via Fourier transform</p><formula xml:id="formula_121">F(•), F( ) ∝ F(h) − F(x). (<label>32</label></formula><formula xml:id="formula_122">)</formula><p>Applying the bounded condition, we scale it by α and take inverse Fourier transform</p><formula xml:id="formula_123">F −1 (•), = αF −1 (F(h) − F(x)) . (<label>33</label></formula><formula xml:id="formula_124">)</formula><p>Recall Equation <ref type="formula" target="#formula_123">33</ref>, we can compute a perturbation as</p><formula xml:id="formula_125">i = F −1 (F(w i ) − F(x)) .</formula><p>Since each pixel of the input image is in the range of [0, 1], the perturbation should be adjusted to the same range. In this application, to make the perturbation hard to be seen, i is scaled by a factor α. The value of α is 0.55 in the experiment below. In short, the perturbation i is normalized as</p><formula xml:id="formula_126">i ← α i − min( i ) max( i ) − min( i )</formula><p>.</p><p>Besides the ten proxy kernels estimated by the hacking network, we also append two fake kernels to check whether the fake kernels can behave similarly. The fake kernels are h 11 ∼ U(0, 1) and h 12 ∼ N (0, 1). We randomly choose twelve images from the test set, x i , i = 1, 2, • • • , 12, and feed both image x i and x i + i into classifier network, where 1 , 2 , • • • , 10 are computed from the approximated order-one proxy kernels, and 11 , 12 are computed from two fake kernels. Results are illustrated in Figure <ref type="figure">11</ref>. The images with patches computed from the approximated order-one proxy kernels are more likely to change the output than the images with patches computed from fake kernels.</p><p>This phenomenon is similar to the adversarial example <ref type="bibr" target="#b28">(Szegedy et al., 2013;</ref><ref type="bibr" target="#b13">Goodfellow et al., 2014)</ref>, adding the human invisible perturbation to an image can mislead the network. Nevertheless, these two methods are different. They iteratively compute perturbations that mislead the network with or without specific objects. Our method is to maximize the energy of order-one convolution, and this increases the probability to change output of the network. More details about the perturbations are presented in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>This paper presents a new perspective on the analysis of convolutional neural networks. It shows that a convolutional neural network can be represented in the form of Volterra convolution. By approximating infinite Volterra convolution using the finite one, we can analyze the proxy kernels of Volterra convolution rather than directly analyze the original network, even if the structure and the parameters of the network are unknown. The proxy kernels can be computed from the weights of a neural network, or be approximated building and training a hacking network. The method of approximating the proxy kernels is more intuitive and easier to implement if the number of layers is large, and the approximated proxy kernels are comparable with the actual kernels.</p><p>In the future work, we plan to do further research on the proxy kernels, i.e., how to compute the high order kernels efficiently, how to determine the order of kernels with given approximation accuracy, etc. We will also investigate the kernels that are input related, i.e., the dynamic networks, as well as practical applications of the hacking network.</p><p>All codes associated with this article are accessible publicly at GitHub https://github. com/tenghuilee/nnvolterra.git.</p><p>Property 4, 5 and 6: We will prove Property 6. Noticing that G is symmetric, swapping order of x i does not change the result.</p><formula xml:id="formula_127">(G * (x 1 + x 2 + • • • + x m ) n ) (t) = τ 1 ,••• ,τn G(τ 1 , • • • , τ n ) n i=1 (x 1 (t − τ i ) + x 2 (t − τ i ) + • • • + x m (t − τ i )) = 1 ,••• ,τn n n 1 n 2 • • • n m G(τ 1 , • • • , τ n ) x 1 (t − τ 1 )x 1 (t − τ 2 ) • • • x 1 (t − τ n 1 ) x 2 (t − τ 1+n 1 )x 2 (t − τ 2+n 1 ) • • • x 2 (t − τ n 2 +n 1 ) • • • x m (t − τ 1+ m−1 k=1 n k )x m (t − τ 2+ m−1 k=1 n k ) • • • x m (t − τ nm+ m−1 k=1 n k ) = n n 1 n 2 • • • n m G * x n 1 1 , x n 2 2 , • • • , x nm m (t),</formula><p>where</p><formula xml:id="formula_128">n n 1 n 2 •••nm = n! n 1 !n 2 ! • • • n m !</formula><p>, the multinomial coefficient, and</p><formula xml:id="formula_129">m i=1 n i = m, n i ≥ 0, for all i = 1, 2, • • • , m. If m = 2,</formula><p>this is Property 5, and if m = 2 and n = 2, this is Property 4.</p><p>Property 7 and 8: Only Property 8 is proved, and we can prove Property 7 by setting stride equal to one.</p><formula xml:id="formula_130">(G * s (H * z x)) (t) = l G(l) τ H( τ ) n i=1 x i (z(st − l) − τ i ) = τ l G(l)H( τ − zl) n i=1 x i (szt − τ i ) = τ (G z H)( τ ) n i=1 x i (szt − τ i ) = ((G z H) * sz x) (t).</formula><p>Property 9 and 10: We will prove Property 9 here and this proof could be intuitively extended to Property 10.</p><formula xml:id="formula_131">(G * H 1 * x, H 2 * y ) (t) = l 1 ,l 2 G(l 1 , l 2 )   τ 1 H 1 ( τ 1 ) n i=1 x i (t − τ 1,i − l 1 )     τ 2 H 2 ( τ 2 ) m j=1 y j (t − τ 2,j − l 2 )   = τ 1 , τ 2   l 1 ,l 2 G(l 1 , l 2 )H 1 ( τ 1 − l 1 )H 2 ( τ 2 − l 2 )   n i=1 x i (t − τ 1,i )   m j=1 y j (t − τ 2,j )   = τ 1 , τ 2 (G H 1 , H 2 )( τ 1 , τ 2 ) n i=1 x i (t − τ 1,i )   m j=1 y j (t − τ 2,j )   = ((G H 1 , H 2 ) * x, y ) (t).</formula><p>Property 11:</p><formula xml:id="formula_132">(G 1 (G 2 G 3 )) ( τ ) = k G 1 (k) l G 2 (l)G 3 ( τ − k − l) = l k G 1 (k)G 2 (l − k) G 3 ( τ − l) = ((G 1 G 2 ) G 3 ) ( τ ).</formula><p>Property 12, 13, and Property 14: We will only prove Property 14 here, and we can prove Property 12 and 13 in the same way.</p><formula xml:id="formula_133">(G * H 1 * x, α, H 2 * y ) (t) = α l 1 ,l 2 ,l 3 G(l 1 , l 2 , l 3 )   τ 1 H 1 ( τ 1 ) i=1 x i (t − τ 1,i − l 1 )     τ 3 H 2 ( τ 3 ) i=1 y i (t − τ 3,i − l 3 )   = α τ 1 , τ 2   l 1 ,l 2 ,l 3 G(l 1 , l 2 , l 3 )H 1 ( τ 1 − l 1 )H 2 ( τ 3 − l 3 )   i=1 x i − τ 1,i − l 1 ) i=1 y i (t − τ 3,i − l 3 ) =     #α (G H 1 , α, H 2 )   * x, y   (t).</formula><p>Property 15:</p><formula xml:id="formula_134">(h * [x] n ) (t) = l h(l) (x(t − l)) n = τ 1 ,τ 2 ,••• ,τn h(l) n i=1 (δ(τ i − l)x(t − τ i )) = τ 1 ,τ 2 ,••• ,τn (diag(n, h)) (τ 1 , τ 2 , • • • , τ n ) n i=1 x(t − τ i ) = (diag(n, h) * x n ) (t).</formula><p>Property 16:</p><formula xml:id="formula_135">g * [h * x] n = diag(n, g) * (h * x) n = (diag(n, g) h n ) * x n .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Perturbations in Volterra Convolution</head><p>During practice, it has been found that some specific perturbations have the potential to make neural networks behave abnormally <ref type="bibr" target="#b28">(Szegedy et al., 2013;</ref><ref type="bibr" target="#b13">Goodfellow et al., 2014)</ref>. An observation of how perturbations influence the proxy kernels will be theoretically explored in this appendix. The neural network is extremely complex, our analysis limits on its impacts on the proxy kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Perturbation Upper Bound</head><p>In this subsection, we express the upper bound for perturbation of ideal Volterra convolution, showing that how the perturbation change the output.</p><p>Theorem 19 Assume input signal is x, and the perturbation is , the approximated neural network is</p><formula xml:id="formula_136">f (x) = N n=0 H n * x n , we have f (x + ) − f (x) 2 ≤ min        N n=0 H n 2 n−1 k=0 en k k x k 1 n−k 1 , N n=0 H n 1 n−1 k=0 en k k x k 2k n−k 2(n−k)        , (<label>34</label></formula><formula xml:id="formula_137">)</formula><p>where e = 2.718281828 • • •, the base of the natural logarithm.</p><p>Proof See appendix C.3.</p><p>The following is an example of effect of perturbations on each convolution from order one to eight. We randomly generate x ∈ M 32 (M is defined in Equation <ref type="formula" target="#formula_111">27</ref>). Two perturbations are produced to simulate those two different cases. To make the plot more clear, we add a considerably large number 3.0 or a considerably small value 0.5 to the middle point of x. For each perturbation and each convolution with order form one to eight, we compute</p><formula xml:id="formula_138">H n * (x + ) n − H n * x n 2 one thousand times with random H n ∈ M 5×•••×5 .</formula><p>The boxplot is shown in Figure <ref type="figure" target="#fig_11">12</ref>. </p><formula xml:id="formula_139">H n * (x + ) n − H n * x n 2 , n = 1, 2, • • • , 8.</formula><p>The horizontal axis indicates the order and the vertical axis indicates the error.</p><p>Figure <ref type="figure" target="#fig_11">12</ref> shows that both upper bound and lower bound is reasonable. It seems that the impact of perturbation will cause exponential blowup or decay. Nevertheless, this result is obtained under the context of ideal Volterra convolution. In reality, the situation will be more complicated, we need to consider the kernels, input signals, and so on.</p><p>If the impact decays, this means that the related network are robust to such perturbation, which is exactly what we want.</p><p>As for the exponential blowup, we need to consider the network before the approximation. If the network is Lipschitz in some domains, the approximated Volterra convolution must be Lipschitz in the same domains. The upper bound of change of output is bounded by the Lipschitz constant, and the bound is not exponential. Otherwise, if the network is non-Lipschitz, but the output is bounded, this impact is also bounded. If the output of the network is not bounded, something wrong must have happened to this network. This is because all operations in a neural network is bounded, i.e., the convolutions are bounded if both kernel and signal are bounded, and the normalization operations are also bounded if not divide by zero. In conclusion, the impact of perturbation will increase but won't exponential blowup in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Perturbation on Order-n Volterra Convolution</head><p>The following is an example of how perturbations affect the order-n Volterra convolution. Due to the computation complexity, n is set to be eight, <ref type="formula" target="#formula_111">27</ref>). Input signal is selected as a sin function x = sin(t), 0 ≤ t ≤ 8π. Pick thirty points from x and plus 0.2 as perturbation x + . The perturbations are denser in the head and sparser in the tail. Result is illustrated in Figure <ref type="figure" target="#fig_12">13</ref>. This result is influenced by both network parameters and perturbations. Different parameters will cause different results. Some are sensitive to perturbations while others are not.</p><formula xml:id="formula_140">f (x) = 8 i=1 H i * x i , H 0 = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kernels are randomly generated as</head><formula xml:id="formula_141">H 1 ∈ M 5 , H 2 ∈ M 5×5 , • • • H 8 ∈ M 5×•••×5 (M is defined in Equation</formula><p>As can be seen in Figure <ref type="figure" target="#fig_12">13</ref>, points in the head are more influenced by perturbations, while points in the tail are not. This is because perturbations are denser in the head than in the tail, and short time energy is larger than that in the tail. It can also be seen that some special points, such as peaks or valleys, are more affected by perturbation, while other points are less affected. Not all perturbations are effective. The effective perturbations are both determined by both kernels and input signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Proof for Theorem 19</head><p>In the following, we will prove Theorem 19. For proving this Lemma, we will first introduce Young's Inequality (Theorem 20), Corollary 21, Lemma 22 and Lemma 23.</p><p>Theorem 20 (Young's Inequality for Convolutions <ref type="bibr" target="#b15">(Henry, 1912)</ref>) Let p, q, r ∈ R, p, r ≥ 1 and 1 + 1 r = 1 p + 1 q . For signals h and x, following inequality is satisfied:</p><formula xml:id="formula_142">h * x r ≤ h p x q . (<label>35</label></formula><formula xml:id="formula_143">)</formula><p>Corollary 21 Let r = p = 2, q = 1. Theorem 20 is simplified as</p><formula xml:id="formula_144">h * x 2 ≤ min ( h 2 x 1 , h 1 x 2 ) . (<label>36</label></formula><formula xml:id="formula_145">)</formula><p>Lemma 22 Suppose that H n is the kernel, and x, y are two signals. We have</p><formula xml:id="formula_146">H n * x k , y n−k 2 ≤ min H n 2 x n 1 y n−k 1 , H n 1 x k 2k y n−k 2(n−k) , k = 0, 1, • • • , n.</formula><p>Proof By definition of order-n convolution, we have</p><formula xml:id="formula_147">H n * x k , y n−k (t) = τ 1 ,••• ,τn H n (τ 1 , τ 2 , • • • , τ n ) k i=1 x(t − τ i ) n i=k+1 y(t − τ i ) (37) Let Z(t 1 , t 2 , • • • , t n ) = k i=1 x(t i ) n i=k+1 y(t i ).</formula><p>Equation 37 becomes</p><formula xml:id="formula_148">H n * x k , y n−k (t) = τ 1 ,••• ,τn H n (τ 1 , τ 2 , • • • , τ n )Z(t − τ 1 , t − τ 2 , • • • , t − τ n ) = (H n * Z) (t, t, • • • , t).</formula><p>Recall Young's Inequality (Theorem 20):</p><formula xml:id="formula_149">H n * Z 2 ≤ min ( H n 2 Z 1 , H n 1 Z 2 ) ,</formula><p>where</p><formula xml:id="formula_150">Z 1 is Z 1 = t |Z(t 1 , t 2 , • • • , t n )| = t k i=1 x(t i ) n i=k+1 y(t i ) = k i=1 t 1 ••• |x(t i )|   n i=k+1 t k+1 ••• |y(t i )|   = x k 1 y n−k 1 ,</formula><p>and Z 2 is</p><formula xml:id="formula_151">Z 2 = t 1 ,t 2 ,••• ,tn (Z(t 1 , t 2 , • • • , t n )) 2 = t 1 ,t 2 ,••• ,tn k i=1 x(t i ) n i=k+1 y(t i ) 2 = t 1 ••• x 2k (t i )   t k+1 ••• y 2(n−k) (t i )   = x k 2k y n−k 2(n−k) .</formula><p>This proof is finished.</p><p>Lemma 23 Suppose that H n is the kernel, and x, y are two signals. We have</p><formula xml:id="formula_152">H n * (x + y) n − H n * x n 2 ≤ min        H n 2 n−1 k=0 en k k x k 1 y n−k 1 , H n 1 n−1 k=0 en k k x k 2k y n−k 2(n−k)        ,<label>(38)</label></formula><p>where k = 0, 1, • • • , n., and e = 2.718281828 • • •, the base of the natural logarithm.</p><p>Proof Recall Property 5,</p><formula xml:id="formula_153">H n * (x + y) n = n k=0 n k H n * x k , y n−k . (<label>39</label></formula><formula xml:id="formula_154">)</formula><p>We move H n * x n to the left and take the L 2 -norm. By Lemma 22, the L 2 -norm is</p><formula xml:id="formula_155">H n * (x + y) n − H n * x n 2 = n−1 k=0 n k H n * x k , y n−k 2 ≤ n−1 k=0 n k H n * x k , y n−k 2 ≤ min        H n 2 n−1 k=0 n k x k 1 y n−k 1 , H n 1 n−1 k=0 n k x k 2k y n−k 2(n−k)        ≤ min        H n 2 n−1 k=0 en k k x k 1 y n−k 1 , H n 1 n−1 k=0 en k k x k 2k y n−k 2(n−k)       </formula><p>, where binomial coefficient n k ≤ en k k (Exercise 0.0.5 of <ref type="bibr" target="#b30">(Vershynin, 2018)</ref>).</p><p>The following is the proof of Theorem 19.</p><formula xml:id="formula_156">Proof [Proof of Theorem 19] f (x + ) − f (x) 2 = N n=0 (H n * (x + ) n − H n * x n ) 2 ≤ N n=0 (H n * (x + ) n − H n * x n ) 2 (apply Lemma 23) ≤ min        N n=0 H n 2 n−1 k=0 en k k x k 1 n−k 1 , N n=0 H n 1 n−1 k=0 en k k x k 2k n−k 2(n−k)        .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Rank for Outer Convolution</head><p>In this appendix we will discuss rank of output from outer convolution and n-dimensional convolution in discrete time. The rank of a matrix equal to the number of non-zero eigenvalues. It is of major importance, which tells us linear dependencies of column or vectors, and is one of the fundamental building block of matrix completion and compress sensing <ref type="bibr" target="#b5">(Candès et al., 2006;</ref><ref type="bibr" target="#b7">Donoho, 2006;</ref><ref type="bibr" target="#b27">Sidiropoulos et al., 2017)</ref>. The rank of a tensor could be defined in various ways, such as the rank based on CANDECOMP/PARAFACT decomposition and Tucker decomposition and others <ref type="bibr" target="#b19">(Kolda and Bader, 2009)</ref>. Before discussing the rank, let's first take a look at linear dependence and independence <ref type="bibr" target="#b16">(Horn and Johnson, 1985)</ref></p><formula xml:id="formula_157">. Vectors v 1 , v 2 , • • • , v n are linear independence if α 1 v 1 + α 2 v 2 + • • • + α n v n = 0 only implies all scalars are zero α 1 = α 2 = • • • = 0. Similar to that, we call G 1 , G 2 , • • • , G n are linear independence if α 1 G 1 + α 2 G 2 + • • • + α n G n = 0 only implies α 1 = α 2 = • • • = α n = 0.</formula><p>If k vectors span a space of dimension r, any of these k vectors can be represented as a linear combination of r linear independence vectors in that space.</p><p>To express the rank of matrix, Tucker rank <ref type="bibr" target="#b27">(Sidiropoulos et al., 2017)</ref> of tensor, and others, we denote</p><formula xml:id="formula_158">rank(k, G) =        1, if G is 1 dimension column rank of G, if G is 2 dimension, k = 1 row rank of G, if G is 2 dimension, k = 2 k-th Tucker rank of G, others .<label>(40)</label></formula><p>Especially, if G is a matrix or one-dimensional vector, this notation can be rewritten as rank(G). Besides, a tensor G is non-zero means that its arbitrary Lp-norm G p = 0, p ≥ 1. Before talking about the rank, we would like to introduce the zero result of convolutions. By the definition of one-dimensional convolution, we can rewrite it into matrix multiplication format,</p><formula xml:id="formula_159">y(t) = T τ =0 g(τ )h(t − τ ) ⇔     . . . y(t) . . .     =     . . . . . . . . . h(t − 0) h(t − 1) • • • h(t − T ) . . . . . . . . .          g(0) g(1) . . . g(T )     </formula><p>, where matrix generated by shifting h(•) is the Hankel matrix. It is clear that the convolution g * h is zero if g is in the null space <ref type="bibr" target="#b16">(Horn and Johnson, 1985)</ref> of the Hankel matrix. For example, the zero padding one-dimensional convolution is zero if kernel and signal have the following format</p><formula xml:id="formula_160">g = 1 1 1 1 1 ; h = 1 −1 0 −1 1 1 −1 0 −1 • • • .</formula><p>The high-dimensional convolution also follows the same rule. We can rewrite this kind of convolutions into matrix multiplication format by flattening G and the patches of</p><formula xml:id="formula_161">H. A patch of H is H( t − τ 1 , τ 2 , • • • , τ k ), where τ i = 0, 1, • • • , N i with i = 1, 2, • • • , k and G ∈ R N 1 ×N 2 ×•••×N k .</formula><p>For instance, a two-dimensional convolution can be rewritten into the following format,</p><formula xml:id="formula_162">    . . . . . . . . . . . . H(t 1 − 0, t 2 − 0) H(t 1 − 0, t 2 − 1) H(t 1 − 1, t 2 − 0) H(t 1 − 1, t 2 − 1) . . . . . . . . . . . .         G(0, 0) G(0, 1) G(1, 0) G(1, 1)     .</formula><p>If the flatted G is in the null space of the matrix generated by flattening patches of H, the result of this high-dimensional convolution is zero.</p><p>By the definition of outer convolution (Definition 12), we have</p><formula xml:id="formula_163">(G h 1 , h 2 )(t 1 , t 2 ) = τ 1 ,τ 2 G(τ 1 , τ 2 )h 1 (t 1 − τ 1 )h 2 (t 2 − τ 2 ) = rank(G) r=1 τ 1 p r (τ 1 )h 1 (t 1 − τ 1 ) τ 2 q r (τ 2 )h 2 (t 2 − τ 2 ) . = rank(G) r=1 (p r * h 1 )(t 1 )(q r * h 2 )(t 2 ),</formula><p>which is a summation of the outer product of vectors p r * h 1 and q r * h 2 . Recall Lemma 24, we have </p><formula xml:id="formula_164">rank(G h 1 , h 2 ) ≤ min (min(rank(G), rank(H 1 )), min(rank(G), rank(H 2 ))) ≤ min (rank(G), rank(H 1 ), rank(H 2 )) ≤ rank(G). If H 1 is full column rank, there is no such vector v ∈ R n 1 such that v *</formula><formula xml:id="formula_165">(G h 1 , h 2 ) = rank(G). Let H(t 1 , t 2 ) = h 1 (t 1 )h 2 (t 2 ). It becomes a two-dimensional convolution, (G h 1 , h 2 ) (t 1 , t 2 ) = τ 1 ,τ 2 G(τ 1 , τ 2 )h 1 (t 1 − τ 1 )h 2 (t 2 − τ 2 ) = τ 1 ,τ 2 G(τ 1 , τ 2 )H(t 1 − τ 1 , t 2 − τ 2 ).</formula><p>More generally, we extend this to the H of rank grater than or equal to one. </p><formula xml:id="formula_166">G * H = G *   rank(H) r=1 H r   = rank(H) r=1 G * H r . By Lemma 25, the rank of this convolution is rank (G * H) = rank   rank(H) r=1 G * H r   ≤ rank(H) r=1 rank(G) ≤ rank(H)rank(G).</formula><p>In addition, the rank of a matrix must be bounded by its size, implying that rank(G * H) ≤ min(s 1 , s 2 , rank(G)rank(H)).</p><p>Remark 27 The two-dimensional convolution often increase the rank of a two-dimensional image.</p><p>Remark 28 Dilated convolutions <ref type="bibr" target="#b34">(Yu and Koltun, 2016)</ref> still follow Lemma 26, since adding zero-filled rows or columns will not alter the rank. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Tucker Rank for</head><formula xml:id="formula_167">Multi-dimensional Signals Lemma 29 rank(k, G h) ≤ rank(k, G), where k = 1, 2, • • • , n, and h = h 1 , h 2 , • • • , h n is a list of non-zero one-dimensional</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof</head><p>If n = 2, this has been proved in Lemma 25.</p><formula xml:id="formula_168">If n &gt; 2, suppose that G ∈ R s 1 ×s 2 ×•••×sn . Focusing on the k-th dimension, we have (G h)( t) = τ G( τ ) n i=1 h i (t i − τ i ) = τ k   τ 1 ,••• ,τ k−1 τ k+1 ,••• ,τn G( τ ) n i=1,i =k h i (t i − τ i )   h k (t k − τ k ). Let P (t 1 , • • • , t k−1 , τ k , t k+1 , • • • , t n ) = τ 1 ,••• ,τ k−1 τ k+1 ,••• ,τn G( τ ) n i=1,i =k h i (t i − τ i ).</formula><p>The convolution becomes</p><formula xml:id="formula_169">(G h)(• • • , t k , • • • ) = τ k P (• • • , τ k , • • • )h k (t k − τ k ).</formula><p>We permute and reshape P into matrix format P ∈ R s k ×• , and rewrite the convolution into matrix multiplication format,</p><formula xml:id="formula_170">    . . . . . . . . . h k (t k − 0) h k (t k − 1) • • • h k (t k − s k ) . . . . . . . . .          P (0, 1) P (0, 2) • • • P (1, 1) P (1, 2) • • • . . . . . . P (s k , 1) P (s k , 2) • • •      ≡ H k P.</formula><p>By the rank inequality of matrix multiplication, we have rank(k, G h) ≤ min(rank(H k ), rank( P)) </p><formula xml:id="formula_171">≤ min(rank(H k ), rank(k, G)) (Lemma 24) ≤ rank(k, G). Let H(t 1 , t 2 , • • • , t n ) = h 1 (t 1 )h 2 (t 2 ) • • • h n (t n ). A tensor with Tucker rank equals [1, 1, • • • , 1]. We have G h = G * H,</formula><formula xml:id="formula_172">G * H = G *   rank(k,H) r=1 H r   = rank(k,H) r=1 G * H r .</formula><p>By Lemma 30, the k-th Tucker rank of this convolution is</p><formula xml:id="formula_173">rank (k, G * H) = rank   k, rank(k,H) r=1 G * H r   ≤ rank(k,H) r=1 rank (k, G) ≤ rank(k, H)rank(k, G).</formula><p>In addition, the k-th Tucker rank is bounded by the size, which implies that</p><formula xml:id="formula_174">rank(k, G * H) ≤ min(s k , rank(k, G)rank(k, H)).</formula><p>This operation can be considered as a collection of multidimensional convolutions in group k, where all kernels are super-diagonal tensors. The Tucker rank of these super-diagonal tensors are rank k, i , P ≤ z k , with equality if and only if all diagonal entries are non-zero. Recall Lemma 30, we conclude that Tucker rank of this group is</p><formula xml:id="formula_175">rank k, i , G H ≤ min(s k,i , rank k, i , P rank(i, H k )) ≤ min(s k,i , z k rank(i, H k )).</formula><p>In conclusion, we completed this proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Validation of the Rank Properties</head><p>Data description. To cover more situations, the following tensors are taken from three sets, M, U and O.</p><p>The first set is the Gaussian distribution with normalization,</p><formula xml:id="formula_176">M = x : x = y y 2 , y(•) ∼ N (0, 1) ,</formula><p>where N (0, 1) is standard Gaussian distribution with mean zero and variance one. This is the same as Equation <ref type="formula" target="#formula_111">27</ref>.</p><p>The second set is the Uniform distribution with normalization,</p><formula xml:id="formula_177">U = x : x = y y 2 , y(•) ∼ U(0, 1) ,</formula><p>where U(0, 1) is the Uniform distribution form zero to one. The third set is O = {h : ∃g, g * h is zero, g and h are non-zero} .</p><p>One method for generating the desired signal h with a known kernel g is illustrated as follows. For any fixed index t, if (g * h)(t) = 0, we have</p><formula xml:id="formula_178">T τ =0 g(τ )h(t − τ ) = g(0)h(t) + T τ =1 g(τ )h(t − τ ) = 0.</formula><p>If the weighted sum T τ =1 g(τ )h(t − τ ) is known, and g(0) is non-zero, h(t) can be uniquely determined by</p><formula xml:id="formula_179">h(t) = −1 g(0) T τ =1 g(τ )h(t − τ ). (<label>41</label></formula><formula xml:id="formula_180">)</formula><p>We can iteratively compute the upcoming sequence h(T ), h(T + 1), • • •, if all initial terms, h(0), h(1), • • • , h(T − 1), are manually specified. It is clear that we cannot pad any other elements to the head or tail of h, because the new elements may not follow Equation <ref type="formula" target="#formula_179">41</ref>, and padding these elements may leave g * h non-zero. Validation process. The main object is to compare the numerical rank with the theoretical rank. To begin with, we compute the convolutions of the generated kernels and signals. After that, we clip the singular values of the convolution output to range [1e −16 , 1e 16 ]. At last, we plot the singular values to a figure with y-axis scaled by log 10 . We can easily distinguish the zero singular values from the non-zero ones in the figures. If there is a sharp slope, the singular values in the left of this slop are non-zero, and the number of the non-zero singular values equals the rank.</p><p>Validate Lemma 25: rank(G h 1 , h 2 ) ≤ min (rank(G), rank(H 1 ), rank(H 2 )). G is randomly generated with rank(G) = r g . h 1 is iteratively computed by T 1 random initial terms and kernel [1, 1, • • • , 1]. h 2 is computed in the same way. Since zero padding of h 1 or h 2 is not allowed here, the outer convolution G h 1 , h 2 is zero padded. The singular values are plotted in Figure <ref type="figure" target="#fig_16">14</ref>. As shown in figure, rank(G h 1 , h 2 ) is less than the smallest number of r g , T 1 , T 2 , and r g = rank(G), rank(H 1 ) ≤ T 1 , rank(H 2 ) ≤ T 2 .  Validate Lemma 30. Regardless of size, the Tucker rank of an n-dimensional convolution is rank(k, G * H) ≤ rank(k, G)rank(k, H). This inequality of three-dimensional convolution is validated below. G is randomly generated with Tucker rank equals [2, 4, 3] and H is randomly generated with Tucker rank equals <ref type="bibr">[3,</ref><ref type="bibr">2,</ref><ref type="bibr">4]</ref>. The k-th Tucker rank of a tensor equals to the matrix rank of mode-k matricization of that tensor, where the mode-k matricization is to permute and reshape the tensor with shape (• • • , s, • • • ) to (• • • , s) <ref type="bibr" target="#b19">(Kolda and Bader, 2009)</ref>. The singular values of mode-(one, two, three) matricization of G * H are plotted in Figure <ref type="figure" target="#fig_18">16</ref>. The k-th Tucker rank of G * H is less than or equal to the multiplication of the k-th Tucker rank of G and H. ,</p><p>where H = H 1 , H 2 , • • • , H n is a list of tensors, and G ∈ R z 1 ×z 2 ×•••×zn . This inequality of outer convolution G h 1 , h 2 , H 3 , H 4 is validated below. Four-dimensional tensor G is randomly generated with Tucker rank equals <ref type="bibr">[2,</ref><ref type="bibr">3,</ref><ref type="bibr">3,</ref><ref type="bibr">2]</ref>. h 1 and h 2 are one-dimensional vectors, H 3 is a two-dimensional matrix with rank(H 3 ) = 2, and H 4 is a three-dimensional tensor with Tucker rank equals <ref type="bibr">[2,</ref><ref type="bibr">3,</ref><ref type="bibr">4]</ref>. The singular values of mode-k matricization of G h 1 , h 2 , H 3 , H 4 is plotted in Figure <ref type="figure" target="#fig_19">17</ref>. h 1 and h 2 are one-dimensional, the first and second Tucker rank of G h 1 , h 2 , H 3 , H 4 is less than or equal to that of G. H 3 and H 4 are not one-dimensional, the corresponding Tucker rank of G h 1 , h 2 , H 3 , H 4 is not grater than z k times the Tucker rank of H 3 and H 4 . </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A brief preview of the diag(•) operator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of outer Convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: (7a) The output of stacking two order-two Volterra convolutions (left-hand side of Equation29); (7b) The output of order-four Volterra convolutions (right-hand side of Equation29). The Reconstruct error (L 2 -norm) between (7a) and (7b) is 1.79705e −15 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The inferred order-one proxy kernels from the two-layer network. The reconstruction error (L 2 -norm) between left and right is 3.04194e −04 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>kernel from the trained hacking network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The inferred order-one proxy kernels of the three-layer network. The reconstruction error (L 2 -norm) between left and right is 1.94667e −03 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The boxplot of reconstruction errors between two methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Boxplot of H n * (x + ) n − H n * x n 2 , n = 1, 2, • • • , 8.The horizontal axis indicates the order and the vertical axis indicates the error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Example for perturbation of order-eight Volterra convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>signals, and G is an n-dimensional tensor, and rank(k, G) is the k-th Tucker rank of G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>.</head><label></label><figDesc>According to the computing process, the columns in the right of vertical line are linear combinations of the columns in the left. It implies that rank(H) ≤ T , with equality if the columns in the left of vertical line are linear independence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>G ∈ U 9×9 , h1, h2 ∈ O 27</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: The singular values of G h 1 , h 2 with no padding. The min operator with three numbers is min(r g , T 1 , T 2 ). Validate Lemma 26. Regardless of size, the rank of a two-dimensional convolution G * H is rank(G * H) ≤ rank(G)rank(H). Pairs of matrices G and H are randomly generated with rank(G) = r g and rank(H) = r h . The singular values of G * H are plotted in Figure 15. As the figure shows, rank(G * H) is less than or equal to the multiplication of rank(G) and rank(H).Validate Lemma 30. Regardless of size, the Tucker rank of an n-dimensional convolution is rank(k, G * H) ≤ rank(k, G)rank(k, H). This inequality of three-dimensional convolution is validated below.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Singular values of two-dimensional convolutions G * H.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Singular values of mode-(one, two, three) matricization of three-dimensional convolution G * H.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Singular values of mode-k, k = 1, 2, • • • , 7, matricization of G h 1 , h 2 , H 3 , H 4 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>m×m×</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>••×m . The second observation is that there exists symmetric H such that H * x n = H * x n , where symmetry means Ĥ</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Lemma 16 Stacking order o 1 , o 2 , • • • , o n Volterra convolutions can be converted to the form of order-n i=1 o i Volterra convolution, where o 1 , o 2 , • • • , o n &gt; 0. Proof To prove this, we recursively apply Lemma 14. Taking the first two layers into consideration, we have order-(o 1 o 2 ) Volterra convolution. Appending the third layer, we have order-(o 1 o 2 o</figDesc><table /><note>3 ) Volterra convolution. Recursively, after appending the n-th layer, order becomes k+1 i=1 o i , which completes this proof.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 :</head><label>1</label><figDesc>Structure of classifier network and hacking network. Parameters of conv2d layers are input channels, output channels, kernel size, stride, and padding.</figDesc><table><row><cell></cell><cell>classifier network</cell><cell>hacking network</cell></row><row><cell></cell><cell>input</cell><cell>input</cell></row><row><cell>1</cell><cell>conv2d(1, 10, k=3, s=2, p=1)</cell></row><row><cell>2</cell><cell>sigmoid</cell></row><row><cell cols="2">3 conv2d(10, 10, k=3, s=2, p=1)</cell><cell>conv2d(1, 10, k=15, s=8, p=3)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Predictions of MNIST digits polluted by perturbations. For each frame, top left is the original image x i , top right is the patched image x i + i , and table below is prediction. Block "C" indicate top two predict values of x i , and block "P" indicate top two predict values of x i + i . Line "L" indicate labels and Line "V" indicate values.</figDesc><table><row><cell>C</cell><cell>L V 8.7471 2.4500 6 0</cell><cell>C</cell><cell>L V 7.7386 0.0578 1 5</cell><cell>C</cell><cell>L V 9.0018 1.1149 3 2</cell><cell>C</cell><cell>L V 9.7701 1.5166 3 5</cell></row><row><cell>P</cell><cell>L V 3.0685 2.2283 8 6</cell><cell>P</cell><cell>L V 4.8160 1.3297 8 5</cell><cell>P</cell><cell>L V 4.8440 1.5732 8 3</cell><cell>P</cell><cell>L V 2.7900 2.7868 8 3</cell></row><row><cell>C</cell><cell>L V 8.5626 2.1267 3 9</cell><cell>C</cell><cell>L V 7.6841 2.4219 6 4</cell><cell>C</cell><cell>L V 8.3944 3.0419 6 0</cell><cell>C</cell><cell>L V 8.8827 1.5762 9 4</cell></row><row><cell>P</cell><cell>L V 2.8069 1.7993 8 5</cell><cell>P</cell><cell>L V 4.1646 2.1322 8 5</cell><cell>P</cell><cell>L V 4.2719 1.8364 8 2</cell><cell>P</cell><cell>L V 3.8951 2.1197 8 5</cell></row><row><cell>C</cell><cell>L V 9.5912 1.4444 2 7</cell><cell>C</cell><cell>L V 7.8130 1.9318 9 4</cell><cell>C</cell><cell>L V 9.4836 2.8961 6 5</cell><cell>C</cell><cell>L V 6.9076 2.4816 7 9</cell></row><row><cell>P</cell><cell>L V 1.8954 1.7449 8 2</cell><cell>P</cell><cell>L V 3.9808 1.4411 8 5</cell><cell>P</cell><cell>L V 4.4007 2.6830 6 8</cell><cell>P</cell><cell>L V 3.4972 1.5161 7 8</cell></row><row><cell>Figure 11:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>h 1 is zero, and p 1 , p 2 , • • • , p rank(G) are linear independence. If H 2 is full column rank, the same rule can also be applied to H 2 . Under this assumption, we have rank</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>ProofSuppose that H 1 , H 2 , • • • , H rank(H) are linear independence matrices, and H =</figDesc><table><row><cell>rank(H) r=1</cell><cell>H r .</cell></row><row><cell>The convolution becomes</cell><cell></cell></row></table><note>Lemma 26 rank(G * H) ≤ min (s 1 , s 2 , rank(G)rank(H)), where both G and H are twodimensional matrices, and G * H ∈ R s 1 ×s 2 .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>and we extend this to a more general H in Lemma 30.Lemma 30 rank(k, G * H) ≤ min(s k , rank(k, G)rank(k, H)), where G, H are both non-zero n-dimensional tensors, and G * H ∈ R s 1 ×s 2 ×•••×sn .</figDesc><table><row><cell>Proof</cell></row><row><cell>If n = 2, it has been proved in Lemma 26.</cell></row><row><cell>If n &gt; 2, suppose that</cell></row><row><cell>rank(k,H)</cell></row><row><cell>H =</cell></row></table><note>r=1H r , where H 1 , H 2 , • • • , H rank(k,H) are linear independent and all rank(k, H r ) = 1. The ndimensional convolution becomes,</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">. https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>The authors thank Andong Wang for helpful discussions. Sincere thanks to all anonymous reviewers, that greatly helped to improve this paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Convolution From Order One to Two</head><p>In Appendix A, we will show how to convert a one-dimensional order-one convolution to a one-dimensional order-two convolution in continuous time domain only. By the wellknown continuous one-dimensional convolution (Equation <ref type="formula">3</ref>), and the differential property of one-dimensional convolution, we have</p><p>x(l)dl dτ.</p><p>Applying the integration,</p><p>where Dirac delta</p><p>During this, we are surprised to find that g is insensitive to time irrelevant additive noise.</p><p>Suppose ĥ(τ ) = h(τ ) + n(τ ), with dn(τ ) dτ = 0, we have</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof for Combination Properties</head><p>In this appendix, only properties of discrete one-dimensional signals are proved, and this proof can be generalized to other situations. To prevent indexing out of bound, zero padding is always considered. Proof Property 1 and 3: They can be proved by linearity of integration or summation. Property 2: Due to linear property of integration or summation, we can separate these two term as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Linear Independence in Convolutions</head><p>Lemma 24 There are r ≤ min (rank(H), rank(G)) linear independence tensors in the linear combinations of</p><p>where H is the matrix generated by flattening patches of H, and G is the matrix generated by flattening</p><p>Let matrix Y be the matrix generated by flattening Y 1 , Y 2 , • • •. We have (Horn and Johnson, 1985)</p><p>If H is not full column rank, it is possible that some linear combinations of the columns of G are in the null space of H. The matrix multiplication with H and these combinations are zero, which implies that the convolution of H and the same combinations of G i are zero.</p><p>Otherwise, if H is full column rank, left multiplication by a full column rank matrix leaves rank unchanged, r = rank(HG) = rank(G), and rank(G) equals the number of linear independence tensors in the linear combinations of G 1 , G 2 , • • •. It means that there are no combinations of G i that lead to a zero convolution result.</p><p>By Lemma 24, if the H is full column rank, the number of linear independent tensors in linear combinations of</p><p>For instance, if H meets the condition, and G 1 = αG 2 for any scalar α, we have</p><p>×n 2 , the two matrices generated by shifting h 1 (•) and h 2 (•) with padding size p 1 and p 2 , are full column rank.</p><p>Proof Since the rank of G is rank(G), we can factorize G into the form of summation by outer product of linear independent vectors,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Main Theorem of Rank for Outer Convolution</head><p>Now, all previous discussion of outer convolutions are combined, expanding Lemma 29 to Theorem 31. To simplify discussions, outer convolution is divided into groups,</p><p>Theorem 31 (Rank for outer convolution)</p><p>,</p><p>If H k is not one-dimensional, we focus on the operations in group k. By the definition of outer convolution (Equation <ref type="formula">13</ref>), we have</p><p>The convolution becomes</p><p>where P is a super-diagonal format of G,</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Layer Normalization</title>
		<author>
			<persName><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1607.06450" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Universal approximation bounds for superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">R</forename><surname>Barron</surname></persName>
		</author>
		<idno type="DOI">10.1109/18.256500</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="930" to="945" />
			<date type="published" when="1993-05">may 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fading Memory and the Problem of Approximating Nonlinear Operators With Volterra Series</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><forename type="middle">O</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.1109/tcs.1985.1085649</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on circuits and systems</title>
				<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1150" to="1161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges</title>
		<author>
			<persName><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><surname>Veličković</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2104.13478" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Introductory combinatorics. Pearson/Prentice Hall, 4 edition</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Brualdi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust Uncertainty Principles : Exact Signal Frequency Information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Emmanuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terence</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="489" to="509" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02551274</idno>
		<idno>doi: 10.1007/ BF02551274</idno>
		<ptr target="http://link.springer.com/10.1007/BF02551274" />
	</analytic>
	<monogr>
		<title level="j">Mathematics of Control, Signals, and Systems</title>
		<idno type="ISSN">0932-4194</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989-12">dec 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">For most large underdetermined systems of linear equations the minimal L1-norm solution is also the sparsest solution</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<idno type="DOI">10.1002/cpa.20132</idno>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="797" to="829" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Machine learning from a continuous viewpoint</title>
		<author>
			<persName><forename type="first">E</forename><surname>Weinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wu</surname></persName>
		</author>
		<idno>doi: 10.1007/ s11425-020-1773-8</idno>
	</analytic>
	<monogr>
		<title level="j">I. Science China Mathematics</title>
		<idno type="ISSN">1674-7283</idno>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2233" to="2266" />
			<date type="published" when="2020-11">nov 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Hybrid Higher Order Neural Classifier for handling classification problems</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Fallahnezhad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Hassan</forename><surname>Moradi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Zaferanlouei</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2010.06.077</idno>
		<ptr target="http://dx.doi.org/10.1016/j.eswa.2010.06.077" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="386" to="393" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Generalised Transfer Functions of Neural Networks</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Billings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996-05">may 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning, invariance, and generalization in high-order neural networks</title>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Maxwell</surname></persName>
		</author>
		<idno type="DOI">10.1364/ao.26.004972</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<idno type="ISSN">0003-6935</idno>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page">4972</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Rafael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><surname>Woods</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
				<meeting><address><addrLine>London, England</addrLine></address></meeting>
		<imprint>
			<publisher>Pearson Education</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>4 edition</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Explaining and Harnessing Adversarial Examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6572" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015 -Conference Track Proceedings</title>
				<imprint>
			<date type="published" when="2014-12">dec 2014</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the multiplication of successions of Fourier constants</title>
		<author>
			<persName><forename type="first">Young</forename><forename type="middle">William</forename><surname>Henry</surname></persName>
		</author>
		<idno type="DOI">10.1098/rspa.1912.0086</idno>
		<ptr target="https://royalsocietypublishing.org/doi/10.1098/rspa.1912.0086" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character</title>
		<idno type="ISSN">0950-1207</idno>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">596</biblScope>
			<biblScope unit="page" from="331" to="339" />
			<date type="published" when="1912-10">oct 1912</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.1017/CBO9780511810817</idno>
		<title level="m">Matrix Analysis</title>
				<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1985-12">dec 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Halbert</forename><surname>White</surname></persName>
		</author>
		<idno type="DOI">10.1016/0893-6080(89)90020-8</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd International Conference on Machine Learning, ICML 2015</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName><forename type="first">Tamara</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brett</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
		<idno type="DOI">10.1137/07070111X</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.726791</idno>
		<ptr target="http://ieeexplore.ieee.org/document/726791/" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Phase diagram for two-layer ReLU neural networks at infinite-width limit</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi Qin John</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<idno type="ISSN">15337928</idno>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rectified Linear Units Improve Restricted Boltzmann Machines</title>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1123/jab.2016-0355</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Confer-ence on Machine Learning</title>
				<meeting>the 27th International Confer-ence on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Topology of deep neural networks</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Naitzat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhitnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lek Heng</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<idno type="ISSN">15337928</idno>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Nonlinear system theory</title>
		<author>
			<persName><forename type="first">Wilson John</forename><surname>Rugh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>Johns Hopkins University Press</publisher>
			<pubPlace>Baltimore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Approximation of multivariate functions using ridge polynomial</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<idno type="DOI">10.1109/ijcnn.1992.226958</idno>
		<imprint>
			<date type="published" when="1992-07">July 1992. 2003</date>
			<biblScope unit="page" from="380" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ridge polynomial networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joydeep</forename><surname>Ghosh</surname></persName>
		</author>
		<idno type="DOI">10.1109/72.377967</idno>
		<ptr target="http://ieeexplore.ieee.org/document/377967/" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="610" to="622" />
			<date type="published" when="1995-05">may 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tensor Decomposition for Signal Processing and Machine Learning</title>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lieven</forename><surname>De Lathauwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><forename type="middle">E</forename><surname>Papalexakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSP.2017.2690524</idno>
		<ptr target="http://ieeexplore.ieee.org/document/7891546/" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<idno type="ISSN">1053-587X</idno>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3551" to="3582" />
			<date type="published" when="2017-07">jul 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1312.6199" />
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations, ICLR 2014 -Conference Track Proceedings</title>
				<imprint>
			<date type="published" when="2013-12">dec 2013</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298594</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">High-dimensional probability: an introduction with applications in data science</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Vershynin</surname></persName>
		</author>
		<ptr target="https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.pdf" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Theory of functionals and of integral and integro-differential equations</title>
	</analytic>
	<monogr>
		<title level="j">Bull. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">623</biblScope>
			<date type="published" when="1932">1932</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Proposal on Machine Learning via Dynamical Systems</title>
		<author>
			<persName><forename type="first">W</forename><surname>Weinan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Mathematics and Statistics</title>
		<idno type="ISSN">2194671X. doi: 10.1007/ s40304-017-0103-z</idno>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Calculation of the Volterra kernels of non-linear dynamic systems using an artificial neural network</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName><surname>Green</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00202758</idno>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="187" to="195" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations, ICLR 2016 -Conference Track Proceedings</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On admm in deep learning: Convergence and saturation-avoidance</title>
		<author>
			<persName><forename type="first">Jinshan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shao-Bo</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding-Xuan</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v22/20-1006.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2021">199. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
