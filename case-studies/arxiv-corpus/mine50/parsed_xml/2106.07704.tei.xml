<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient (Soft) Q-Learning for Text Generation with Limited Good Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Han</forename><surname>Guo</surname></persName>
							<email>hanguo@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bowen</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
							<email>epxing@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Mohamed bin Zayed University of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Inc</forename><surname>Petuum</surname></persName>
						</author>
						<author>
							<persName><forename type="first">U</forename><forename type="middle">C</forename><surname>San</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Silver</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Fidje</surname></persName>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Morris</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Eli</forename><surname>Lifland</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jin</forename><forename type="middle">Yong</forename><surname>Yoo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jake</forename><surname>Grigsby</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yanjun</forename><forename type="middle">2020</forename><surname>Qi</surname></persName>
						</author>
						<author>
							<persName><surname>Textattack</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
						</author>
						<title level="a" type="main">Efficient (Soft) Q-Learning for Text Generation with Limited Good Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">645D81AE22B7554855C8A90AB3F437EE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-10-27T19:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Maximum likelihood estimation (MLE) is the predominant algorithm for training text generation models. This paradigm relies on direct supervision examples, which is not applicable to many emerging applications, such as generating adversarial attacks or generating prompts to control language models. Reinforcement learning (RL) on the other hand offers a more flexible solution by allowing users to plug in arbitrary task metrics as reward. Yet previous RL algorithms for text generation, such as policy gradient (on-policy RL) and Q-learning (off-policy RL), are often notoriously inefficient or unstable to train due to the large sequence space and the sparse reward received only at the end of sequences. In this paper, we introduce a new RL formulation for text generation from the soft Q-learning (SQL) perspective. It enables us to draw from the latest RL advances, such as path consistency learning, to combine the best of on-/off-policy updates, and learn effectively from sparse reward. We apply the approach to a wide range of novel text generation tasks, including learning from noisy/negative examples, adversarial attacks, and prompt generation. Experiments show our approach consistently outperforms both task-specialized algorithms and the previous RL methods. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent natural language generation systems have made remarkable progress in producing wellformed text, especially with massive pretrained language models. Those models are typically trained using maximum likelihood estimation (MLE) with a large amount of data supervisions. Despite its successes, the standard training method suffers from limited applicability to many emerging text generation problems, where little or no supervised data is available. Prominent examples of such low-data problems include generating prompts to control the massive LMs <ref type="bibr" target="#b59">(Yin et al., 2019;</ref><ref type="bibr" target="#b50">Shin et al., 2020;</ref><ref type="bibr" target="#b61">Zhong et al., 2021;</ref><ref type="bibr">Liu et al., 2021)</ref>, learning text generation from noisy or even negative data, generating adversarial text attacks for robustness study <ref type="bibr" target="#b54">(Wallace et al., 2019;</ref><ref type="bibr" target="#b0">Atanasova et al., 2020)</ref>, and others (Figure <ref type="figure" target="#fig_6">1</ref>, right). Due to the failure of standard MLE, people have had to devise specialized algorithms for those problems respectively.</p><p>Reinforcement learning (RL) (Sutton and Barto, 2018) offers an alternative principled framework for learning from arbitrary reward functions. However, RL by far has made limited success for training text generation, primarily due to the key challenges of sparse reward (i.e., a single reward signal is received only after the whole text sequence is generated) and large action space (i.e., a vocabulary of millions of words). For instance, a popular family of RL algorithms studied extensively for text generation is the policy-based <ref type="bibr" target="#b56">(Williams, 1992)</ref> or actor-critic based <ref type="bibr" target="#b1">(Bahdanau et al., 2016;</ref><ref type="bibr" target="#b45">Rennie et al., 2017)</ref> algorithms, with policy gradient (PG) being the most prevalent example <ref type="bibr" target="#b44">(Ranzato et al., 2016;</ref><ref type="bibr" target="#b30">Li et al., 2016;</ref><ref type="bibr" target="#b45">Rennie et al., 2017;</ref><ref type="bibr" target="#b52">Tan et al., 2018;</ref><ref type="bibr" target="#b40">Pasunuru and Bansal, 2018;</ref><ref type="bibr" target="#b41">Paulus et al., 2018)</ref>. Those algorithms train the model with on-policy updates, i.e., the text samples used for estimating policy gradients are from the target model itself. Due to the exponentially large space of sequences, on-policy updates often suffer from extremely high variance and low data efficiency (e.g., most model samples are not useful for learning). Thus directly training with PG from scratch is usually impossible. In practice, the model has to be initialized by MLE training, followed by PG as finetuning, which often leads to limited improvement <ref type="bibr" target="#b6">(Choshen et al., 2020;</ref><ref type="bibr" target="#b57">Wu et al., 2018)</ref>.</p><p>Another set of work has resorted to off-policy RL. The key advantage is that samples from other sources, e.g., human-written text, can be used, making them more data efficient than on-policy meth- Figure <ref type="figure" target="#fig_6">1</ref>: Left: An overview of the proposed SQL algorithm. Text generation is challenging due to sparse reward (i.e., the rewards of all intermediate steps are 0) and large action space (i.e., large vocabulary). Our SQL formulation enables several key algorithmic features as highlighted with yellow color, including (1) the combined on-and off-policy updates for the best of both, (2) bridging the final non-zero reward to directly supervise the Q-value estimation at intermediate steps for learning stability, and (3) simultaneously updating the Q-values of all candidate actions for efficiency. Right: We explore diverse applications of the text-generation RL algorithm.</p><p>ods. Previous work has used either importance weighted PG <ref type="bibr" target="#b38">(Pang and He, 2021;</ref><ref type="bibr" target="#b62">Zhou et al., 2017;</ref><ref type="bibr" target="#b25">Kandasamy et al., 2017)</ref> or Q-learning based algorithms <ref type="bibr" target="#b14">(Guo, 2015;</ref><ref type="bibr" target="#b23">Jaques et al., 2020;</ref><ref type="bibr">Narasimhan et al., 2015)</ref>. However, off-policy methods have been considered to be less stable. For example, the Q-learning performance relies heavily on how accurate the learned Q-function assesses the quality of intermediate subsequences -a challenging task due to the sparse reward signals.</p><p>In this paper, we develop a new RL formulation for text generation that tackles the above issues (Figure <ref type="figure" target="#fig_6">1</ref>, left). We reframe the text generation problem from the soft Q-learning perspective originally developed in robotics <ref type="bibr" target="#b16">(Haarnoja et al., 2017;</ref><ref type="bibr" target="#b47">Schulman et al., 2017)</ref>. The resulting connection allows us to seamlessly take advantage of the latest successful techniques from the RL literature. In particular, we introduce and adapt the principled path consistency learning <ref type="bibr">(Nachum et al., 2017)</ref> to text generation, that (1) offers a natural way to train the model with both on-and off-policy updates, hence combining the best of the two strategies, (2) bridges the sparse reward signal to directly supervise the Q function learning, leading to more accurate Q estimation and credit assignment, and (3) makes efficient updates to Q-values by considering all candidate actions together.</p><p>The generality and efficiency of the proposed method allows us to train text generation in a wide range of applications: (1) With noisy and negative training examples, our approach learns to generate accurate entailment text that greatly improves upon the data itself as well as other various training methods; (2) Our approach also manages to train an effective adversarial text generator for robustness test for classifiers; (3) We train a prompt generator with our algorithm to achieve controllable generation of pretrained LMs in terms of topics. 2 On all the three tasks, our approach consistently improves over not only previous RL algorithms for text generation, but also diverse task-specialized methods designed specifically for each of the problems, respectively. In the appendix ( §A.1.4), we also show that on standard supervised tasks where MLE prevails, our approach is competitive to train text generation models from scratch, which was usually impossible for previous RL algorithms.</p><p>The contributions can be summarized as follows. On the technical side, we propose a new RL formulation for text generation based on soft Q-Learning. This new formulation allows us to seamlessly take advantage of the RL literature's latest successful techniques (notably the path con-sistency algorithm) to overcome the longstanding challenges (e.g., sparse reward and large action space) in text generation. On the empirical side, we conduct studies on a wide variety of text generation tasks with limited data (i.e., generating from noisy/negative data, adversarial text generation, prompt generation). We propose their RL formulations, and show that our general approach consistently improves over not only previous text RL algorithms, but also diverse task-specialized methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Challenges</head><p>We aim to learn a generation model p θ (y) = T t=0 p θ (y t |y &lt;t ), where y t is a token from a vocabulary V. The distribution at each step t is obtained by applying softmax on the logits f θ (y|y &lt;t ):</p><formula xml:id="formula_0">p θ (yt|y&lt;t) = exp f θ (yt|y&lt;t) y ∈V exp f θ (y |y&lt;t)</formula><p>.</p><p>(1)</p><p>Despite its popularity, MLE-based training only applies when clean supervised data y * is available, and cannot be used to optimize arbitrary task metrics (e.g., BLEU, entailment score) which are typically the goal in many text generation tasks. Previous research has formulated text generation as an RL problem by considering the following finitetime Markov Decision Process (MDP). At each time step t, let the "state" be s t = y &lt;t , namely the partial sequence generated so far. The model ("agent") takes as input the current state s t and outputs a token ("action") a t ∈ V according to a policy π(a t |s t ). The agent then receives a reward r t = r(s t , a t ) and deterministically transitions to next state s t+1 (i.e., the concatenation of the tokens in s t and the new token a t ).</p><p>Following the notation convention in RL, let τ be the trajectory (i.e., text sample) generated by the policy. The agent's objective is to maximize the accumulative reward,</p><formula xml:id="formula_1">J(π) = E τ ∼π T t=0 γ t r t , where γ is the discount factor. A central concept in RL is the Q-function of policy π, Q π (s t , a t ) = E π T t =t γ t r t | s t ,</formula><p>a t , the expected future reward of taking action a t (i.e., generating token a t ) in state s t and continuing with the policy π. Challenges. Text generation poses significant challenges to RL, particularly because (1) the reward signal is usually sparse, i.e., r t = 0, ∀t &lt; T and the agent receives a non-zero reward r T only after it generates the full sequence, (2) the action space (i.e., the vocabulary V) is extremely large.</p><p>The challenges have led to difficulties of the two major families of RL approaches applied to text generation problems, as detailed below. Policy-based RL techniques directly parameterize the policy π θ with parameters θ. Thus the policy π θ (a t |s t ) exactly corresponds to the above generation model p θ (y t |y &lt;t ). Policy gradient (PG) is one of the most widely used algorithms for text generation <ref type="bibr" target="#b44">(Ranzato et al., 2016)</ref>. It optimizes the cumulative reward with the policy gradient using the estimated Q π θ value based on sample τ . PG is an on-policy algorithm, meaning that the sample τ needs to come from the the current policy π θ itself. In practice, however, optimizing this objective alone from scratch is unlikely going to work because most samples τ ∼ π θ are just gibberish with zero reward, failing to provide meaningful training signals for updating the policy. Previous literature either initializes the policy π θ with MLE training, and/or use a combination of MLE and PG updates, which often leads to marginal gains in practice <ref type="bibr" target="#b57">(Wu et al., 2018;</ref><ref type="bibr" target="#b6">Choshen et al., 2020)</ref>. Value-based RL techniques, such as Q-learning, implicitly learn the policy π by approximating the value Q π (s, a) directly. Deep Q-learning <ref type="bibr">(Mnih et al., 2013)</ref> parameterizes the Q-function as Q θ (x, a), and train the parameters by minimizing the following regression objective L(θ) based on the Bellman temporal consistency:</p><formula xml:id="formula_2">E π 1 2 rt+γ max a t+1 Qθ(st+1, at+1)−Q θ (st, at) 2 (2)</formula><p>where θ is the parameters of the target Q-network, which is a slow copy of θ and considered as constant for gradient computation of θ. Here π is an behavior policy which can be an arbitrary distribution over text, such as the data distribution or replay buffer <ref type="bibr">(Mnih et al., 2013)</ref>. This makes Q-learning an off-policy algorithm because of its ability to use samples coming from other policies. After learning Q θ , one can induce a policy π from it that takes arg max a Q θ (s, a) at each state s. <ref type="bibr" target="#b22">Jaques et al. (2017)</ref> instead sample tokens from the softmax function applied to Q θ . However, the training can be unstable and inefficient due to several challenges: (1) The bootstrapping nature of the above regression problem can make the training unstable. That is, the regression target r t + γ max a t+1 Qθ(s t+1 , a t+1 ) itself is derived from the Q-function to be learned <ref type="bibr" target="#b27">(Kumar et al., 2019)</ref>. The problem is exacerbated in the presence of sparse reward in text generation, where the real observed signal r t is zero for all intermediate t &lt; T ; (2) The large action space (e.g., 10 4 ) in text generation results in slow updates. In particular, notice that Eq.( <ref type="formula">2</ref>) applies the gradient update to the Q θ -value of the only one particular token a t (out of, say, the 10 4 candidate tokens in the vocabulary), making the training inefficient; (3) Besides, pure off-policy updates could be highly sensitive to the quality of training data, and miss the opportunity of on-policy exploration that maximizes the reward of interest in a more direct way.</p><p>3 The Soft Q-Learning Framework</p><p>We introduce the soft Q-learning (SQL) formulation of text generation. It is seamlessly compatible with the common architecture of text generation model (Eq.1), permits easy implementation ( §3.1), and enables efficient and stable RL training in practice ( §3.2). Figure <ref type="figure" target="#fig_1">2</ref> and Algorithm 1 summarizes the resulting SQL framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SQL Formulation for Text Generation</head><p>Soft Q-learning <ref type="bibr" target="#b16">(Haarnoja et al., 2017;</ref><ref type="bibr" target="#b47">Schulman et al., 2017;</ref><ref type="bibr">Nachum et al., 2017</ref>) is an maximum-entropy (MaxEnt) extension to the standard (hard) Q-learning <ref type="bibr">(Mnih et al., 2015;</ref><ref type="bibr" target="#b51">Sutton and Barto, 2018)</ref>. Under this framework, the agent is encouraged to optimize the reward while staying as stochastic as possible, with the objective J MaxEnt (π) =</p><formula xml:id="formula_3">E τ ∼π T t=0 γ t r t + αH (π (• | s t ))</formula><p>, which augments the vanilla J(π) with the additional Shannon entropy term H with coefficient α. 3 This is appealing because it seamlessly connects the Q-values to the familiar output logits of a text generation model, which enables straightforward implementation of the SQL formulation. Q-values as Generation Model Logits. We show the connection of the Q-values with the logits, i.e., outputs right before the softmax layer. Concretely, with the SQL objective, the following relationship between optimal policy π * and action-value Q * holds <ref type="bibr" target="#b16">(Haarnoja et al., 2017;</ref><ref type="bibr" target="#b47">Schulman et al., 2017)</ref>:</p><formula xml:id="formula_4">π * (a|s) = exp Q * (s, a) a exp Q * (s, a ) . (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>This form is highly reminiscent of the softmax layer of the generation model in Eq.( <ref type="formula">1</ref>). The con-3 WLOG, we can assume α=1, as it can be folded into the reward function by scaling the latter with 1/α. nection suggests that we can naturally parameterize the Q-function in SQL as the generation model logit function, i.e., Q θ (s, a) ≡ f θ (a|s). In other words, the model output f θ (a|s), originally interpretted as the "logit" of token a given the preceding tokens s, is now re-interpretted as the Q-value of action a in state s. When achieving optimality, f θ * (a|s), namely Q * (s, a), represents the best possible future reward achievable by generating token a in state s. Similarly, the full generation model p θ (a|s) in Eq.( <ref type="formula">1</ref>) that applies softmax to f θ now precisely corresponds to the policy π θ induced from Q θ (s, a). That is,</p><formula xml:id="formula_6">π θ (a|s) = exp Q θ (s, a) a exp Q θ (s, a ) ≡ exp f θ (a|s) a exp f θ (a |s) = p θ (a|s).<label>(4)</label></formula><p>We could further gain even more intuitive interpretation of the above generation policy π * from the lens of advantage function <ref type="bibr" target="#b51">(Sutton and Barto, 2018)</ref>. Specifically, in SQL, the optimal statevalue function is the log-normalizer of the optimal Q-values <ref type="bibr" target="#b16">(Haarnoja et al., 2017;</ref><ref type="bibr" target="#b47">Schulman et al., 2017)</ref>. This allows a more concise form of Eq.( <ref type="formula" target="#formula_4">3</ref>):</p><formula xml:id="formula_7">V * (s) = log a exp Q * s, a π * (a|s) = exp Q * (s, a) − V * (s) = exp A * (s, a),<label>(5)</label></formula><p>where A * is the optimal advantage function. The equation says that, in the proposed text generation SQL formulation, the optimal policy generates token a in state s according to the token's advantage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Efficient Training with Path Consistency</head><p>Vanilla training based on the Bellman temporal consistency can suffer from the instability and inefficiency issues similar to the conventional Qlearning ( §2), as we discuss more in the appendix ( §A.3.2). Fortunately, our SQL formulation allows us to import latest advances of RL techniques to overcome the difficulties. Specifically, we adapt the unified path consistency learning (PCL) that has excelled in game control <ref type="bibr">(Nachum et al., 2017)</ref>.</p><p>The PCL-based training updates Q-values of all tokens at once through a connection between the value function and the induced policy. More specifically, <ref type="bibr">Nachum et al. (2017)</ref> showed that the optimal policy π * (Eq.3) and the optimal state value function V * (Eq.5) in SQL must satisfy the following consistency property for all states and actions: where for each (s t , a t ), the computation involves step t and t + 1. Dashed boxes in dark green and gray indicate the regression target, where the intermediate reward r t is often 0 due to sparsity. The gradient is applied to parameters θ at step t (indicated by orange color). Right: Multi-step objective (Eq.9) which aggregates from step t all the way to T . In this way, the final-step non-zero reward r T is used as the regression target.</p><formula xml:id="formula_8">V * (st) − γV * (st+1) = rt − log π * (at|st) , ∀st, at.<label>(6</label></formula><p>Accordingly, the PCL-based training attempts to encourage the satisfaction of the consistency with the following regression objective L SQL, PCL (θ):</p><formula xml:id="formula_9">E π 1 2 −Vθ (st) +γVθ (st+1) +rt− log π θ (at|st) 2 ,<label>(7)</label></formula><p>where π θ is the induced policy defined in Eq.( <ref type="formula" target="#formula_6">4</ref>); Vθ is defined similarly as in Eq.( <ref type="formula" target="#formula_7">5</ref>) but depends on the target Qθ network (i.e., a slow copy of the Q θ to be learned), and recall that π is an arbitrary behavior policy (e.g., data distribution). Please see Figure <ref type="figure" target="#fig_1">2</ref> (left) for an illustration. Crucially, notice that the gradient update is applied to θ through the log π θ term which explicitly involves the Q θ -values of all tokens a in the vocabulary. This shows an important difference from the above vanilla training in conventional Q-learning ( §2) where Q θ is updated only through the particular a t token. The PCL training thus offers more efficient updates for the Q θ function. In the appendix ( §A.3.1), we also discuss the difference from the MLE objective.</p><p>Intuitively, MLE trains the model to (blindly) increase the probability of the observed tokens, while PCL encourages the (log) probability of the tokens to match the approximate advantage values.</p><p>Multi-step PCL for Sparse Reward. The above PCL objective Eq.( <ref type="formula" target="#formula_9">7</ref>) alone does not resolve the potential instability issue due to the bootstrapped Vθ(s t+1 ) value and the sparse reward (i.e., r(s t , a t ) = 0 for t &lt; T ). Our SQL formulation allows us to additionally incorporate the multi-step variant of the PCL training <ref type="bibr">(Nachum et al., 2017)</ref> to resolve the issue. Specifically, by applying a telescoping sum on the consistency equation (Eq.6) starting from t up to T , we arrive at the multi-step temporal consistency:</p><formula xml:id="formula_10">V * (st) −γ T −t V * (sT +1) = T l=t γ l−t r l − log π * (a l |s l ) ,<label>(8)</label></formula><p>where the value of past-terminal state is zero, V * (s T +1 ) = 0; and the rewards are only available at the end, T l=t γ l−t r l = γ T −t r T . We can then come to the following multi-step objective function L SQL, PCL-ms (θ),</p><formula xml:id="formula_11">E π   1 2 −Vθ (st) +γ T −t rT − T l=t γ l−t log π θ (a l |s l ) 2   .<label>(9)</label></formula><p>We can see the objective side-steps the need to bootstrap intermediate value functions Vθ(s t ) for t &gt; t. Instead, it directly uses the non-zero end reward r T to derive the update for θ. Please see <ref type="bibr">Figure 2 (right)</ref> for an illustration. In practice, we combine the single-and multi-step objectives (Eqs.7 and 9) together for training.</p><p>Joint On-and Off-policy Training. Finally, we highlight that the behavior policy π involved in the objectives Eqs.( <ref type="formula" target="#formula_9">7</ref>) and ( <ref type="formula" target="#formula_11">9</ref>) can be an arbitrary policy. For example, π can be a (possibly noisy) text dataset, or a set of text samples produced by other generation models, resulting in off-policy training.</p><p>We can also set π to be the current generation model π θ to be learned, resulting in on-policy training. In practice, we could first train the model with only off-policy data for warming up, and then continue with joint on-and off-policy training to further maximize the reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Applications and Experiments</head><p>We show broad applications of the proposed RL text generation framework to a variety of problems Draw a batch of on-policy samples {τ on } by decoding with policy π θ (a t | s t ) (Eq.4)</p><p>5:</p><p>Compute Q θ (s t , a t ) values (the model logits) and target Qθ(s t , a t ) for (s t , a t ) ∈ {τ off } ∪ {τ on } 6:</p><p>Compute the objectives in Eqs.( <ref type="formula" target="#formula_9">7</ref>) and ( <ref type="formula" target="#formula_11">9</ref>)</p><p>7:</p><p>Update the model parameters θ via gradient descent 8:</p><p>Update the target model parameters θ by θ ← ρ θ + (1 − ρ)θ with update rate ρ 9: until convergence Output: The trained Q θ * and the induced generator π θ * where no clean supervision data is available. These include learning with noisy or even negative data ( §4.1), generating adversarial text attacks ( §4.2), and generating prompts to steer pretrained LMs ( §4.3). We also study the performance on standard supervised generation tasks ( §A.1.4) and show that our approach is competitive to train text generation models from scratch. We provide detailed configurations in the appendix ( §A.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learning from Noisy (Negative) Text</head><p>The popular MLE algorithm learns by (blindly) imitating training data. However, it is often expensive to curate clean quality data. It is thus highly desirable to be able to learn from data with noises, or even negative examples. With the guidance of task metrics (rewards), the model can even learn to "outperform" the training data and achieve desired generation behaviors. To this end, we consider the task of entailment generation <ref type="bibr" target="#b39">(Pasunuru and Bansal, 2017)</ref>. Given a sentence (premise), the goal is to generate a new sentence (hypothesis) that logically follows the premise.</p><p>Setup (more in §A.2.1). We sub-sampled 50k training examples from the SNLI dataset <ref type="bibr" target="#b2">(Bowman et al., 2015)</ref>, a commonly used entailment classification dataset. The hypotheses have an average entailment probability of only 50%, and over 2/5 of them less than 20% (negative/contradictive examples) -a significant challenge for the models to learn from the noises. The rewards include (1) the entailment score of the generation measured by a robust entailment classifier <ref type="bibr" target="#b33">(Nie et al., 2020)</ref>, (2) the log-likelihood of the generation as an indicator of language quality measured by a GPT-2 language model <ref type="bibr" target="#b43">(Radford et al., 2019)</ref>, and (3) BLEU score w.r.t the input premises as another language quality reward that avoids trivial outputs. We sum together all rewards with weights 1.0.</p><p>We compare our approach with a broad range of baselines, including ( <ref type="formula">1</ref>  <ref type="formula" target="#formula_8">6</ref>) one of the latest methods GOLD-s (Pang and He, 2021) which is a pure off-policy method based on importance-sampling PG. To ablate the effect of multi-step training ( §3.2), we additionally compare with a simplified variant of our approach that uses only vanilla single-step PCL training (SQL(single)). We include more baselines such as MLE weighted by rewards in §A.1.1.</p><p>We evaluate generation results in terms of entailment rate, language quality (perplexity), and diversity which is measured by the Shannon entropy over unigrams and bigrams (H 1 , H 2 ) <ref type="bibr" target="#b13">(Gehrmann et al., 2021)</ref>. Since text generation models intrinsically trade off diversity and quality <ref type="bibr" target="#b4">(Caccia et al., 2019;</ref><ref type="bibr" target="#b17">Hashimoto et al., 2019)</ref>, we vary the generation diversity by generating samples via top-p sampling <ref type="bibr" target="#b18">(Holtzman et al., 2019)</ref> with different p values, and plot the entailment rate and perplexity against diversity, resp. We also evaluate the samples produced by beam-search decoding.  Results. Figure <ref type="figure" target="#fig_3">3</ref> (left) shows the results, and Table A.5 shows samples. First, notice that MLE performs poorly, while MLE+reward improves upon it. This is not surprising as the training data contain noisy/negative examples. Similarly, since the pure off-policy algorithm GOLD-s relies heavily on the data distribution, we observed that it achieves suboptimal performance. The on-policy MLE+PG with MLE initialization gives better entailment rate. In comparison, our full SQL framework achieves the best entailment-diversity trade-off. The comparison between SQL and SQL(single) highlights the importance of having the multi-step objective which directly uses the end reward rather than bootstrapping intermediate Q-values for supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Universal Adversarial Attacks</head><p>We next study the application in text adversarial attacks, where again no supervised data is available. Adversarial attacks is an increasingly important research topic as they reveal models' vulnerabilities and flaws. This is especially true for universal attacks <ref type="bibr" target="#b54">(Wallace et al., 2019;</ref><ref type="bibr" target="#b0">Atanasova et al., 2020)</ref>, where we want to generate universal examples that trick the model on all possible inputs. For instance, consider the context of entailment classification. Our goal is to find universal humanreadable hypotheses that are going to be classified as "entailment" with as high probability as possible, regardless of the input premises. This is a more challenging setting compared to previous instancespecific attack <ref type="bibr">(Morris et al., 2020;</ref><ref type="bibr" target="#b24">Jin et al., 2020;</ref><ref type="bibr" target="#b10">Ebrahimi et al., 2017)</ref> where the attack model conditions on a premise and generates an adversarial hypothesis specific to the premise.</p><p>Setup (more in §A.2.2). We aim to attack one of the most popular MultiNLI <ref type="bibr" target="#b55">(Williams et al., 2018)</ref> entailment classifiers on HuggingFaceHub. 4 The attack generation model generates adversarial text without conditioning on any inputs so that the generated attacks are universal to all premises. We compare our SQL with MLE+PG. We use all hypotheses in the MultiNLI dataset as the training data for the MLE training in MLE+PG and the offpolicy updates for our SQL. We do not compare with previous specialized adversarial text attack methods, because they either are not applicable to the challenging universal attack setting <ref type="bibr">(Morris et al., 2020;</ref><ref type="bibr" target="#b24">Jin et al., 2020;</ref><ref type="bibr" target="#b10">Ebrahimi et al., 2017)</ref>, or were not designed to generate human-readable sentences <ref type="bibr" target="#b54">(Wallace et al., 2019)</ref>. We use similar settings as in §4.1 to explore the diversity-quality trade-off by plotting the entailment rate and perplexity against diversity, respectively. The entailment classifier to be attacked is used as entailment score reward functions. We also include a tokenlevel repetition penalty reward for readability.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. Figure 3 (right) shows the results, and</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Prompt Generation for Controlling Pretrained Language Models</head><p>A reward function does not just have to be a metric like the BLEU score, but also a complicated pipeline that eventually returns a score. To demonstrate this, we consider the emerging task of prompting a large pretrained LM for controllable generation <ref type="bibr" target="#b21">(Hu et al., 2017;</ref><ref type="bibr" target="#b43">Radford et al., 2019;</ref><ref type="bibr">Brown et al., 2020)</ref>. The goal is to learn to generate text prompts that steer the LM to generate sentences of certain desired attributes (e.g., topics). The problem of controlling the generation of pretrained LMs was previously approached through specialized algorithms such as modifying the LM hidden states during decoding <ref type="bibr" target="#b8">(Dathathri et al., 2020;</ref><ref type="bibr" target="#b26">Krause et al., 2020;</ref><ref type="bibr" target="#b42">Qin et al., 2020)</ref>.</p><p>Here we show that prompts offer an easier, faster, more effective way for controlled generation.</p><p>Learning to generate/tune prompts is gaining increasing attention recently. It side-steps the needs for expensive LM fine-tuning, and adapts LMs to new scenarios with prompt as the (computefriendly) interface. Most existing approaches <ref type="bibr" target="#b54">(Wallace et al., 2019;</ref><ref type="bibr" target="#b31">Li and Liang, 2021;</ref><ref type="bibr" target="#b28">Lester et al., 2021)</ref> rely on gradient backpropagation and are applicable only when the whole training pipeline is differentiable. This does not hold for the text generation setting, as illustrated in Figure <ref type="figure" target="#fig_5">5</ref>. In contrast, the RL framework is generally applicable to any differentiable or discrete pipelines.</p><p>Setup (more in §A.2.3). Following <ref type="bibr" target="#b7">(Dathathri et al., 2019)</ref>, we aim to control the generation to have one of 7 topics (e.g., "science"); the generated prompt is prepended to one of 20 input sentences for the pretrained LM to generate continuation sentences. Figure <ref type="figure" target="#fig_5">5</ref> shows the architecture of prompt-based controllable generation. We compare our SQL method with MLE+PG as before.</p><p>Since the prompt length could impact the generated sentences, we conducted experiments with maximum prompt length 5, 10, and 15. As ablation study, we also evaluate the SQL algorithm with only off-policy updates (i.e., without on-policy exploration), denoted as SQL(off), and compare it with vanilla MLE training. Finally, we also compare with two specialized controllable generation techniques based on pretrained LMs, namely PPLM <ref type="bibr" target="#b7">(Dathathri et al., 2019)</ref> and GeDi <ref type="bibr" target="#b26">(Krause et al., 2020)</ref>, following similar procedures using their open-sourced code. We use a distilled GPT-2 model 5 as the pretrained LM to be controlled.</p><p>For rewards, we use the topic accuracy of the continuation sentences measured by a zero-shot classifier, plus the the log-likelihood of continuation sentences as the language quality reward measured by a distilled GPT-2. 6</p><p>Results. Figure <ref type="figure" target="#fig_4">4</ref> shows the topic accuracy of the controlled LM outputs averaged across the 7 topics, and Table <ref type="table" target="#tab_3">1</ref> shows the respective language quality results. More detailed topic accuracy results and samples are provided in the appendix ( §A.1.3) (where GeDi obtained low accuracy on 2 of the 7 topics, possibly because the topic tokens are tokenized into two subwords for which the model released by the authors was not specifically trained).</p><p>We can see that the prompts generated by our SQL cause the LM to generate sentences with high topic accuracy while maintaining low perplexity in most settings. Increasing the prompt length positively impacts the topic accuracy, which makes sense because longer prompts give more flexible for steering the LM. The comparison between MLE and SQL(off) shows that the off-policy component of SQL is better than standard MLE training, as it incorporates reward signals instead of just blindly following the (noisy) data.</p><p>Next, comparing with the previous steered decoding such as PPLM and GeDi, we can see the prompt-based control trained with RL achieves better trade-off between topic accuracy and language quality. Moreover, once a prompt is produced, we can use the pretrained LM to generate text of desired topics efficiently, with the same time cost as standard non-controlled decoding. In comparison, the dedicated steered decoding is often orders-ofmagnitude slower, as shown in Table <ref type="table" target="#tab_4">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Standard RL algorithms can sometimes be oversensitive to the randomness in the environment. Recent works have considered maximum-entropy RL extensions, such as the soft Q-learning (SQL) <ref type="bibr" target="#b16">(Haarnoja et al., 2017;</ref><ref type="bibr">Nachum et al., 2017;</ref><ref type="bibr" target="#b47">Schulman et al., 2017)</ref>, that maximize the entropy of policy besides the rewards, and demonstrated substantial improvement in robotic and game control <ref type="bibr" target="#b64">(Ziebart et al., 2008;</ref><ref type="bibr" target="#b36">O'Donoghue et al., 2017;</ref><ref type="bibr">Nachum et al., 2018;</ref><ref type="bibr" target="#b11">Eysenbach and Levine, 2021)</ref>. Our work is the first to adapt SQL and its advanced variants (in particular the path consistency learning <ref type="bibr">(Nachum et al., 2017)</ref>) to the challenging text generation problem and show significant results on diverse applications.</p><p>Applying RL for text generation has been discussed in alleviating the exposure bias problem and optimizing task metrics <ref type="bibr" target="#b14">(Guo, 2015;</ref><ref type="bibr" target="#b30">Li et al., 2016;</ref><ref type="bibr">Wu et al., 2016;</ref><ref type="bibr" target="#b45">Rennie et al., 2017;</ref><ref type="bibr" target="#b41">Paulus et al., 2018;</ref><ref type="bibr" target="#b5">Chen and Bansal, 2018;</ref><ref type="bibr">Liu et al., 2020;</ref><ref type="bibr" target="#b38">Pang et al., 2021)</ref>. For example, Ranzato et al. (2016) used the REINFORCE algorithm <ref type="bibr" target="#b56">(Williams, 1992)</ref>, and <ref type="bibr" target="#b1">Bahdanau et al. (2016)</ref> used the actorcritic algorithm; <ref type="bibr" target="#b15">Guo et al. (2018)</ref> and <ref type="bibr" target="#b49">Shi et al. (2018)</ref> tried to relieve the sparsity problem via hierarchical and inverse RL methods, resp. They are all on-policy RL algorithms with the need of pretraining their models using MLE. RAML <ref type="bibr">(Norouzi et al., 2016)</ref> implicitly relies on the quality of off-policy data; this does not necessarily apply in our experiments with limited good data. Tan et al. ( <ref type="formula">2018</ref>) and Hu and Xing (2022) offer a unified view of RAML, RL, and other training methods. Another line of work focused mostly on using only off-policy data, often for offline training of chatbots <ref type="bibr" target="#b25">(Kandasamy et al., 2017;</ref><ref type="bibr" target="#b62">Zhou et al., 2017;</ref><ref type="bibr" target="#b23">Jaques et al., 2020;</ref><ref type="bibr" target="#b38">Pang and He, 2021)</ref>. As a result, the opportunity of directly improving the reward (as in on-policy updates) for other rich tasks is missed. Our proposed framework combines onand off-policy training, and further offers solutions for efficient training from scratch in the presence of large action space and sparse sequence-level reward in text generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We develop a new RL formulation for text generation based on soft Q-learning and path consistency learning. We conduct experiments on learning with noisy and negative data, black box adversarial attack, prompting a pretrained language model for controllable generation, and standard supervised tasks. This formulation opens up new opportunities to integrate more advances made in the fertile RL literature to improve text generation problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>A well-documented limitation of RL methods is the importance of the reward function. The proposed methods are no different in this aspect. This is especially relevant as our reward function could involve a learned model itself, which we proactively leveraged in Sec. 4.2. We refer interested readers to <ref type="bibr" target="#b9">Deng et al. (2022)</ref> for more algorithmic considerations. We also noticed that adapting the pretraining-finetuning paradigm to the proposed methods requires careful designs. A hypothesis points to the discrepancy between MLE objectives (commonly used in pretraining context) and SQL objectives. As discussed in Sec. 3.1, the SQL formulation re-interprets the "logit" as the Q-value, for many good reasons. However, our preliminary experiments suggest that, as a downside, this makes finetuning an MLE-trained model with SQL objectives more challenging. Future work to scale the proposed methods to tasks such as machine translation and language modeling, and with significantly larger and (MLE-)pretrained models would be exciting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>This work develops a new RL formulation for text generation. While we demonstrate the framework in four applications, it could be adapted to other (emerging) applications. One major component in these applications is the design of the reward function, which influences the behavior of the trained agent. While we believe the MaxEnt RL framework is more robust against reward misspecification (Eysenbach and Levine, 2021), the potential failures of sub-optimal reward functions are widely known and discussed. 7 To this end, deploying this model to the wild requires careful and extensive examination, using tools such as <ref type="bibr" target="#b46">Ribeiro et al. (2020)</ref>. Further, we highlight the application for (blackbox) adversarial attacks in the paper, with the intention of using adversarial attacks to understand the model's inner workings. That being said, this could potentially be misused to conduct malicious attacks against systems. Hence, users of this framework might want to conduct adversarial attacks against their own models to avoid being attacked by other people with bad intentions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.4 Supervised Text Generation Tasks</head><p>Finally, we conduct experiment on standard generation tasks where clean supervised data is available.</p><p>The study is to examine the capabilities of the proposed RL method to train a text generation model from scratch, which has been considered as exceedingly challenging for previous RL algorithms.</p><p>Setup. We study on two tasks, E2E <ref type="bibr" target="#b35">(Novikova et al., 2017) and</ref><ref type="bibr">CommonGEN (Lin et al., 2020)</ref>, and use the respective datasets pre-processed by <ref type="bibr" target="#b13">(Gehrmann et al., 2021)</ref>  We further demonstrate the sensitive of MLE+PG and SQL w.r.t the reward scale as a key hyperparameter. Figure <ref type="bibr">A.2 (middle and right)</ref> shows the training curves of the two methods with varying reward scales. We can see SQL is significantly more robust as reward scale changes, while MLE+PG tends to collapse with improper reward scale configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Setup Details</head><p>Our evaluation follows the GEM Benchmark <ref type="bibr" target="#b13">(Gehrmann et al., 2021)</ref> when applicable, 8 and otherwise same with the reward function used in training. We use a transformer model <ref type="bibr" target="#b53">(Vaswani et al., 2017)</ref> based on Texar-Pytorch <ref type="bibr" target="#b19">(Hu et al., 2019)</ref> by default, with 64 hidden dimension, 3 blocks, and 4 heads. For experiments that involve policy gradient training, we initialize the model with maximum likelihood training by default unless specified otherwise. We train soft Q-learning model from scratch with both off-policy (using data) and on-policy (using samples) by default except in §4.1 and §4.3, in which we find it beneficial to warm-up the model with just off-policy training. We apply similar tuning budgets to both soft Q-learning model, and policy-gradient (mostly the reward scale and top-k), based on performance on the validation dataset and sample qualities. Most of the experiments are conducted using Nvidia 1080 or 2080 series GPUs with around 12GB memory. Most of the datasets are based in English.</p><p>Reward Functions We use the robust entailment classifier <ref type="bibr" target="#b33">(Nie et al., 2020)</ref> in §4.1, 9 one of the most used entailment classifiers on HuggingFace-Hub in §4.2, 10 and a zero-shot classifier based on  BART <ref type="bibr" target="#b29">(Lewis et al., 2020)</ref> to compute the topic score in §4.3. 11 To compute perplexities, we use a GPT-2 model (124M parameters) <ref type="bibr" target="#b43">(Radford et al., 2019)</ref> fine-tuned on the corresponding datasets for computing perplexity in §4.1 and 4.2, and a distilled GPT-2 model in §4.3 without fine-tuning. 12  We simply set reward weights to 1.0, except in §4.2, where we changed the entailment weight to 0.5, log-likelihood and repetition penalty weight to 5.0.</p><p>A.2.1 Setup Details: §4.1</p><p>We study using the SNLI dataset <ref type="bibr" target="#b2">(Bowman et al., 2015)</ref>, a dataset commonly used in training an entailment classifier. The original dataset contains (premise, hypothesis) sentence pairs, where the hypothesis may or may not entail the premise. We sub-sampled 50, 000 training examples from the corpus such that the hypotheses have an average entailment probability of only 50% in terms of the premises, and over 2/5 examples have entailment probabilities less than 20%, which can be seen as negative (contradictive) examples. The resulting training set poses a significant challenge for the models to learn from the noises. The RL algorithms (including PG and ours) permit us to plug in arbitrary reward functions to drive learning. Based on the goal of the task, we use the following intuitive rewards to ensure entailment accuracy and language quality: (1) a robust entailment classifier <ref type="bibr" target="#b33">(Nie et al., 2020)</ref> that measures the entailment score of a generation in terms of the input premise, (2) a GPT-2 language model <ref type="bibr" target="#b43">(Radford et al., 2019)</ref> that measures the log-likelihood of the generation as an indicator of language quality, and (3) BLEU score w.r.t the input premises as another language quality reward that avoids trivial outputs.</p><p>We sum together all rewards with weights 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Setup Details: §4.2</head><p>We study the task of attacking an entailment classifier. In particular, we aim to attack one of the most popular entailment classifiers on Hugging-FaceHub. 13 The attack generation model gener- ates adversarial text without conditioning on any inputs so that the generated attacks are universal to all premises. The generation model is trained with mostly the same setting as in §4.1, where the entailment classifier to be attacked is used as entailment score reward functions. Besides, we additionally include a token-level repetition penalty reward, which empirically benefits readability. Finally, we use the MultiNLI dataset <ref type="bibr" target="#b55">(Williams et al., 2018)</ref> which includes more diverse examples than the SNLI used above. We compare our SQL with MLE+PG. We use all hypotheses in the MultiNLI dataset as the training data for the MLE training in MLE+PG and the offpolicy updates for our SQL. We do not compare with previous specialized adversarial text attack methods, because they either are not applicable to the universal attack setting <ref type="bibr">(Morris et al., 2020;</ref><ref type="bibr" target="#b24">Jin et al., 2020;</ref><ref type="bibr" target="#b10">Ebrahimi et al., 2017)</ref>, or were not designed to generate human-readable sentences <ref type="bibr" target="#b54">(Wallace et al., 2019)</ref>. Besides, it is worth noting that the general RL algorithms have an additional advantage of doing black-box attacks. That is, the algorithms only require the ability to query the entailment classifier for entailment probability, without need of knowing the internal structure of the classifier (e.g., for computing gradients) as in previous attack algorithms <ref type="bibr" target="#b10">(Ebrahimi et al., 2017;</ref><ref type="bibr" target="#b54">Wallace et al., 2019)</ref>.</p><p>For top-p sampling results, we sample a hypothesis for each premise and measure the average attack rate across the dataset. This is because sampling multiple hypotheses, each for all premises, and measure performance are expensive. Since the hypotheses are sampled input-independently, this should be a good approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.3 Setup Details: §4.3</head><p>Following <ref type="bibr" target="#b7">(Dathathri et al., 2019)</ref>, we aim to control the generation to have one of 7 topics (e.g., "science"); the generated prompt is prepended to one //huggingface.co/models?search=nli.</p><p>of 20 input sentences (Figure <ref type="figure" target="#fig_5">5</ref>) for the pretrained LM to generate continuation sentences. There is no direct supervision data available for training the prompt generator. We randomly create some noisy text as the training data for MLE baselines below and for off-policy updates for our algorithm. Specifically, the noisy text is created by sampling keywords and topics from the list used in <ref type="bibr" target="#b8">(Dathathri et al., 2020)</ref> and a paraphrase generation model.</p><p>Figure <ref type="figure" target="#fig_5">5</ref> shows the architecture of prompt-based controllable generation. We compare our SQL method with MLE+PG as before. At training time, for each generated prompt sample, the pretrained LM generates 2 continuation sentences for evaluating average reward. We use a zero-shot classifier to evaluate the topic accuracy of the continuation sentences. That is, we do not assume access to classifiers pretrained on topic-specific sentences, because generating such topic-specific sentences is the goal of the task in the first place. We additionally use an LM to evaluate the loglikelihood of continuation sentences for measuring language quality. Since the prompt length could impact the generated sentences, we conducted experiments with maximum prompt length 5, 10, and 15. As ablation study, we also evaluate the SQL algorithm with only off-policy updates (i.e., without on-policy exploration), denoted as SQL(off), and compare it with vanilla MLE training. At test time, given a topic, the trained prompt generator produces one prompt using beam search decoding. For each generated prompt, the pretrained LM generates 100 sentences using top-k decoding (with k = 50) for evaluation. Finally, we also compare with two specialized controllable generation techniques based on pretrained LMs, namely PPLM <ref type="bibr" target="#b7">(Dathathri et al., 2019)</ref> and GeDi <ref type="bibr" target="#b26">(Krause et al., 2020)</ref>, following similar procedures using their open-sourced code. We use a distilled GPT-2 model 14 as the pretrained LM to be controlled. We use the paraphrase generation model based on <ref type="bibr">Zhang et al. (2019)</ref>. 15 During decoding, we include no_repeat_ngram_size= 2, which improves readability. 16  A.3 The Soft Q-Learning Framework</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 Comparison with MLE Objective</head><p>It is interesting to take a closer look at the above objective and compare with the common MLE training. Specifically, we notice the relations between the optimal Q * , V * , and A * functions: A * (s t , a t ) = Q * (s t , a t ) − V * (s t ) = r t + γV * (s t+1 ) − V * (s t ), where the first equation is the definition of A * (see Eq.5) and the second equation is due to Eqs.( <ref type="formula" target="#formula_12">10</ref>) and ( <ref type="formula" target="#formula_7">5</ref>). We thus can see the regression target in the above objective as an approximation to the advantage function: Ãθ (s t , a t ) := −Vθ (s t ) + γVθ (s t+1 ) + r t . Therefore, by optimizing the regression objective, log π θ (a t |s t ), which is the log probability of generating token a t given preceding tokens s t , is encouraged to match the approximate advantage value Ãθ (s t , a t ), no more and no less. This is different from the objective of MLE where the model is trained to (blindly) increase the probability of the observed token a t given s t and decrease the probability of the rest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 Vanilla Training with Temporal Consistency</head><p>Much like the Bellman temporal consistency in standard Q-learning, in SQL, the optimal actionvalue function follows the softmax form of the temporal consistency <ref type="bibr" target="#b64">(Ziebart et al., 2008;</ref><ref type="bibr" target="#b63">Ziebart, 2010;</ref><ref type="bibr" target="#b12">Fox et al., 2016;</ref><ref type="bibr">Nachum et al., 2017)</ref>:</p><formula xml:id="formula_12">Q * (st, at) = rt + γ log a t+1 exp Q * (st+1, at+1) .<label>(10)</label></formula><p>We thus can derive a regression objective similar to the standard Q-learning (Eq.2):</p><formula xml:id="formula_13">LSQL, vanilla(θ) = E π 0.5 • rt + γ log a t+1 exp Qθ (st+1, at+1) − Q θ (st, at) 2 . (<label>11</label></formula><formula xml:id="formula_14">)</formula><p>Recall that π is an arbitrary behavior policy (e.g., data distribution), and Qθ is the target Q-network which is a slow copy of the Q θ to be learned and is held fixed during the gradient updates. However, the above objective is inefficient due to exact the same reasons as in standard  legal: legal space religion and space In summary, a good understanding of these concepts is that by giving an explicit understanding to a person, they provide an avenue to be studied and studied. But the concept of one person being a space is also very confusing, and can be very difficult to obtain.\n So, politics: the primary referendum is In summary, the outcome will be a referendum on the EU membership for the first time of its kind for EU citizens, and the full extent of the benefits of a single market and a flexible single EU state." computers: macintoshintoshintoshintosh In summary, it appears that the company and IBM products are currently in need of upgrading the computer. This can be seen in a detailed review of the Macintosh version of Windows Vista and XP. However, when looking at the changes made by the HP Macintosh hardware and software versions of space: legal space science and space In summary:\n\n The purpose of this paper is to investigate and test the theory of space space and other objects. This project will support NASA.s efforts to demonstrate these theories, and to propose other relevant new theories.\n This paper provides the following arguments for the religion: space legal religion religion religion In summary, to the author the current discussion is the position of the Church and the community. While we acknowledge that we should not be commenting upon claims such as our recent cases or the other ones that contradict our view, we conclude it is appropriate to include these cases. Further science: the chemical microscope is In summary, the most sophisticated of these experiments is a technique that gives no obvious, no apparent way of revealing that the material was obtained. In this study, we examine how the compounds in the samples in question make up the composition of the chemical and its properties. The chemical composition military: arms defense battalion battalion cavalry In summary: 6th Panzer Field Division, Second Division.\n\n The main task of the battalion in the main counterinsurgency campaign was to counter the enemy in any counter-incursion. The main objective of this campaign is to eliminate enemy groups and the remnants of legal: legal space religion and space This essay discusses the idea of space and time as a space, in both theoretical and conceptual terms, as not an individual time period or anything else. The emphasis is on time itself, rather than having a fixed central space. Space was the object of the first chapter, and politics: the primary referendum is This essay discusses the nature of the EU referendum. The purpose of this essay is to shed light on the importance of a public referendum, on a question of whether the decision of an EU member states to remain in the European Union is constitutional and thus in accord with constitutional guarantees of sovereignty computers: macintoshintoshintoshintosh This essay discusses hardware devices and software systems for Mac OS X, MacOS X and Linux. To view the latest version of Macintosh OS: Mac 8.7.x\n\n For more information or for information about Macintosh systems, visit Mac MacSystems.\n More space: legal space science and space This essay discusses science for teens, adults and teenagers.\n\n When the idea of studying space was first implemented as a method to test, the question was: What if a student has been "comfortable" with space without its body? What would their body like to be religion: space legal religion religion religion This essay discusses an alternative religion that focuses on the role of a particular religion and views some form of religious ethics as the form when the law is applied to that particular religious community . This discussion is concerned with the status of faith for individuals or groups which may be members and members science: the chemical microscope is This essay discusses the mechanisms of reaction with a focus on the molecular structure of nucleite and of enzymes within the cytoskeleton, thus making it easier to understand the process of metabolism and other elements of cellular life. In this essay, we use techniques such as the photochemical transfer military: arms defense battalion battalion cavalry This essay discusses three main themes:\n\n 1) Lack of uniformed soldiers is an unacceptable and unconscionable strategy for the Army.\n 2) Poor and inadequate does not compensate the soldiers, and may deprive them of the necessary and competitive training from their instructors <ref type="bibr">Table A.6</ref>: Prompt samples from SQL.</p><p>legal: there liable injunction In summary, the court decided that defendants had a right to petition the high court to intervene, and therefore granted the injunction. However, a Court of Appeal decision in the US District Court ruled with no effect on the petitioned by the plaintiffs, citing two reasons.\n\n The US politics: the primary referendum was In summary, the majority of Russians would support the idea of banning all human rights, and that would be a major part of the government's effort to build a new liberal economy, which President Vladimir Putin says would boost Russian tourism.\n\n The results were published in the computers: the macintoshintosh In summary, the kernel has the best quality of life, and kernel security is at the heart of any OS development. The first time Linux released was when i386 released the Macintosh version, which had a rather low standard. Other issues like the introduction of the OS6 operating system space: comet asteroid spaceship In summary, this asteroid asteroid ship is about to make its maiden flight to explore deep-space.\n\n "The asteroid craft was a very early attempt at the discovery of a new home for the world's first black hole," NASA said in a news release. "Today we religion: faith faith faith salvation In summary, Christian beliefs are not a new way to use the time spent thinking about God's world as a source for faith. Faith is an effort to think of the world without fear that it might become a dangerous place for the human family. Because it represents the very essence that science: climate research chemistry In summary of the study, this review aims to determine how in a single study where the same number of data was analysed, a new methodology is needed to better understand who produced a different graph than the one suggested. The paper will be published in issue #5, Issue #18. military: the cavalry battalion a In summary, the army are a unit of the same type and in all, so there is no need to declare one. The unit does not constitute a cavalry unit or for use on troops.\n\n The army is not under the command of a brigade from the front. For legal: there liable injunction This essay discusses the potential legal consequences of a stay in the United States for an indefinite period of time if the government continues to delay the process of de-instituting it. To apply such a request, all applicable laws shall apply either the same terms as the existing statutes. In politics: the primary referendum was This essay discusses the electoral strategy against a candidate for governor of the Commonwealth.\n\n The survey of British voters in this survey provides an overview of what the candidates for the United Kingdom will be seeking in the next Parliament. In the general election a few seats will lead up to a computers: the macintoshintosh This essay discusses the various problems of the Macintosh, the first two-year running environment. An early version of this paper was originally published in 1982. The MacSX was not designed and managed by Kia.\n\n Macintosh\n The mac has been a family invention space: comet asteroid spaceship This essay discusses a topic: the impact of two of the Earth's two-thirds comet-sized moon Charon on Earth, and why asteroids are so close to the sun; why people are looking for ways to find a way to keep Earth-shaped asteroids out of orbit. religion: faith faith faith salvation This essay discusses the impact religion has on the American experience and in American culture. Since the beginning of my career I have found that faith and belief have often been linked to economic growth, social development and education. I believe that all people need to know that there is no reason for science: climate research chemistry This essay discusses the role of molecular information and its interaction with the general organism and human health.\n\n "The idea of biological information is not really a new concept. We used genetic information as a medium to define, identify, and store information about biology and biology," explains Dr. military: the cavalry battalion a This essay discusses the potential for the development of a small infantry brigade as an infantry regiment. It is also a contribution to the larger cavalry corps as it would require a larger brigade for battle. For more information see the original article on this page. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Soft Q-Learning with path consistency learning (PCL) objectives. Left: Single-step objective (Eq.7), where for each (s t , a t ), the computation involves step t and t + 1. Dashed boxes in dark green and gray indicate the regression target, where the intermediate reward r t is often 0 due to sparsity. The gradient is applied to parameters θ at step t (indicated by orange color). Right: Multi-step objective (Eq.9) which aggregates from step t all the way to T . In this way, the final-step non-zero reward r T is used as the regression target.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) the standard MLE training (MLE); (2) MLE+reward, where we use the reward function to filter examples; (3) joint MLE and PG training with MLE initialization (MLE+PG), where we initialize the model with MLE training, then train it with combined MLE and PG losses; previous text-generation RL algorithms including (4) MIXER (Ranzato et al., 2016), (5) Self-critic (Rennie et al., 2017), and (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Left: entailment generation performance plotted against diversity (average of H 1 and H 2 ). Circles represent results of top-p sample outputs, and triangles represent results of beam-search outputs. Please seeTable A.3 for additional results. Right: entailment attack performance against diversity. Only a few MLE+PG dots are visible because the model is not able to generate more diverse samples even with increasing p value in top-p decoding, i.e., the model collapses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Average topic accuracy. Please see TableA.4 for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The scheme of prompt generation for controlling the outputs of pretraind LMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure A. 1 :</head><label>1</label><figDesc>Figure A.1: Entailment generation performance plotted against diversity (average of H 1 and H 2 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure A. 2 :</head><label>2</label><figDesc>Figure A.2: Training curves on validation sets. Left: Training curves on E2E with best hyperparameter configurations. Middle: Training curves on E2E with varying reward scale. Right: Training curves on CommonGen with varying reward scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table A.3 for additional results. Right: entailment attack performance against diversity. Only a few MLE+PG dots are visible because the model is not able to generate more diverse samples even with increasing p value in top-p decoding, i.e., the model collapses.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table A .</head><label>A</label><figDesc>2 shows samples. We can see that SQL outperforms MLE+PG consistently across different diversity values. The outputs from MLE+PG are not diverse even with high p's, indicating the model collapses and can only generate a small set of unique adversarial examples. The model by SQL discovers the pattern "saint-pierre-et-saint-paul" (an entity name), and exploits this to generate samples with high universal entailment rate.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Table A.4   for more details. Average perplexity across topics. The lower, the more fluent the generated continuation sentences.</figDesc><table><row><cell cols="2">PPLM GeDi</cell><cell cols="2">MLE (5) SQL (off, 5)</cell></row><row><cell>13.07</cell><cell>123.88</cell><cell>25.70</cell><cell>25.77</cell></row><row><cell cols="4">MLE+PG (5/10/15) SQL (5/10/15, ours)</cell></row><row><cell cols="2">25.52/28.16/28.71</cell><cell cols="2">25.94/26.95/29.10</cell></row><row><cell></cell><cell cols="3">Model PPLM GeDi SQL</cell></row><row><cell></cell><cell cols="3">Seconds 5.58 1.05 0.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Average sentence generation time cost.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table A .</head><label>A</label><figDesc>1: BLEU results on the E2E val/test sets.</figDesc><table><row><cell>A Appendix</cell></row><row><cell>A.1 Applications and Experiments</cell></row><row><cell>A.1.1 Learning from Noisy (Negative) Text</cell></row><row><cell>Please see Table A.3 for beam search results, Fig-</cell></row><row><cell>ure A.1 for additional results for MLE+reward,</cell></row><row><cell>and Table A.5 for examples.</cell></row><row><cell>A.1.2 Universal Adversarial Attacks</cell></row><row><cell>Please see Table A.2 for examples.</cell></row><row><cell>A.1.3 Prompt Generation for Controlling</cell></row><row><cell>Pretrained Language Models</cell></row><row><cell>Please see Table A.4 for detailed results breakdown,</cell></row><row><cell>and Table A.6-A.9 for examples. Examples are in</cell></row><row><cell>the format: topic: [prompt] input sentence gener-</cell></row><row><cell>ated text.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Results. TableA.1 shows the performance on E2E of different models whose hyperparameters are picked using the validation set. We can see the proposed SQL that trains models from scratch achieves competitive results with the common MLE and MLE+PG. In contrast, the PG algorithm alone without MLE fails the training. FigureA.2 (left)   shows the respective training curves (on the validation set), demonstrating that SQL converges in an efficient and stable way as MLE.</figDesc><table><row><cell>which allow sequence-to-</cell></row><row><cell>sequence modeling with standard transformers. We</cell></row><row><cell>run four sets of methods: the standard MLE train-</cell></row><row><cell>ing (MLE); PG training from scratch (PG); joint</cell></row><row><cell>MLE and PG training, with MLE initialization</cell></row><row><cell>(MLE+PG); and our SQL training from scratch with</cell></row><row><cell>both off-policy and on-policy updates (SQL). We</cell></row><row><cell>use the standard BLEU as reward. We addition-</cell></row><row><cell>ally investigate the training stability and sensitivity</cell></row><row><cell>w.r.t hyperparameters, in particular the scale of re-</cell></row><row><cell>ward. To this end, for MLE+PG and SQL, we vary</cell></row><row><cell>the reward scale in {1, 10, 50, 100, 500, 1000} and</cell></row><row><cell>evaluate the respective performance under different</cell></row><row><cell>scales.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Q-learning discussed earlier, namely the unstable per-step bootstrappingstyle training with sparse reward signals, plus the slow updates w.r.t only one token a t out of the large vocabulary (action space).Table A.3: Beam search results on entailment generation, in the format val/test. ↑/↓ indicates higher/lower is better. † SQL (single) achieves zero in H 1 /H 2 as it generates a single token.</figDesc><table><row><cell cols="2">Model</cell><cell></cell><cell cols="4">Entl. Prob ↑ Entl. Rate ↑ PPL ↓</cell><cell>H1 ↑</cell><cell>H2 ↑</cell><cell></cell></row><row><cell cols="2">MLE</cell><cell></cell><cell cols="2">75.62/75.86</cell><cell>79.75/80.23</cell><cell>5.49/5.45</cell><cell cols="3">5.46/5.42 8.47/8.40</cell></row><row><cell cols="5">GOLD-s (Pang and He, 2021) 74.55/76.03</cell><cell>78.69/79.89</cell><cell>5.55/5.50</cell><cell cols="3">5.50/5.49 8.48/8.45</cell></row><row><cell cols="2">MLE+PG</cell><cell></cell><cell cols="2">90.16/89.73</cell><cell>95.18/94.13</cell><cell>6.38/6.31</cell><cell cols="3">5.23/5.20 8.02/7.99</cell></row><row><cell>SQL</cell><cell></cell><cell></cell><cell cols="2">91.94/91.55</cell><cell>96.26/96.21</cell><cell>8.41/8.42</cell><cell cols="3">5.59/5.58 8.20/8.21</cell></row><row><cell cols="2">SQL (single)  †</cell><cell></cell><cell cols="2">89.90/89.92</cell><cell>94.94/94.82</cell><cell cols="4">214.42/214.42 0.00/0.00 0.00/0.00</cell></row><row><cell cols="2">Length Model</cell><cell>legal</cell><cell>politics</cell><cell cols="2">computers space</cell><cell>religion</cell><cell>science</cell><cell cols="2">military Average</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Topic Scores</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>/</cell><cell>PPLM</cell><cell>16.52</cell><cell>25.09</cell><cell>13.35</cell><cell>26.23</cell><cell>5.39</cell><cell>38.87</cell><cell>19.33</cell><cell>20.68</cell></row><row><cell>/</cell><cell>GeDi</cell><cell>40.51</cell><cell>83.40</cell><cell>9.32</cell><cell>70.90</cell><cell>18.69</cell><cell>12.46</cell><cell>86.40</cell><cell>45.96</cell></row><row><cell>5</cell><cell>MLE</cell><cell>17.28</cell><cell>13.44</cell><cell>7.26</cell><cell>42.27</cell><cell>45.24</cell><cell>39.31</cell><cell>63.75</cell><cell>32.65</cell></row><row><cell>5</cell><cell cols="2">SQL (off) 23.79</cell><cell>61.11</cell><cell>24.07</cell><cell>7.91</cell><cell>61.77</cell><cell>64.67</cell><cell>67.83</cell><cell>44.45</cell></row><row><cell>5</cell><cell cols="2">MLE+PG 29.45</cell><cell>74.16</cell><cell>72.49</cell><cell>57.39</cell><cell>65.62</cell><cell>74.31</cell><cell>76.86</cell><cell>64.33</cell></row><row><cell>5</cell><cell>SQL</cell><cell>11.79</cell><cell>70.57</cell><cell>66.37</cell><cell>58.80</cell><cell>65.60</cell><cell>69.24</cell><cell>83.15</cell><cell>60.79</cell></row><row><cell>10</cell><cell cols="2">MLE+PG 17.72</cell><cell>75.29</cell><cell>71.01</cell><cell>73.92</cell><cell>58.29</cell><cell>80.85</cell><cell>80.84</cell><cell>65.42</cell></row><row><cell>10</cell><cell>SQL</cell><cell>29.62</cell><cell>86.58</cell><cell>75.72</cell><cell>58.38</cell><cell>71.29</cell><cell>81.05</cell><cell>91.40</cell><cell>70.58</cell></row><row><cell>15</cell><cell cols="2">MLE+PG 40.18</cell><cell>81.47</cell><cell>47.14</cell><cell>82.64</cell><cell>76.21</cell><cell>84.82</cell><cell>89.31</cell><cell>71.68</cell></row><row><cell>15</cell><cell>SQL</cell><cell>48.08</cell><cell>77.94</cell><cell>70.04</cell><cell>87.43</cell><cell>75.46</cell><cell>85.94</cell><cell>77.36</cell><cell>74.61</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Perplexity</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>/</cell><cell>PPLM</cell><cell>13.52</cell><cell>12.81</cell><cell>12.79</cell><cell>13.56</cell><cell>12.98</cell><cell>12.43</cell><cell>13.38</cell><cell>13.07</cell></row><row><cell>/</cell><cell>GeDi</cell><cell>204.44</cell><cell>80.01</cell><cell>132.82</cell><cell>116.94</cell><cell>132.19</cell><cell>90.00</cell><cell>110.77</cell><cell>123.88</cell></row><row><cell>5</cell><cell>MLE</cell><cell>24.52</cell><cell>25.05</cell><cell>23.79</cell><cell>26.26</cell><cell>26.07</cell><cell>25.63</cell><cell>28.56</cell><cell>25.70</cell></row><row><cell>5</cell><cell cols="2">SQL (off) 25.48</cell><cell>22.70</cell><cell>25.10</cell><cell>26.64</cell><cell>25.84</cell><cell>27.45</cell><cell>27.19</cell><cell>25.77</cell></row><row><cell>5</cell><cell cols="2">MLE+PG 24.42</cell><cell>22.60</cell><cell>27.74</cell><cell>23.17</cell><cell>25.38</cell><cell>24.84</cell><cell>30.50</cell><cell>25.52</cell></row><row><cell>5</cell><cell>SQL</cell><cell>25.31</cell><cell>24.15</cell><cell>26.40</cell><cell>24.31</cell><cell>27.02</cell><cell>25.73</cell><cell>28.67</cell><cell>25.94</cell></row><row><cell>10</cell><cell cols="2">MLE+PG 28.25</cell><cell>23.49</cell><cell>27.82</cell><cell>26.88</cell><cell>31.62</cell><cell>25.31</cell><cell>33.74</cell><cell>28.16</cell></row><row><cell>10</cell><cell>SQL</cell><cell>25.23</cell><cell>25.37</cell><cell>26.20</cell><cell>26.97</cell><cell>25.02</cell><cell>27.11</cell><cell>32.76</cell><cell>26.95</cell></row><row><cell>15</cell><cell cols="2">MLE+PG 28.38</cell><cell>28.24</cell><cell>28.16</cell><cell>27.21</cell><cell>26.43</cell><cell>29.99</cell><cell>32.54</cell><cell>28.71</cell></row><row><cell>15</cell><cell>SQL</cell><cell>35.16</cell><cell>27.72</cell><cell>29.70</cell><cell>31.89</cell><cell>24.04</cell><cell>28.46</cell><cell>26.74</cell><cell>29.10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table A .</head><label>A</label><figDesc>4: Prompt generation results. Note that some of the numbers from GeDi are low because the topics are tokenized into two subword tokens, which the model was not trained with.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table A .</head><label>A</label><figDesc>7: Prompt samples from MLE+PG.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code at https://github.com/HanGuo97/ soft-Q-learning-for-text-generation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2"> More recently, Deng et al. (2022)  extend this line of work to optimize discrete text prompts with reinforcement learning.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/pytorch/fairseq/ tree/master/examples/roberta</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://huggingface.co/distilgpt2 6 Note that the language quality emphasis is on the generated sentences. Prompts themselves do not necessarily have to be human-readable<ref type="bibr" target="#b54">(Wallace et al., 2019;</ref><ref type="bibr" target="#b48">Sheng et al., 2020)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/GEM-benchmark/ GEM-metrics 9 https://huggingface.co/ynie/ roberta-large-snli_mnli_fever_anli_R1_ R2_R3-nli. 355M parameters. 10 https://github.com/pytorch/fairseq/ tree/master/examples/roberta. This classifier is ranked #1 (as of May 20, 2021) based on https://huggingface.co/models?search=nli. 355M parameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">https://huggingface.co/facebook/ bart-large-mnli. 407M parameters.12 https://huggingface.co/distilgpt2. 82M parameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">https://github.com/pytorch/fairseq/ tree/master/examples/roberta, which is ranked #1 as of May 20, 2021 based on https:</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">https://huggingface.co/distilgpt2 15 https://huggingface.co/tuner007/ pegasus_paraphrase 16 https://huggingface.co/blog/ how-to-generate</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank all reviewers for their invaluable comments and feedback. This research was supported by NSF IIS1563887, NSF CCF1629559, NSF IIS1617583, NGA HM04762010002, NSF IIS1955532, NSF CNS2008248, NSF IIS2123952, and NSF BCS2040381. The views in this article are those of the authors and not the funding agency.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>science: This essay discusses Linux desktop computing, and IRI video-game applications.\n\n The zooming in -even after the GNOME 3 transition came to an end, is all about figuring out how you have run a software operating system so vital that any hacker can mine it military: This essay discusses military courage that included in the combat operations in Iraq and Afghanistan. <ref type="bibr">Table A.8</ref>: Prompt samples from GeDi. legal: In summary we have published in the journal Nature Neuroscience: A systematic review of human brain tissue has found no evidence for any association between the presence of the presence of a particular form of human neuropathy in the brain, a condition that is not normally associated with cognitive impairment. We found that politics: In summary we have a list of 10 of the best and most common types of drugs for people with HIV. This is a very short list of recommendations from a national and international community.\n\n\n\n This article has been updated to make the official state of the EU state of computers: In summary, we believe that the current system has no way of doing anything about it.\n\n\n\n The following steps are taken to get the system working.\n\n 1. Install a new operating system with a Linux Mint operating system\n 2. Start a new Linux Mint operating space: In summary we have some important news from the moment of the year and some important information about these two major planets. This new discovery is the first to confirm this important planet has an active life in its home planet, a planet with a mass of about 5.8 billion tons. It religion: In summary, we believe that the current administration has no way of doing anything about the Benghazi attacks. This is a very interesting story, and I think it has been a very nice surprise. This is a very nice and well thought out piece that is a must for the science: In summary we use this approach to evaluate if the number of data points (in the dataset) that are relevant for each data set is the same (in this case, the data are not in one data set). In this approach we can test the data points in a different way. military: In summary we have some important news from the moment of the year and some important information from the moment of the year.\n\n\n\n\n We've also added an additional update for our new feature, which includes:\n • Improved access and access in all of the main legal: This essay discusses how you can build a community of dedicated people. If you're a member of a community of people who want to contribute to the environment, you'll also be helping them build communities in order to support the local economy, and future of the city. The latest report politics: This essay discusses how we can build on previous research findings about the role religion plays in human development in human development. This is a very interesting and highly entertaining story. What is an "independent" political party in the United States, the U.S. political party, and the United computers: This essay discusses how you can build a new browser to view and share your favorite web sites.\n\n\n A browser that is open source can also be built from a web browser, which can be a browser that does not allow browser extensions (e.g. Firefox, Chrome, Opera space: This essay discusses how you can build a life with a healthy diet and how you can use it when you're ready to move forward. It's a very simple approach to building a life with a healthy diet and what it means to be healthy and healthy for the religion: This essay discusses how you can build a new game without having to play the original game, and how you can make a new title that is completely different to the original. It has been around since 2007, when the first game, The Elder Scrolls IV: Oblivion, was released in the PlayStation science: This essay discusses how we can build on previous research findings about the role of obesity in human metabolism and how we can improve our health.\n\n\n\n In this essay, we explore why eating a whole whole diet does not help prevent obesity (1). We find that a whole food diet military: This essay discusses how you can build a community with the help of friends and family.\n\n\n\n\n "The people around me are the ones who need help. They are the ones who need help. They are the ones who are not alone."\n -Michael\n "It's Table <ref type="table">A</ref>.9: Prompt samples from PPLM.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generating label cohesive and wellformed adversarial claims</title>
		<author>
			<persName><forename type="first">Pepa</forename><surname>Atanasova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3168" to="3177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An actor-critic algorithm for sequence prediction</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philemon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07086</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language GANs falling short</title>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Caccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Caccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast abstractive summarization with reinforce-selected sentence rewriting</title>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="675" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the weaknesses of reinforcement learning for neural machine translation</title>
		<author>
			<persName><forename type="first">Leshem</forename><surname>Choshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zohar</forename><surname>Aizenbud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Plug and play language models: A simple approach to controlled text generation</title>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janice</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Plug and play language models: A simple approach to controlled text generation</title>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janice</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">RLPrompt: Optimizing discrete text prompts with reinforcement learning</title>
		<author>
			<persName><forename type="first">Mingkai</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Ping</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianmin</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Javid</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejing</forename><surname>Dou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.06751</idno>
		<title level="m">Hotflip: White-box adversarial examples for text classification</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Maximum entropy rl (provably) solves some robust rl problems</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Eysenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06257</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Taming the noise in reinforcement learning via soft updates</title>
		<author>
			<persName><forename type="first">Roy</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Pakman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence</title>
				<meeting>the Thirty-Second Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tosin</forename><surname>Adewumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karmanya</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawan</forename><surname>Sasanka Ammanamanchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aremu</forename><surname>Anuoluwapo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miruna</forename><surname>Khyathi Raghavi Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Clinciu</surname></persName>
		</author>
		<author>
			<persName><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kaustubh</surname></persName>
		</author>
		<author>
			<persName><surname>Dhole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01672</idno>
		<title level="m">The gem benchmark: Natural language generation, its evaluation and metrics</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.09202</idno>
		<title level="m">Generating text with deep reinforcement learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long text generation via adversarial training with leaked information</title>
		<author>
			<persName><forename type="first">Jiaxian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sidi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reinforcement learning with deep energy-based policies</title>
		<author>
			<persName><forename type="first">Tuomas</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1352" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unifying human and statistical evaluation for natural language generation</title>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugh</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1689" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Texar: A modularized, versatile, and extensible toolkit for text generation</title>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="159" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards a &quot;standard model&quot; of machine learning</title>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harvard Data Science Review</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Toward controlled generation of text</title>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control</title>
		<author>
			<persName><forename type="first">Natasha</forename><surname>Jaques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Miguel Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1645" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Human-centric dialog training via offline reinforcement learning</title>
		<author>
			<persName><forename type="first">Natasha</forename><surname>Jaques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><forename type="middle">Hanwen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asma</forename><surname>Ghandeharioun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosalind</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3985" to="4003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Is bert really robust? a strong baseline for natural language attack on text classification and entailment</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
				<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Batch policy gradient methods for improving neural conversation models</title>
		<author>
			<persName><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Carter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Ben</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhilesh</forename><surname>Deepak Gotmare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mc-Cann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazneen Fatema</forename><surname>Rajani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06367</idno>
		<title level="m">Gedi: Generative discriminator guided sequence generation</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stabilizing off-policy q-learning via bootstrapping error reduction</title>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bart</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1192" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Prefixtuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00190</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">CommonGen: A constrained text generation challenge for generative commonsense reasoning</title>
		<author>
			<persName><forename type="first">Wangchunshu</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.165</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1823" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adversarial nli: A new benchmark for natural language understanding</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4885" to="4901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reward augmented maximum likelihood for neural structured prediction</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1723" to="1731" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The e2e dataset: New challenges for end-toend generation</title>
		<author>
			<persName><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
				<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="201" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Combining policy gradient and q-learning</title>
		<author>
			<persName><forename type="first">O'</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Donoghue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Text generation by learning from demonstrations</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Richard Yuanzhe Pang and He He</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Amortized noisy channel neural machine translation</title>
		<author>
			<persName><forename type="first">He</forename><surname>Richard Yuanzhe Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.08670</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multitask video captioning with video and entailment generation</title>
		<author>
			<persName><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1273" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multireward reinforced summarization with saliency and entailment</title>
		<author>
			<persName><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="646" to="653" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Backpropagationbased decoding for unsupervised counterfactual and abductive reasoning</title>
		<author>
			<persName><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jena</forename><forename type="middle">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="794" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Zaremba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7008" to="7024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Beyond accuracy: Behavioral testing of nlp models with checklist</title>
		<author>
			<persName><forename type="first">Tongshuang</forename><surname>Marco Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4902" to="4912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06440</idno>
		<title level="m">Equivalence between policy gradients and soft Qlearning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Towards controllable biases in language generation</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3239" to="3254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Toward diverse text generation with inverse reinforcement learning</title>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
				<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4361" to="4367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Autoprompt: Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Robert L Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.15980</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Connecting the dots between mle and rl for sequence prediction</title>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Bowen Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09740</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Ruslan Salakhutdinov, and Eric Xing</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Universal adversarial triggers for attacking and analyzing nlp</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Kandpal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2153" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A study of reinforcement learning for neural machine translation</title>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3612" to="3621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach</title>
		<author>
			<persName><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamaal</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3905" to="3914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristy</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04670</idno>
		<title level="m">Meta-tuning language models to answer prompts better</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">End-to-end offline goal-oriented dialog policy learning via policy gradient</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Rokhlenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Elkan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02838</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Modeling purposeful adaptive behavior with the principle of maximum causal entropy</title>
		<author>
			<persName><forename type="first">D</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName><surname>Ziebart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Generated: families at a amusement park . Input: man in a black suit , white shirt and black bowtie playing an instrument with the rest of his symphony surrounding him . Generated: a man is playing music . Input: a white dog with long hair jumps to catch a red and green toy</title>
		<author>
			<persName><forename type="first">D</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anind K</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aaai</title>
				<meeting><address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1433" to="1438" />
		</imprint>
	</monogr>
	<note>Generated: a dog is jumping Input: a man in a black shirt is playing golf outside . Generated: a man is playing golf Input: a man wearing sunglasses is sitting on the steps outside. reading a magazine . Generated: a man is sitting outside</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">5: Entailment generation samples from SQL (beam search, validation dataset)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Table</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
