<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fundamental limits for learning hidden Markov model parameters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-24">24 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kweku</forename><surname>Abraham</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Elisabeth</forename><surname>Gassiat</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zacharie</forename><surname>Naulet</surname></persName>
						</author>
						<title level="a" type="main">Fundamental limits for learning hidden Markov model parameters</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-24">24 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">C5DB16C2D8C7AEC744913744FF61D43D</idno>
					<idno type="arXiv">arXiv:2106.12936v3[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-10-27T19:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Hidden Markov Models</term>
					<term>Minimax estimation</term>
					<term>Sample complexity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the frontier between learnable and unlearnable hidden Markov models (HMMs). HMMs are flexible tools for clustering dependent data coming from unknown populations. The model parameters are known to be fully identifiable (up to label-switching) without any modelling assumption on the distributions of the populations as soon as the clusters are distinct and the hidden chain is ergodic with a full rank transition matrix. In the limit as any one of these conditions fails, it becomes impossible in general to identify parameters. For a chain with two hidden states we prove nonasymptotic minimax upper and lower bounds, matching up to constants, which exhibit thresholds at which the parameters become learnable. We also provide an upper bound on the relative entropy rate for parameters in a neighbourhood of the unlearnable region which may have interest in itself.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Context and motivation</head><p>Finite state space hidden Markov models (HMMs) are widely used in applications to model observations coming from different populations. HMMs can be viewed as particular mixture models. In the latter, given a latent sequence of cluster labels (X n ) n∈N taking values in a finite set, the observed data (Y n ) n∈N is a sequence of independent random variables with, for each n, the distribution of Y n depending only on X n . When the X n are independent, a mixture model is not identifiable: various convex combinations of population probability distributions can lead to the same distribution for the observations. This is true even for observations taking values in a finite alphabet: one cannot recover two different multinomial distributions from a convex combination of them.</p><p>For a HMM, one adds the extra structure that (X n ) n∈N forms a Markov chain. In sharp contrast to the independent setting, with hidden Markov structure one can recover the distribution of data for each population absent virtually any constraint on these distributions (known in this context as the emission distributions). This fact had been observed in applied papers, and a theoretical proof that parameters can be identified with minimal assumptions is relatively recent, given for HMMs taking values in a finite set in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> and extended to allow for emission distributions modelled nonparametrically (but still with the underlying Markov chain having finite state space) in <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. HMMs therefore form a 0 ©2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. tractable class of models nevertheless rich enough to model many practical clustering settings well: see for instance <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. In this context note that given good estimates of the model parameters one can almost match the optimal clustering and testing behaviour of the Bayes classifier (e.g. see <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>); let us emphasise once more that this is possible essentially absent any constraint on the emission distributions, in contrast to typical clustering algorithms which may require parametric modelling or separation of clusters.</p><p>In drawing a contrast between the independent and dependent cases, we have so far omitted to mention that of course an independent model is a degenerate subcase of a Markov model. There are three ways in which the data (Y n ) n∈N can fail to exhibit dependence: when the population labels themselves are in reality independently distributed; when the emission distributions are identical; or when only one population is observed. Without extra modelling assumption on the populations distributions, learnability of the parameters with a finite number of observations becomes difficult near the independent case. This occurs when one of the populations is sparse, which is a typical setting for multiple testing. It also occurs when the two populations have close distributions; knowing how far separated the populations have to be for clustering to be possible without further structural assumptions is of interest. Finally, this occurs when the cluster labels have very weak dependence. It has also been observed empirically in <ref type="bibr" target="#b13">[14]</ref> that the EM algorithm can exhibit bad behavior in some regions of the parameter space. It is thus of theoretical and practical importance to understand quantitatively what happens when these limiting situations are approached in the setting of finitely many observations of the HMM sequence.</p><p>The present work initiates an exploration of the limits of learnability of the hidden Markov parameters as the independent subcase is approached. We focus on the setting of two hidden states and multinomial data, and exhibit principles which should generalise to much wider settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Contribution</head><p>Our main result, Theorem 1, gives upper and lower bounds showing the minimax estimation rate for the model parameters, exhibiting that these parameters can be learned if and only if the sample size n is large enough compared to a suitable measure of the closeness of the data to the independent subcase.</p><p>Important steps to get the main result are as follows. We introduce a reparametrisation of the model leading to a statistical distance which appears to be a key tool for the understanding of the fundamental limits of learning the HMM parameters near the independent subcase. This statistical distance is proved in Proposition 1 to be equivalent to the distance between the distribution of three consecutive observations, and leads to an explicit upper bound of the relative entropy rate for a specific part of parameters domain, see Proposition 2, which we believe could have interest in itself. Upper bounds for the learning of the new parameters are proved in Theorem 2 while (almost) matching lower bounds are proved in Theorem 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Related work</head><p>Theoretical justification of a range of learning methods for HMMs with emission distributions modelled parametrically or nonparametrically have been developed in recent years: moment and tensor methods in <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b14">[15]</ref>, and model selection using penalized least squares estimation in <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, using penalized likelihood methods in <ref type="bibr" target="#b17">[18]</ref>, or using other techniques in <ref type="bibr" target="#b18">[19]</ref>. These works all give both asymptotic and nonasymptotic upper bounds controlling the distance between estimators and the unknown parameters. All require the data to truly be dependent, but none quantify explicitly how their sample complexity results depend on the "distance" to independence. Indeed, quantifying this dependence requires a sharp understanding of how the distances between distributions evolve with respect to the distances between parameters, as done for particular parametric finite mixture models in <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>.</p><p>Results in <ref type="bibr" target="#b14">[15]</ref> control the propagation of errors from parameter estimation to the posterior probabilities when calculating the latter via plug-in, implying that good control on the risk of the estimators will ensure the performance of the empirical Bayes classifier is close to that of the true Bayes classifier (whose optimality for clustering is a standard result in decision theory <ref type="bibr" target="#b22">[23]</ref>).</p><p>A topic closely related to binary classification/clustering is multiple testing, in which one aims to identify within some large data set a collection of data points which come from a "discovery" hypothesis, rather than from the conservative null hypothesis. In this setting control of the false discovery rate has been obtained recently for a knockoffs-based method in <ref type="bibr" target="#b23">[24]</ref> and for an empirical Bayes method in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>; in each case estimation of the HMM parameters is an essential first step. Modelling the proportion of non-null signals as vanishingly small, as our results permit, would allow for further links to the setting of sparse multiple testing, considered for example (with independent data) in <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>.</p><p>Relative entropy rate, or equivalently Kullback-Leibler rate, between HMMs can be expressed using Blackwell's invariant measure <ref type="bibr" target="#b27">[27]</ref>, but no explicit formulation exists <ref type="bibr" target="#b28">[28]</ref>. Providing useful or meaningful upper and lower bounds is a subject of ongoing research <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref>. In Proposition 2 we obtain a new bound on the Kullback-Leibler rate between HMMs which, compared to the aforementioned works, does a better job at capturing the effects of the underlying Markov dependency structure, at the expense of holding only for a restricted subspace of parameters.</p><p>To the best of our knowledge no prior theoretical result exists addressing the learning of parameters of a HMM when approaching the independent case. By experimentally studying the EM algorithm when the multinomial emission distributions approach each other, the authors in <ref type="bibr" target="#b13">[14]</ref> found a range of parameters for which the EM algorithm behaves badly. Their results provided some of the inspiration for the work herein, which we believe shows that such behaviour is primarily a result of the investigated region approaching the limit where the parameters become unlearnable, not of a limitation of the EM algorithm specifically.</p><p>Finally, let us mention that departure from the independence assumption has been noted to allow for better learning also in HMM settings free from the assumption that the Markov chain has a finite state space <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b33">[33]</ref> (at the expense of stricter assumptions on the emission distributions), and also in other problems including dynamic networks <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b35">[35]</ref>, image denoising <ref type="bibr" target="#b36">[36]</ref>, and deconvolution <ref type="bibr" target="#b37">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Organisation of the paper</head><p>We describe the setting in Section II and state our main result in Section III. The key reparametrisation is given in Section IV where we state the basic propositions involving the statistical distance we define. Intermediate upper bound results are given in Section V while lower bounds are in Section VI. In Section VII we discuss our results and possible further work. All proofs are deferred to Section VIII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Notation</head><p>We write f = f, f 1/2 for the usual Euclidean norm and inner product. We write a ∨ b := max(a, b) and a ∧ b := min(a, b). We write KL for the Kullback-Leibler divergence between densities p, q or between the corresponding distribution P, Q, KL(p, q) ≡ KL(P, Q) = E p (log(p/q)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. SETTING</head><p>Consider a two-state HMM with multinomial emissions, in which we observe the first n entries of a sequence Y = (Y 1 , Y 2 , . . . ) ∈ {1, . . . , K} N which, under a parameter θ = (p, q, f 0 , f 1 ), satisfies</p><formula xml:id="formula_0">P θ (Y n = k | X) = f Xn (k), X = (X n ) n∈N ∼ Markov(π, Q),<label>(1)</label></formula><p>with the Y j , j ∈ N conditionally independent given X. The vector X of 'hidden states' takes values in {0, 1} N and the transition matrix of the chain is given by</p><formula xml:id="formula_1">Q := 1 − p p q 1 − q ,<label>(2)</label></formula><p>with the convention that for j ≥ 1,</p><formula xml:id="formula_2">P θ (X j+1 = 0 | X j = 0) = 1 − p &lt; 1 and P θ (X j+1 = 0 | X j = 1) = q &gt; 0.</formula><p>The densities f 0 , f 1 are the 'emission densities' with respect to counting measure on {1, . . . , K}. Note that any function g on {1, . . . , K} (such as f 0 and f 1 ) may be identified with a vector (g(a)) 1≤a≤K in R K . Grant also that X 1 is drawn from the stationary distribution of the chain, i.e. P θ (X 1 = 1) = p/(p + q). We throughout use P θ to denote the law of (X, Y ), and all induced marginal and conditional laws.</p><p>In the limit where the sequence Y becomes independent and identically distributed (i.i.d.), learning the parameters becomes impossible due to standard identifiability issues for mixture models: the distribution of Y 1 may be decomposed in many ways as a convex combination of multinomials. This i.i.d. limit can be approached in three ways:</p><p>1) p ≈ 0 or q ≈ 0, and thus the chain X passes long periods of time in one of the two states; 2) the transition matrix Q is nearly singular, so that X itself is almost i.i.d; this is the case if |1 − p − q| ≈ 0; 3) the emission distributions are close to each other: f 0 − f 1 ≈ 0, where • denotes the usual Euclidean norm,</p><formula xml:id="formula_3">f 2 = |f (k)| 2 .</formula><p>We adopt a minimax point of view and encapsulate all the above scenarios within the class of parameters defined, for some δ, ǫ ∈ (0, 1) and some ζ &gt; 0, by</p><formula xml:id="formula_4">Θ = Θ(δ, ǫ, ζ) = {θ : p, q ≥ δ, |1−p−q| ≥ ǫ, f 0 −f 1 ≥ ζ}.</formula><p>Introduce also the subset</p><formula xml:id="formula_5">Θ L = Θ L (δ, ǫ, ζ) = Θ ∩ {1 − |1 − p − q| ≥ L}.</formula><p>Remark 1. Note that 1−|1−p−q| is the absolute spectral gap of the chain X, and hence the mixing time of the chain can be upper bounded uniformly in Θ L since the state space has size 2 (so the chain is automatically reversible). Here L may be arbitrarily small but we think of it as fixed, in contrast to δ, ǫ and ζ which are allowed to depend on n. With the introduction of this lower bound we still allow one of p, q to be vanishingly small (or arbitrarily close -even equal -to 1), but not both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MAIN RESULTS</head><p>To avoid a label-switching issue discussed in the next section we assume that f 0 − f 1 lies in some specified halfplane. Our main result is the following. The estimator θ is built via plug-in from those constructed later in Theorem 2.</p><p>Theorem 1. There exist an estimator θ = (p, q, f0 , f1 ) and</p><formula xml:id="formula_6">a constant C = C(K, L) &gt; 0 such that for all 1 ≤ x 2 ≤ nδ 2 ǫ 4 ζ 6 , sup θ∈ΘL P θ |p − p| ∨ |q − q| &gt; Cx(δ ∨ ǫζ) nδ 2 ǫ 4 ζ 6 ≤ e −x 2 , sup θ∈ΘL P θ f0 − f 0 ∨ f1 − f 1 &gt; Cx nδ 2 ǫ 4 ζ 4 ≤ e −x 2 .</formula><p>Furthermore, there exist constants c = c(K) &gt; 0, ǫ 1 &gt; 0 and</p><formula xml:id="formula_7">ζ 1 &gt; 0 such that for δ ≤ 1/6, ǫ ≤ ǫ 1 , ζ ≤ ζ 1 , L ≤ 1/3 and nδ 2 ǫ 4 ζ 6 ≥ 1, inf θ sup θ∈ΘL P θ |p − p| ∨ |q − q| &gt; c(δ ∨ ǫζ) nδ 2 ǫ 4 ζ 6 ≥ 1/4, inf θ sup θ∈ΘL P θ f0 − f 0 ∨ f1 − f 1 &gt; c nδ 2 ǫ 4 ζ 4 ≥ 1/4,</formula><p>where the infima are over all estimators θ = (p, q, f0 , f1 ).</p><p>The particular value 1/4 on the right sides is not essential: what is important is that the probabilities are bounded away from zero.</p><p>We deduce immediately the sample complexity for learning the parameters. We do not seek sharp dependence on K in the bounds because we believe our results can be extended to the nonparametric setting, which we leave for further work.</p><p>Corollary 1. Fix a target error magnitude E &gt; 0 and a probability level α &gt; 0. For the same estimators as in Theorem 1, there exists a constant C = C(K, L) such that for any θ ∈ Θ L we have</p><formula xml:id="formula_8">n ≥ log(1/α) δ 2 ǫ 4 ζ 6 Cδ 2 E 2 Cǫ 2 ζ 2 E 2 1 =⇒ P θ (|p − p| ∨ |q − q| &gt; E) ≤ α,</formula><p>and,</p><formula xml:id="formula_9">n ≥ log(1/α) δ 2 ǫ 4 ζ 4 C E 2 1 ζ 2 =⇒ P θ ( f0 − f 0 ∨ f1 − f 1 &gt; E) ≤ α.</formula><p>Conversely there exists a constant c = c(K) &gt; 0 such that for all 0 &lt; E ≤ c(K) and for any estimator θ = (p, q, f0 , f1 )</p><formula xml:id="formula_10">there exists θ ∈ Θ L such that n ≤ c 2 (δ 2 ∨ ǫ 2 ζ 2 ) E 2 δ 2 ǫ 4 ζ 6 =⇒ P θ (|p − p| ∨ |q − q| &gt; E) ≥ 1/4,</formula><p>and,</p><formula xml:id="formula_11">n ≤ c 2 E 2 δ 2 ǫ 4 ζ 4 =⇒ P θ ( f0 − f 0 ∨ f1 − f 1 &gt; E) ≥ 1/4.</formula><p>Note that to apply Theorem 1 for the lower bounds we would initially also need n ≥ (δ 2 ǫ 4 ζ 6 ) −1 but by monotonicity -i.e. the fact that any measurable function of (Y 1 , . . . , Y n ) is also a measurable function of (Y 1 , . . . , Y N ) for N ≥ nthe restriction can be removed.</p><p>Let us sketch the main ideas behind the proof of Theorem 1. The full proof is deferred to Section VIII, along with all other proofs for this article.</p><p>The minimax upper bounds are obtained by producing an estimator that attains the bounds. Building on the work of <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> we know that θ is identifiable from the law of three consecutive observations, and we propose a reparametrisation of the model to simplify the analysis. Let us denote by p (3) θ the density of three consecutive observations. Motivated by a desire to simplify the expression for p (see equations <ref type="bibr" target="#b7">(8)</ref> and <ref type="bibr" target="#b8">(9)</ref> in Section IV), we introduce new parameters φ, ψ and we show in Proposition 1 that p</p><formula xml:id="formula_12">(3) θ(φ,ψ) − p (3)</formula><p>θ( φ, ψ) is equivalent to ρ(φ, ψ; φ, ψ), where ρ is defined in the proposition and can be seen as an adequate statistical distance of the problem (see also the discussion after Proposition 2). Then, we leverage that p </p><formula xml:id="formula_13">θ(φ,ψ) − p<label>(3)</label></formula><p>n will give a good estimator ( φ, ψ) for (φ, ψ). By standard calculations and using the equivalence between p  θ( φ, ψ) and ρ(φ, ψ; φ, ψ) derived in Proposition 1, we obtain bounds on maximum risk of such ( φ, ψ) for estimating (φ, ψ) in Theorem 2. Finally, the upper bounds for the original parameters in Theorem 1 are obtained by taking θ = θ( φ, ψ).</p><p>Incidentally, we remark that the parametrisation (φ, ψ) turns out to be of special interest: the components of φ determine how close the sequence Y is to being i.i.d in an interpretable way (see Section IV), and the parameter ψ is related to the stationary distribution of the sequence Y . For this reason, we also establish minimax bounds for the estimation of φ and ψ themselves in Theorems 2 and 3.</p><p>The minimax lower bounds are obtained by an argument à la Le Cam. In particular, it is a famous result of Le Cam <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b39">[39]</ref> that the minimax rate (under quadratic loss) of estimating a functional g : Θ → R is always greater than the maximum value that |g(θ)−g( θ)| 2 can take for θ, θ ∈ Θ under the constraint that KL(p</p><formula xml:id="formula_15">(n) θ ; p (n) θ ) ≤ c, where KL(p (n) θ ; p (n) θ )</formula><p>denotes the Kullback-Leibler (KL) divergence between the laws of (Y 1 , . . . , Y n ) under parameters θ and θ, and 0 &lt; c &lt; 1 is a small positive constant (see Lemma 2 for the precise formulation we use). Understanding bounds on |g(θ) − g( θ)| in terms of bounds on KL(p</p><formula xml:id="formula_16">(n) θ ; p (n) θ</formula><p>) is also sufficient for obtaining an upper bound on the minimax estimation rate. Since we have dependent observations, the main difficulty of the proof is to relate KL(p</p><formula xml:id="formula_17">(n) θ ; p (n)</formula><p>θ ) to a suitable notion of distance between θ and θ. A key result is Proposition 2 showing that under mild assumptions KL(p</p><formula xml:id="formula_18">(n) θ(φ,ψ) ; p (n) θ( φ, ψ)</formula><p>) is upper bounded by a constant times nρ 2 (φ, ψ; φ, ψ). Then the lower bounds for φ (respectively ψ) in Theorem 3 are obtained by lower bounding the value of the optimisation problems max|φ j − φj | 2 (respectively max|ψ j − ψj | 2 ) subject to nρ 2 (φ, ψ; φ, ψ) ≤ c and θ(φ, ψ), θ( φ, ψ) ∈ Θ for a small enough constant c &gt; 0. Finally, the lower bounds for the original parameters in the Theorem 1 are essentially deduced from the bounds for (φ, ψ) and inversion of the parametrisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CHANGE OF PARAMETRISATION</head><p>We reparametrise the model in such a way that the i.i.d. limiting cases are highlighted, by changing variables to φ = (φ 1 , φ 2 , φ 3 ) and ψ = (ψ 1 , ψ 2 ) defined as</p><formula xml:id="formula_19">φ(θ) = q−p p+q 1 − p − q f 0 − f 1 , ψ(θ) = qf0+pf1 p+q f0−f1 f0−f1 .</formula><p>Here we have separated the scalar parameters φ from the vector parameters ψ. Defining</p><formula xml:id="formula_20">r(φ) = 1 4 (1 − φ 2 1 )φ 2 φ 2 3 ,<label>(3)</label></formula><p>it follows from the discussion in Section II that the data Y is close to i.i.d. exactly when r(φ) ≈ 0. [This is of course true also of other combinations of the components of φ, but as equation ( <ref type="formula">9</ref>) will show, r(φ) is the appropriate combination measuring the "distance" to the i.i.d. case.] Define</p><formula xml:id="formula_21">Φ = Φ(δ, ǫ, ζ) = {(φ(θ), ψ(θ)) : θ ∈ Θ(δ, ǫ, ζ)}, Φ L = Φ L (δ, ǫ, ζ) = {(φ(θ), ψ(θ)) : θ ∈ Θ L (δ, ǫ, ζ)},</formula><p>and note that for (φ, ψ) ∈ Φ we have</p><formula xml:id="formula_22">− 1 − δ 1 + δ ≤ φ 1 ≤ 1 − δ 1 + δ , ǫ ≤ |φ 2 | ≤ 1 − 2δ, ζ ≤ φ 3 ≤ √ 2, |r(φ)| ≥ δǫζ 2 /4,<label>(4)</label></formula><p>while for (φ, ψ) ∈ Φ L we additionally have</p><formula xml:id="formula_23">|φ 2 | ≤ 1 − L.<label>(5)</label></formula><p>Remark 2. When K = 2, in view of identifiability issues discussed in the next subsection, ψ 2 is not needed in the parametrisation, since we may universally make the choice</p><formula xml:id="formula_24">ψ 2 = 1 √ 2 , − 1 √ 2 . Remark 3. The parametrisation θ → (φ, ψ) is invertible: we calculate p = 1 2 (1 − φ 2 )(1 − φ 1 ), q = 1 2 (1 − φ 2 )(1 + φ 1 ), f 0 = ψ 1 − 1 2 φ 1 φ 3 ψ 2 + 1 2 φ 3 ψ 2 , f 1 = ψ 1 − 1 2 φ 1 φ 3 ψ 2 − 1 2 φ 3 ψ 2 . Remark 4. Suppose ψ 1 is a probability density function with respect to counting measure on {1, . . . , K}, ψ 2 is a function satisfying ψ 2 = 1 and k ψ 2 (k) = 0, and φ satisfies |φ 1 | ≤ 1, |φ 2 | ≤ 1 and φ 3 ≥ 0. Then (φ, ψ) lies in Φ(δ, ǫ, ζ) if and only if q p + q g ⊗ f 0 ⊗ g + p p + q h ⊗ f 1 ⊗ h,<label>(8)</label></formula><p>where g = (1 − p)f 0 + pf 1 and h = qf 0 + (1 − q)f 1 , and where ⊗ denotes the tensor product so that</p><formula xml:id="formula_25">(f ⊗g ⊗h)(a, b, c) = f (a)g(b)h(c), (a, b, c) ∈ {1, . . . , K} 3 .</formula><p>In the (φ, ψ) parametrisation, writing just p</p><p>φ,ψ for p</p><p>θ(φ,ψ) in a slight abuse of notation, we have</p><formula xml:id="formula_28">p (3) φ,ψ = ψ 1 ⊗ ψ 1 ⊗ ψ 1 + r(φ) ψ 2 ⊗ ψ 2 ⊗ ψ 1 + ψ 1 ⊗ ψ 2 ⊗ ψ 2 + φ 2 r(φ)ψ 2 ⊗ ψ 1 ⊗ ψ 2 − φ 1 φ 2 φ 3 r(φ)ψ 2 ⊗ ψ 2 ⊗ ψ 2 , (9)</formula><p>where we recall the notation r(φ) = 1 4 (1 − φ 2 1 )φ 2 φ 2 3 . We define a statistical distance ρ directly on the parameter space Φ which is equivalent to the Euclidean distance between the densities p (3) φ,ψ and p (3) φ, ψ. The function ρ is not a true metric because it may not satisfy the triangle inequality and because, due to the identifiability issues reflected by the appearance of factors of sgn( ψ 2 , ψ2 ) in its definition, we may have ρ(φ, ψ; φ, ψ) = 0 with (φ, ψ) = ( φ, ψ). Here •, • denotes the Euclidean inner product on R K , f, g</p><formula xml:id="formula_29">= K i=1 f (k)g(k). Proposition 1. For r as in equation (3) define m by m(φ) = (r(φ), φ 2 r(φ), φ 1 φ 2 φ 3 r(φ)), (<label>10</label></formula><formula xml:id="formula_30">)</formula><formula xml:id="formula_31">and define ρ(φ, ψ; φ, ψ) = max{|m 1 (φ) − m 1 ( φ)|, |m 2 (φ) − m 2 ( φ)|, |m 3 (φ) − sgn( ψ 2 , ψ2 ) • m 3 ( φ)|, ψ 1 − ψ1 , (|m 1 (φ)| ∨ |m 1 ( φ)|) • ψ 2 − sgn( ψ 2 , ψ2 ) • ψ2 }. (<label>11</label></formula><formula xml:id="formula_32">) There exist constants c 1 , c 2 &gt; 0 (which depend on K) such that for all (φ, ψ), ( φ, ψ) ∈ δ,ǫ,ζ Φ(δ, ǫ, ζ) we have c 1 ρ(φ, ψ; φ, ψ) ≤ p (3) φ,ψ − p (3) φ, ψ ≤ c 2 ρ(φ, ψ; φ, ψ).</formula><p>Optimal estimation rates can be obtained if we adequately understand the Kullback-Leibler divergence between distributions with different parameters. The Kullback-Leibler divergence between P (n) θ(φ,ψ) and P (n) θ( φ, ψ) can be related to the statistical distance ρ(φ, ψ; φ, ψ) in a neighbourhood of the independent subcase. Proposition 2. Assume there exists c ∈ (0, 1) such that min(f 0 , f 1 , f0 , f1 ) ≥ c. There exist constants C, ǫ 0 &gt; 0 depending only on c such that if max(|φ 2 |, | φ2 |) ≤ ǫ 0 , then with ρ as in equation <ref type="bibr" target="#b10">(11)</ref>,</p><formula xml:id="formula_33">KL(P (n) θ(φ,ψ) , P (n) θ( φ, ψ) ) ≤ Cnρ(φ, ψ; φ, ψ) 2 .</formula><p>We note that only the lower bound on p</p><formula xml:id="formula_34">(3) φ,ψ − p (3) φ, ψ in Proposition 1 is used in our paper (it is used in proving Theorem 2). The upper bound on p (3) φ,ψ − p (3)</formula><p>φ, ψ is still of interest as it establishes the tightness (up to constants) of the corresponding lower bound, thereby proving the equivalence between p</p><formula xml:id="formula_35">(3) φ,ψ −p (3)</formula><p>φ, ψ and ρ(φ, ψ; φ, ψ) and showing that ρ is an adequate statistical metric for this problem. Furthermore, in combination with Proposition 2, Pinsker's inequality, and the fact that all norms on the set {1, . . . , K} 3 are equivalent, it shows that whenever max(</p><formula xml:id="formula_36">|φ 2 |, | φ2 |) is small enough, KL(P (n) θ(φ,ψ) , P (n) θ( φ, ψ) ) ≤ C ′ n p (3) φ,ψ − p (3) φ, ψ 2 ≤ C ′′ nKL(P<label>(3)</label></formula><p>θ(φ,ψ) , P</p><p>θ( φ, ψ) ), for constants C ′ , C ′′ &gt; 0, once again highlighting the prominent role of the law of 3 consecutive observations in HMM modelling, and illustrating that optimal estimators (up to numerical constants) can be built solely on the basis of the empirical distribution of blocks of 3 consecutive observations. This shows that as long as the chain Y is not "too dependent", it behaves almost as if we had observed i.i.d. blocks of 3 consecutive observations (in which case we would have that KL(P</p><formula xml:id="formula_38">(n) θ(φ,ψ) , P (n) θ( φ, ψ) ) = (n/3)KL(P (3) θ(φ,ψ) , P<label>(3)</label></formula><p>θ( φ, ψ) ), for all n divisible by 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. UPPER BOUNDS</head><p>We obtain the following upper bounds for estimating φ and ψ. Since we are studying limits as the quantities of interest become small, the relative risk may be of as much interest as the absolute risk, and we provide bounds for both quantities. The bounds demonstrate that learning model parameters is possible in the regime where n is large enough in relation to δ, ǫ and ζ. Observe firstly that estimation of p (3) is possible at a parametric rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1. Define the empirical estimator</head><formula xml:id="formula_39">p(3) n : {1, . . . , K} n → [0, 1] by p(3) n (a, b, c) = 1 n n−2 i=1 ½{Y i = a, Y i+1 = b, Y i+2 = c}. (12)</formula><p>Then for some constant C = C(K, L) and any x ≥ 1 sup</p><formula xml:id="formula_40">(φ,ψ)∈ΦL(δ,ǫ,ζ) P (φ,ψ) ( p(3) − p (3) ≥ Cx/ √ n) ≤ e −x 2 .</formula><p>Theorem 2. Assume Φ L is non-empty and let φ, ψ be any measurable functions satisfying, for p(3) n as in equation <ref type="bibr" target="#b11">(12)</ref>, p</p><formula xml:id="formula_41">n ≤ 2 inf ( φ, ψ)∈ΦL p<label>(3) φ, ψ − p(3)</label></formula><p>There exists a constant C = C(K, L) &gt; 0 such that the following hold.</p><formula xml:id="formula_43">1) Assume 1 ≤ x 2 ≤ nδ 2 ǫ 4 ζ 6 . Then sup (φ,ψ)∈ΦL(δ,ǫ,ζ) P φ,ψ 1 − φ2 1 1 − φ 2 1 − 1 ≥ √ 2Cx √ nδǫ 2 ζ 3 ≤ sup (φ,ψ)∈ΦL(δ,ǫ,ζ) P φ,ψ | φ1 − φ 1 | ∧ | φ1 + φ 1 | ≥ Cx √ nǫ 2 ζ 3 ≤ e −x 2 . 2) Assume 1 ≤ x 2 ≤ nδ 2 ǫ 2 ζ 4 . Then sup (φ,ψ)∈ΦL(δ,ǫ,ζ) P φ,ψ φ2 φ 2 − 1 ≥ Cx √ nδǫ 2 ζ 2 ≤ sup (φ,ψ)∈ΦL(δ,ǫ,ζ) P φ,ψ | φ2 − φ 2 | ≥ Cx √ nδǫζ 2 ≤ e −x 2 . 3) Assume 1 ≤ x 2 ≤ nδ 2 ǫ 4 ζ 6 . Then sup (φ,ψ)∈ΦL(δ,ǫ,ζ) P φ,ψ φ3 φ 3 − 1 ≥ Cx √ nδǫ 2 ζ 3 ≤ sup (φ,ψ)∈ΦL(δ,ǫ,ζ) P φ,ψ | φ3 − φ 3 | ≥ Cx √ nδǫ 2 ζ 2 ≤ e −x 2 . 4) Assume 1 ≤ x 2 ≤ n. Then sup (φ,ψ)∈ΦL(δ,ǫ,ζ) P φ,ψ ψ1 − ψ 1 ≥ Cx √ n ≤ e −x 2 . 5) Assume 1 ≤ x 2 ≤ nδ 2 ǫ 2 ζ 4 and K &gt; 2. Then sup (φ,ψ)∈ΦL(δ,ǫ,ζ) P φ,ψ ψ2 − ψ 2 ∧ ψ2 + ψ 2 ≥ Cx √ nδǫζ 2 ≤ e −x 2 .</formula><p>Recall that estimating ψ 2 is unnecessary when K = 2 (see Remark 2). Note that the absolute loss in each case is bounded, and one can deduce that the bounds for φ 2 and for ψ hold without an upper bound on x, with e −x 2 on the right replaced by zero (for C large enough).</p><p>The estimator proposed in Theorem 2 is an approximate solution to an optimisation problem which (by taking squares) is a multivariate polynomial function with 2K unknowns. There are several methods in the literature about finding the global optimum of multivariate polynomials, see e.g. <ref type="bibr" target="#b40">[40]</ref>. As mentioned in Section VII, the issue of finding computationally efficient estimation methods requires further investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. LOWER BOUNDS</head><p>We prove lower bounds, matching the previous upper bounds in a suitable regime and demonstrating the impossibility of learning model parameters when n is not large enough in relation to δ, ǫ and ζ. </p><formula xml:id="formula_44">P φ,ψ | φ1 − φ 1 | ∧ | φ1 + φ 1 | ≥ c √ nǫ 2 ζ 3 ≥ inf φ1 sup (φ,ψ)∈ΦL(δ,ǫ,ζ) P φ,ψ 1 − φ2 1 1 − φ 2 1 − 1 ≥ √ 2c √ nδǫ 2 ζ 3 ≥ 1/4. 2) Assume nδ 2 ǫ 4 ζ 4 ≥ 1. Then inf φ2 sup (φ,ψ)∈ΦL(δ,ǫ,ζ) P φ,ψ | φ2 − φ 2 | ≥ c √ nδǫζ 2 ≥ inf φ2 sup (φ,ψ)∈ΦL(δ,ǫ,ζ) P φ,ψ φ2 φ 2 − 1 ≥ c √ nδǫ 2 ζ 2 ≥ 1/4. 3) Assume nδ 2 ǫ 4 ζ 6 ≥ 1. Then inf φ3 sup (φ,ψ)∈ΦL(δ,ǫ,ζ) P φ,ψ | φ3 − φ 3 | ≥ c √ nδǫ 2 ζ 2 ≥ inf φ3 sup (φ,ψ)∈ΦL(δ,ǫ,ζ) P φ,ψ φ3 φ 3 − 1 ≥ c √ nδǫ 2 ζ 3 ≥ 1/4.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4)</head><p>For any n, δ, ǫ and ζ,</p><formula xml:id="formula_45">inf ψ1 sup (φ,ψ)∈ΦL(δ,ǫ,ζ) P φ,ψ ψ1 − ψ 1 ≥ c √ n ≥ 1/4. 5) Assume nδ 2 ǫ 2 ζ 4 ≥ 1 and K &gt; 2. Then inf ψ2 sup (φ,ψ)∈ΦL(δ,ǫ,ζ) P φ,ψ ψ2 − ψ 2 ∧ ψ2 + ψ 2 ≥ c √ nδǫζ 2 ≥ 1/4.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS AND FUTURE DIRECTIONS</head><p>In this work we have quantified the impact on learnability of approaching the i.i.d. boundary within the set of parameters of a hidden Markov model. The limiting cases occur when one hidden state is absorbing, when the underlying Markov chain becomes a sequence of independent variables, or when the emission distributions are equal. We have proved both upper and lower bounds for the estimation rates of the parameters in a hidden Markov model with two hidden states and finitely many possible outcomes. Our results characterize the frontier in the parameter space between learnable and unlearnable parameters and quantify how large the sample has to be in order to get estimators with prescribed error with high probability.</p><p>Some tricky regions of the parameter space are not fully captured in the upper and lower bounds. Specifically, the condition on n in the lower bound for estimating φ 2 differs by a factor of ǫ 2 from the corresponding condition in the upper bound. Also, in the upper bound for φ 1 , we do not describe the precise estimation behaviour in the region nδ 2 ǫ 4 ζ 6 &lt; x 2 &lt; nǫ 4 ζ 6 : in this range we can obtain something by applying the bound with y 2 = min(x 2 , nδ 2 ǫ 4 ζ 6 ) but we cannot expect that this gives the correct dependence on x.</p><p>[There is no issue in the region x 2 ≥ nǫ 4 ζ 6 since we may replace the bound e −x 2 with zero, similarly to the comment after the theorem regarding φ 2 and ψ.] A similar gap exists for estimating φ 3 . The reason for those gaps is that the inversion formulas given in Proposition 3 are only local; finding global inversion formulas, which would allow the remaining regions to be covered, remains an open problem. Our results already work for a wide range of parameters, and extending to the few remaining cases is an interesting issue for future research.</p><p>Regarding the upper bounds, our proof method relies on the fact that the two steps of estimating p (3) and of estimating, given p (3) , the HMM parameters themselves, decouple. This is because, with good mixing properties for the Markov chain, estimation of p (3) can be done uniformly at a rate not depending on the HMM parameters (Lemma 1). When the spectral gap is small the underlying Markov chain mixes slowly, spending long periods remaining in whichever of the two states it is in, so that estimation of p (3) becomes hard for parameters for which there is small spectral gap. These are not the same parameters for which recovering the HMM parameters given p (3) is most difficult, and so to obtain accurate rates without a spectral gap requires carefully addressing the two steps simultaneously, which is beyond the scope of the paper (we could obtain a suboptimal rate using the current methods just with careful tracking of the spectral gap, since it is lower bounded by 1 − 2δ, but upper and lower bounds obtained in this way mismatch by a factor of δ). Note the above arguments explain the requirement for a spectral gap, not an absolute spectral gap; we believe our results will in fact hold in the near-periodic case when the spectral gap is close to 2 and the absolute spectral gap is close to zero, but this would require some extra technical calculations in the proof of Lemma 1.</p><p>We believe similar results hold with more than two hidden states and with arbitrary nonparametric emission distributions. Investigation of the fundamental limits for learning more general HMMs and misspecified modelling will be the object of further work. Developments of our findings for clustering, multiple testing and sparse settings will also be the object of further work, and all will depend fundamentally on the results obtained here.</p><p>We analysed a minimum distance estimator for theoretical convenience, and it is possible that the same upper bounds hold for more practical estimators (for example empirical least squares estimators and tensor-based methods). On the practical side, usual estimation algorithms can be expected to exhibit bad computational behaviour when the unknown true parameters lie near the learning frontier, as shown in <ref type="bibr" target="#b13">[14]</ref> for the EM algorithm. We have not tackled this issue here and we believe it merits substantive investigation, both in building robust algorithms and in detecting the poor performance in the problematic region. This last question is interesting both from a practical and a theoretical point of view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. PROOFS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proof of Proposition 1</head><p>Recall the definition <ref type="bibr" target="#b9">(10)</ref> of m as</p><formula xml:id="formula_46">m(φ) = (r(φ), φ 2 r(φ), φ 1 φ 2 φ 3 r(φ)), r(φ) = 1 4 (1 − φ 2 1 )φ 2 φ 2 3 .</formula><p>We write m = m( φ), and we write ψ ijk for ψ i ⊗ ψ j ⊗ ψ k and ψijk for ψi ⊗ ψj ⊗ ψk . Then from equation ( <ref type="formula">9</ref>) we have</p><formula xml:id="formula_47">p (3) φ,ψ − p (3) φ, ψ = (ψ 111 − ψ111 ) + {m 1 (ψ 221 + ψ 122 ) − m1 ( ψ221 + ψ122 )} + {m 2 ψ 212 − m2 ψ212 } − {m 3 ψ 222 − m3 ψ222 }. (<label>13</label></formula><formula xml:id="formula_48">) Recalling that •, • denotes the Euclidean inner product on R K , we have ψ 1 , 1 = 1, ψ 2 , 1 = 0, ψ 2 = 1 and 1 = K 1/2 . Let •,</formula><p>• also denote the Euclidean inner product on R K×K×K , wherein for functions f i , fi : {1, . . . , K} → R, i ≤ 3 we have</p><formula xml:id="formula_49">f 1 ⊗ f 2 ⊗ f 3 , f1 ⊗ f2 ⊗ f3 = f 1 , f1 f 2 , f2 f 3 , f3 . a) Lower bounding p (3) φ,ψ − p (3)</formula><p>φ, ψ : For any function f : {1, . . . , K} → R, we have</p><formula xml:id="formula_50">p (3) φ,ψ −p (3) φ, ψ , f ⊗1⊗1 = ψ 111 − ψ111 , f ⊗1⊗1 = ψ 1 − ψ1 , f .</formula><p>Then</p><formula xml:id="formula_51">ψ 1 − ψ1 = sup f =1 | ψ 1 − ψ1 , f | = sup f =1 | p (3) φ,ψ − p (3) φ, ψ , f ⊗ 1 ⊗ 1 | ≤ p (3) φ,ψ − p (3) φ, ψ sup f =1 f ⊗ 1 ⊗ 1 = K p (3) φ,ψ − p (3) φ, ψ ,<label>(14)</label></formula><p>and similarly, p</p><p>φ,ψ − p</p><formula xml:id="formula_53">(3) φ, ψ , 1 ⊗ f ⊗ f = ψ 111 − ψ111 , 1 ⊗ f ⊗ f + m 1 ψ 122 − m1 ψ122 , 1 ⊗ f ⊗ f = ψ 1 − ψ1 , f 2 + m 1 ψ 2 , f 2 − m1 ψ2 , f 2 . (<label>15</label></formula><formula xml:id="formula_54">)</formula><p>Choosing f = ψ 2 + sgn( ψ 2 , ψ2 ) • ψ2 (with the convention that sgn(0) = +1), we observe that</p><formula xml:id="formula_55">ψ 2 , f = 1 + | ψ 2 , ψ2 | = sgn( ψ 2 , ψ2 ) • ψ2 , f .</formula><p>In particular we note that</p><formula xml:id="formula_56">ψ 2 , f 2 = ψ2 , f 2 = (1 + | ψ 2 , ψ2 |) 2 ≥ 1. Since also f 2 = 2 + 2| ψ 2 , ψ2 | ≤ 4,</formula><p>returning to (15) we observe that</p><formula xml:id="formula_57">|m 1 − m1 | ≤ f 2 ψ 1 − ψ1 2 + 1 ⊗ f ⊗ f p (3) φ,ψ − p (3) φ, ψ ≤ 4 ψ 1 − ψ1 2 + 4K 1/2 p (3) φ,ψ − p (3) φ, ψ ≤ 4(K 7/2 + K 1/2 ) p<label>(3)</label></formula><formula xml:id="formula_58">φ,ψ − p (3) φ, ψ ,<label>(16)</label></formula><p>where for the last line we have used equation ( <ref type="formula" target="#formula_51">14</ref>) and the fact that p</p><p>(3)</p><formula xml:id="formula_59">φ,ψ − p (3) φ, ψ 2 ≤ K 3 .</formula><p>We continue by considering the expression f ⊗ 1 ⊗ f , for which we have</p><formula xml:id="formula_60">p (3) φ,ψ − p (3) φ, ψ , f ⊗ 1 ⊗ f = ψ 111 − ψ111 , f ⊗ 1 ⊗ f + m 2 ψ 212 − m2 ψ212 , f ⊗ 1 ⊗ f = ψ 1 − ψ1 , f 2 + m 2 ψ 2 , f 2 − m2 ψ2 , f 2</formula><p>Recognising symmetry with equation ( <ref type="formula" target="#formula_53">15</ref>), we again choose</p><formula xml:id="formula_61">f = ψ 2 + sgn( ψ 2 , ψ2 ) • ψ2 to obtain |m 2 − m2 | ≤ 4(K 7/2 + K 1/2 ) p (3) φ,ψ − p (3) φ, ψ .<label>(17)</label></formula><p>Finally, considering the expression f ⊗ f ⊗ f , we observe that p</p><formula xml:id="formula_62">(3) φ,ψ − p (3) φ, ψ , f ⊗ f ⊗ f is equal to ψ 111 − ψ111 , f ⊗ f ⊗ f + m 1 (ψ 221 + ψ 122 ) − m1 ( ψ221 + ψ122 ), f ⊗ f ⊗ f + m 2 ψ 212 − m2 ψ212 , f ⊗ f ⊗ f − m 3 ψ 222 − m3 ψ222 , f ⊗ f ⊗ f .</formula><p>In other words, p</p><p>φ,ψ − p</p><formula xml:id="formula_64">(3) φ, ψ, f ⊗ f ⊗ f equals ψ 1 − ψ1 , f 3 + 2 m 1 ψ 2 , f 2 ψ 1 , f − m1 ψ2 , f 2 ψ1 , f + m 2 ψ 2 , f 2 ψ 1 , f − m2 ψ2 , f 2 ψ1 , f − m 3 ψ 2 , f 3 − m3 ψ2 , f 3 .</formula><p>Once more choosing f = ψ 2 + sgn( ψ 2 , ψ2 ) • ψ2 , we obtain (recall that by construction f ≤ 2, 1 ≤ ψ 2 , f 2 = ψ2 , f 2 ≤ 4, and also sgn(</p><formula xml:id="formula_65">ψ 2 , ψ2 ) ψ 2 , f = ψ2 , f ) |m 3 − sgn( ψ 2 , ψ2 ) • m3 | ≤ 8 ψ 1 − ψ1 3 + 8 p (3) φ,ψ − p (3) φ,ψ + 8 m 1 ψ 1 , f − m1 ψ1 , f + 4 m 2 ψ 1 , f − m2 ψ1 , f .</formula><p>For some constant C = C(K) we have</p><formula xml:id="formula_66">m 1 ψ 1 , f − m1 ψ1 , f ≤ | ψ 1 , f | m 1 − m1 + | m1 || ψ 1 − ψ1 , f | ≤ 2 ψ 1 m 1 − m1 + 2| m1 | ψ 1 − ψ1 ≤ C p (3) φ,ψ − p (3) φ, ψ ,</formula><p>where for the last line we have used equations ( <ref type="formula" target="#formula_51">14</ref>) and ( <ref type="formula" target="#formula_58">16</ref>) and that</p><formula xml:id="formula_67">ψ 1 ≤ K 1/2 and | m1 | ≤ φ2 3 /4 ≤ f0 − f1 2 /4 ≤ K/4.</formula><p>Similarly, using equation ( <ref type="formula" target="#formula_61">17</ref>) and the fact that | m2 | is suitably bounded, we have for some</p><formula xml:id="formula_68">C = C(K) m 2 ψ 1 , f − m2 ψ1 , f ≤ C p (3) φ,ψ − p (3) φ, ψ .</formula><p>We deduce for some different constant C = C(K) that</p><formula xml:id="formula_69">|m 3 − sgn( ψ 2 , ψ2 ) • m3 | ≤ C p (3) φ,ψ − p (3) φ, ψ .<label>(18)</label></formula><p>Finally, for ψ 2 we show that for some C we have</p><formula xml:id="formula_70">(|m 1 | ∨ | m1 |) ψ 2 − sgn( ψ 2 , ψ2 ) • ψ2 ≤ C p (3) φ,ψ − p (3) φ, ψ .<label>(19</label></formula><p>) If ψ 2 = ψ2 there is nothing to prove, so we assume without loss of generality that ψ 2 = ψ2 . Also assume that |m 1 | ≥ | m1 |, the final bound then following by symmetry. Returning to equation <ref type="bibr" target="#b14">(15)</ref> with f to be chosen, we see that</p><formula xml:id="formula_71">m 1 ψ 2 , f 2 − ψ2 , f 2 = ψ 1 − ψ1 , f 2 − p (3) φ,ψ − p (3) φ, ψ , 1 ⊗ f ⊗ f − ψ2 , f 2 (m 1 − m1 ). Since ψ 2 , f 2 − ψ2 , f 2 = ψ 2 − ψ2 , f ψ 2 + ψ2 , f we obtain |m 1 ψ 2 − ψ2 , f ψ 2 + ψ2 , f | ≤ f 2 ψ 1 − ψ1 2 + K f 2 p (3) φ,ψ − p (3) φ, ψ + |m 1 − m1 | ψ2 , f 2 . (<label>20</label></formula><formula xml:id="formula_72">)</formula><p>Observe that ψ 2 + ψ2 is orthogonal to ψ 2 − ψ2 (this arises from the fact that ψ 2 and ψ2 have unit norms) and choose</p><formula xml:id="formula_73">f = ψ 2 + ψ2 ψ 2 + ψ2 + ψ 2 − ψ2 ψ 2 − ψ2 ; note that ψ 2 − ψ2 , f ψ 2 + ψ2 , f = ψ 2 − ψ2 ψ 2 + ψ2 .</formula><p>Since also f ≤ 2 and | ψ2 , f | ≤ 2, continuing from equation ( <ref type="formula" target="#formula_71">20</ref>) and using equations ( <ref type="formula" target="#formula_51">14</ref>) and <ref type="bibr" target="#b15">(16)</ref> we see that for a constant C = C(K)</p><formula xml:id="formula_74">|m 1 | ψ 2 − ψ2 ψ 2 + ψ2 ≤ 4 ψ 1 − ψ1 2 + 4K p (3) φ,ψ − p (3) φ, ψ + 4|m 1 − m1 | ≤ 2C p (3) φ,ψ − p (3) φ, ψ .</formula><p>Observing that</p><formula xml:id="formula_75">ψ 2 − ψ2 2 ψ 2 + ψ2 2 = ψ 2 − sgn( ψ 2 , ψ2 ) • ψ2 2 ψ 2 + sgn( ψ 2 , ψ2 ) • ψ2 2 = ψ 2 − sgn( ψ 2 , ψ2 ) • ψ2 2 2 + 2| ψ 2 , ψ2 | ≥ 2 ψ 2 − sgn( ψ 2 , ψ2 ) • ψ2 2 ,</formula><p>and recalling we assumed that</p><formula xml:id="formula_76">|m 1 | ≥ | m1 |, equation<label>(19)</label></formula><p>follows.</p><p>The proof that p</p><formula xml:id="formula_77">(3) φ,ψ − p (3) φ, ψ</formula><p>is lower bounded up to a constant by ρ(φ, ψ; φ, ψ) follows by combining equations ( <ref type="formula" target="#formula_51">14</ref>) and ( <ref type="formula" target="#formula_58">16</ref>)-( <ref type="formula" target="#formula_70">19</ref>) b) Upper bounding p</p><formula xml:id="formula_78">(3) φ,ψ − p<label>(3)</label></formula><p>φ, ψ : From equation ( <ref type="formula" target="#formula_47">13</ref>), p</p><p>φ,ψ − p</p><formula xml:id="formula_80">(3) φ, ψ ≤ ψ 111 − ψ111 + |m 1 − m1 | ψ 221 + ψ 122 + | m1 | ψ 221 − ψ221 + | m1 | ψ 122 − ψ122 + |m 2 − m2 | ψ 212 + | m2 | ψ 212 − ψ212 + |m 3 − m3 | ψ 222 + | m3 | ψ 222 − ψ222 .<label>(21)</label></formula><p>Note that the bound remains valid if we replace the final two terms by</p><formula xml:id="formula_81">|m 3 + m3 | ψ 222 + | m3 | ψ 222 + ψ222 ;</formula><p>we focus on the case where sgn( ψ 2 , ψ2 ) = +1 for which the original decomposition yields suitable bounds, but the proof in the other case is similar using the alternative decomposition. As used already in proving the lower bound on p</p><formula xml:id="formula_82">(3) φ,ψ − p (3) φ, ψ , we note that max( ψ 221 , ψ 122 , | m1 |, ψ 212 , | m2 |, ψ 222 , | m3 |) ≤ C,</formula><p>for some C = C(K). To conclude the proof it thus suffices to bound the tensor product terms ψ ijk − ψijk in terms of the differences ψ 1 − ψ1 , ψ 2 − ψ2 . First we decompose</p><formula xml:id="formula_83">ψ 1 ⊗ψ 1 ⊗ψ 1 − ψ1 ⊗ ψ1 ⊗ ψ1 ≤ ψ 1 ⊗ψ 1 ⊗ψ 1 − ψ1 ⊗ψ 1 ⊗ψ 1 + ψ1 ⊗ψ 1 ⊗ψ 1 − ψ1 ⊗ ψ1 ⊗ψ 1 + ψ1 ⊗ ψ1 ⊗ψ 1 − ψ1 ⊗ ψ1 ⊗ ψ1 , so that ψ 111 − ψ111 ≤ ψ 1 − ψ1 ( ψ 1 2 + ψ 1 ψ1 + ψ1 2 ) ≤ 3K ψ 1 − ψ1 .<label>(22)</label></formula><p>We also note, recalling that ψ 2 and ψ2 have unit norms, that</p><formula xml:id="formula_84">ψ 221 − ψ221 2 = ψ 221 2 + ψ221 2 − 2 ψ 221 , ψ221 = ψ 1 2 + ψ1 2 − 2 ψ 2 , ψ2 2 ψ 1 , ψ1 = ψ 1 − ψ1 2 + 2 ψ 1 , ψ1 1 − ψ 2 , ψ2 2 ≤ ψ 1 − ψ1 2 + 2 ψ 1 ψ1 |1 − ψ 2 , ψ2 2 |.</formula><p>Observe that</p><formula xml:id="formula_85">ψ 2 − ψ2 2 = 2(1 − ψ 2 , ψ2 ),<label>(23)</label></formula><p>and hence</p><formula xml:id="formula_86">1 − ψ 2 , ψ2 2 = 1 + ψ 2 , ψ2 1 − ψ 2 , ψ2 ≤ 2|1 − ψ 2 , ψ2 | = ψ 2 − ψ2 2 .</formula><p>We deduce that</p><formula xml:id="formula_87">ψ 221 − ψ221 2 ≤ ψ 1 − ψ1 2 + 2 ψ 1 ψ1 ψ 2 − ψ2 2 ≤ ψ 1 − ψ1 2 + 2K ψ 2 − ψ2 2 . (<label>24</label></formula><formula xml:id="formula_88">)</formula><p>By symmetry, the same bound holds for ψ 122 − ψ122 and for ψ 212 − ψ212 . Furthermore ψ 222 = 1, and using <ref type="bibr" target="#b22">(23)</ref>,</p><formula xml:id="formula_89">ψ 222 − ψ222 2 = ψ 222 2 + ψ222 2 − 2 ψ 222 , ψ222 = 2 − 2 ψ 2 , ψ2 3 = 2 1 − ψ 2 , ψ2 1 + ψ 2 , ψ2 + ψ 2 , ψ2 2 ≤ 3 ψ 2 − ψ2 2 . (<label>25</label></formula><formula xml:id="formula_90">)</formula><p>The claim follows from inserting equations ( <ref type="formula" target="#formula_83">22</ref>), ( <ref type="formula" target="#formula_87">24</ref>) and ( <ref type="formula" target="#formula_89">25</ref>) into equation <ref type="bibr" target="#b20">(21)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Proposition 2</head><p>Write X 1:k and Y 1:k for the vectors (X 1 , . . . , X k ) and (Y 1 , . . . , Y k ) respectively, and recall that P (n) θ denotes the law of Y 1:n for parameter θ. Without loss of generality we may assume that sgn( ψ 2 , ψ2 ) = +1, since one may substitute φ′ = (− φ1 , φ2 , φ3 ) and ψ′ = ( ψ1 , − ψ2 ) for φ and ψ and obtain</p><formula xml:id="formula_91">P (n) θ = P (n) θ′ , hence KL(P (n) θ ; P (n) θ ) = KL(P (n) θ ; P (n) θ′ ), but sgn( ψ 2 , ψ′ 2 ) = −sgn( ψ 2 , ψ2 ). Recall that KL(P ; Q) is upper bounded by the chi-square distance χ 2 (P, Q) = E Q [(dP/dQ − 1) 2 ] (e.g. [39, Lemma 2.7]). Then using that P θ (Y 1 = •) = ψ 1 (•) and P θ(Y 1 = •) = ψ1 (•) ≥ c, we have KL(P (1) θ ; P (1) θ ) ≤ y∈Y [P θ (Y 1 = y) − P θ (Y 1 = y)] 2 P θ(Y 1 = y) ≤ ψ 1 − ψ1 2 c .<label>(26)</label></formula><p>This yields the case n = 1 since the definition <ref type="bibr" target="#b10">(11)</ref> implies that ρ(φ, ψ; φ, ψ) ≥ ψ 1 − ψ1 2 . Now assume that n ≥ 2. By the chain rule for relative divergence (used inductively), we have</p><formula xml:id="formula_92">KL(P (n) θ ; P (n) θ ) = KL(P (1) θ ; P (1) θ ) + n−1 k=1 E θ [KL(P θ (Y k+1 ∈ • | Y 1:k ); P θ(Y k+1 ∈ • | Y 1:k ))].<label>(27)</label></formula><p>The first term was addressed above, and we now consider the remaining terms. Again bounding the KL divergence by the chi-square distance, we have</p><formula xml:id="formula_93">KL(P θ (Y k+1 ∈ • | Y 1:k ); P θ (Y k+1 ∈ • | Y 1:k )) ≤ y∈Y [P θ (Y k+1 = y | Y 1:k ) − P θ(Y k+1 = y | Y 1:k )] 2 P θ(Y k+1 = y | Y 1:k ) ≤ P θ (Y k+1 = • | Y 1:k ) − P θ (Y k+1 = • | Y 1:k ) 2 min y∈Y P θ(Y k+1 = y | Y 1:k ) . But, for any k ≥ 1, noting that P θ (Y k+1 = y | Y 1:k , X 1:k+1 = x) = f x k+1 (y), P θ (Y k+1 = y | Y 1:k ) = x∈{0,1} k+1 f x k+1 (y)P θ (X 1:k+1 = x | Y 1:k ) = x∈{0,1} f x (y)P θ (X k+1 = x | Y 1:k ),<label>(28)</label></formula><p>where we have used that Y k+1 | (Y 1:k , X 1:k+1 ) has the same law as Y k+1 | X k+1 . Therefore when min( f0 , f1 ) ≥ c we must have, for all Y 1:k and all k ≥ 1,</p><formula xml:id="formula_94">P θ(Y k+1 = y | Y 1:k ) ≥ c x∈{0,1} P θ(X k+1 = x | Y 1:k ) = c.</formula><p>Hence we have established that, for all Y 1:k and all k ≥ 1,</p><formula xml:id="formula_95">KL(P θ (Y k+1 ∈ • | Y 1:k ); P θ (Y k+1 ∈ • | Y 1:k )) ≤ P θ (Y k+1 = • | Y 1:k ) − P θ(Y k+1 = • | Y 1:k ) 2 c .<label>(29)</label></formula><p>Let us now rewrite P θ (Y k+1 = y | Y 1:k ) in the parametrisation (φ, ψ). For convenience we introduce the notation P k (x) := P θ (X k+1 = x | Y 1:k ) for the prediction filters, and we similarly write Pk (x) := P θ(X k+1 = x | Y 1:k ). By equation <ref type="bibr" target="#b28">(28)</ref> and Remark 3,</p><formula xml:id="formula_96">P θ (Y k+1 = y | Y 1:k ) = f 0 (y)P k (0) + f 1 (y)P k (1) = ψ 1 (y) − 1 2 φ 1 φ 3 ψ 2 (y) + 1 2 φ 3 ψ 2 (y) P k (0) + ψ 1 (y) − 1 2 φ 1 φ 3 ψ 2 (y) − 1 2 φ 3 ψ 2 (y) P k (1) = ψ 1 (y) + 1 2 P k (0) − P k (1) − φ 1 φ 3 ψ 2 (y). For k ≥ 1, define V k := φ 3 (P k (0) − P k (1) − φ 1 ) = φ 3 (1 − 2P k (1) − φ 1 ), Ṽk := φ3 (1 − 2 Pk (1) − φ1 ).</formula><p>Then combining <ref type="bibr" target="#b25">(26)</ref>, <ref type="bibr" target="#b27">(27)</ref> and <ref type="bibr" target="#b29">(29)</ref>, we obtain that KL(P</p><formula xml:id="formula_97">(n) θ ; P (n) θ ) is bounded above by ψ 1 − ψ1 2 c + 1 c n−1 k=1 E θ ψ 1 − ψ1 + 1 2 V k ψ 2 − 1 2 Ṽk ψ2 2</formula><p>which is in turn bounded above by</p><formula xml:id="formula_98">2n − 1 c ψ 1 − ψ1 2 + 1 2c n−1 k=1 E θ [ V k ψ 2 − Ṽk ψ2 2 ]</formula><p>so that in the end, using that ψ2 2 = 1,</p><formula xml:id="formula_99">KL(P (n) θ ; P (n) θ ) ≤ 2n − 1 c ψ 1 − ψ1 2 + ψ 2 − ψ2 2 c n−1 k=1 E θ [V 2 k ] + 1 c n−1 k=1 E θ [(V k − Ṽk ) 2 ].<label>(30)</label></formula><p>Let us now find an inductive formula for V k . Let us define</p><formula xml:id="formula_100">P k (x) := P θ (X k+1 = x | Y 1:k ). First we observe that for any k ≥ 2, since P θ (X k+1 = x | Y 1:k , X k = x ′ ) = P θ (X k+1 = x | X k = x ′ ), P k (x) = x ′ ∈{0,1} P θ (X k+1 = x | X k = x ′ )P θ (X k = x ′ | Y 1:k ) = x ′ ∈{0,1} Q x ′ ,x P θ (X k = x ′ | Y 1:k−1 , Y k ),</formula><p>and we further calculate</p><formula xml:id="formula_101">P θ (X k = x ′ | Y 1:k−1 , Y k = y k ) = P θ (X k = x ′ , Y k = y k | Y 1:k−1 ) P θ (Y k | Y 1:k−1 ) = f x ′ (y k )P θ (X k = x ′ | Y 1:k−1 ) x ′′ ∈{0,1} P θ (y k | Y 1:k−1 , X k = x ′′ )P θ (X k = x ′′ | Y 1:k−1 ) = f x ′ (y k )P k−1 (x ′ ) x ′′ ∈{0,1} f x ′′ (y k )P k−1 (x ′′ )</formula><p>.</p><p>Similarly, for k = 1,</p><formula xml:id="formula_102">P θ (X 2 = x | Y 1 = y 1 ) = P θ (X 2 = x, Y 1 = y 1 ) P θ (y 1 ) = x ′ ∈{0,1} P θ (X 2 = x, Y 1 = y 1 | X 1 = x ′ )P θ (X 1 = x ′ ) x ′ ∈{0,1} f x ′ (y 1 )P θ (X 1 = x ′ ) = x ′ ∈{0,1} f x ′ (y 1 )Q x ′ ,x P θ (X 1 = x ′ ) x ′ ∈{0,1} f x ′ (y 1 )P θ (X 1 = x ′ )</formula><p>.</p><p>To summarise, we have proved the recursive formula</p><formula xml:id="formula_103">P k (x) =    x ′ ∈{0,1} Q x ′ ,x f x ′ (Y k )P k−1 (x ′ ) x ′ ∈{0,1} f x ′ (Y k )P k−1 (x ′ ) if k ≥ 2, x ′ ∈{0,1} f x ′ (Y1)Q x ′ ,x P θ (X1=x ′ ) x ′ ∈{0,1} f x ′ (Y1)P θ (X1=x ′ ) if k = 1.</formula><p>Therefore when k ≥ 2, V k equals</p><formula xml:id="formula_104">φ 3 1 − 2 Q 0,1 f 0 (Y k )P k−1 (0) + Q 1,1 f 1 (Y k )P k−1 (1) f 0 (Y k )P k−1 (0) + f 1 (Y k )P k−1 (1) − φ 1</formula><p>which rewrites as</p><formula xml:id="formula_105">φ 3 1 − 2pf 0 (Y k ) + 2P k−1 (1)[(1 − q)f 1 (Y k ) − pf 0 (Y k )] f 0 (Y k ) + P k−1 (1)[f 1 (Y k ) − f 0 (Y k )] − φ 1 .</formula><p>We write for convenience</p><formula xml:id="formula_106">D k = f 0 (Y k ) + P k−1 (1)[f 1 (Y k ) − f 0 (Y k )],</formula><p>and,</p><formula xml:id="formula_107">N k = (1 − φ 1 )φ 3 D k − 2φ 3 pf 0 (Y k ) − 2φ 3 P k−1 (1)[(1 − q)f 1 (Y k ) − pf 0 (Y k )],</formula><p>so that V k = N k /D k . We rewrite the previous expressions solely in terms of the parameters (φ, ψ) [recall the inversion formulae in <ref type="bibr">Remark 3]</ref>. First, D k is equal to</p><formula xml:id="formula_108">ψ 1 (Y k ) − 1 2 φ 1 φ 3 ψ 2 (Y k ) + 1 2 φ 3 ψ 2 (Y k ) − P k−1 (1)φ 3 ψ 2 (Y k ) = ψ 1 (Y k ) + 1 2 φ 3 ψ 2 (Y k )[1 − 2P k−1 (1) − φ 1 ] = ψ 1 (Y k ) + V k−1 2 ψ 2 (Y k ).</formula><p>Also,</p><formula xml:id="formula_109">2pf 0 = (1 − φ 1 )(1 − φ 2 ) ψ 1 − 1 2 φ 1 φ 3 ψ 2 + 1 2 φ 3 ψ 2 ,</formula><p>and</p><formula xml:id="formula_110">(1 − q)f 1 − pf 0 = (1 − q − p)f 1 − p(f 0 − f 1 ) = φ 2 ψ 1 − 1 2 φ 1 φ 3 ψ 2 − 1 2 φ 3 ψ 2 − 1 2 (1 − φ 2 )(1 − φ 1 )φ 3 ψ 2 = φ 2 ψ 1 − 1 2 φ 3 ψ 2 φ 2 + φ 1 φ 2 + (1 − φ 2 )(1 − φ 1 ) = φ 2 ψ 1 − 1 2 φ 3 ψ 2 1 − φ 1 + 2φ 1 φ 2 .</formula><p>Using the last three displays and the fact that</p><formula xml:id="formula_111">2φ 3 P k−1 (1) = −V k−1 + φ 3 − φ 1 φ 3 , we obtain that N k = (1 − φ 1 )φ 3 ψ 1 (Y k ) + V k−1 2 ψ 2 (Y k ) − φ 3 (1 − φ 1 )(1 − φ 2 ) × ψ 1 (Y k ) − 1 2 φ 1 φ 3 ψ 2 (Y k ) + 1 2 φ 3 ψ 2 (Y k ) + (V k−1 − φ 3 + φ 1 φ 3 ) × φ 2 ψ 1 (Y k ) − 1 2 φ 3 ψ 2 (Y k ) 1 − φ 1 + 2φ 1 φ 2 .</formula><p>Grouping together the terms proportional to V k−1 and the others, N k equals</p><formula xml:id="formula_112">V k−1 φ 2 ψ 1 (Y k ) + 1 2 − φ 3 + φ 1 φ 3 − 2φ 1 φ 2 φ 3 + (1 − φ 1 )φ 3 ψ 2 (Y k ) + ψ 1 (Y k ) (1 − φ 1 )φ 3 − φ 3 (1 − φ 1 )(1 − φ 2 ) − φ 3 (1 − φ 3 )φ 2 + 1 2 ψ 2 (Y k ) −φ 2 3 (1−φ 1 ) 2 (1−φ 2 )+φ 2 3 (1−φ 1 )(1−φ 1 +2φ 1 φ 2 ) .</formula><p>We remark that</p><formula xml:id="formula_113">−φ 3 + φ 1 φ 3 − 2φ 1 φ 2 φ 3 + (1 − φ 1 )φ 3 = −2φ 1 φ 2 φ 3 ,</formula><p>and</p><formula xml:id="formula_114">(1 − φ 1 )φ 3 − φ 3 (1 − φ 1 )(1 − φ 2 ) − φ 3 (1 − φ 3 )φ 2 = 0,<label>and</label></formula><formula xml:id="formula_115">− φ 2 3 (1 − φ 1 ) 2 (1 − φ 2 ) + φ 2 3 (1 − φ 1 )(1 − φ 1 + 2φ 1 φ 2 ) = −φ 2 3 (1 − φ 1 ) 2 (1 − φ 2 ) + φ 2 3 (1 − φ 1 ) 2 + 2φ 1 φ 2 φ 2 3 (1 − φ 1 ) = φ 2 φ 2 3 (1 − φ 1 ) 2 + 2φ 1 φ 2 φ 2 3 (1 − φ 1 ) = φ 2 φ 2 3 [1 − 2φ 1 + φ 2 1 + 2φ 1 − 2φ 2 1 ] = φ 2 φ 2 3 (1 − φ 2 1 ).</formula><p>That is,</p><formula xml:id="formula_116">N k = φ 2 V k−1 ψ 1 (Y k ) − φ 1 φ 3 ψ 2 (Y k ) + φ 2 φ 2 3 (1 − φ 2 1 ) 2 ψ 2 (Y k ),</formula><p>which means that for k ≥ 2,</p><formula xml:id="formula_117">V k = φ 2 [ψ 1 (Y k ) − φ 1 φ 3 ψ 2 (Y k )]V k−1 + 2r(φ)ψ 2 (Y k ) ψ 1 (Y k ) + 1 2 ψ 2 (Y k )V k−1</formula><p>.</p><p>For k = 1, recalling that Y 1 ∼ ψ 1 and P θ (X 1 = 1) = p/(p + q), we have</p><formula xml:id="formula_118">V 1 = φ 3 (1 − 2P 1 (1) − φ 1 ) = φ 3 1 − φ 1 − 2 f 0 (Y 1 ) pq p+q + f 1 (Y 1 ) (1−q)p p+q ψ 1 (Y 1 ) = φ 3 1 − φ 1 − 2 f 1 (Y 1 ) p p+q + φ 3 ψ 2 (Y 1 ) pq p+q ψ 1 (Y 1 ) = −φ 2 3 ψ 2 (Y 1 ) − (1−φ1)(1+φ1) 2 + (1−φ2)(1−φ 2 1 ) 2 ψ 1 (Y 1 ) = 1 2 (1 − φ 2 1 )φ 2 φ 2 3 ψ 2 (Y 1 ) ψ 1 (Y 1 )</formula><p>,</p><p>where to go from the third to fourth line we have used the expressions derived in Remark 3 for f 1 , p and q. Letting m 1 = r(φ), m 2 = r(φ)φ 2 , and m 3 = r(φ)φ 1 φ 2 φ 3 , we have obtained the inductive formula</p><formula xml:id="formula_119">V k =    [m2ψ1(Y k )−m3ψ2(Y k )] V k−1 m 1 +2m1ψ2(Y k ) ψ1(Y k )+ 1 2 ψ2(Y k )V k−1 if k ≥ 2, 2m1ψ2(Y1) ψ1(Y1) if k = 1.</formula><p>The strategy is now to bound V k − Ṽk for k ≥ 2 in terms of V 1 − Ṽ1 using the above inductive formula. To do so, we will need an upper bound for V k (respectively Ṽk ) which we establish now. We claim that</p><formula xml:id="formula_120">|V k | ≤ 4|m 1 |/c for all k ≥ 1 provided ǫ 0 is taken small enough. Indeed, |ψ 2 (Y 1 )| ≤ ψ 2 = 1 and c ≤ ψ 1 (Y 1 ) ≤ 1, hence |V 1 | ≤ 2|m 1 |/c ≤ 4|m 1 |/c. Now suppose that |V k−1 | ≤ 4|m 1 |/c</formula><p>; then, under the assumptions of the proposition with for ǫ 0 = ǫ 0 (c) small enough, using equation ( <ref type="formula" target="#formula_22">4</ref>) to see that</p><formula xml:id="formula_121">|m 1 | ≤ |φ 2 | ≤ ǫ 0 , |φ 1 φ 2 φ 3 | ≤ √ 2|φ 2 | ≤ √ 2ǫ 0 , we have |V k | ≤ (|m 2 | + |m 3 |) 4 c + 2|m 1 | c − 1 2 4|m1| c ≤ |m 1 | (|φ 2 | + |φ 1 φ 2 φ 3 |) 4 c + 2 c − 4|m 1 |/c ≤ 4|m 1 | c . (<label>31</label></formula><formula xml:id="formula_122">) Similarly | Ṽk | ≤ 4| m1 |/c for all k ≥ 1. We are now in position to bound V k − Ṽk for k ≥ 2. Recall V k = N k /D k and similarly write Ṽk = Ñk / Dk . Then V k − Ṽk = N k D k − Ñk Dk = Dk N k − D k Ñk D k Dk = ( Dk − D k )N k D k Dk + N k − Ñk Dk .</formula><p>As when bounding |V k |, we can assume that ǫ 0 is small enough to have D k ≥ c/2 and Dk ≥ c/2, and</p><formula xml:id="formula_123">|N k | ≤ (|m 2 | + |m 3 |) 4 c + 2|m 1 | ≤ 4|m 1 |.</formula><p>Therefore,</p><formula xml:id="formula_124">|V k − Ṽk | ≤ 16|m 1 | c 2 |D k − Dk | + 2 c |N k − Ñk |. (<label>32</label></formula><formula xml:id="formula_125">)</formula><p>But, recalling the definition <ref type="bibr" target="#b10">(11)</ref> of ρ, we have</p><formula xml:id="formula_126">|D k − Dk | = ψ 1 (Y k ) − ψ1 (Y k ) + 1 2 ψ 2 (Y k )V k−1 − ψ2 (Y k ) Ṽk−1 ≤ |ψ 1 (Y k ) − ψ1 (Y k )| + |ψ 2 (Y k )| 2 |V k−1 − Ṽk−1 | + | Ṽk−1 | 2 |ψ 2 (Y k ) − ψ2 (Y k )| ≤ ψ 1 − ψ1 + |V k−1 − Ṽk−1 | 2 + 2|m 1 | c ψ 2 − ψ2 ≤ 1 + 2 c ρ(φ, ψ; φ, ψ) + |V k−1 − Ṽk−1 | 2 ,</formula><p>and the difference N k − Ñk is equal to</p><formula xml:id="formula_127">[m 2 ψ 1 (Y k ) − m2 ψ1 (Y k ) − m 3 ψ 2 (Y k ) + m3 ψ2 (Y k )] V k−1 m 1 + V k−1 m 1 − Ṽk−1 m1 m2 ψ1 (Y k ) − m3 ψ2 (Y k ) + 2m 1 ψ 2 (Y k ) − 2 m1 ψ2 (Y k ),</formula><p>from which we deduce that |N k − Ñk | is upper bounded by</p><formula xml:id="formula_128">|m 2 | ψ 1 − ψ1 +|m 2 − m2 |+|m 3 ψ 2 − ψ2 +|m 3 − m3 | V k−1 m 1 + |V k−1 − Ṽk−1 | + |m 1 − m1 | V k−1 m 1 | m2 | + | m3 | | m1 | + 2|m 1 | ψ 2 − ψ2 + 2|m 1 − m1 |,</formula><p>which, for ǫ 0 &gt; 0 small enough, is further upper bounded by</p><formula xml:id="formula_129">4|m 1 − m1 |+ 4|m 2 − m2 | c + 4|m 3 − m3 | c + 4|m 2 | ψ 1 − ψ1 c + 4|m 3 | c|m 1 | + 2 |m 1 | ψ 2 − ψ2 + c|V k−1 − Ṽk−1 | 8 .</formula><p>Inserting these bounds into equation ( <ref type="formula" target="#formula_124">32</ref>), we find that there is a constant B depending solely on c such that for all k ≥ 2</p><formula xml:id="formula_130">|V k − Ṽk | ≤ Bρ(φ, ψ; φ, ψ) + 1 4 + 8|m 1 | c 2 |V k−1 − Ṽk−1 | ≤ Bρ(φ, ψ; φ, ψ) + 1 2 |V k−1 − Ṽk−1 |,</formula><p>again when ǫ 0 is small enough. Hence for k ≥ 2,</p><formula xml:id="formula_131">|V k − Ṽk | ≤ 2(1 − 2 −k )Bρ(φ, ψ; φ, ψ) + 2 1−k |V 1 − Ṽ1 | ≤ 3B 2 ρ(φ, ψ; φ, ψ) + |V 1 − Ṽ1 | 2 .</formula><p>To finish the proof, it is enough to show that |V 1 − Ṽ1 | is bounded by a constant multiple of ρ(φ, ψ; φ, ψ), which follows from its definition and the same arguments as above. Thus for some constant B ′ &gt; 0 depending only on c max k=1,...,n</p><formula xml:id="formula_132">|V k − Ṽk | ≤ B ′ ρ(φ, ψ; φ, ψ). (<label>33</label></formula><formula xml:id="formula_133">)</formula><p>The conclusion follows by combining equations ( <ref type="formula" target="#formula_99">30</ref>), <ref type="bibr" target="#b31">(31)</ref> and <ref type="bibr" target="#b33">(33)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proof of Theorem 2</head><p>We start with the proof of Lemma 1, that p (3) can be estimated at a parametric rate.</p><p>Proof of Lemma 1. We use a Markov chain concentration result from <ref type="bibr" target="#b41">[41]</ref>. Theorem 3.4 therein (but note there is an updated version of the paper on arXiv) tells us that for any stationary Markov chain Z = (Z (1) , Z (2) , . . . ) of pseudospectral gap γ ps (defined as in <ref type="bibr" target="#b41">[41]</ref>) and any function h</p><formula xml:id="formula_134">satisfying E[h(Z (1) ) 2 ] ≤ σ 2 and h ∞ ≤ b, P(| n i=1 h(Z (i) ) − Eh(Z (1) )| ≥ x) ≤ 2 exp − x 2 γ ps 8(n + 1/γ ps )σ 2 + 20bx .<label>(34)</label></formula><p>We apply to the chain Z defined by</p><formula xml:id="formula_135">Z (n) = (X n , X n+1 , X n+2 , Y n , Y n+1 , Y n+2 )</formula><p>; we begin by showing the pseudo-spectral gap of this chain is bounded from below. Proposition 3.4 of the same reference shows that the reciprocal of the pseudo-spectral gap of any chain is bounded above by twice the mixing time t Z mix of the chain, defined as the first time that the law of Z, regardless of the starting distribution, is within 1/4 of its invariant distribution in total variation distance. We note that t Z mix is equal to the mixing time t X (3)   mix of the chain ((X n , X n+1 , X n+2 ) n≥0 ). This latter quantity is upper bounded by t X mix + 2 where t X mix denotes the mixing time of the chain X itself. Finally, the matrix Q has eigenvalues 1 and φ 2 , and an explicit computation yields that</p><formula xml:id="formula_136">max ij |Q n ij − π j | = max i (π i )|φ 2 | n so that the mixing time of X is at most log 4 log(1/|φ 2 |) ≤ log 4 log(1/(1 − L)) ≤ log 4 L ,</formula><p>which is a constant since L is fixed. The pseudo-spectral gap of the chain Z is thus lower bounded by some constant γ = γ(L).</p><p>Applying equation <ref type="bibr" target="#b34">(34)</ref> with h(Z) = ½{Z 4 = a, Z 5 = b, Z 6 = c}, which satisfies Eh 2 ≤ 1 and h ∞ ≤ 1, we see that</p><formula xml:id="formula_137">P φ,ψ n|p (3) (a, b, c) − p (3) φ,ψ (a, b, c)| ≥ x ≤ 2 exp − γx 2 8n + 8/γ + 20x</formula><p>, hence for some constant c ′ &gt; 0</p><formula xml:id="formula_138">P φ,ψ |p (3) (a, b, c) − p (3) φ,ψ (a, b, c)| ≥ x/ √ n ≤ 2 exp −c ′ min x 2 , x 2 n, x √ n .</formula><p>Using that p(3) − p (3)  ≤ K 3 max a,b,c |p (3) (a, b, c) − p (3) (a, b, c)| and a union bound, we deduce for some C = C(K, L) and for x ≤ √ n that</p><formula xml:id="formula_139">P φ,ψ ( p(3) − p (3) ≥ K 3 x/ √ n) ≤ 2K 3 exp(−Cx 2 ).</formula><p>For x ≥ 1 we may absorb the factor 2K 3 into the exponential by changing the constant C, and by replacing x with C ′ x we can remove this constant, yielding the result in the case where C ′ x ≤ √ n. In the other case, since p(3) − p (3) is bounded (by K 3/2 ), by increasing the constant C ′ if necessary we have C ′ x/ √ n ≥ K 3/2 so that the probability in question is equal</p><formula xml:id="formula_140">to 0 ≤ e −x 2 .</formula><p>To prove Theorem 2, observe that by Lemma 1 there exist events A n of probability at least e −x 2 on which</p><formula xml:id="formula_141">p(3) n − p (3) φ,ψ ≤ Cx/ √ n.</formula><p>The true parameter (φ, ψ) lies in Φ L so that any estimators constructed in Theorem 2 satisfy</p><formula xml:id="formula_142">p (3) φ, ψ − p(3) n ≤ 2 p (3) φ,ψ − p(3) n ,</formula><p>and hence on the event A n further satisfy p</p><formula xml:id="formula_143">≤ 3Cx/ √ n,<label>(3) φ, ψ − p (3) φ,ψ ≤ p (3) φ, ψ − p(3) n + p (3) φ,ψ − p(3) n ≤ 3 p (3) φ,ψ − p(3) n</label></formula><p>By Proposition 1 we deduce for a constant C ′ that ρ( φ, ψ; φ, ψ) ≤ C ′ x/ √ n on A n . For estimating ψ, observe that ψ1 − ψ 1 ≤ ρ( φ, ψ; φ, ψ) and |r(φ)| min( ψ2 − ψ 2 , ψ2 + ψ 2 ) ≤ ρ( φ, ψ; φ, ψ). The upper bound for estimating ψ 1 is immediate and, recalling from equation (4) that |r(φ)| ≥ δǫζ 2 /4, we also deduce the bound for ψ 2 .</p><p>For the bounds on φ, observe firstly that it suffices to prove the upper bounds on the absolute risk since, taking φ 2 as an example, for (φ, ψ) ∈ Φ L (δ, ǫ, ζ) we have</p><formula xml:id="formula_144">P φ,ψ | φ2 /φ 2 − 1| 2 ≥ C nδ 2 ǫ 4 ζ 4 = P φ,ψ | φ2 − φ 2 | 2 ≥ Cφ 2 2 nδ 2 ǫ 4 ζ 4 ≤ P φ,ψ | φ2 − φ 2 | ≥ Cǫ 2 nδ 2 ǫ 4 ζ 4 .<label>(35)</label></formula><p>(See also after equation ( <ref type="formula" target="#formula_198">36</ref>) for a similar argument with φ 1 .) Define</p><formula xml:id="formula_145">ω 1 (φ, ψ; η) := sup |φ 1 − sgn( ψ 2 , ψ2 ) • φ1 | : ρ(φ, ψ; φ, ψ) ≤ η ,</formula><p>and for j = 2, 3</p><formula xml:id="formula_146">ω j (φ, ψ; η) := sup |φ j − φj | : ρ(φ, ψ; φ, ψ) ≤ η .</formula><p>We have the following.</p><formula xml:id="formula_147">Proposition 3. Let η ∈ [0, 1]</formula><p>. There exist constants c, C for which the following hold.</p><formula xml:id="formula_148">η &lt; c(1 − φ 2 1 )φ 2 2 φ 3 3 =⇒ ω 1 (φ, ψ; η) ≤ Cη φ 2 2 φ 3 3 , η &lt; c(1 − φ 2 1 )|φ 2 |φ 2 3 =⇒ ω 2 (φ, ψ; η) ≤ Cη (1 − φ 2 1 )|φ 2 |φ 2 3 , η &lt; c(1 − φ 2 1 )φ 2 2 φ 3 3 =⇒ ω 3 (φ, ψ; η) ≤ Cη (1 − φ 2 1 )φ 2 2 φ 2 3 .</formula><p>The conditions of Theorem 2 ensure that on the event A n we may apply Proposition 3 with η = C ′ x/ √ n. We deduce the upper bounds for estimating the components of φ immediately upon replacing φ 1 , φ 2 and φ 3 on the right sides in Proposition 3 by their lower bounds [for φ 1 we note that</p><formula xml:id="formula_149">min(| φ1 − φ 1 |, | φ1 + φ 1 |) ≤ |φ 1 − sgn( ψ 2 , ψ2 ) • φ1 |]. Proof of Proposition 3. Recall that m(φ) = (r(φ), φ 2 r(φ), φ 1 φ 2 φ 3 r(φ)) with r(φ) = 1 4 (1 − φ 2 1 )φ 2 φ 2 3 . If r(φ) = 0 then in each case no η ∈ [0, 1]</formula><p>satisfies the conditions and so there is nothing to prove. Otherwise, note that m is invertible when restricted to {φ : r(φ) = 0} ⊃ Φ(δ, ǫ, ζ) and its inverse is given by φ(m) defined by</p><formula xml:id="formula_150">φ 1 (m) = m 3 /(4m 2 1 m 2 + m 2 3 ) 1/2 φ 2 (m) = m 2 /m 1 , φ 3 (m) = (4m 2 1 m 2 + m 2 3 ) 1/2 /m 2 .</formula><p>For arbitrary (φ, ψ; φ, ψ) satisfying ρ(φ, ψ; φ, ψ) ≤ η, we define</p><formula xml:id="formula_151">∆ 1 := m 1 ( φ) − m 1 (φ), ∆ 2 := m 2 ( φ) − m 2 (φ), ∆ 3 := sgn( ψ 2 , ψ2 ) • m 3 ( φ) − m 3 (φ).</formula><p>Define also</p><formula xml:id="formula_152">g(φ) := 4m 1 (φ) 2 m 2 (φ) + m 3 (φ) 2 = {m 2 (φ)φ 3 } 2 = 1 4 (1 − φ 2 1 )φ 2 2 φ 3 3 2 ,</formula><p>and, for</p><formula xml:id="formula_153">∆ = (∆ 1 , ∆ 2 , ∆ 3 ), h φ (∆) := g( φ) − g(φ) = 4(m 1 (φ) + ∆ 1 ) 2 (m 2 (φ) + ∆ 2 ) + (m 3 (φ) + ∆ 3 ) 2 − {4m 1 (φ) 2 m 2 (φ) + m 3 (φ) 2 }.</formula><p>Observe that</p><formula xml:id="formula_154">h φ (∆) = 8m 1 (φ)m 2 (φ)∆ 1 + 8m 1 (φ)∆ 1 ∆ 2 + 4m 2 (φ)∆ 2 1 + 4∆ 2 1 ∆ 2 + 4m 1 (φ) 2 ∆ 2 + 2m 3 (φ)∆ 3 + ∆ 2 3 .</formula><p>a) Bounding ω 1 : We decompose,</p><formula xml:id="formula_155">φ 1 − sgn( ψ 2 , ψ2 ) • φ1 = m 3 (φ) g(φ) − m 3 (φ) + ∆ 3 g(φ) + h φ (∆)</formula><p>which is in turn equal to</p><formula xml:id="formula_156">m 3 (φ) 1 g(φ) − 1 g(φ) + h φ (∆) − ∆ 3 g(φ) + h φ (∆) i.e. equal to m 3 (φ) g(φ)(g(φ) + h φ (∆)) g(φ) + h φ (∆) − g(φ) − ∆ 3 g(φ) + h φ (∆) , which is m 3 (φ) g(φ)(g(φ) + h φ (∆)) h φ (∆) g(φ) + h φ (∆) + g(φ) − ∆ 3 g(φ) + h φ (∆)</formula><p>.</p><p>Now we observe that m 3 (φ)/ g(φ) is equal to φ 1 , so indeed</p><formula xml:id="formula_157">φ 1 − sgn( ψ 2 , ψ2 ) • φ1 = φ 1 h φ (∆) − ∆ 3 ( g(φ) + h φ (∆) + g(φ)) g(φ) + h φ (∆)( g(φ) + h φ (∆) + g(φ)</formula><p>) .</p><p>Call the numerator of this last fraction N and call its denominator D. Writing h φ (∆) as h φ (∆) = ξ φ (∆) + γ φ (∆), where γ φ (∆) := 2m 3 (φ)∆ 3 + ∆ 2 3 , we see that</p><formula xml:id="formula_158">N = φ 1 ξ φ (∆)+φ 1 γ φ (∆)−∆ 3 (g(φ)+h φ (∆)) 1/2 +g(φ) 1/2 .</formula><p>In order to obtain the optimal upper bound, we need to do a fine analysis of this expression. To this end, we calculate</p><formula xml:id="formula_159">A := φ 1 γ φ (∆) − ∆ 3 {(g + h) 1/2 + g 1/2 } = 2∆ 3 {φ 1 m 3 (φ) − g 1/2 } + φ 1 ∆ 2 3 − ∆ 3 {(g + h) 1/2 − g 1/2 } = −2∆ 3 (1 − φ 2 1 )g 1/2 + φ 1 ∆ 2 3 − ∆ 3 h (g + h) 1/2 + g 1/2 , i.e. A = −2∆ 3 (1 − φ 2 1 )g 1/2 − ∆ 3 γ φ (∆) − φ 1 ∆ 3 ((g + h) 1/2 + g 1/2 ) (g + h) 1/2 + g 1/2 − ∆ 3 ξ φ (∆) (g + h) 1/2 + g 1/2 ,</formula><p>where the last line follows because φ 1 m 3 (φ) = φ 2 1 g(φ) 1/2 . We now focus on the middle term of the last display, which we will express as a function of A.</p><formula xml:id="formula_160">B := γ φ (∆) − φ 1 ∆ 3 ((g + h) 1/2 + g 1/2 ) = 2∆ 3 (m 3 (φ) − φ 1 g 1/2 ) + ∆ 2 3 − φ 1 ∆ 3 {(g + h) 1/2 − g 1/2 } = ∆ 2 3 − φ 1 ∆ 3 h (g + h) 1/2 + g 1/2 = ∆ 2 3 − φ 1 ∆ 3 γ φ (∆) (g + h) 1/2 + g 1/2 − φ 1 ∆ 3 ξ φ (∆) (g + h) 1/2 + g 1/2</formula><p>that is,</p><formula xml:id="formula_161">B = − ∆ 3 A + h) 1/2 + g 1/2 − φ 1 ∆ 3 ξ φ (∆) (g + h) 1/2 + g 1/2 .</formula><p>Thus,</p><formula xml:id="formula_162">A = −2∆ 3 (1 − φ 2 1 )g 1/2 − ∆ 3 B (g + h) 1/2 + g 1/2 − ∆ 3 ξ φ (∆) (g + h) 1/2 + g 1/2 = −2∆ 3 (1 − φ 2 1 )g 1/2 + ∆ 2 3 A {(g + h) 1/2 + g 1/2 } 2 + φ 1 ∆ 2 3 ξ φ (∆) {(g + h) 1/2 + g 1/2 } 2 − ∆ 3 ξ φ (∆) (g + h) 1/2 + g 1/2</formula><p>, from which we deduce that</p><formula xml:id="formula_163">N = φ 1 ξ φ (∆)+ −2∆ 3 (1 − φ 2 1 )g 1/2 + φ1∆ 2 3 ξ φ (∆) {(g+h) 1/2 +g 1/2 } 2 − ∆3ξ φ (∆) (g+h) 1/2 +g 1/2 1 − ∆ 2 3 /{(g + h) 1/2 + g 1/2 } 2 .</formula><p>Since m 2 (φ) ≥ 0, we see that ξ φ (∆) has maximal amplitude when ∆ 1 = sgn(m 1 (φ))η and when ∆ 2 = η, in which case we have</p><formula xml:id="formula_164">|ξ φ (∆)| = 8|m 1 (φ)|m 2 (φ)η + 8|m 1 (φ)|η 2 + 4m 2 (φ)η 2 + 4η 3 + 4m 1 (φ) 2 η ≤ 12m 1 (φ) 2 η + 12|m 1 (φ)|η 2 + 4η 3 ,</formula><p>where the last line follows since m 2 (φ) ≤ |m 1 (φ)|. Now we observe that under the condition of the lemma, we have η |m 1 (φ)|, and so we can find a constant C &gt; 0 such that</p><formula xml:id="formula_165">|ξ φ (∆)| ≤ Cm 1 (φ) 2 η.</formula><p>Also, we have that |γ φ (∆)| ≤ 2|m 3 (φ)|η + η 2 , and so</p><formula xml:id="formula_166">|h φ (∆)| ≤ Cm 1 (φ) 2 η + 2|m 3 (φ)|η + η 2 ,</formula><p>Noting that φ 3 ≤ √ K, for c 0 = c 0 (K) sufficiently small in the assumption of the proposition we have |h φ (∆)| ≤ g(φ)/2. Consequently, noting also that |∆ 3 | ≤ η and η ≤ 4c 0 g 1/2 , we find that</p><formula xml:id="formula_167">|N | |φ 1 |m 1 (φ) 2 η + η(1 − φ 2 1 )g(φ) 1/2 η(1 − φ 2 1 ) 2 φ 2 2 φ 3 3 , and |D| g(φ) (1 − φ 2 1 ) 2 φ 4 2 φ 6 3 .</formula><p>Hence we have</p><formula xml:id="formula_168">|φ 1 − sgn( ψ 2 , ψ2 ) • φ1 | η φ 2 2 φ 3 3 . b) Bounding ω 2 :</formula><p>We rewrite,</p><formula xml:id="formula_169">φ 2 − φ2 = m 2 (φ) m 1 (φ) − m 2 (φ) + ∆ 2 m 1 (φ) + ∆ 1 = m 2 (φ)(m 1 (φ) + ∆ 1 ) − (m 2 (φ) + ∆ 2 )m 1 (φ) m 1 (φ)(m 1 (φ) + ∆ 1 )<label>.</label></formula><p>Hence,</p><formula xml:id="formula_170">φ 2 − φ2 = ∆ 1 m 2 (φ) − ∆ 2 m 1 (φ) m 1 (φ)(m 1 (φ) + ∆ 1 )</formula><p>.</p><p>Under the assumptions of the theorem, we have that η ≤ m 1 (φ)/2, and thus</p><formula xml:id="formula_171">|φ 2 − φ2 | ≤ 2η(m 2 (φ) + |m 1 (φ)|) m 1 (φ) 2 ≤ 4η |m 1 (φ)| = 16η (1 − φ 2 1 )|φ 2 |φ 2 3 .</formula><p>c) Bounding ω 3 : We rewrite,</p><formula xml:id="formula_172">φ 3 − φ3 = g(φ) m 2 (φ) − g(φ) + h φ (∆) m 2 (φ) + ∆ 2 = m 2 (φ)( g(φ) − g(φ) + h φ (∆)) m 2 (φ)(m 2 (φ) + ∆ 2 ) + ∆ 2 g(φ) m 2 (φ)(m 2 (φ) + ∆ 2 ) = −h φ (∆) (m 2 (φ) + ∆ 2 )( g(φ) + h φ (∆) + g(φ)) + ∆ 2 φ 3 m 2 (φ) + ∆ 2 = −h φ (∆) + ∆ 2 φ 3 ( g(φ) + h φ (∆) + g(φ)) (m 2 (φ) + ∆ 2 )( g(φ) + h φ (∆) + g(φ))</formula><p>Let us call the numerator of the fraction on the right of the last display N , and the denominator D. We further decompose h φ (∆) as h φ (∆) = ξ φ (∆) + γ φ (∆), where γ φ (∆) := 4m 1 (φ) 2 ∆ 2 . We see that</p><formula xml:id="formula_173">N = −ξ φ (φ) − 4m 1 (φ) 2 ∆ 2 + φ 3 ∆ 2 ((g + h) 1/2 + g 1/2 ) = −ξ φ (φ) − 4m 1 (φ) 2 ∆ 2 + φ 3 ∆ 2 {(g + h) 1/2 + g 1/2 } = −ξ φ (φ) − 4m 1 (φ) 2 ∆ 2 + 2φ 3 ∆ 2 g 1/2 + φ 3 ∆ 2 {(g + h) 1/2 − g 1/2 } = −ξ φ (φ) + ∆ 2 (1 + φ 2 1 )φ 3 g 1/2 + φ 3 ∆ 2 h (g + h) 1/2 + g 1/2 ,</formula><p>where the last line follows because m 1 (φ</p><formula xml:id="formula_174">) 2 = 1 4 (1 − φ 2 1 )φ 3 g 1/2</formula><p>. Since m 2 (φ) ≥ 0, we see that ξ φ (∆) has maximal amplitude when ∆ 1 = sgn(m 1 (φ))η and when ∆ 2 = η, in which case we have</p><formula xml:id="formula_175">|ξ φ (∆)| = 8|m 1 (φ)|m 2 (φ)η + 8|m 1 (φ)|η 2 + 4m 2 (φ)η 2 + 4η 3 + 2|m 3 (φ)|η + η 2 {|m 1 (φ)|m 2 (φ) + 2|m 3 (φ)|}η + η 2 (1 − φ 2 1 )φ 2 2 φ 3 3 max{(1 − φ 2 1 )|φ 2 |φ 3 , |φ 1 |}η + η 2 ,</formula><p>where the second line follows because under the assumptions of the proposition we have that m 2 (φ) |m 1 (φ)| and η ≤ m 2 (φ)/2 (note that φ 3 ≤ K 1/2 ). Since h φ (∆) = ξ φ (∆) + 4m 1 (φ) 2 ∆ 2 , we also have</p><formula xml:id="formula_176">|h φ (∆)| (1 − φ 2 1 )φ 2 2 φ 3 3 max{(1 − φ 2 1 )φ 3 , |φ 1 |}η + η 2 , Hence, |N | (1 − φ 2 1 )φ 2 2 3 3 max{(1 − φ 2 1 )|φ 2 |φ 3 , |φ 1 |}η + η 2 + ηφ 3 g 1/2 + η 2 φ 3 (1 − φ 2 1 )φ 2 2 φ 3 3 max{(1 − φ 2 1 )φ 3 , |φ 1 |} + η 3 φ 3 g 1/2 (1 − φ 2 1 )φ 2 2 φ 3 3 max{(1 − φ 2 1 )|φ 2 |φ 3 , |φ 1 |}η + η 2 + ηφ 3 g 1/2 + η 2 φ 3 max{(1 − φ 2 1 )φ 3 , |φ 1 |} + η 3 (1 − φ 2 1 )φ 2 2 φ 2 3</formula><p>But by assumption η (1 − φ 2 1 )φ 2 2 φ 3 3 , and</p><formula xml:id="formula_177">4g 1/2 = (1 − φ 2 1 )φ 2 2 φ 2 3 , thus |N | (1 − φ 2 1 )φ 2 2 φ 3 3 max{φ 3 , |φ 1 |}η + η 2 .</formula><p>Note that max(φ 3 , |φ| 1 ) ≤ √ K. Moreover, under the assumptions of the proposition and using that φ 3 ≤ √ K, it is the case that |∆ 2 | ≤ η m 2 (φ). Therefore |D| m 2 (φ) g(φ), and</p><formula xml:id="formula_178">|φ 3 − φ3 | η (1 − φ 2 1 )φ 2 2 φ 2 3 + η 2 (1 − φ 2 1 ) 2 φ 4 2 φ 5 3 .</formula><p>Finally, since have assumed that η &lt;</p><formula xml:id="formula_179">(1−φ 2 1 )φ 2 2 φ 2 3 8</formula><p>, we see that the second term is at most a constant times the first, so that it can be absorbed by increasing the constant C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Proof of Theorem 3</head><p>We give a standard two-point testing lower bound, summarising ideas that can be found for example in Chapter 2 of <ref type="bibr" target="#b39">[39]</ref>. Lemma 2. Given data X (n) ∼ p (n) u for parameter u ∈ U, the following lower bounds hold for estimating u.</p><p>Suppose U ⊆ R and for some r ≤ 1/2 assume that there exist parameters u 0 , u </p><formula xml:id="formula_180">1 satisfying i. |u 1 /u 0 − 1| ≥ 4r, ii. KL(p (n) u1 ; p (n) u0 ) ≤ 1/</formula><formula xml:id="formula_181">P u (|û/u − 1| ≥ r) ≥ 1/4,</formula><p>where the infimum is over all estimators û based on the data X (n) . If instead (U, d) is a pseudo-metric space and for some r ≥ 0 there exist parameters u 0 , u</p><formula xml:id="formula_182">1 satisfying i. d(u 0 , u 1 ) ≥ 2r ii. KL(p (n) u1 , p (n) u0 ) ≤ 1/100, then inf û sup u∈U P u (d(û, u) ≥ r) ≥ 1/4.</formula><p>Proof. In the case U ⊆ R, given an estimator û we may construct a test T of u = u 0 vs u = u 1 ,</p><formula xml:id="formula_183">T = ½ û u 0 − 1 &gt; û u 1 − 1 .</formula><p>Observe that</p><formula xml:id="formula_184">û u 0 − 1 = u 1 u 0 − 1 + û − u 1 u 1 u 1 u 0 ≥ 4r − û u 1 − 1 (1 + 4r).</formula><p>Then</p><formula xml:id="formula_185">P u1 (T = 0) = P u1 û u 0 − 1 ≤ û u 1 − 1 ≤ P u1 4r − û u 1 − 1 (1 + 4r) ≤ û u 1 − 1 ≤ P u1 û u 1 − 1 ≥ r ,</formula><p>where for the last line we have used that 4r/(2 + 4r) ≥ r for r ≤ 1/2. Also note that on the event {T = 1} ∩ {|û/u 0 − 1| &lt; r} we have also |û/u 1 − 1| &lt; r and hence</p><formula xml:id="formula_186">|u 1 /u 0 − 1| = |û/u 0 − 1 − (û/u 1 − 1) − (û/u 1 − 1)(u 1 /u 0 − 1)| &lt; 2r + r|u 1 /u 0 − 1|, so that |u 1 /u 0 −1| &lt; 2r/(1−r) on this event. Having assumed r ≤ 1/2 and |u 1 /u 0 − 1| ≥ 4r we deduce that {T = 1} ∩ {|û/u 0 − 1| &lt; r} = ∅ so that {T = 1} ⊆ {|û/u 0 − 1| ≥ r},</formula><p>and hence we have shown</p><formula xml:id="formula_187">inf û sup u P u û u − 1 ≥ r ≥ inf û max i=0,1 P ui û u i − 1 ≥ r ≥ inf T max i=0,1 P ui (T = i),</formula><p>where the latter infimum is over all tests T . In the pseudometric case a reduction considering the test T = ½{d(û, u 0 ) &gt; d(û, u 1 )} and directly using the triangle inequality likewise yields</p><formula xml:id="formula_188">inf û sup u P u d(û, u) ≥ r ≥ inf T max i=0,1 P ui (T = i).</formula><p>It remains to lower bound the maximum probability of testing error by 1/4. Introducing the event A =</p><formula xml:id="formula_189">p (n) u 0 p (n) u 1 ≥ 1/2 , we see P u0 (T = 0) ≥ E u1 p (n) u 0 p (n) u 1 ½ A T ≥ 1 2 [P u1 (T = 1) − P u1 (A c )]</formula><p>Thus, writing p 1 = P u1 (T = 1), we see</p><formula xml:id="formula_190">max(P u0 (T = 0), P u1 (T = 1)) ≥ max( 1 2 (p 1 − P u1 (A c )), 1 − p 1 ) ≥ inf p∈[0,1] max( 1 2 (p − P u1 (A c )), 1 − p).</formula><p>The infimum is attained when 1 2 (p − P u1 (A c )) = 1 − p and takes the value 1  3 P u1 (A), so that</p><formula xml:id="formula_191">inf T max i=0,1 P ui (T = i) ≥ 1 3 P u1 (A).</formula><p>Next observe</p><formula xml:id="formula_192">P u1 (A) = P u1 p (n) u 1 p (n) u 0 ≤ 2 = 1 − P n u1 log p (n) u 1 p (n) u 0 &gt; log 2 ≥ 1 − P n θ1 |log( p (n) u 1 p (n) u 0 )| &gt; log 2 ≥ 1 − (log 2) −1 E u1 log p (n) u 1 p (n) u 0</formula><p>, where we have used Markov's inequality to attain the final expression. By the second Pinsker inequality (e.g. Proposition 6.1.7b in <ref type="bibr" target="#b42">[42]</ref>), using the upper bound on the Kullback-Leibler divergence we can continue the chain of inequalities to see</p><formula xml:id="formula_193">P u1 (A) ≥ 1 − (log 2) −1 KL(p (n) u1 , p (n) u0 ) + 2KL(p (n) u1 , p (n) u0 ) ≥ 1 − (log 2) −1 (µ + 2µ).</formula><p>For any c &lt; 1/3, we may choose µ = µ(c) small enough that the testing error satisfies</p><formula xml:id="formula_194">inf T max i=0,1 P ui (T = i) ≥ 1 3 1 − µ + √ 2µ log 2 &gt; c,</formula><p>and in particular a numerical calculation shows that µ = 1 +</p><formula xml:id="formula_195">1 4 log 2 − 1 + 1 2 log 2 &gt; 1/100 works for c = 1/4.</formula><p>In view of Proposition 2, for any (φ, ψ), ( φ, ψ) ∈ Φ corresponding to strictly positive emission densities, we have for φ 2 and φ2 small enough that</p><formula xml:id="formula_196">KL(p (n) φ,ψ , p (n) φ, ψ ) ≤ Cnρ(φ, ψ; φ, ψ) 2 ,</formula><p>where C &gt; 0 is a constant depending only on K and a lower bound for the emission densities. We remark that for all the hypotheses we will exhibit below, we will have that φ 2 and φ2 are of order ǫ, which is upper bounded by ǫ 1 by assumption, so that choosing the latter small enough the above bound on KL(p</p><formula xml:id="formula_197">(n) φ,ψ , p (n) φ,</formula><p>ψ ) will apply. Then, to prove Item (1), it suffices to apply Lemma 2 to u = 1 − φ 2 1 and prove the existence of parameters (φ, ψ), ( φ, ψ) ∈ Φ L (δ, ǫ, ζ) satisfying for small enough c 1 &gt; 0 and some c 2 &gt; 0 ρ(φ, ψ; φ, ψ) ≤ c 1 / √ n, and 1 − φ2</p><formula xml:id="formula_198">1 1 − φ 2 1 − 1 ≥ c 2 / nδ 2 ǫ 4 ζ 6<label>(36</label></formula><p>) which will give the lower bound on the absolute risk. Regarding the relative risk, we then note that for any a ≥ 0, since |φ 1 | ≤ 1 and 1 − φ 2 1 ≥ δ, so that we may assume the same of φ1 , we have</p><formula xml:id="formula_199">P φ,ψ (| φ1 − φ 1 | ∧ | φ1 + φ 1 | ≥ a) ≥ P φ,ψ (|(1 − φ2 1 ) − (1 − φ 2 1 )| ≥ 2a) ≥ P φ,ψ (|(1 − φ2 1 )/(1 − φ 2 1 ) − 1| ≥ 2a/δ</formula><p>). (See also equation ( <ref type="formula" target="#formula_144">35</ref>) for a similar calculation with φ 2 .) Similar conditions to <ref type="bibr" target="#b36">(36)</ref> suffice for proving the other parts of Theorem 3 and we proceed now to verifying the existence of suitable parameters (φ, ψ) and ( φ, ψ), with the help of the following lemma.</p><p>Lemma 3. For a given φ, assume conditions (5) and ( <ref type="formula">6</ref>) and assume that φ 3 ≤ 2⌊K/2⌋/(2K). Then there exists ψ such that (φ, ψ) lies in Φ L and the corresponding emission densities f 0 , f 1 are bounded below by some constant c = c(K) &gt; 0.</p><p>In particular, for |φ</p><formula xml:id="formula_200">1 | ≤ 1 − 3δ, ǫ ≤ φ 2 ≤ min(1/3, 1 − L), ζ ≤ φ 3 ≤ 2ζ, such a ψ exists under the condition ζ ≤ 2⌊K/2⌋/(4K).</formula><p>Proof. For k ≤ K, set ψ 1 (k) = 1/K and ψ 2 (k) = (2⌊K/2⌋) −1/2 (½{k odd, k &lt; K} − ½{k even}).</p><p>[Or, similarly, ψ 2 (k) = (2⌊K/2⌋) −1/2 (½{k &lt; (K + 1)/2} − ½{k &gt; (K + 1)/2}).] Under the assumed condition on φ 3 and recalling that |φ 1 | ≤ 1 by assumption, we observe from the expressions for f 0 , f 1 given in Remark 3 that these are lower bounded by 1/(2K). In the particular case, one simply notes that all the conditions hold for such φ.</p><p>a) Proof of Items ( <ref type="formula" target="#formula_0">1</ref>) and ( <ref type="formula" target="#formula_13">3</ref>): We prove the lower bounds for estimating φ 1 and φ 3 together. For some small constant c &gt; 0</p><formula xml:id="formula_201">, set R = cǫ −2 ζ −3 n −1/2 and, writing S = (2 − 6δ − R)R/(6δ − 9δ 2 ), set φ = (1 − 3δ, ǫ, ζ √ 1 + S), φ = (1 − 3δ − R, ǫ, ζ).</formula><p>Recalling the definition r(φ) = (1 − φ 2 1 )φ 2 φ 2 3 /4, the choice of φ 3 ensures that r(φ) = r( φ), and we note that under the assumptions of the theorem we have R ≤ δ ≤ 1/6 so that S ≤ R/δ ≤ 1 and ζ ≤ φ 3 ≤ 2ζ. By Lemma 3 there exists some ψ = ψ such that (φ, ψ), ( φ, ψ) ∈ Φ L and for this ψ = ψ we see that</p><formula xml:id="formula_202">ρ(φ, ψ; φ, ψ) = |φ 1 φ 2 φ 3 r(φ) − φ1 φ2 φ3 r( φ)| = φ 2 r(φ)|φ 1 φ 3 − φ1 φ3 |.</formula><p>Using that √ 1 + t ≤ 1 + t for t ≥ 0 we have Recalling that R ≤ δ ≤ 1/6 and that r(φ) = r( φ) one calculates 1 − φ2</p><formula xml:id="formula_203">|φ 1 φ 3 − φ1 φ3 | = (1 − 3δ)ζ( √ 1 + S − 1) + Rζ ≤ (S + R)ζ ≤ 2Rζ/δ,</formula><formula xml:id="formula_204">1 1 − φ 2 1 − 1 = S ≥ R/(12δ).</formula><p>For c small enough we see that the conditions in equation <ref type="bibr" target="#b36">(36)</ref> are satisfied, yielding the claimed bound for estimating φ 1 .</p><p>To prove the lower bound for estimating and observe that by construction r(φ) = r( φ). Noting that φ 2 ≤ 2ǫ ≤ 1 − L and φ 3 ≤ 2ζ because the assumptions of Theorem 3 ensure that R ≤ ǫ ≤ 1/3, we deduce using Lemma 3 that there exists some ψ = ψ such that (φ, ψ), ( φ, ψ) ∈ Φ L (δ, ǫ, ζ).</p><p>Next observe, using that (1 + x) 1/2 ≤ 1 + x, Again using that φ 3 ≤ 2ζ and noting also that (1 − φ 2 1 ) = 6δ − 9δ 2 ≤ 6δ, we see that for some C ′ &gt; 0 we have</p><formula xml:id="formula_205">ρ(φ, ψ; φ, ψ) ≤ C ′ δǫζ 2 R ≤ cC ′ n −1/2 .</formula><p>As with Items (1) and ( <ref type="formula" target="#formula_13">3</ref>), for c small enough in the definition of R we may apply Lemma 2 to deduce the claimed lower bound since | φ2 /φ 2 − 1| = R/ǫ. c) Proof of Item (4): Set φ = φ = (0, ǫ, ζ). For k ≤ K, as in Lemma 3 define ψ 1 (k) = 1/K and ψ 2 (k) = (2⌊K/2⌋) −1/2 (½{k odd, k &lt; K} − ½{k even}), and set ψ1 = ψ 1 +cn −1/2 ψ 2 . Note that for the upper bound ζ 0 small enough we have (φ, ψ), ( φ, ψ) ∈ Φ L for n larger than some C = C(K, c), or for all n ≥ 1 if c is small enough. Then ρ(φ, ψ; φ, ψ) = cn −1/2 and we apply Lemma 2 to deduce the result. d) Proof of Item ( <ref type="formula" target="#formula_23">5</ref>): Set φ = φ = (1 − 3δ, ǫ, ζ), choose ψ 1 = ψ1 to be the uniform density on {1, . . . , K}. As with the previous parts, an application of Lemma 2 will yield the theorem if we can exhibit ψ 2 , ψ2 such that the induced emission densities are bounded below by some c ′ = c ′ (K) &gt; 0, ψ 2 − ψ2 = R := c(nδ 2 ǫ 2 ζ 4 ) −1/2 for some c &gt; 0, sgn( ψ 2 , ψ2 ) = +1, and ρ(φ, ψ; φ, ψ) ≤ c 1 n −1/2 for a small constant c 1 . Such a choice is possible if the constants c and ζ 0 are small enough constants, for nδ 2 ǫ 2 ζ 4 ≥ 1 with ζ ≤ ζ 0 ; for example, define ψ 2 (k), k ≤ K as in Lemma 3 by ψ 2 (k) = (2⌊K/2⌋) −1/2 (½{k odd, k &lt; K} − ½{k even}), and, for h defined by h(1) = 2 −1/2 , h(3) = −2 −1/2 and h(k) = 0 for all other k, define ψ2 = (ψ 2 + αh)/(1 + α), α = R/(2 − R).</p><p>This satisfies ψ2 − ψ 2 = R, ψ2 = 1, ψ2 , 1 = 0 and ψ2 , ψ 2 ≥ 0. For k ∈ {1, 3} the condition (7) of Remark 4 holds with 1/(2K) in place of 0 on the right, and for k ∈ {1, 3} a direct calculation shows that the condition with 1/(4K) in on the right if R is upper bounded by some c ′ = c ′ (K), which is the case for c = c(K) sufficiently small. Then ρ(φ, ψ; φ, ψ) = |r(φ)| ψ 2 − ψ2 ≤ δǫζ 2 R ≤ cn −1/2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Proofs for Section III</head><p>Proof of Theorem 1. We begin with the upper bounds. From the inversion formulae in Remark 3 we have</p><formula xml:id="formula_206">f0 − f 0 ∨ f1 − f 1 ≤ ψ1 − ψ 1 + 1 2 φ1 φ3 ψ2 − φ 1 φ 3 ψ 2 + 1 2 φ3 ψ2 − φ 3 ψ 2 .</formula><p>Recalling that | φ1 | ≤ 1, that 0 ≤ φ 3 ≤ K 1/2 and that ψ 2 = ψ2 = 1, we decompose the second term on the right, with an implicit decomposition of the third term included:</p><formula xml:id="formula_207">φ1 φ3 ψ2 − φ 1 φ 3 ψ 2 ≤ | φ1 | φ3 ψ2 − φ 3 ψ 2 + |φ 3 || φ1 − φ 1 | ≤ | φ3 − φ 3 | + K 1/2 ψ2 − ψ 2 + φ 3 | φ1 − φ 1 |.</formula><p>It follows that for some constant C we have</p><formula xml:id="formula_208">f0 − f 0 ∨ f1 − f 1 ≤ C max( ψ1 − ψ 1 , ψ2 − ψ 2 , | φ3 − φ 3 |, φ 3 | φ1 − φ 1 |).</formula><p>Applying Proposition 3 as in the proof of Theorem 2, one can show that for some C &gt; 0</p><formula xml:id="formula_209">P φ,ψ φ 2 3 | φ1 − φ 1 | 2 ≥ Cx 2 nǫ 4 ζ 4 ≤ e −x 2 .</formula><p>The upper bounds for estimating f 0 and f 1 then follow from Theorem 2. Similarly, Remark 3 and the fact that</p><formula xml:id="formula_210">|φ 2 | ≤ 1 give |p − p| ∨ |q − q| ≤ 1 2 (1 + | φ1 |)| φ2 − φ 2 | + 1 2 | φ1 − φ 1 ||1 − φ 2 | ≤ 2(| φ1 − φ 1 | ∨ | φ2 − φ 2 |).</formula><p>The upper bounds then again follow from Theorem 2.</p><p>For the lower bounds, writing θ(φ, ψ) = (p, q, f 0 , f 1 ) and θ( φ, ψ) = (p, q, f0 , f1 ), observe by Lemma 2 that it suffices to lower bound max(|p − p|, |q − q|) and max( f 0 − f0 , f 1 − f1 ) corresponding to choices of (φ, ψ), ( φ, ψ) made in the proof of Theorem 3.</p><p>From the inversion formulae in Remark 3 we calculate, for any φ, φ, Inserting from equation <ref type="bibr" target="#b37">(37)</ref> we conclude the bound in either case.</p><p>For (f 0 , f 1 ), again set φ = (1 − 3δ, ǫ, ζ(1 + S) 1/2 ), φ = (1 − 3δ − R, ǫ, ζ) where R = cǫ −2 ζ −3 n −1/2 , and choose ψ = ψ by Lemma 3. As with p and q we deduce that for some c ′ &gt; 0 we have</p><formula xml:id="formula_211">inf θ sup θ∈Θ P θ f0 −f 0 ∨ f1 −f 1 &gt; c ′ ( f 0 − f0 ∨ f 1 − f1 ) ≥ 1/4.</formula><p>Using the inversion formulae in Remark 3 and the fact that ψ = ψ and ψ 2 = 1, one calculates</p><formula xml:id="formula_212">2( f 0 − f0 ∨ f 1 − f1 ) = |φ 1 φ 3 − φ1 φ3 |+|φ 3 − φ3 | ≥ |φ 3 − φ3 |</formula><p>For the current choice of φ, φ, calculating as in proving Theorem 3 Item (3), we have |φ 3 − φ3 | ≥ CζR/δ for some C &gt; 0 and we deduce the lower bound.</p><p>Proof of Corollary 1. It suffices to substitute α = e −x 2 into Theorem 1 and solve for error equal to E, while ensuring that x 2 = log(1/α) is suitably bounded.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>in Euclidean distance at the parametric rate n −1/2 by the empirical estimator p(3)n defined in Section V, Lemma 1. This suggests that solving for ( φ, ψ) ∈ arg min φ,ψ p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 3 )</head><label>3</label><figDesc>θ(φ,ψ) − p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Theorem 3 .</head><label>3</label><figDesc>There exist constants c = c(K) &gt; 0, and ǫ 0 , ζ 0 &gt; 0 such that whenever ǫ ≤ ǫ 0 , ζ ≤ ζ 0 , δ ≤ 1/6 and L ≤ 1/3 the following hold. [The infima are over all estimators, i.e. all measurable functions of the data (Y 1 , . . . , Y n ).] 1) Assume nδ 2 ǫ 4 ζ 6 ≥ 1. Then inf φ1 sup (φ,ψ)∈ΦL(δ,ǫ,ζ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>hence since r(φ) = (6δ − 9δ 2 )ǫζ 2 (1 + S)/4 ≤ 3δǫζ 2 , we obtain ρ(φ, ψ; φ, ψ) ≤ 6ǫ 2 ζ 3 R ≤ 6cn −1/2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>φ 3 it suffices to lower bound |φ 3 / φ3 −1|. Here we the bound√ 1 + x−1 ≥ x/(2 √ 1 + x) ≥ x/(2 √ 2) for 0 ≤ x ≤ 1 to see for a constant c ′ &gt; 0 that |φ 3 / φ3 − 1| ≥ c ′ R/δ.The bound for φ 3 follows from applying Lemma 2.b) Proof of Item (2):For a constant c &gt; 0, define R = cδ −1 ǫ −1 ζ −2 n −1/2 , define φ, φ by φ = (1 − 3δ, ǫ, ζ(1 + R/ǫ) 1/2 ) φ = (1 − 3δ, ǫ + R, ζ),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>φ 1 |φ 2 φ 3 −</head><label>3</label><figDesc>φ2 φ3 | ≤ |φ 2 ||φ 3 − φ3 | + | φ3 ||φ 2 − φ2 | = ǫζ( 1 + R/ǫ − 1) + ζR ≤ 2ζR ≤ R, the last inequality holding if ζ ≤ ζ 0 ≤ 1/2. We deduce ρ(φ, ψ; φ, ψ) = r(φ) max(|φ 2 − φ2 |, |φ 1 φ 2 φ 3 − φ1 φ2 φ3 |)= Rr(φ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>2 (</head><label>2</label><figDesc>|p − p| ∨ |q − q|) ≥ max((1 + |φ 1 |)|φ 2 − φ2 | − |1 − φ2 ||φ 1 − φ1 |, |φ 1 − φ1 ||1 − φ2 | − (1 − |φ 1 |)|φ 2 − φ2 |). (37) If δ &gt; ǫζ set φ = (1 − 3δ, ǫ, ζ(1 + S) 1/2 ) and φ = (1 − 3δ − R, ǫ, ζ), where R = c(nǫ 4 ζ 6 ) −1/2 for some c &gt; 0 and where S ∈ [R/(12δ), R/δ] is, as in the proof of Theorem 3 Item (1), such that r(φ) = r( φ). If δ ≤ ǫζ instead set φ = (1 − 3δ, ǫ, ζ(1 + R/ǫ) 1/2 ), φ = (1 − 3δ, ǫ + R, ζ) with R = c(nǫ 2 δ 2 ζ 4 ) −1/2 . Ineither case the proof of Theorem 3 demonstrates that for suitable ψ = ψ we have KL(p 1/100 for c small enough hence by Lemma 2 inf θ sup θ∈Θ P θ |p − p| ∨ |q − q| &gt; c ′ (|p − p| ∨ |q − q|) ≥ 1/4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>100, where we recall KL denotes the Kullback-Leibler divergence.</figDesc><table><row><cell>Then</cell></row><row><cell>inf û sup u∈U</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Identifiability of parameters in latent structure models with many observed variables</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Allman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Rhodes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6A</biblScope>
			<biblScope unit="page" from="3099" to="3132" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A spectral algorithm for learning hidden Markov models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. System Sci</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1460" to="1480" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tensor decompositions for learning latent variable models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anandumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Telgarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2773" to="2832" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Inference in finite state space non parametric hidden Markov models and applications</title>
		<author>
			<persName><forename type="first">É</forename><surname>Gassiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cleynen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Comput</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="61" to="71" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nonparametric identification and maximum likelihood estimation for hidden Markov models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Alexandrovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Holzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="423" to="434" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wavelet based non-parametric HMMs: theory and methods</title>
		<author>
			<persName><forename type="first">L</forename><surname>Couvreur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couvreur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP &apos;00 Proceedings</title>
				<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="604" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Non-parametric probability estimation for HMM-based automatic speech recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lefèvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speach and Language</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="113" to="136" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A non-parametric hidden Markov model for climate state identification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Whiting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Metcalfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hydrology and Earth System Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="652" to="667" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nonparametric discriminant HMM and application to facial expression recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2090" to="2096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bayesian non-parametric hidden Markov models with applications in genomics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Papaspiliopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">O</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Holmes</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-9868.2010.00756.x</idno>
		<ptr target="http://dx.doi.org/10.1111/j.1467-9868.2010.00756.x" />
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B Stat. Methodol</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="57" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hidden Markov Models with mixtures as emission distributions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Volant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bérard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Martin-Magniette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-scale multiple testing under dependence</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Cai</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-9868.2008.00694.x</idno>
		<ptr target="https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2008.00694.x" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="393" to="424" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multiple testing in nonparametric hidden Markov models: An empirical Bayes approach</title>
		<author>
			<persName><forename type="first">K</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Gassiat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1" to="57" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Phase transition for parameter learning of Hidden Markov Models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Hartmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Consistent estimation of the filtering and marginal smoothing distributions in nonparametric hidden Markov models</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">De</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gassiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Le</forename><surname>Corff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4758" to="4777" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Minimax adaptive estimation of nonparametric hidden Markov models</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">De</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Gassiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lacour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">111</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Consistent order estimation for nonparametric hidden Markov models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lehéricy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="464" to="498" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nonasymptotic control of the MLE for misspecified nonparametric hidden Markov models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lehéricy</surname></persName>
		</author>
		<idno type="DOI">10.1214/21-EJS1890</idno>
		<ptr target="https://doi.org/10.1214/21-EJS1890" />
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4916" to="4965" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">State-by-state minimax adaptive estimation for nonparametric hidden Markov models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lehéricy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">39</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The local geometry of finite mixtures</title>
		<author>
			<persName><forename type="first">É</forename><surname>Gassiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Handel</surname></persName>
		</author>
		<idno type="DOI">10.1090/S0002-9947-2013-06041-2</idno>
		<ptr target="https://doi-org.revues.math.u-psud.fr/10.1090/S0002-9947-2013-06041-2" />
	</analytic>
	<monogr>
		<title level="j">Trans. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">366</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1047" to="1072" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Strong identifiability and optimal minimax rates for finite mixture estimation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<idno type="DOI">10.1214/17-AOS1641</idno>
		<ptr target="https://doi-org.revues.math.u-psud.fr/10.1214/17-AOS1641" />
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">6A</biblScope>
			<biblScope unit="page" from="2844" to="2870" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Optimal estimation of highdimensional Gaussian mixtures</title>
		<author>
			<persName><forename type="first">N</forename><surname>Doss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05818</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A probabilistic theory of pattern recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Devroye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Györfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gene hunting with hidden Markov model knockoffs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sesia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sabatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<idno type="DOI">10.1093/biomet/asy033</idno>
		<ptr target="https://doi-org.revues.math.u-psud.fr/10.1093/biomet/asy033" />
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adapting to unknown sparsity by controlling the false discovery rate</title>
		<author>
			<persName><forename type="first">F</forename><surname>Abramovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Benjamini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Johnstone</surname></persName>
		</author>
		<idno type="DOI">10.1214/009053606000000074</idno>
		<ptr target="https://doi-org.revues.math.u-psud.fr/10.1214/009053606000000074" />
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="584" to="653" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On spike and slab empirical Bayes multiple testing</title>
		<author>
			<persName><forename type="first">I</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Roquain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2548" to="2574" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<idno type="DOI">10.1214/19-AOS1897</idno>
		<ptr target="https://doi-org.revues.math.u-psud.fr/10.1214/19-AOS1897" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Statistical decision functions, random processes held at Liblice near Prague from November 28 to 30</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blackwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the first Prague conference on information theory</title>
				<meeting><address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1956">1956. 1957</date>
			<biblScope unit="page" from="13" to="20" />
		</imprint>
	</monogr>
	<note>Publishing House of the Czechoslovak Academy of Sciences</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Upper bound Kullback-Leibler divergence for transient hidden Markov models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSP.2008.924137</idno>
		<ptr target="https://doi-org.revues.math.u-psud.fr/10.1109/TSP.2008.924137" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4176" to="4188" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast approximation of Kullback-Leibler distance for dependence trees and hidden Markov models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="115" to="118" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the entropy rate of hidden Markov processes observed through arbitrary memoryless channels</title>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIT.2009.2013030</idno>
		<ptr target="https://doi-org.revues.math.u-psud.fr/10.1109/TIT.2009.2013030" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1460" to="1467" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Quickest change detection and Kullback-Leibler divergence for two-state hidden Markov models</title>
		<author>
			<persName><forename type="first">C.-D</forename><surname>Fuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mei</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSP.2015.2447506</idno>
		<ptr target="https://doi-org.revues.math.u-psud.fr/10.1109/TSP.2015.2447506" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="4866" to="4878" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Identifiability and consistent estimation of nonparametric translation hidden Markov models with general state space</title>
		<author>
			<persName><forename type="first">É</forename><surname>Gassiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Le</forename><surname>Corff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lehéricy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">115</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hidden Markov model for parameter estimation of a random walk in a Markov environment</title>
		<author>
			<persName><forename type="first">P</forename><surname>Andreoletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Loukianova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Matias</surname></persName>
		</author>
		<idno type="DOI">10.1051/ps/2015008</idno>
		<ptr target="https://doi-org.revues.math.u-psud.fr/10.1051/ps/2015008" />
	</analytic>
	<monogr>
		<title level="j">ESAIM Probab. Stat</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="605" to="625" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Statistical clustering of temporal networks through a dynamic stochastic block model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Miele</surname></persName>
		</author>
		<idno type="DOI">10.1111/rssb.12200</idno>
		<ptr target="https://doi-org.revues.math.u-psud.fr/10.1111/rssb.12200" />
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B. Stat. Methodol</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1119" to="1141" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Nonparametric identification in the dynamic stochastic block model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Holzmann</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIT.2019.2893947</idno>
		<ptr target="https://doi-org.revues.math.u-psud.fr/10.1109/TIT.2019.2893947" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="4335" to="4344" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Joint selfsupervised blind denoising and noise estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ollion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ollion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Gassiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lehéricy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Le</forename><surname>Corff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08023</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deconvolution with unknown noise distribution is possible for multivariate signals</title>
		<author>
			<persName><forename type="first">É</forename><surname>Gassiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Le</forename><surname>Corff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lehéricy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="303" to="323" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Asymptotic methods in statistical decision theory</title>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Cam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Introduction to nonparametric estimation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Tsybakov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">GloptiPoly 3: moments, optimization and semidefinite programming</title>
		<author>
			<persName><forename type="first">D</forename><surname>Henrion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Lasserre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Löfberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optim. Methods Softw</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="761" to="779" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Concentration inequalities for Markov chains by Marton couplings and spectral methods</title>
		<author>
			<persName><forename type="first">D</forename><surname>Paulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. J. Probab</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">79</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mathematical foundations of infinite-dimensional statistical models, ser</title>
		<author>
			<persName><forename type="first">E</forename><surname>Giné</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nickl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cambridge Series in Statistical and Probabilistic Mathematics</title>
				<imprint>
			<biblScope unit="volume">40</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
