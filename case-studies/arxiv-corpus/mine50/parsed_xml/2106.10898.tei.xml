<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BanditMF: Multi-Armed Bandit Based Matrix Factorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-24">24 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Recommender</forename><surname>System</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shenghao</forename><surname>Xu</surname></persName>
						</author>
						<title level="a" type="main">BanditMF: Multi-Armed Bandit Based Matrix Factorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-24">24 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">8DA32A0BB2B903C6ABAEC6A20114E8B7</idno>
					<idno type="arXiv">arXiv:2106.10898v4[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-10-31T05:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-armed bandits (MAB) provide a principled online learning approach to attain the balance between exploration and exploitation. Generally speaking, in a multi-armed bandit problem, to obtain a higher reward, the agent must choose the optimal action in various states based on previous experience (exploit) known actions to obtain a higher score; to discover these actions, the necessary discovery is required (exploration). Due to the superior performance and low feedback learning without the learning to act in multiple situations, multi-armed bandits are drawing widespread attention in applications ranging from recommender systems. Likewise, within the recommender system, collaborative filtering (CF) is arguably the earliest and most influential method in the recommender system. The meaning of collaboration is to filter the information through the relationship between the users and the feedback of the user's rating of the items together to find the target users' preferences. Crucially, new users and an ever-changing pool of recommended items are the challenges that recommender systems need to address. For collaborative filtering, the classical method is to train the model offline, then perform the online testing, but this approach can no longer handle the dynamic changes in user preferences, which is the so-called cold start. So, how to effectively recommend items to users in the absence of effective information?</p><p>To address the aforementioned problems, a multi-armed bandit based collaborative filtering recommender system has been proposed, named BanditMF. BanditMF is designed to address two challenges in the multi-armed bandits algorithm and collaborative filtering:</p><p>(1) how to solve the cold start problem for collaborative filtering under the condition of scarcity of valid information, (2) how to solve the sub-optimal problem of bandit algorithms in strong social relations domains caused by independently estimating unknown parameters associated with each user and ignoring correlations between users.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 1 Introduction</head><p>In this section, first, a detailed account of the recommender system is given, followed by the general information about multi-armed bandits (MAB) and collaborative filtering (CF), which will be introduced. The problem statement will also be covered in this chapter. Also, the aims and contributions of this project will have to be indicated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">General Introduction</head><p>Nowadays, with the rapid development of social networks, e-commerce, and sharing economies, it has become a core link of Internet services to discover users' needs, understand users' behaviors, and screen out the most relevant information and products for users. There is a huge amount of information on the Internet: YouTube users upload over 400 hours of video every minute; WeChat Moments receive 10 billion clicks per day; Instagram Stories have 500 million active users per day 1 . The recommendation system has become an essential part of various video websites and e-commerce websites to provide various personalized services for users to interact with the system.</p><p>Early recommendation systems relied more on simple models or algorithms guided by intuition. For example, the recommendation system based on information retrieval treats the user's information as a query and baesd on various types of information to express the item to be recommended as a document. Thus, the problem of recommending the most relevant set of items is transformed into the problem of finding the most relevant documents in information retrieval. As the field of recommendation systems has evolved,  <ref type="bibr" target="#b16">[17]</ref>. The content-based approach describes the nature of users and items by defining explicit attributes (usually defined by experts with professional knowledge) and then recommends items that are "compatible" with the user's nature. Traditional collaborative filtering, as shown in Figure <ref type="figure">1</ref>.1, is categorized into two categories <ref type="bibr" target="#b0">[1]</ref>: memory-based methods and model-based methods. Item-based collaborative filtering and user-based collaborative filtering fall under the category of memory-based methods. For matrix factorization (MF), it is the most popular collaborative filtering technique that belongs to model-based methods <ref type="bibr" target="#b8">[9]</ref>.The three methods mentioned above will be elaborated and analyzed in the next chapter.</p><p>Matrix factorization attracts a ton of attention due to its good scalability and ease of implementation. In <ref type="bibr" target="#b10">[11]</ref>, the authors comprehensively introduce the matrix factorization algorithm, which integrates machine learning technology. Other than that, the authors in <ref type="bibr" target="#b8">[9]</ref> proposed a method that combines matrix factorization with convolutional neural networks (CNN). Matrix factorization is scalable; it can be seamlessly integrated with mainstream models and can also do feature fusion with multiple information sources. So, in <ref type="bibr" target="#b7">[8]</ref>, a model that fusion the text comment information, geographic neighborhood information, project category information, and popularity information has been proposed to promote the precision of rating prediction. However, these recommender systems focus on how to improve the accuracy of algorithms for a single problem (e.g., rating prediction for matrix factorization), thus neglecting the timeliness and systematicness of humanmachine interaction, which makes it difficult to model the vagaries of user behavior and the rapidly changing external environment completely. These matrix factorization-based recommendation systems only exploit the trained model for the recommendation and do no proper exploration. Thus, these recommendation systems tend to produce cookiecutter results.</p><p>In order to achieve a dynamic balance between the system and the user and to shift away from viewing the recommendation system as a running fragment of one recommendation result at a time. Some researchers, such as the authors in <ref type="bibr" target="#b11">[12]</ref> have started to explore how to apply some concepts of reinforcement learning to the scenario of recommendation systems. Reinforcement learning often represents a "feedback system" in which the user and the system interact to achieve the optimization of the target. The interaction process between the system and users is no longer a "one-shot deal", which is replaced by the dynamic balance between the system and users that achieved through a series of results on the dimension of time. A number of algorithms have been proposed based on the context-free bandit and the contextual bandit. Further detailed concepts and algorithms of context-free bandit and contextual bandit will be conducted in chapter 2. However, in the more complex scenario of online mass recommendation systems, the above algorithm has not been favored. Although such algorithms have beautiful proofs and mathematical properties in theory, there are often insurmountable barriers to product experience in practice. As the authors affirm in <ref type="bibr" target="#b20">[20]</ref>, the contextual bandit algorithm requires a tremendous amount of exploration, resulting in a slow speed. Most of these algorithms are not familiar with the user in the initial period, so the algorithm tends to explore more items to understand the user. Therefore, during this period, the user may be confronted with a number of items that are not as relevant. In these algorithms, the assumption is that once the exploration period is over, the algorithm will be able to learn the user's preferences better and provide more "trustworthy" recommendations.</p><p>However, this hypothetical scenario does not hold in reality. In the real world, only highly loyal users may have the tolerance to accept less relevant recommendations. From the perspective of new users or users with low loyalty, the frequency of use of the product is not high, and when these users find some irrelevant recommendation results in a few uses, it is likely to cause new users to abandon the product permanently.</p><p>As alluded to above, collaborative filtering is the process of sifting and filtering information together with the relationship between users and their feedback on the items, to recommend items of interest to the target users. Since coordinated filtering focuses more on judging the similarity between historical records and a user's similarity to make recommendations, the result is that the content universe of collaborative filtering is almost static and loses the ability to recommend new items. Unfortunately, in many recommendation scenarios, where the universe of content experiences rapid turnover and the popularity of content changes over time. Furthermore, the cold start <ref type="bibr" target="#b13">[14]</ref> problem is also an insurmountable chasm for collaborative filtering. In former research <ref type="bibr" target="#b4">[5]</ref>, it was demonstrated that these problems led to poor performance and difficulty in applying traditional collaborative filtering-based recommender systems.</p><p>To address the aforementioned problems, in this work, BanditMF has been proposed, which is a multi-armed bandit based matrix factorization recommender system. This system combines the matrix factorization (MF) which is model-based collaborative filtering with the multi-armed bandit algorithm. BanditMF contains an offline subsystem focusing on matrix factorization and an online subsystem with a multi-armed bandit algorithm as its core. With the power of the multi-armed bandit algorithm, BanditMF solves the coldstart problem of the collaborative filtering method and gives the ability to recommend new items to users. Meanwhile, the bandit algorithm in the online subsystem accepts parameters from the matrix factorization in the offline subsystem to make recommendations. Since the matrix factorization takes into account the connections and similarities between users, the multi-armed bandit algorithm uses these parameters to reduce the irrelevance of the recommended items when exploring, thus reducing the loss of new users with low loyalty due to irrelevant recommendations from the bandit algorithm during the exploration period. The detailed discussion of BanditMF will be introduced in chapter 3.</p><p>The remainder of this report is organized as follows: In Chapter 2, the representative multi-armed bandit and collaborative filtering methods will be reviewed and formulated to solve the problem. In Chapter 3, two traditional systems will be illustrated: a collaborative filtering-based recommender system and a hybrid recommender system. Then, based on those traditional systems, BanditMF will be introduced in detail. In chapter 4, the two traditional systems are implemented and experimentally evaluated with Ban-ditMF. We propose the future work and the conclusion of the entire work will be present in chapter 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 2 Formulation</head><p>In this chapter, context-free bandit and contextual bandit will be introduced, followed by a brief review of the collaborative filtering method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-Armed Bandit</head><p>The Multi-armed bandits (MAB) framework provides principled solutions for the dilemma between exploration and exploitation.</p><p>As one of the classic problems of reinforcement learning-Multi-armed bandits (MAB)-is different from machine learning (e.g., supervised learning), it is based on environmental interaction and trial-and-error to achieve the effect of learning. Such a learning mode is more similar to the human learning mode.  In Table <ref type="table" target="#tab_1">2</ref>.1 2 , four different scenarios have been described, which are the scenarios when making decisions under uncertainty. In the setting of multi-armed bandits, the outcomes (rewards) are unknown, and the outcomes can be stochastic or adversarial.</p><p>The state of the world can not be changed by actions.</p><p>The reinforcement learning process consists of five major elements: agent, environment, state, action, and reward. At a certain instant, the intelligence is in a certain state, and after executing an action, the environment receives the action, prompting the agent to move to the next state while feeding back the reward. The agent's purpose is to maximize the accumulated reward and adjust the action accordingly to the amount of the reward to get a higher reward.</p><p>For reinforcement learning can using Markov decision process (MDP) to formalize <ref type="bibr" target="#b18">[18]</ref>.</p><p>Formally, a Markov decision process is a 4 tuple (S, A, P a , R a ), where: Multi-armed bandits is a classic reinforcement learning problem (essentially a simplified class of reinforcement learning problems) that has non-associative states (learning from only one situation at a time, losing or winning) and focuses only on evaluable feedback. Suppose there is a bandit with K arms. Each arm will have a certain probability of getting a reward, so that we have K possible actions (each arm corresponds to an action), and the result of each action is only associated with the current state and is not affected by the results of historical actions. This means the reward of each arm is only related to the probability set by the bandit. The previous winning and losing results will not affect this action. So we can formally define multi-armed bandits problems as Markov decision processes with a single state as follows:</p><formula xml:id="formula_0">• S is</formula><p>• No sate S, but S 0 can denote as hypothetical constant state.</p><p>• K arms correspond to K different actions, where A=a 1 , a 2 , a 3 , ,• • •,a K .</p><p>• Do not have probability P, but it can be assumed that the state always transitions from S 0 to S 0 .</p><p>• Have a reward function R dependence on action A instead of state S.</p><p>The key idea of the multi-armed bandits is to make the optimal decisions to maximize the total reward through exploration and exploitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Context-Free Bandit</head><p>Bandit algorithms can be categorized into context-free bandit and contextual bandit depending on whether or not contextual features are taken into account. For the traditional K-armed bandit due to:</p><p>1. The arm set A t = {a 1 , a 2 , . . . , a k } with K actions are fixed for all t ∈ T , where T denote the total rounds.</p><p>2. Both user u t and context x t,a are unchanged for all t ∈ T .</p><p>Whenever the arm is pulled, as arm set A t and context x t,a are constant, therefore they do not affect the algorithm. So, for this kind of bandit algorithm, we refer to them as the context-free bandit <ref type="bibr" target="#b11">[12]</ref>.</p><p>For the above context-free bandit, we formulated as follows:</p><p>Problem Formulation: The context-free bandit Given: T rounds, K actions, 1 agent.</p><p>In each round, for t ∈ [T ], agent chooses the policy :</p><p>1. Policy choose action a t .</p><p>2. For the chosen action, policy observes the rewards r t , with r t ∈ [0, 1].</p><p>For context-free bandit, µ denoting the mean reward vector, where µ ∈ [0, 1] K . The mean reward of action a is expressed as µ(a) = E(D a ). The µ * is used to define the best mean reward, where µ * := max a i ∈{a i } K i=1 µ(a). We use the notation ∆(a) to represent the gap of action a. The gap describes the difference between the reward received for action a and the reward received for the optimal action, that ∆(a) := µ * − µ(a) and a * is used to denote the optimal action, which action with µ(a) = µ * . Formally, regret define as follows:</p><formula xml:id="formula_1">R(T ) T t=1 (µ * − E[µ (at) ]) = T • µ * − E T t=1 µ (at) (2.1)</formula><p>The R(T ) is called as regret at round T , where a t is denote the action chosen at round t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Contextual Bandit</head><p>As opposed to the context-free bandit, the contextual bandit is based on contextual information of the user and items (i.e. feature vector x t,a ) to inferring the conditionally expected reward of an action.</p><p>For the above contextual bandit, we formulated as follows:</p><p>Problem Formulation: The contextual bandit Given: T rounds, users u t , arm set A t , feature vector (context) x t,a .</p><p>In each round, for t ∈ [T ], do :</p><p>1. For all a ∈ A t , algorithm observers the current user u t and arm set A t with correspond feature vector x t,a .</p><p>2. Algorithm chosen action a t , and revives the rewards r t,at .</p><p>3. With new observation x t,a , a t , and r t,at , the algorithm improves its arm selection strategy π.</p><p>As the variant of the multi-armed bandits, the contextual multi-armed bandits (CMAB) are different from the stochastic bandit and adversarial bandits. In the setting of the stochastic setting, the reward of an arm is sampled from the unknown distribution, or so-called reward distribution. In an adversarial environment, the reward for one arm is chosen by the adversarial, rather than drawn from any distribution. Conversely, under the setting of the contextual bandits, what we are interested in is the situation that we need to observe side information at each time slot, where side in formation is the so-called contexts x t,a . For the contextual bandit, there is a set of actions A t , and each action maps a context to an arm, where at each iteration, before the arm selection, the agent will observe the d -dimensional context, where x t,a ∈ R d . Based on this context, along with the rewards of arms played in the past, to select the arms to be chosen in the current iteration. The goal of the algorithm is to maximize the reward over finite times t ∈ [T ].</p><p>T t=1 r t,at denote the total rewards of the algorithm and define the optimal expected cumulative reward as E T t=1 r t,a * t , where a * t denote the optimal arm at time t. The regret of the contextual bandit algorithm is defined as:</p><formula xml:id="formula_2">R(T ) E T t=1 r t,a * t − E T t=1 r t,at<label>(2.2)</label></formula><p>To address the general situation, different algorithms are proposed, which include LIN-UCB proposed at <ref type="bibr" target="#b11">[12]</ref>, Contextual Thompson Sampling (CTS) described in <ref type="bibr" target="#b1">[2]</ref>, and Neural</p><p>Bandit in <ref type="bibr" target="#b2">[3]</ref>, in these algorithms, a linear dependency between the expected payoff of an action and its context is generally hypothesized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Collaborative Filtering</head><p>In this section, two categories of methods involved in collaborative filtering that will be discussed: memory-based methods and model-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Memory-Based Methods</head><p>Collaborative filtering takes into account the fact that there is a relationship between the items and the user's preference. In memory-based approaches, as the name suggests, a memory-based recommender system typically needs to leverage the entire user-item dataset and make recommendations accordingly. Once we have the user rating data for corresponding number is the user's rating of the item. The question mark within the matrix indicates that the user has not rated the item, and our goal is to use collaborative filtering to predict unknown ratings. Memory-based collaborative filtering to make recommendations by similar users or similar items, then the question arises how to define similar users or similar items? The answer is similarity. After we have the user-item rating matrix, as shown above, the row vectors in the matrix represent the preferences of each user, and the column vectors represent the attributes of each item. This can be interpreted as meaning that there exists an attribute of the item that attracts users to provide a corresponding rating. So, by computing the similarity between row vectors, we will get the user similarity, and by computing the similarity of vertical vectors, the item similarity will get. Cosine similarity, Pearson correlation coefficient, and Euclidean distance are the common methods to compute the similarity.</p><p>This chapter has demonstrated the concept of memory-based collaborative filtering.</p><p>It is now necessary to explain the two basic approaches of memory-based collaborative filtering: the user-based method and the item-based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.1">User-Based Collaborative Filtering</head><p>For user-based collaborative filtering, in each recommendation, we have a target user who will be recommended. The user-based collaborative filtering algorithm first identifies users that are similar to that particular target user, i.e., users that share the target user's rating pattern. User-based collaborative filtering predicated on similarities including history, preferences, and choices made by users when purchasing, viewing, or certain other behaviors. Based on the ability conferred by similarity, we can estimate the ratings that a target user might give based on similar users' ratings of items that the target user did not give a rating on. Based on the predicted rating, the user-based collaborative filtering algorithm can recommend items with higher ratings to the target users.</p><p>User-based collaborative filtering can be illustrated briefly in Figure <ref type="figure">2</ref>.2. Recall that the recommendation of the user-based method is based on similar users who share the same preference with the target user. As the example shown in Figure <ref type="figure">2</ref>.2. Both user1 and user3 show a preference for item3 and item4, and for this reason, we consider user1 and user3 to be similar users. Then, the user-based collaborative filtering algorithm will recommend item1 to user3 that item1 is the item that wins the positive rating by user3 's We use Figure <ref type="figure">2</ref>.3 to interpret the process of item-based collaborative filtering. The first step is to identify the similar item through the behavior of users instead of the contents of the items ( if we use the content of the item as the similarity criterion, then the system will morph into a content-based recommendation system). In the example in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Model-Based Methods</head><p>It is worth noting that in the real world, the number of users is generally huge (e.g., Netflix has millions of users), compared to the number and type of items that are relatively fixed. In this scenario, calculating the similarity between items offline by using item-based collaborative filtering is preferred in order to optimize the efficiency of recommendation.</p><p>On the contrary, in the case of a small number of users, user-based collaborative filtering is preferred. It can be seen from this that there is no good or bad recommendation system, and the important thing is to choose the right recommendation method in the right situation.</p><p>As can be seen from the above, memory-based methods are intuitive and highly interpretable models. Users can easily figure out why the memory-based system recommends these items to them (due to their similar users preferring this item or the item is similar to the one you have purchased or viewed). But on the other hand, the effect of memorybased methods to recommend popular products is more obvious. That is to say, if a lot of people like item1, under memory-based methods, the probability of recommending it to you will be higher, although if item item2 also matches your interest, due to the rating for item2 is sparse, so the similarity of item item2 is greatly reduced when calculating the similarity.</p><p>So, is there a method that can handle sparse data with better generalization ability?</p><p>Turning now to the methods to address the above question, which are called model-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.1">Latent Factor Models</head><p>Model-based collaborative filtering has been studied for decades, and many methods have been proposed. Such as in <ref type="bibr" target="#b9">[10]</ref>, a multifaceted collaborative filtering model has been proposed or the probabilistic matrix factorization be described in <ref type="bibr" target="#b15">[16]</ref>. In general, modelbased collaborative filtering can produce a more accurate recommendation <ref type="bibr" target="#b9">[10]</ref>. In this section, matrix factorization (MF) which is the most popular latent factor model, will be illustrated.</p><p>The purpose of matrix factorization is to discover latent factor models of users and items in a shared latent space where the user-item relationship (e.g., user rating of the item) is calculated from the inner product of the user matrix and the item matrix.For a user-item rating matrix R consisting of m users and n items, where R ∈ R m×n . Through matrix factorization, the m × n user-item matrix R be factorized into user latent matrix p (m × k matrix) and item latent matrix q (k × n matrix), where k is the dimension of the latent factor. In matrix factorization, the latent model of user u and item i are denoted by p u and q i respectively, where p u ∈ R k , q i ∈ R k . The notation r u,i is used to represent the rating of user u on item i. The predicted rating of user u on item i can be described as :</p><formula xml:id="formula_3">ru,i = q T i p u (2.3)</formula><p>A general approach to training the latent model is to minimize the gap between predicted rating ru,i and real rating r u,i that can be express as r u,i − q T i p u , by adopting root mean square error (RMSE) as the loss function, we have:</p><formula xml:id="formula_4">Loss = min q * ,p * (u,i)∈K r u,i − q T i p u 2 (2.4)</formula><p>where K denotes the set of (u, i) pairs. By adopting l 2 regularization to void over-fitting problem, we have:</p><formula xml:id="formula_5">Loss = min q * ,p * (u,i)∈K r u,i − q T i p u 2 + λ ( q i + p u ) 2 (2.5)</formula><p>Chapter 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodologies</head><p>In this chapter, we will discuss the techniques mentioned above in the manner that from the shallow to the deep. We will first discuss the methods individually, then we will diverge from a single method to a recommendation system that contains multiple methods, and finally, we will provide details of the proposed model, multi-armed bandit-based matrix factorization recommender system (BanditMF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Matrix Factorization</head><p>As discussed above, to reduce the gap between predicted rating ru,i and real rating r u,i , root mean square error (RMSE) is used as the loss function and l 2 norm is adopt as the regularization term. Then, when we already have the loss function, the next step is to minimize the loss function. Here we use the stochastic gradient descent (SGD) method to minimize the loss function. Next, the derivation of stochastic gradient descent will be provided. By Taylor expansions, we have the following equation:</p><formula xml:id="formula_6">f (θ) ≈ f (θ 0 ) + (θ − θ 0 ) • f (θ 0 ) (3.1)</formula><p>where (θ − θ 0 ) is a vector, it represents the distance of change. We can denote it by η. η is a scalar, and the unit vector of (θ − θ 0 ) is expressed by v:</p><formula xml:id="formula_7">(θ − θ 0 ) = η • v (3.2)</formula><p>By formula 3.2, the formula 3.1 can change as:</p><formula xml:id="formula_8">f (θ) ≈ f (θ 0 ) + ηv • f (θ 0 ) (3.3)</formula><p>To minimize the loss function, we need minimize the f (θ). In other words, we want to make the value of the f (θ) smaller each time we update the θ. So that f (θ) − f (θ 0 ) &lt; 0, by this we have:</p><formula xml:id="formula_9">f (θ) − f (θ 0 ) ≈ ηv • f (θ 0 ) &lt; 0 (3.4)</formula><p>Where, η is scalar and generally positive, which can be ignored. Therefore:</p><formula xml:id="formula_10">v • f (θ 0 ) &lt; 0 (3.5)</formula><p>From vector multiplication, when the angle between two vectors is 180 degrees, the product of vector multiplication is less than 0, and minimal. So, when the direction of v and f (θ 0 ) is opposite (i.e. included angle is 180 degrees), v • f (θ 0 ) &lt; 0 and have minimal value. From this we can get:</p><formula xml:id="formula_11">v = − f (θ) f (θ) (3.6)</formula><p>where v is a unit vector, so we divide it by the norm of the f (θ), substitute the formula 3.6 into formula 3.2:</p><formula xml:id="formula_12">θ = θ 0 − η f (θ) f (θ) (3.7)</formula><p>Due to the norm of a vector being a scalar, we obtain:</p><formula xml:id="formula_13">θ = θ 0 − ηf (θ) (3.8)</formula><p>where θ 0 is is the current parameter, η denote the stride or so call learn rate, f (θ) is the direction of the gradient. However, our loss function is a multivariate function, so we need to calculate the partial derivatives of it. So the formula of θ become:</p><formula xml:id="formula_14">θ = θ 0 − η ∂ ∂θ 0 J(θ) (3.9)</formula><p>For our loss function,</p><formula xml:id="formula_15">Loss : J(p, q) = min q * ,p * (u,i)∈K r u,i − q T i p u 2 + λ ( q i + p u ) 2 (3.10)</formula><p>Where q i and p u are the variables which need to optimize, λ is the regularization factor.</p><p>Our goal is to optimize the q i and p u and find the minimum value of the loss function.</p><p>Perform partial derivative on both q i and p u :</p><formula xml:id="formula_16">∂E ∂q i = −2 r u,i − q T i p u p u + 2λq i (3.11) ∂E ∂p u = −2 r u,i − q T i p u q i + 2λp u (3.12)</formula><p>Substituted two partial derivatives into the gradient descent formula 3.9:</p><formula xml:id="formula_17">q i = q i + 2η r u,i − q T i p u p u − λq i<label>(3.13)</label></formula><formula xml:id="formula_18">p u = p u + 2η r u,i − q T i p u q i − λp u<label>(3.14)</label></formula><p>By define e u,i = r u,i − q T i p u . we have:</p><formula xml:id="formula_19">q i ← q i + η (e u,i p u − λq i ) (3.15) p u ← p u + η (e u,i q i − λp u ) (3.16)</formula><p>To sum up, the above process is the derivation process of stochastic gradient descent.By the stochastic gradient descent algorithm, we train two latent vector matrices: user matrix p and item matrix q. Through the inner product of user matrix p and item matrix q, we can have the predicted user-item matrix. If the user matrix and item matrix are fitted by the loss function perfectly, then our predicted rating will be infinitely close to the real rating.</p><p>In this project, we consider another scenario in which some harsh users give a low rating, but some users are more tolerant, giving a relevant high score for bad items. By this intuition, we trying to add bias into matrix factorization.</p><p>ru,i = q T i p u basic + b ui bias (3.17)</p><p>So that our predicted rating become:</p><formula xml:id="formula_20">ru,i = b u,i + q T i * p u (3.18)</formula><p>And the loss function is:</p><formula xml:id="formula_21">Loss = min q * ,p * (u,i)∈K r u,i − b u,i − q T i p u 2 + λ q i 2 + p u 2 + b 2 u + b 2 i (3.19)</formula><p>where b u,i = µ+b u +b i . We use µ to denote the average rating for the entire rating matrix, b u denote the user's bias, and b i is represent the item bias. In the next chapter, we will simulate these two approaches with real data and evaluate the performance between the two approaches .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Memory-Based Collaborative Filtering Recommender System</head><p>As was pointed out in the formulation of this report, memory-based collaborative filtering makes recommendations based on the similarity between the users or the items. The detailed steps of the memory-based collaborative filtering can be stated as follows:</p><p>1. Identify a target user who will be recommended.</p><p>2. Figure out the items that the target user has given ratings for.</p><p>3. Based on the target user's rating pattern, find similar users.</p><p>4. Get the rating records of similar users for items.</p><p>5. Calculate the similarity between the target user and a similar user by formula (e.g.,</p><p>Euclidean Distance, Pearson Correlation, Cosine Similarity).</p><p>6. Recommend the items with the highest score to the target user.</p><p>In the following, an example will be demonstrated to elaborate on the above steps. two users, we base it on the ratings of item2, item3, and item4 which are the items that all users have rated. Here we make a hypothesis that the similarity between the target user and other users is 0.4, 0.9, and 0.6, respectively, as shown in Figure <ref type="figure">3</ref>.2. The next step is that, based on the value of similarity, we can calculate the predicted rating for the target user. What can be seen in Figure <ref type="figure">3</ref>.3 is the so-called weighted rating matrix because it provides more weight to those users who have a higher similarity to the target user. For the final step, we tally up all weighted rates to create the recommender matrix.</p><p>Due to the sparsity of the weighted rating matrix (e.g. only user2 and user3 provided the rating for item5 , we normalize the value of the weighted rating by dividing the sum of weighted ratings by the sum of the similarity for users. According to the result of user- describes the potential rating for the target user that will arise for the items based on the similarity between them and other users. The target user potentially provides a higher rating to item5 than item1. So, through the result, the user-based collaborative filtering recommender system will choose item5 to recommend to the target user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hybrid Recommender System: Combination of</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Content-Based Method and Matrix Factorization</head><p>In the above paper, matrix factorization has been discussed. For the content-based method, the concept is straightforward, which is to recommend items that are similar to what the target user liked before. Now, the intuition is whether we can combine the two approaches into a hybrid recommender system. Recall that in matrix factorization, by the stochastic gradient descent algorithm, we train two latent vector matrices: user matrix p and item matrix q. Through the inner product of user matrix p and item matrix q, we can have the predicted user-item matrix. In the predicted user-item matrix, row vectors represent the preferences of each user, and column vectors represent the attributes of each item, which means the user similarity can be obtained by calculating the similarity between row vectors, and similarly, the item similarity can be obtained by calculating the similarity of column vectors.</p><p>For the proposed hybrid system, the content-based approach is performed based on the matrix factorization (i.e., the content-based approach calculates the similarity through the user-item rating matrix derived by the result of matrix factorization). Consider that the detailed methodology of matrix factorization is already stated in section 3.1, so there we will illustrate the methodology after obtaining the user-item rating matrix, which derives from the result of matrix factorization.</p><p>Assume the predicted user-item rating matrix is available to recommend items to the target user. We have the following steps:</p><p>1. List all the item indexes rated by the target user.</p><p>2. Sort the items that have not yet been rated by the target user (i.e., the item indexes are not listed in the step1 ) through our predicted rating score.</p><p>3. Get the top ranked item of the list created by step2.</p><p>Then we use the cosine similarity for similarity measurement to measure how similar the items are in attractiveness to the target user, where:</p><formula xml:id="formula_22">similar(i, j) = cos(i, j) = i • j i • j (3.20)</formula><p>The formula uses the angle between two vectors to calculate the cosine value. The angle between the vectors i and j is the thing this formula cares about, instead of the magnitude.</p><p>So that the value of similarity is within the interval [-1,1], and 1 means "exactly the same".</p><p>After calculating the cosine similarity, we sort the items by the value of similarity, and then we can get the items that are most similar to the target user's favorite items. The implementation of this hybrid recommender system with real-world data will be presented in the next chapter.</p><p>It is foreseeable that if it combines memory-based collaborative filtering with a contentbased approach, it can solve the problems faced by memory-based collaborative filtering, such as sparsity and loss of information. Similarly, as can be seen from our method description, hybrid systems increase computational steps and complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">BanditMF: Multi-Armed Bandit Based Matrix Factorization Recommender System</head><p>The authors in <ref type="bibr" target="#b3">[4]</ref> proposed a method that is based on implicit and explicit user feed-   By analyzing the user-item rating matrix as shown in Table <ref type="table">3</ref>.1, we can obtain the predicted user-item rating matrix as shown in Table <ref type="table">3</ref>.2. For the sake of explanation, we consider each row in Table <ref type="table">3</ref>.2 as a predictive model. In another world, we consider the row vectors that represent the preferences of each user as models. When a new user enters the system, a predictive model is selected and used as a basis to make recommendations for the new user. However, in the real world, the number of users is generally in the millions, so the above approach is rendered impractical due to the m × n scaling rating matrix, where the m is millions of orders in size.</p><p>Whether we can cluster the rows in the matrix to reduce the size of the rating matrix(i.e., reduce the number of predictive models)? Inspired by collaborative filtering, it is possible to cluster the rows of the rating matrix such that each row of the matrix represents a row vector, and each row vector represents the preferences of each user, so that each cluster is composed of users with similar rating preferences. Another important reason for matrix factorization is the very widespread assumption that the user-item rating matrix is a low-rank matrix, so the clustering can be performed on the rows of the user-item rating matrix. We treat BanditMF as an algorithm consisting of two stages:</p><p>(1) prediction, and (2) recommendation.</p><p>For the offline subsystem, we define the prediction model as the following steps: matrix factorization based rating prediction, clustering, and unified rating Matrix factorization based rating prediction. As we illustrated in section 3.1, the non-cold user formed the user-item rating matrix Rand, based on the matrix factorization, we can get the predicted user-item rating matrix R. The notation ru,i is used to represent the rating of user u on item i. Recall that in this project, our predicted rating is: And the loss function is:</p><formula xml:id="formula_23">ru,i = b u,i + q T i * p u (3.21)</formula><formula xml:id="formula_24">Loss = min q * ,p * (u,i)∈K r u,i − b u,i − q T i p u 2 + λ q i 2 + p u 2 + b 2 u + b 2 i (3.22)</formula><p>where b u,i = µ + b u + b i . We use µ denote the average rating for the entire rating matrix, b u denote the user's bias, and b i is represent the item bias.</p><p>Clustering . According to the user's preference vectors of the predicted user-item rating matrix R, we can perform the clustering on the row of the R. In this system, Kmeans algorithm is adopted, which is measured by Euclidean distance. After performing the clustering on the user's preference vectors, we will have a set of clusters where C =</p><formula xml:id="formula_25">{c 1 , c 2 • • • c n }.</formula><p>Unified rating . For the clusters in C, we perform the average rating on it to get the unified user preference vectors, where α denote the unified user preference vectors. For example, as shown in Table <ref type="table">3</ref>.3, by the hypothetical that u 1 and u 2 be clustered into c 1 , the unified user preference vectors α = {1.2, 0.55, 1.05, 0.7, 0.7}, which is the average of the u 1 , u 2 .</p><p>i 1 i 2 i 5 i 4 i 5 u 1 1.4 0.8 1.1 0.7 0.9 u 1 1.0 0.3 1.0 0.7 0.5 α1 1.2 0.55 1.05 0.7 0.7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3.3: Unified user preference vectors</head><p>The online recommendation module is mainly composed of multi-armed bandits, and this online module runs as follows:</p><p>1. Select a predictive model S n using the MAB algorithm π 2. Select and recommend the item i * with the top rating score in α which is not recommended to user u.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Receive the user u's rating for item i * .</p><p>4. calculate the reward and regret.</p><p>5. Determine whether a user is a cold user by a threshold τ .</p><p>6. If yes, update the algorithm π. Otherwise, turn to the offline module.</p><p>The online recommendation module will provide recommendations to new users until the new user is no longer cold (i.e., some feedback on the recommended items or preferences has been provided to the system). Nevertheless, if a system obliges the user to make excessive ratings, the user will lose interest in the system <ref type="bibr" target="#b5">[6]</ref>. For the above reasons, we cannot run the bandit algorithm too many times to get the optimal recommendation, and online systems need to get user preferences in a limited number of recommendations(i.e., within the threshold τ ). Now let's walk through the entire BanditFM workflow from scratch, which is the flowchart shown in Figure <ref type="figure">3</ref>.5. The input information is first fed into the offline prediction module, and through matrix factorization, we get the predicted user-item rating matrix.</p><p>Then each row of the matrix, namely the user preference vector, is clustered to reduce the size of the prediction matrix. If we denote the original size predicted user-item rating matrix as m × n, after clustering, we have a m × n matrix, where m m. Then pass the clustered predictive model to the online recommendation module. Through the multi-armed bandit algorithm, it selects the predictive model from the clusters. Get user preferences through interaction with users (i.e., an algorithm recommends items to users and users provide feedback). The online module determines whether a user is a cold user through a threshold τ , and if the system has collected certain user rating preferences, then the user is no longer a cold user. At this point, we switch to the offline prediction module and append the user preferences obtained from the online module to the user-item rating matrix. We can then accurately make recommendations to new users through the matrix factorization technique.</p><p>Reward and Regret. In BanditMF, we define the reward as the equation shown in the formula 3.23.</p><formula xml:id="formula_26">µ Sn (t) = r u,i r * (3.23)</formula><p>where r * denote the maximum rating in the dataset. r u,i denote the rating of user u for recommended item i according to the algorithm selected predictive model S n . The reward interval is µ Sn (t) ∈ [0, 1].This intuition of this reward can be interpreted as the higher the feedback received for the recommendation, the higher the reward will be. If by the reward, then we define the regret as:</p><formula xml:id="formula_27">R(T ) T t=1 (E [µ * ] − E [µ Sn (t)]) (3.24)</formula><p>where E [µ * ] denotes the maximum expected cumulative rewards.</p><p>In the next chapter, we will verify the BanditMF using various criteria and compare the performance of different multi-armed bandit algorithms.</p><p>Chapter 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Implementation</head><p>In this chapter, the methods mentioned in the previous chapters will be implemented and validated with different real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Contextual Bandit: LinUCB</head><p>We introduced the context-free algorithm in detail last semester, so we will not continue to elaborate on it in this report. Instead, we will focus on the contextual bandit, implementing and verifying one of the most famous algorithms -LinUCB which was proposed by the authors in <ref type="bibr" target="#b11">[12]</ref>. LinUCB is an extension of the UCB algorithm. The algorithm chooses an arm based on observation of the current user and set of arms together with the context feature. Also, in the LinUCB algorithm, there exists a factor α, which is used to control the trade-off between exploration and exploitation.</p><p>Dataset. For this experiment, the personalization dataset 3 has been used. Compute click-through rate (CTR). The click-through rate (CTR) proposed in <ref type="bibr" target="#b12">[13]</ref> is used to evaluate the performance of the algorithm. The click-through rate replay  </p><formula xml:id="formula_28">C(T ) = T t=1 y t × 1 [π t−1 (x t ) = a t ] T t=1 1 [π t−1 (x t ) = a t ]<label>(4.1)</label></formula><p>where, π t−1 denote the algorithm trained on data up to time t − 1. At time t, if the context x t been observed by algorithm π t−1 , then we use the notation π t−1 (x t ) denote the chosen action. a t and y t denote the real action that was taken in the dataset at time t and the real reward that was obtained at time t respectively.</p><p>Experiment. In this experiment (ideas and parameters are referenced from <ref type="bibr" target="#b14">[15]</ref>), the size of the context is 100, where x t ∈ R 100 , the number of arms K = 10, total iteration T = 10000. For the α, as the only input in the algorithm, optimizing this parameter may result in higher total payoffs in practice. So, we will experiment with the blow values of α:</p><p>1. For Alpha = 1 2. For Alpha = 0.25 3. For Alpha = 1/ √ t 4. For Alpha = 0.001 5. For Alpha = 0.001/(0.1 * P redictionCount) To understand the trade-off between exploration and exploitation, for each alpha value, first the mean value of the upper confidence bound for 10 arms will be plotted. We will then compare the number of predictions made by each arm with the number of true predictions it made.</p><p>As it is evident from Figure <ref type="figure" target="#fig_11">4</ref>.4, when the α value is 1, the exploration and exploitation here is at a minimum, which can be represented by strips of nearly the same length. That implies that the upper confidence bound of each arm is very approximate. For this cause, we can also observe that the predicted counts are nearly the same for all arms,thus when the α value equals to 1, the algorithm giving the lowest click rate value which is 0.18 as shown in Figure <ref type="figure" target="#fig_11">4</ref>.3. For Figure <ref type="figure" target="#fig_11">4</ref>.5, When the α value change to 0.25, the improvement is not significant compare to the result when α value equals to 1 . Now, when we change our α to the time-dependent values (i.e., square root of the time), as shown in Figure    4.6, the result receives a significant improvement. This is mainly caused by the fact that we adjust our exploration according to the changes in time and limit our predictions in order to maximize the use of our trained algorithms over time.     ) the experiment, we further perform an α value with correct prediction dependent which is the Figure <ref type="figure" target="#fig_11">4</ref>.8. The intuition for doing so is that the alpha value related to the correct prediction increases the exploitation of the arm that performs better (i.e., the larger the correct prediction, the smaller the α value). On the other hand, the number of explorations From the above figures, we can see that the arm with the largest upper confidence bound is also the arm with the highest number of predictions. This reflects the nature of the LinUCB algorithm that selects the arm with the highest upper confidence bound as the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Matrix Factorization Implementation</head><p>To implement the matrix factorization mentioned in section 3.1, a smaller data set was chosen in order for us to carry out a comparison between the two approaches: the base method and the bias method (i.e., the method of matrix factorization that considers both user bias and item bias).</p><p>Dataset. The partial dataset is shown in Figure <ref type="figure" target="#fig_11">4</ref>.9. The user-item rating matrix R is a 25 × 100 matrix, which represent 25 users and 100 items. Furthermore, we can see from the figure that R is a sparse matrix (i.e. matrix with many zeros)</p><p>Training . Recall our loss function:</p><formula xml:id="formula_29">Loss : J(p, q) = min q * ,p * (u,i)∈K r u,i − q T i p u 2 + λ ( q i + p u ) 2 (4.2)</formula><p>Based on the methods proposed in <ref type="bibr" target="#b10">[11]</ref>, we use a stochastic gradient descent (SGD) algorithm to train two latent vector matrices: a user latent matrix p and item latent matrix q.By defining e u,i = r u,i − q T i p u . We update the two matrices by the following formulas:</p><formula xml:id="formula_30">q i ← q i + η (e u,i p u − λq i ) (4.3) p u ← p u + η (e u,i q i − λp u ) (4.4)</formula><p>In the experiment, we set the number of latent features k = 2, then we initiated the user matrix and item matrix with a size of 25 × 2 (i.e., number of users × number of latent features). Furthermore, the learn rateη = 0.001, regularization factorλ = 0.1 and iteration 1000 times. After iteration, we can get our user latent matrix p and item latent  Adding bias.Now, we will experiment with the bias method, which considers both user bias and item bias. As mentioned before, when we consider the bias, then the loss function becomes:</p><formula xml:id="formula_31">Loss = min q * ,p * (u,i)∈K r u,i − b u,i − q T i p u 2 + λ q i 2 + p u 2 + b 2 u + b 2 i (4.5)</formula><p>The formula used to iterate over the user latent matrix and the item latent matrix becomes:</p><formula xml:id="formula_32">q i ← q i + η r ui − q T i p u − b u,i p u − λq i (4.6) p u ← p u + η r ui − q T i p u − b u,i q i − λp u (4.7)</formula><p>The rest of the steps are consistent with the base matrix factorization method.After the training, we get the user and item latent matrix based on the bias method. By the visualization method,  What is striking in Table <ref type="table">4</ref>.1 is the dramatic decline of MSE, when we consider user bias and item bias, the value of MSE is reduced by 20% which proved that consider both user and item bias when perform the matrix factorization is effective.</p><p>'title':'Pulp Fiction', 'rating':3, 'title':"Heat", 'rating':3.   As mentioned in the methodology chapter, there exist many formulae to calculate the similarity, but in this system, Pearson Correlation Coefficient is adopted, and it can be represented as :</p><formula xml:id="formula_33">sim(i, j) = p∈P R i,p − Ri R j,p − Rj p∈P R i,p − Ri 2 p∈P R j,p − Rj 2<label>(4.8)</label></formula><p>where R i,p denote the rating of the user i on movie p and Ri represents the average rating of all movies by the user i. Similarly, R j,p denote the rating of the target user j on movie p and Rj represents the average rating of all movies by the target user j. Also, P represents the set of all movies.</p><p>After calculating the similarity, we can list out the similarity of all similar users to the target user, as shown in Figure <ref type="figure" target="#fig_11">4</ref>.16. As Figure <ref type="figure" target="#fig_11">4</ref>.17 shows, by sorting the similar users by similarity, the user with ID 240 is the most similar user with the target user, for whom the similarity is 0.981981.</p><p>By the method of user-based collaborative filtering, we multiply the similar user's  rating by the similarity to get the weighted rating. As shown in Figure <ref type="figure" target="#fig_11">4</ref>.18 (a), the intuition here is that the higher the similarity with the target user, the higher the weight of the target user's prediction score.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Hybrid Recommender System</head><p>In this section, we will demonstrate the implementation of a hybrid system that combines matrix factorization and the content-based approach.</p><p>Dataset. To verify the generality of the system, in addition to the MovieLens dataset we have used in the user-based system, we will also use a dataset called Jester 4 which contains over 1.7 million continuous ratings (-10.00 to +10.00) of 150 jokes from 59,132 users.</p><p>Recommendation. Based on the method and implementation of matrix factorization described above, we can get our user latent matrix p and item latent matrix q. By performing the inner product between the above two latent matrices, we get the predicted user-item rating matrix R. For the example shown in Figure <ref type="figure" target="#fig_11">4</ref>.20, if we want to get the predicted rating of user 1 on item 1, we can use the inner product between two matrices to get the predicted rating. The process is shown in (a), and the predicted rating here is 3.999. When compared with the grand true rating shown in (b), we can see that the approximation of the predicted rating to the true rating is 99.975%.  As per the methods that we talk about in section 3.3, the next step is to sort the movies that have not been rated by the target user through our predicted rating score and recommend the top-ranked movie to the target user. Through Figure <ref type="figure" target="#fig_11">4</ref>.21, we can get the information that a movie with ID 910 is the movie with the highest predicted rating. Like what we described before, in order to measure how similar the movies are in attractiveness to the target user, we need to calculate the similarity by some formula.</p><p>To test the ability of different similarity formulas, different from the Pearson Correlation Coefficient used in the previous user-based system, we use Cosine similarity to calculate the similarity, where the following formula can be used to calculate the similarity between movie i and movie j:   </p><formula xml:id="formula_34">similar(i, j) = cos(i, j) = i • j i • j<label>(4.9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">BanditMF</head><p>In this section, we will implement the BanditMF proposed in Section 3.4 and compare the experimental results obtained using different multi-armed bandit algorithms according to different criteria. In this experiment, the -greedy, UCB, and Thompson sampling algorithms will be used.We will implement BanditMF's process one by one as follows.</p><p>Dataset. MovieLens will be used in this experiment, which contains the same data as we described in the last two experiments.</p><p>Matrix factorization. For BanditMF we use the bias method to implement matrix factorization, where ru,i = b u,i + q T i p u and b u,i is the combination of user bias, item bias, and average rating.</p><p>For the result shown in Figure <ref type="figure" target="#fig_11">4</ref>.24, we generate a predicted user-item rating matrix with a size of 610 × 9724 Clustering . We perform the clustering on the row of the predicted user-item rating matrix R, which means that we cluster the users who share similar rating preferences.</p><p>In this experiment, we adopt a machine learning algorithm library called scikit-learn.</p><p>Based on the library, K-means algorithm is adopted, which is measured by Euclidean distance. We set the parameter n clusters=3, which means that we will cluster all users into three clusters. Also, we set the value of parameter n init to 20, which means that the K-means algorithm will be run with 20 different centroid seeds and output the best result in 20 consecutive runs as the final result. Figure <ref type="figure" target="#fig_11">4</ref>.25 shows the above iterations.</p><p>As the number of iterations increases (i.e., x-axis), the sum of the Euclidean distances decreases (i.e., y-axis).   rating matrix as data, and then selects and recommends items to the target users. After receiving the user's rating, the reward and regret are calculated. Then judge whether the user is still a cold user. If so, continue to update the Bandit algorithm with the received data; if not, transfer the user to the offline system and append the user-item rating matrix.</p><p>Evaluation criteria. In addition to the rewards and regrets which are the common criteria in the multi-armed bandit algorithm, we have introduced two new criteria: discounted cumulative gain (DCG) and normalized discounted cumulative gain (NDCG) Chapter 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this report, we first reviewed and formulated the representative multi-armed bandit and collaborative filtering methods. Subsequently, we introduce the matrix factorization methods, and a matrix factorization considering bias is proposed. Based on it, we illustrate the memory-based collaborative filtering recommender system and a hybrid recommender system that combines the content-based method with matrix factorization techniques. Importantly, in this work, BanditMF has been proposed, which is a multi-armed bandit-based matrix factorization recommender system. This system combines the matrix factorization (MF) which is model-based collaborative filtering with the multi-armed bandit algorithm. BanditMF uses a multi-armed bandit algorithm to solve the cold start problem in collaborative filtering. Also, with the help of matrix factorization results, the bandit algorithm can quickly derive user preferences. When we get the user preferences of the new users, we can transfer them to the offline module for the recommendation. That is, we add the user preference vector obtained from the online module to the user-item rating matrix. The advantage of this is that the collaborative filtering technology in the offline module fills a gap in the multi-armed bandit algorithm where similarity and user relationships are not taken into account. Also, for BanditMF, there are disadvantages.</p><p>As we explain in the report, the contextual bandit algorithm can be based on contextual information to achieve a better balance between exploration and exploitation. However, in our BanditMF, we apply the context-free bandit, i.e., a bandit algorithm that does not consider contextual feature information, which may result in the recommended items not being optimal. The future work is to extend the contextual bandit to the BanditMF.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 Formulation 2.1 Multi-Armed Bandit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1.1 Context-Free Bandit . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1.2 Contextual Bandit . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Collaborative Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2.1 Memory-Based Methods . . . . . . . . . . . . . . . . . . . . . . . . 2.2.1.1 User-Based Collaborative Filtering . . . . . . . . . . . . . 2.2.1.2 Item-Based Collaborative Filtering . . . . . . . . . . . . . 2.2.2 Model-Based Methods . . . . . . . . . . . . . . . . . . . . . . . . . 2.2.2.1 Latent Factor Models . . . . . . . . . . . . . . . . . . . . 3 Methodologies 3.1 Matrix Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Memory-Based Collaborative Filtering Recommender System . . . . . . . . 3.3 Hybrid Recommender System: Combination of Content-Based Method and Matrix Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 BanditMF: Multi-Armed Bandit Based Matrix Factorization Recommender System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Experiments and Implementation 4.1 Contextual Bandit: LinUCB . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Matrix Factorization Implementation . . . . . . . . . . . . . . . . . . . . . 4.3 Collaborative Filtering Recommender System: User-Based . . . . . . . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 . 1 :</head><label>11</label><figDesc>Figure 1.1: Relation diagram of collaborative filtering in recommender systems</figDesc><graphic coords="6,171.64,70.87,252.00,245.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 . 1 :</head><label>21</label><figDesc>Figure 2.1: User−item rating matrix</figDesc><graphic coords="14,139.17,501.88,316.95,188.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 . 2 :</head><label>22</label><figDesc>Figure 2.2: User-based collaborative filtering</figDesc><graphic coords="16,185.65,70.87,223.97,248.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 . 3 ,</head><label>23</label><figDesc>Figure 2.3, item2 and item3 are regarded as similar items due to this two items were received a positive rating from both user1 and user2. By the figure, we can see that user3 shows the preference on item3, so the item-based collaborative filtering algorithm will choose item2 which is the similar item of item3 to recommend to the user3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 . 3 :</head><label>23</label><figDesc>Figure 2.3: Item-based collaborative filtering</figDesc><graphic coords="17,182.15,70.87,230.98,248.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 . 1 :</head><label>31</label><figDesc>Figure 3.1: Example of User−item rating matrix</figDesc><graphic coords="24,94.46,104.95,406.37,254.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 . 2 :Figure 3 . 3 :</head><label>3233</label><figDesc>Figure 3.2: Similarity between target user and other users</figDesc><graphic coords="25,235.54,76.80,124.19,205.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>back, agents use machine learning algorithms to inscribe an individual user model and use the user model to make recommendations. The method called BanditMF, a matrix factorization recommendation system based on multi-arm bandit, is proposed to address the limitations of collaborative filtering and bandit algorithms as mentioned earlier. The system combines model-based collaborative filtering matrix factorization (MF) with a multi-armed bandit algorithm. BanditMF incorporates an offline subsystem centered on matrix factorization and an online subsystem focused on a multi-armed bandit algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 3 . 5 :</head><label>35</label><figDesc>Figure 3.5: Flow chart of the BanditMF</figDesc><graphic coords="29,80.84,174.74,433.60,468.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 4 .</head><label>4</label><figDesc>2 demonstrates the format of the dataset. This dataset consists of a matrix of size 10000 × 102. The data in the first column indicates the action has been chosen. The second column is value indicate the obtained reward, where y t denote the real reward that was obtained at time t and y t ∈ B. From column 3 to column 103 are the context which is represented as the feature vector x t ∈ R 100 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 4 . 1 :</head><label>41</label><figDesc>Figure 4.1: LinUCB with disjoint linear models<ref type="bibr" target="#b11">[12]</ref> </figDesc><graphic coords="34,162.64,70.87,270.00,211.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 4 . 2 :</head><label>42</label><figDesc>Figure 4.2: Format of the personalization dataset</figDesc><graphic coords="34,72.94,320.33,449.40,54.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 4 . 3 :</head><label>43</label><figDesc>Figure 4.3: CTR of LinUCB for varying Alphas</figDesc><graphic coords="35,136.36,146.62,322.56,239.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 4 . 4 :</head><label>44</label><figDesc>Figure 4.4: Mean UCB and Total VS. Correct prediction for Alpha = 1</figDesc><graphic coords="36,74.77,314.31,230.40,171.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 4 . 5 :</head><label>45</label><figDesc>Figure 4.5: Mean UCB and Total VS. Correct prediction for Alpha = 0.25</figDesc><graphic coords="36,305.44,550.83,230.40,171.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 4 . 6 :</head><label>46</label><figDesc>Figure 4.6: Mean UCB and Total VS. Correct prediction for Alpha = 1/ √ t</figDesc><graphic coords="36,74.77,550.83,230.40,171.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 4 .</head><label>4</label><figDesc><ref type="bibr" target="#b6">7</ref> shows that(a) MeanUCB (b) correct prediction comparison</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 4 . 7 :</head><label>47</label><figDesc>Figure 4.7: Mean UCB and Total VS. Correct prediction for Alpha = 0.001</figDesc><graphic coords="37,74.77,147.23,230.40,171.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 4 . 8 :</head><label>48</label><figDesc>Figure 4.8: Mean UCB and Total VS. Correct prediction for Alpha = 0.001/( correct−predictions</figDesc><graphic coords="37,74.77,440.77,230.40,171.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>10</head><label>10</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 4 . 9 :</head><label>49</label><figDesc>Figure 4.9: Partial of the user-item rating matrix R</figDesc><graphic coords="38,118.84,70.87,357.60,193.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>( a )</head><label>a</label><figDesc>User latent matrix (b) Item latent matrix (first 25 out of 100 items)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 4 . 10 :Figure 4 . 11 :</head><label>410411</label><figDesc>Figure 4.10: User and item matrix</figDesc><graphic coords="39,161.60,249.32,53.10,193.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 4 .</head><label>4</label><figDesc>12 plots the latent vectors of users and items in the twodimensional coordinate. Through multiply the red dot coordinate by the blue cross's coordinate in the figure, you can get the predicted rating score of user u on item i.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.12: visualization of user and item latent vector</figDesc><graphic coords="40,124.84,375.51,345.60,230.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 4 . 14 :</head><label>414</label><figDesc>Figure 4.14: Target user data frame</figDesc><graphic coords="42,206.14,251.93,183.00,117.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 4 . 15 :</head><label>415</label><figDesc>Figure 4.15: Top three similar groups</figDesc><graphic coords="42,165.08,571.30,265.13,144.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Figure 4 . 16 :Figure 4 . 17 :</head><label>416417</label><figDesc>Figure 4.16: Similarity value of all similar users to the target user</figDesc><graphic coords="43,73.76,70.87,447.75,151.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head></head><label></label><figDesc>Sum of the weighted rating (c) Weighted average score</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Figure 4 . 18 :</head><label>418</label><figDesc>Figure 4.18: Calculate the score for recommendation (5 out of 9000 movies)</figDesc><graphic coords="44,74.77,88.45,172.35,70.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Figure 4 .</head><label>4</label><figDesc>18 (b) demonstrates the result of the sum of weighted ratings and the sum of similarity. Finally, in Figure 4.18 (c), by dividing the sum of weighted ratings by the sum of similarity, we can get the predicted weighted average recommendation score of what the target users probably rate the movie. Recommendation. For the recommendation, based on the above steps, first, sort the movies by average recommendation score, which is shown in Figure 4.19 (a). Then, by the movie ID in the sort list, we can get the top 10 movies with titles recommended by the user-based collaborative filtering system to the target user, as shown in Figure 4.19 (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>( a )</head><label>a</label><figDesc>Sorted by average recommendation score (b) recommend movies</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Figure 4 . 19 :</head><label>419</label><figDesc>Figure 4.19: Top 10 movies recommend by user-based system</figDesc><graphic coords="44,86.04,523.41,204.23,164.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.20: comparison between predicted rating and true rating</figDesc><graphic coords="45,74.77,422.08,231.30,153.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.21: Top 10 movies with high predicted rating</figDesc><graphic coords="46,230.51,70.87,134.25,133.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.22, (a)  shows the top 10 movies that are similar to the movie "Some Like It Hot (1959)". In (b), if we want to recommend two more movies to the target user, as the recommendation result of this hybrid system, movie " The Fate of the Furious (2017)" which ID is 170875 and movie " Withnail &amp; I (1987)" which ID is 1202 will be recommended to the target user who likes the movie "Some Like It Hot (1959)".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.22: Content-based recommendation</figDesc><graphic coords="47,124.40,112.38,127.50,130.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.24: Partial of user-item predicted matrix</figDesc><graphic coords="48,135.94,346.40,323.40,193.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_44"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.25: Iterations of K-means</figDesc><graphic coords="49,140.61,103.20,314.06,232.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_45"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.26, which means that we converted the original 610 × 9724 predicted user-item rating matrix to the 3 × 9724 clustered user-item rating matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_46"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.26: Predicted user-item rating matrix after clustering</figDesc><graphic coords="49,145.54,459.59,304.20,53.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell>1: Four scenarios when making decisions under uncertainty</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>denote the set of states called the state space, whereS =S 1 , S 2 , S 3 , S 4 ,• • • • A is denote the set of actions called the action space, where A=A 1 , A 2 , A 3 , A 4 ,• • • • P a is denote the probability, P a (s, s ) = Pr (s t+1 = s | s t = s, a t = a)is the probability from state s to sate s .• R a represent the reward. where R a (s, s ) = E (r t | s t = s, a t = a).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Data sources: Instagram Revenue and Usage Statistics (2021).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Table from the paper: A Survey on Contextual Multi-armed Bandits.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Dataset from: http://www.cs.columbia.edu/ jebara/6998/dataset.txt</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Dataset available at: http://eigentaste.berkeley.edu/dataset</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSE Improvement rate</head><p>Base method ru,i = q T i p u 0.6530 Bias method ru,i = b u,i + q T i p u 0.5178 20.715% Table <ref type="table">4</ref>.1: Evaluation between base method and bias method 4.3 Collaborative Filtering Recommender System: User-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Based</head><p>In this chapter, we will demonstrate the implementation of a user-based recommender system.</p><p>Dataset. In this system, we adopt the dataset called MovieLen <ref type="bibr" target="#b6">[7]</ref>. MovieLen contains 100,000 ratings and 3,600 tag applications applied to 9,000 movies by 600 users.</p><p>Implementation. By pandas library, we read movies and ratings into the data frame.</p><p>The format of both movie and rating data frames is shown in Figure <ref type="figure">4</ref>.13.</p><p>(a) movies data-frame (b) ratings data-frame According to the process for creating memory-based collaborative filtering, the next step is to identify a target user who will be recommended.</p><p>Identify a target user . We define a target user and their rating in the way shown in the frame below. As in the image shown in Figure <ref type="figure">4</ref>.14, the target user data frame involved the movie title with its ID and the target user's rating.</p><p>Target user input targetUserInput = [ 'title':'Grumpier Old Men', 'rating':4, 'title':'Get Shorty', 'rating':2.5, <ref type="bibr" target="#b19">[19]</ref>. Their formulas are shown below:</p><p>r u,t log 2 t (4.10)</p><p>We can see from the formula that these two criteria will discount the reward at a later stage, which means that the accuracy (i.e., user satisfaction) of early recommendations is more important than that of late recommendations, which is why these two criteria are applied here. Since we focus on solving the user's cold start problem, we need the algorithm to make accurate recommendations in a shorter period of time. However, when the user is no longer a cold user, accurate recommendations lose value because the user may give up using the server before you can make accurate recommendations.</p><p>Result. We use T to denote the total rounds, and N denotes the number of new users. Further, R t represent the cumulative regret at t rounds. As Table <ref type="table">4</ref>.5 shows, when we increase T to 50 rounds, Thompson sampling reverts to the algorithm that obtains the best results, both in terms of cumulative regret and NDCG, -greedy achieves results comparable to Thompson sampling, and the gap between UCB and the two algorithms also exists. In Table <ref type="table">4</ref>.6, when we increase the number of new users N to 5, T = 5, compared to the previous results in Table <ref type="table">4</ref>.2, although the UCB algorithm still performs the worst, the difference between it and Thompson sampling and -greedy is slightly reduced. In Table <ref type="table">4</ref>.7, when we further set the number of new users N to 50, the above conclusion still holds.In Table <ref type="table">4</ref>.8, -greedy achieves a better performance than Thompson sampling when the number of users is relatively large. The regret of the UCB algorithm is 6.4 times higher than that of Thompson sampling.</p><p>In summary, both Thompson sampling and -greedy can achieve lower regret and higher NDCG. In most cases, Thompson sampling performs better, but the difference between the two is very small. In other words, users' preferences can be drawn and performs the worst among the three algorithms and cannot quickly learn the preferences of new users. The possible reason is that the UCB algorithm needs to select all the arms once, a move that is fatal for the cold start problem. Because users will lose interest in the system if the system forces them to rate too much <ref type="bibr" target="#b5">[6]</ref>, it is important to get user preferences and make accurate recommendations in the shortest possible time for the cold start problem. However, if the number of new users is very large, e.g., 10,000 new users, then the UCB algorithm is equivalent to conducting 10,000 random selection strategies, which results in UCB not being able to make accurate recommendations for new users quickly and suffering from huge regret.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Adomavicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tuzhilin</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2005.99</idno>
		<idno>doi: 10 . 1109/TKDE.2005.99</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="734" to="749" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Thompson Sampling for Contextual Bandits with Linear Payoffs</title>
		<author>
			<persName><forename type="first">Shipra</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navin</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</editor>
		<meeting>the 30th International Conference on Machine Learning<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2013-06">June 2013</date>
			<biblScope unit="page" from="127" to="135" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research 3</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A Neural Networks Committee for the Contextual Bandit Problem</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Allesiardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Feraud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djallel</forename><surname>Bouneffouf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.8191</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>cs.NE</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">User modeling for adaptive news access</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Billsus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Pazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">User modeling and user-adapted interaction</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="147" to="180" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Personalized Recommendation on Dynamic Content Using Predictive Bilinear Models</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung-Taek</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1145/1526709.1526802</idno>
		<ptr target="https://doi.org/10.1145/1526709.1526802" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on World Wide Web. WWW &apos;09</title>
				<meeting>the 18th International Conference on World Wide Web. WWW &apos;09<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page">9781605584874</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Active Learning Strategies for Rating Elicitation in Collaborative Filtering: A System-Wide Perspective</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Elahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Rubens</surname></persName>
		</author>
		<idno type="DOI">10.1145/2542182.2542195</idno>
		<idno>doi: 10 . 1145 / 2542182.2542195</idno>
		<ptr target="https://doi.org/10.1145/2542182.2542195" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<idno type="ISSN">2157-6904</idno>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The MovieLens Datasets: History and Context</title>
		<author>
			<persName><forename type="first">F</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<idno type="DOI">10.1145/2827872</idno>
		<ptr target="https://doi.org/10.1145/2827872" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Interact. Intell. Syst</title>
		<idno type="ISSN">2160-6455</idno>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015-12">Dec. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Your Neighbors Affect Your Ratings: On Geographical Neighborhood Influence to Rating Prediction</title>
		<author>
			<persName><forename type="first">Longke</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/2600428.2609593</idno>
		<idno>doi: 10.1145/ 2600428.2609593</idno>
		<ptr target="https://doi.org/10.1145/2600428.2609593" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International ACM SIGIR Conference on Research and amp; Development in Information Retrieval. SIGIR &apos;14</title>
				<meeting>the 37th International ACM SIGIR Conference on Research and amp; Development in Information Retrieval. SIGIR &apos;14<address><addrLine>Gold Coast, Queensland, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">9781450322577</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional Matrix Factorization for Document Context-Aware Recommendation</title>
		<author>
			<persName><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1145/2959100.2959165</idno>
		<ptr target="https://doi.org/10.1145/2959100.2959165" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems. RecSys &apos;16</title>
				<meeting>the 10th ACM Conference on Recommender Systems. RecSys &apos;16<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">9781450340359</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Factorization Meets the Neighborhood: A Multifaceted Collaborative Filtering Model</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<idno type="DOI">10.1145/1401890.1401944</idno>
		<idno>doi: 10 . 1145 / 1401890.1401944</idno>
		<ptr target="https://doi.org/10.1145/1401890.1401944" />
	</analytic>
	<monogr>
		<title level="m">KDD &apos;08</title>
				<meeting><address><addrLine>Las Vegas, Nevada, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page">9781605581934</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Matrix Factorization Techniques for Recommender Systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
		<idno type="DOI">10.1109/MC.2009.263</idno>
		<idno>doi: 10.1109/ MC.2009.263</idno>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Contextual-Bandit Approach to Personalized News Article Recommendation</title>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/1772690.1772758</idno>
		<ptr target="https://doi.org/10.1145/1772690.1772758" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on World Wide Web. WWW &apos;10</title>
				<meeting>the 19th International Conference on World Wide Web. WWW &apos;10<address><addrLine>Raleigh, North Carolina, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">9781605587998</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Contextual-Bandit Approach to Personalized News Article Recommendation</title>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/1772690.1772758</idno>
		<ptr target="https://doi.org/10.1145/1772690.1772758" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on World Wide Web. WWW &apos;10</title>
				<meeting>the 19th International Conference on World Wide Web. WWW &apos;10<address><addrLine>Raleigh, North Carolina, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">9781605587998</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Naive Filterbots for Robust Cold-Start Recommendations</title>
		<author>
			<persName><forename type="first">Seung-Taek</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1145/1150402.1150490</idno>
		<ptr target="https://doi.org/10.1145/1150402.1150490" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. KDD &apos;06</title>
				<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. KDD &apos;06<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">1595933395</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contextual bandit algorithm using disjoint LinUCB</title>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Purwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GitHub</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probabilistic Matrix Factorization</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recommender Systems in E-Commerce</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM Conference on Electronic Commerce</title>
				<meeting>the 1st ACM Conference on Electronic Commerce</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<idno type="DOI">10.1145/336992.337035</idno>
		<ptr target="https://doi.org/10.1145/336992.337035" />
		<title level="m">EC &apos;99</title>
				<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page">1581131763</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Introduction to Reinforcement Learning. 1st</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="page">262193981</biblScope>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A Theoretical Analysis of NDCG Type Ranking Measures</title>
		<author>
			<persName><forename type="first">Yining</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1304.6480</idno>
		<ptr target="http://arxiv.org/abs/1304.6480" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Conversational Contextual Bandit: Algorithm and Application</title>
		<author>
			<persName><forename type="first">Xiaoying</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366423.3380148</idno>
		<ptr target="https://doi.org/10.1145/3366423.3380148" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020. WWW &apos;20. Taipei, Taiwan: Association for Computing Machinery</title>
				<meeting>The Web Conference 2020. WWW &apos;20. Taipei, Taiwan: Association for Computing Machinery</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">9781450370233</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
