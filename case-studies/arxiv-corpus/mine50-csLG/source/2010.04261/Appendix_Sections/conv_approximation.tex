\label{sec:appendix_conv}
The approximation of weight Hessian of convolutional layer is a trivial extension from the approximation of Fisher information matrix of convolutional layer by \citet{grosse2016kronecker}.

Consider a two dimensional convolutional layer of neural network with $m$ input channels and $n$ output channels. Let its input feature map $\tX$ be of shape $(n, X_1, X_2)$ and output feature map $\tZ$ be of shape $(m, P_1, P_2)$. Let its convolution kernel be of size $K_1\times K_2$. Then the weight $\tW$ is of shape $(m, n, K_1, K_2)$, and the bias $\vb$ is of shape $(m)$. Let $P$ be the number of patches slide over by the convolution kernel, we have $P=P_1P_2$.

Follow  \citet{dangel2020modular}, we define $\mZ\in\R^{m\times P}$ as the reshaped matrix of $\tZ$ and $\mW\in\R^{m \times nK_1K_2}$ as the reshaped matrix of $\tW.$
Define $\mB\in\R^{m\times P}$ by broadcasting $\vb$ to $P$ dimensions. Let $\mX\in\R^{nK_1K_2 \times P}$ be the unfolded $\tX$ with respect to the convolutional layer. The unfold operation \citep{NEURIPS2019_9015} is commonly used in computation to model convolution as matrix operations.

After the above transformation, we have the linear expression of the $p$-th convolutional layer similar to FC layers:\begin{equation}
    \label{eqn:conv_linear}
    \mZ^{(p)} = \mW^{(p)}\mX^{(p)} + \mB^{(p)}
\end{equation}
We still omit superscription of $(p)$ for dimensions for simplicity. We also denote $\vz^{(p)}$ as the vector form of $\mZ^{(p)}$ and has size $mP$.
Similar to fully connected layer, we have analogue of \equationref{eqn:appendix_closeformhessian} for convolutional layer as
\begin{equation}
    \label{eqn:appendix_closeformhessian_conv}
    \mH_\ell(\vw^{(p)}, \mX) = \left(\mI_{m} \otimes \mX^{(p)}\right)\mM_\vx^{(p)}\left(\mI_{m} \otimes \mX^{(p)\T}\right),
\end{equation}
where $\mM_\vx^{(p)} = \mH_\ell(\vz^{(p)}, \mX)$ and is a $mP \times mP$ matrix.
Also, since convolutional layers can also be considered as linear operations (matrix multiplication with reshape) together with FC layers and ReLUs, \equationref{eqn:appendix_zero_logit_output_hessian} still holds. Thus, we still have  \begin{equation}
    \mH_\ell(\vz^{(p)}, \mX) = \mM_\vx^{(p)} = \mG_\vx^{(p)\T}\mA_\vx\mG_\vx^{(p)},
\end{equation}
where $\mG_\vx^{(p)} = \frac{\partial \vz}{\partial\vz^{(p)}}$ and has dimension $c \times mP$, although is cannot be further decomposed as direct multiplication of weight matrices as in the FC layers.

However, for convolutional layers, $\mX^{(p)}$ is a matrix instead of a vector. Thus, we cannot make \equationref{eqn:appendix_closeformhessian_conv} into the form of a Kronecker product as in \equationref{eqn:appendix_closeformhessian}.

Despite this, it is still possible to have a Kronecker factorization of the weight Hessian in the form
\begin{equation}
   \mH_\ell(\vw^{(p)}, \mX) \approx \tmM^{(p)}_\vx \otimes \mX^{(p)}\mX^{(p)\T},
\end{equation}
using further approximation motivated by \cite{grosse2016kronecker}.
Note that $\tmM^{(p)}_\vx$ need to have a different shape ($m\times m$) from $\mM^{(p)}_\vx$ ($mP\times mP$), since $\mH_\ell(\vw^{(p)},\mX)$ is $mnK1K2 \times mnK1K2$ and $\mX^{(p)}\mX^{(p)\T}$ is $nK1K2 \times nK1K2$.

Since we can further decompose $\Ax = \Qx^\T\Qx$, we then have
\begin{equation}
    \Mx^{(p)} = \mG_\vx^{(p)\T}\Ax\mG_\vx^{(p)} = \left(\Qx\Gx^{(p)}\right)^\T\left(\Qx\Gx^{(p)}\right).
\end{equation}
We define $\mN_\vx^{(p)} =\Qx\Gx^{(p)}$. Here $\Qx$ is $c\times c$ and $\Gx^{(p)}$ is $c\times mP$ so that $\mN_\vx^{(p)}$ is $c \times mP$. We can reshape $\mN_\vx^{(p)}$ into a $cP\times m$ matrix $\tmN_\vx^{(p)}$. We then reduce $\mM^{(p)}_\vx$ ($mP\times mP$) into a $m\times m$ matrix as 
\begin{equation}
    \tmM^{(p)}_\vx = \frac{1}{P}\tmN_\vx^{(p)\T}\tmN_\vx^{(p)}.
\end{equation}
The scalar $\frac{1}{P}$ is a normalization factor since we squeeze a dimension of size $P$ into size 1.

Thus, we can have similar Kronecker factorization approximation as
\begin{align}
    \HessL(\vw^{(p)}) &= \E\left[\mH_\ell(\vw^{(p)}, \mX)\right] = 
    \E\left[\left(\mI_{m} \otimes \mX^{(p)}\right)\mM_\vx^{(p)}\left(\mI_{m} \otimes \mX^{(p)\T}\right)\right] \\ &\approx \E\left[\tmM^{(p)}_\vx \otimes \mX^{(p)}\mX^{(p)\T}\right] \approx \E\left[\tmM^{(p)}_\vx\right] \otimes \E\left[\mX^{(p)}\mX^{(p)\T}\right].
\end{align}