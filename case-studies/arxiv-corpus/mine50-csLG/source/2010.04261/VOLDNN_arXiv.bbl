\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Dangel et~al.(2020)Dangel, Harmeling, and Hennig]{dangel2020modular}
Dangel, F., Harmeling, S., and Hennig, P.
\newblock Modular block-diagonal curvature approximations for feedforward
  architectures.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  799--808, 2020.

\bibitem[Dziugaite \& Roy(2017)Dziugaite and Roy]{dziugaite2017computing}
Dziugaite, G.~K. and Roy, D.~M.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock In \emph{Proceedings of the Thirty-Third Conference on Uncertainty in
  Artificial Intelligence, {UAI}}, 2017.

\bibitem[Fort \& Ganguli(2019)Fort and Ganguli]{fort2019emergent}
Fort, S. and Ganguli, S.
\newblock Emergent properties of the local geometry of neural loss landscapes.
\newblock \emph{arXiv preprint arXiv:1910.05929}, 2019.

\bibitem[George et~al.(2018)George, Laurent, Bouthillier, Ballas, and
  Vincent]{george2018fast}
George, T., Laurent, C., Bouthillier, X., Ballas, N., and Vincent, P.
\newblock Fast approximate natural gradient descent in a kronecker factored
  eigenbasis.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9550--9560, 2018.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Krishnan, and
  Xiao]{ghorbani2019investigation}
Ghorbani, B., Krishnan, S., and Xiao, Y.
\newblock An investigation into neural net optimization via hessian eigenvalue
  density.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2232--2241, 2019.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010understanding}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pp.\  249--256, 2010.

\bibitem[Golmant et~al.(2018)Golmant, Yao, Gholami, Mahoney, and
  Gonzalez]{hessian-eigenthings}
Golmant, N., Yao, Z., Gholami, A., Mahoney, M., and Gonzalez, J.
\newblock pytorch-hessian-eigentings: efficient pytorch hessian
  eigendecomposition, 2018.
\newblock URL \url{https://github.com/noahgolmant/pytorch-hessian-eigenthings}.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{goodfellow2016deep}
Goodfellow, I., Bengio, Y., and Courville, A.
\newblock \emph{Deep Learning}.
\newblock MIT Press, 2016.

\bibitem[Grosse \& Martens(2016)Grosse and Martens]{grosse2016kronecker}
Grosse, R. and Martens, J.
\newblock A kronecker-factored approximate fisher matrix for convolution
  layers.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  573--582, 2016.

\bibitem[Gur-Ari et~al.(2018)Gur-Ari, Roberts, and Dyer]{gur2018gradient}
Gur-Ari, G., Roberts, D.~A., and Dyer, E.
\newblock Gradient descent happens in a tiny subspace.
\newblock \emph{arXiv preprint arXiv:1812.04754}, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{kaiming2015}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{2016 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  770--778, 2016.
\newblock \doi{10.1109/CVPR.2016.90}.

\bibitem[Heskes(2000)]{heskes2000natural}
Heskes, T.
\newblock On “natural” learning and pruning in multilayered perceptrons.
\newblock \emph{Neural Computation}, 12\penalty0 (4):\penalty0 881--901, 2000.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  448--456, 2015.

\bibitem[Jacot et~al.(2020)Jacot, Gabriel, and Hongler]{jacot2019asymptotic}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock The asymptotic spectrum of the hessian of {DNN} throughout training.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR}}, 2020.

\bibitem[Jastrzebski et~al.(2019)Jastrzebski, Kenton, Ballas, Fischer, Bengio,
  and Storkey]{jastrzebski2018relation}
Jastrzebski, S., Kenton, Z., Ballas, N., Fischer, A., Bengio, Y., and Storkey,
  A.~J.
\newblock On the relation between the sharpest directions of {DNN} loss and the
  {SGD} step length.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR}}, 2019.

\bibitem[Karakida et~al.(2019{\natexlab{a}})Karakida, Akaho, and
  Amari]{karakida2019normalization}
Karakida, R., Akaho, S., and Amari, S.-i.
\newblock The normalization method for alleviating pathological sharpness in
  wide neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, pp.\  6406--6416, 2019{\natexlab{a}}.

\bibitem[Karakida et~al.(2019{\natexlab{b}})Karakida, Akaho, and
  Amari]{karakida2019pathological}
Karakida, R., Akaho, S., and Amari, S.-i.
\newblock Pathological spectra of the fisher information metric and its
  variants in deep neural networks.
\newblock \emph{arXiv preprint arXiv:1910.05992}, 2019{\natexlab{b}}.

\bibitem[Karakida et~al.(2019{\natexlab{c}})Karakida, Akaho, and
  Amari]{karakida2019universal}
Karakida, R., Akaho, S., and Amari, S.-i.
\newblock Universal statistics of fisher information in deep neural networks:
  Mean field approach.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  1032--1041. PMLR, 2019{\natexlab{c}}.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR}}, 2017.

\bibitem[Kleinman \& Athans(1968)Kleinman and Athans]{kleinman1968design}
Kleinman, D. and Athans, M.
\newblock The design of suboptimal linear time-varying systems.
\newblock \emph{IEEE Transactions on Automatic Control}, 13\penalty0
  (2):\penalty0 150--159, 1968.

\bibitem[Krizhevsky(2009)]{Krizhevsky09learningmultiple}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Langford \& Seeger(2001)Langford and Seeger]{langford2001bounds}
Langford, J. and Seeger, M.
\newblock Bounds for averaging classifiers.
\newblock Technical report, 2001.

\bibitem[Laurent \& Massart(2000)Laurent and Massart]{laurent2000adaptive}
Laurent, B. and Massart, P.
\newblock Adaptive estimation of a quadratic functional by model selection.
\newblock \emph{Annals of Statistics}, pp.\  1302--1338, 2000.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Li et~al.(2020)Li, Gu, Zhou, Chen, and Banerjee]{li2020hessian}
Li, X., Gu, Q., Zhou, Y., Chen, T., and Banerjee, A.
\newblock Hessian based analysis of sgd for deep nets: Dynamics and
  generalization.
\newblock In \emph{Proceedings of the 2020 SIAM International Conference on
  Data Mining}, pp.\  190--198. SIAM, 2020.

\bibitem[Liao \& Mahoney(2021)Liao and Mahoney]{liao2021hessian}
Liao, Z. and Mahoney, M.~W.
\newblock Hessian eigenspectra of more realistic nonlinear models.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Martens \& Grosse(2015)Martens and Grosse]{martens2015optimizing}
Martens, J. and Grosse, R.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In \emph{International conference on machine learning}, pp.\
  2408--2417, 2015.

\bibitem[McAllester(1999)]{mcallester1999some}
McAllester, D.~A.
\newblock Some pac-bayesian theorems.
\newblock \emph{Machine Learning}, 37\penalty0 (3):\penalty0 355--363, 1999.

\bibitem[Papyan(2018)]{papyan2018full}
Papyan, V.
\newblock The full spectrum of deepnet hessians at scale: Dynamics with sgd
  training and sample size.
\newblock \emph{arXiv preprint arXiv:1811.07062}, 2018.

\bibitem[Papyan(2019)]{papyan2019measurements}
Papyan, V.
\newblock Measurements of three-level hierarchical structure in the outliers in
  the spectrum of deepnet hessians.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5012--5021, 2019.

\bibitem[Papyan(2020)]{papyan2020traces}
Papyan, V.
\newblock Traces of class/cross-class structure pervade deep learning spectra.
\newblock \emph{arXiv preprint arXiv:2008.11865}, 2020.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A.
\newblock Automatic differentiation in pytorch.
\newblock Technical report, 2017.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{NEURIPS2019_9015}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
  DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
  Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\
  8024--8035. Curran Associates, Inc., 2019.

\bibitem[Sagun et~al.(2016)Sagun, Bottou, and LeCun]{sagun2016eigenvalues}
Sagun, L., Bottou, L., and LeCun, Y.
\newblock Eigenvalues of the hessian in deep learning: Singularity and beyond.
\newblock \emph{arXiv preprint arXiv:1611.07476}, 2016.

\bibitem[Sagun et~al.(2018)Sagun, Evci, G{\"{u}}ney, Dauphin, and
  Bottou]{sagun2017empirical}
Sagun, L., Evci, U., G{\"{u}}ney, V.~U., Dauphin, Y.~N., and Bottou, L.
\newblock Empirical analysis of the hessian of over-parametrized neural
  networks.
\newblock In \emph{6th International Conference on Learning Representations,
  {ICLR} 2018, Workshop Track Proceedings}, 2018.

\bibitem[Schoenholz et~al.(2017)Schoenholz, Gilmer, Ganguli, and
  Sohl-Dickstein]{schoenholz2016deep}
Schoenholz, S.~S., Gilmer, J., Ganguli, S., and Sohl-Dickstein, J.
\newblock Deep information propagation.
\newblock In \emph{International Conference on Learning Representations, ICLR},
  2017.

\bibitem[Simonyan \& Zisserman(2015)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{3rd International
  Conference on Learning Representations, {ICLR}}, 2015.

\bibitem[Singh et~al.(2021)Singh, Bachmann, and Hofmann]{singh2021analytic}
Singh, S.~P., Bachmann, G., and Hofmann, T.
\newblock Analytic insights into structure and rank of neural network hessian
  maps.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Singla et~al.(2019)Singla, Wallace, Feng, and
  Feizi]{singla2019understanding}
Singla, S., Wallace, E., Feng, S., and Feizi, S.
\newblock Understanding impacts of high-order loss approximations and features
  in deep learning interpretation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5848--5856, 2019.

\bibitem[Skorski(2019)]{skorski2019chain}
Skorski, M.
\newblock Chain rules for hessian and higher derivatives made easy by tensor
  calculus.
\newblock \emph{arXiv preprint arXiv:1911.13292}, 2019.

\bibitem[Sra et~al.(2012)Sra, Nowozin, and Wright]{sra2012optimization}
Sra, S., Nowozin, S., and Wright, S.~J.
\newblock \emph{Optimization for machine learning}.
\newblock Mit Press, 2012.

\bibitem[Torralba et~al.(2008)Torralba, Fergus, and Freeman]{torralba2007tiny}
Torralba, A., Fergus, R., and Freeman, W.~T.
\newblock 80 million tiny images: A large data set for nonparametric object and
  scene recognition.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 30\penalty0 (11):\penalty0 1958--1970, 2008.

\bibitem[Van~der Maaten \& Hinton(2008)Van~der Maaten and
  Hinton]{vanDerMaaten2008}
Van~der Maaten, L. and Hinton, G.
\newblock Visualizing data using t-sne.
\newblock \emph{Journal of machine learning research}, 9\penalty0 (11), 2008.

\bibitem[Yao et~al.(2018)Yao, Gholami, Lei, Keutzer, and
  Mahoney]{yao2018hessian}
Yao, Z., Gholami, A., Lei, Q., Keutzer, K., and Mahoney, M.~W.
\newblock Hessian-based analysis of large batch training and robustness to
  adversaries.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4949--4959, 2018.

\bibitem[Yao et~al.(2019)Yao, Gholami, Keutzer, and Mahoney]{yao2019pyhessian}
Yao, Z., Gholami, A., Keutzer, K., and Mahoney, M.
\newblock Pyhessian: Neural networks through the lens of the hessian.
\newblock \emph{arXiv preprint arXiv:1912.07145}, 2019.

\bibitem[Zhu(2012)]{zhu2012short}
Zhu, S.
\newblock A short note on the tail bound of wishart distribution.
\newblock \emph{arXiv preprint arXiv:1212.5860}, 2012.

\end{thebibliography}
