\label{sec:empirical}
We observe several interesting phenomena which can be explained by the structural analysis of Hessian in \cref{sec:hessian}, demonstrating that the Hessians are far from random matrices and Hessians of different models are related. This observation also suggests that more meaningful information may be extracted by assessing these common properties. We also further predict and verfy the conditions these phenomenon rely on, and thus provide some connection between network structures and Hessians. \znote{Need reorganization, probably some parts can move to conclusion}

\subsection{Eigenspace Overlap of different models}
\label{sec:models}
Consider models trained using different random initializations with the same network structure and dataset. We observe surpisingly high top eigenspace overlap between their layerwise Hessians, despite no obvious similarity between their parameters.
%For overparameterized networks, one would expect that they should have very different parameters and thus very different layerwise Hessians. However, we observe that the top eigenspace of their layerwise Hessians have suprisingly high overlap, despite no obvious similarity between their parameters.

\begin{figure}[th]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[width=\textwidth]{Figures/SubspaceOverlap/Mix/Overlap_LeNet5_varying_conv2.pdf}
        \caption{conv2-LeNet with 16/25 output channels}
        \label{fig:Overlap_conv2}
    \end{subfigure}%
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[width=\textwidth]{Figures/SubspaceOverlap/Mix/Overlap_LeNet5_varying_fc1.pdf}
        \caption{fc1-LeNet5 with 80/100/120/150 output neuron}
        \label{fig:Overlap_fc1}
    \end{subfigure}
    \captionsetup{justification=centering}
    \caption{Eigenspace overlap between models\znote{currently working on experiments which have different number of neurons.}}
    \label{fig:overlap}
\end{figure}

\cref{fig:overlap} includes 5 different structural variants of LeNet5 trained on CIFAR-10. For each structural variant, 5 models are trained independently from different random initializations. We plot the average pairwise top eigenspace overlap between layerwise Hessians among those 5 models.\znote{Not sure if this explanation is clear enough.} For example, in \cref{fig:Overlap_conv2} we consider the vanilla LeNet5, whose conv2 layer has $n=16$ output channels, and a modified LeNet5, whose conv2 has $n=25$ output channels. It is clear that for the same structure, the top eigenspaces of different models exhibits a highly non-trivial overlap, and the overlap peaks near $n$, which is the dimension of the layer's output. For sanity check, pairwise dot product between the weight matrices of different models are computed, and there does not appear to be notable correlation.


%We computed the average pairwise overlap (absolute value of normalized dot product) between their parameter vectors ($\xi$ in \cref{fig:overlap}). They are very low and similar to the expectation of random overlap $1/\sqrt{n}$. Also, if this is due to similarity of parameters, we should expect the overlap to stay high instead of having an obvious peak. We also cannot model them as 2 uncorrelated subspace in $\R^d$. In that case, their overlap should be $\frac{k}{d}$ at dimension $k$. Take layer fc1 as an example, the eigenvectors lives in dimension 48000 and are expected to have overlap 0.0025, but the actual overlap is around 0.75 as shown in \cref{fig:Overlap_fc1}.

This phenomenon can be explained using the approximation in \cref{sec:approx_top_eig}. For simplicity, in this subsection we assume exactly $n$ top eigenvectors of layerwise Hessian are in $\Krsp(\hE[\vx])$. A more generalized model is in the Appendix.

Now suppose we have 2 different models with $\hE[\vx]_1$ and $\hE[\vx]_2$ respectively. We can approximate their top $k$ eigenspace as $\mU_k \otimes \hE[\vx]_1$ and $\mV_k \otimes \hE[\vx]_2$ respectively, for $k \leq n$. The eigenspace overlap at dimension $k$ is thus
\begin{equation}
\begin{aligned}
    \Overlap\left(\mU_k \otimes \hE[\vx]_1, \mV_k \otimes \hE[\vx]_2\right) &=\frac{1}{k}\left\|\mV_1^T\mV_2 \otimes \hE[\vx]_1^T\hE[\vx]_2\right\|^2_F\\
    &= \left(\hE[\vx]_1 \cdot \hE[\vx]_2\right)^2\Overlap\left(\mU_k, \mV_k\right).
\end{aligned}
\label{eqn:model_overlap}
\end{equation}
Moreover, columns of $\mU_k$ and $\mV_k$ lives in $\R^n$, the space corresponding to the neurons. Since output neurons (channels for conv) can be permuted to give equivalent models while changing eigenvectors, we can assume that columns of $\mU_k$ and $\mV_k$ have no correlation and thus have an expected inner product of $\frac{1}{\sqrt{n}}$. Thus, $\Overlap(\mU_k, \mV_k) = \frac{k}{n}$ and the eigenspace overlap of at dimension $k$ would be approximately $\frac{k}{n}(\hE[\vx]_1 \cdot \hE[\vx]_2)^2$. This explains the peak at dimension $n$ and the linear growth before it. 

When $k=n$, the overlap is approximately $\hE[\vx]_1 \cdot \hE[\vx]_2$. Since $\hE[\vx]_1$ and $\hE[\vx]_2$ are the same for the input layer and all non-negative for other layers, the overlap is expected to be high at dimension $n$. \ynote{One problem for this statement is the overlap is not very close to 1 for the input layer. Probably we can also explain by saying our explanation is not very accurate.}

Note that fc3 does not have the linear growth since neurons cannot be permuted in the last layer. \ynote{In addition, for the first layer, models with the same dataset would have the same $\hE[\vx]$, and that would lead to an overlap close to 1 at dimension $n$. Omit since we did not show the first layer}

Then, consider the $(n+1)$th eigenvector of the first model. Since top $n$ eigenvectors span the full space $\Krsp(\hE[\vx]_1)$, it will be orthogonal to this space. It will also have low overlap with $\Krsp(\hE[\vx]_2)$ since $(\hE[\vx]_1 \cdot \hE[\vx]_2)$ is large. This explains the immediate drop in eigenspace overlap at dimension $n+1$. \ynote{May not be necessary}

\subsection{Dominating Eigenvectors of Layer-wise Hessian are Low Rank}
\label{sec:domin_eig}
The structure of Hessians' eigenvectors is also important for assessing Hessian properties. Let $\vh_i$ be the $i$th eigenvector of a layerwise Hessian. Since it lives in the same space as the weight vector $\vw$ of that layer, we can define an operation $\Mat(\vh_i)$ to reshape $\vh_i$ into a matrix with the same shape as the weight matrix $\mW$. \ynote{The definition of $\Mat$ can be removed if we define it in Section 3 or 4}

The rank of $\Mat(\vh_i)$ can be considered as an indicator of the complexity of the eigenvector. Consider the case that $\vh_i$ is one of the top eigenvectors ($i \leq t$). From \cref{sec:approx_top_eig}, we have $\vh_i \approx \vu_i \otimes \hE[\vx]$ for some $\vu_i \in \R^n$. Thus, $\Mat(\vh_i) \approx \vu_i\hE[\vx]^T$ and is approximately rank 1.
\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{Figures/Eigenvec_Lowrank/NLeNet_base/Top_Eigenvector_rank_CIFAR10_Exp1_LeNet5_normnew_fixlr0.01R1_E-1_20.pdf}
    \captionsetup{justification=centering}
    \caption{Ratio between top singular value and Frobenius norm. (LeNet5 on CIFAR10)}
    \label{fig:eigen_lowrank}
\end{figure}

\cref{fig:eigen_lowrank} shows the L2 norm of top $k$ singular values divided by the Frobenius Norm of $\Mat(\vh_i)$, for $k = 1$. This value measures how each eigenvector is close to rank $k$. We can see the top few eigenvectors are close to rank 1. This agrees with our prediction. It also shows that the approximation is usually more accurate when $i$ is smaller.

% The values are all high for $k=10$ so that all these eigenvectors can be approximated as rank 10 or lower. This results also suggest that eigenvectors of layerwise Hessians have internal structure and cannot be modelled as randomly chosen vectors in a subspace. \ynote{Can be deleted}

\subsection{Outliers in Eigenvalue Spectrum and \texorpdfstring{$\E[\mM]$}{EM}}
\label{sec:emp_outlier}
One characteristic of Hessian that has been mentioned by many is the outliers in the spectrum of eigenvalues. \citet{sagun2017empirical} suggests that there is a gap in Hessian eigenvalue distribution around the number of classes $C$ in most cases, where $C=10$ in our case. \citet{papyan2019measurements} attempted further explanation for the $C$ outliers using class clustering. 

Since $\E[\vx\vx^T]$ is close to rank 1 and the Kronecker factorization is a good approximation for top eigenspace, the top eigenvalues of layerwise Hessian can be approximated as the top eigenvalues of $\E[\mM]$ multiplied by the first eigenvalue of $\E[\vx\vx^T]$. Thus, the top eigenvalues of Hessians should have the same relative ratios as the top eigenvalues of their corresponding $\E[\mM]$'s. We can also conjecture that the outliers also appear in $\E[\mM]$.

\begin{figure}[th]
    \centering
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[width=\textwidth]{Figures/EM_vs_H/FC2_MNIST/UTAU_vs_full_sigval_d30_MNIST_Exp1_FC2_fixlr0.01R1_E-1_fc1.pdf}
        \caption{FC2 fc1 at minimum\\(MNIST)}
        \label{fig:UTAU_H_spec_FC2}
    \end{subfigure}%
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[width=\textwidth]{Figures/EM_vs_H/LeNet5_fixlr0.01/UTAU_vs_full_sigval_d30_CIFAR10_Exp1_LeNet5_fixlr0.01R1_E-1_fc1.pdf}
        \caption{LeNet5 fc1 at minimum\\(CIFAR10)}
        \label{fig:UTAU_H_spec_Lenet}
    \end{subfigure}%
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[width=\textwidth]{Figures/EM_vs_H/LeNet5_RL/UTAU_vs_full_sigval_d30_CIFAR10_RandomLabel_LeNet5_fixlr0.01_RLR4_E-1_fc1.pdf}
        \caption{LeNet5 fc1 at minimum\\(CIFAR10 Random Label)}
        \label{fig:UTAU_H_spec_RL}
    \end{subfigure}
    \captionsetup{justification=centering}
    \caption{Eigenspectrum of $\E[\mM]$ and $\E[\mH]$}
    \label{fig:UTAU_H_spec}
\end{figure}

Figure 7 shows the similarity of eigenvalue spectrum between $\E[\mM]$ and layerwise Hessians in different situations, agreeing with our prediction. However, the outliers only appear at initialization and at minimum for true labels but not at miminum for random labels.

\citet{papyan2019measurements} provides explanation for the gap in eigenspectrum using class clustering. Although we reproduced their results on their networks, there is no clear class clustering for both $\E[M]$ and layerwise Hessian at the Minimum for networks we experiment on. The reason is unclear but we conjecture that class clustering is only significant for very large networks.

The outliers at initialization, however, are easier to explain. Similar to \citet{papyan2019measurements} suggests for full Hessian, we observe logit clustering in $\E[\mM]$'s. Since there are $C$ logits, we would expect there are $C$ outliers. The results are shown in Appendix.

\subsection{Zero-mean Input and Batch Normalization}
\snote{The approximation worsen, but is still relevant. Reason of low overlap: low correlation between $x$ in different layers, random permutation of neurons in FC layers.}
According to our explanation, the good approximation and high overlap of top eigenspace both depend on the low rank structure of $\E[\vx\vx^T]$. Also, the low rank strcuture is caused by the fact $\E[\vx]\E[\vx]^T]$ dominates $\mSigma_\vx$ in most cases. Thus, we conjecture using Batch normalization (BN) \citep{ioffe2015batch} will change these phenomena as $\E[\vx]$ will be zero and $\E[\vx\vx^T] = \mSigma_\vx$.

We experiment on the same networks but with BN. The results are shown in the Appendix. We found that $\E[\vx\vx^T]$ is no longer close to rank 1 for models trained with BN. However, $\E[\vx\vx^T]$ still have a few large eigenvalues and is not similar to a random matrix.

In the case, our approximation in \cref{sec:approx_top_eig} is not accurate. Thus, the overlap of top $k$ eigenspace between different models can no longer be approximated as in \cref{eqn:model_overlap}. As expected, models with BN give a much lower eigenspace overlap at dimension smaller than $n$ and there is no obvious peak.

The approximation of Hessian eigenvalues using Kronecker factorization is also not so accurate in models using BN, agreeing with our conjecture. However, the approximation still gives meaningful information and the top eigenspace overlap between true and approximated hessian is still (). It is lower than the case without BN but much larger than the expected value for 2 random subspaces.