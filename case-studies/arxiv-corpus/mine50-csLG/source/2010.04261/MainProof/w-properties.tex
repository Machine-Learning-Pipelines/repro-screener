\subsubsection{Properties of Infinite Width Weight Matrices}
\label{sec:pf-W-properties}
We will first prove some simple properties of the Gaussian initialized weight matrices $\mW^{(1)}$ and $\mW^{(2)}$ that will facilitate our analysis. Recall that $\mW^{(1)}\in\R^{d\times n}$ and $\mW^{(2)}\in\R^{n\times c}$ where the output dimension $c$ is a finite constant, the hidden layer width $n$ goes to infinity, and the input dimension $d=n^{1+\alpha}$ for some constant $\alpha>0$.
\begin{lemma}
\label{lemma:W-expectation}
 For all $i\in[c]$, for all $\eps>0$,
 \begin{equation}
    \lim_{n\to\infty}\pr{\left|\sum_{j=1}^n\mW_{ij}^{(2)}\right|\geq\eps}=0.
 \end{equation}
\end{lemma}
\begin{proofof}{\lemmaref{lemma:W-expectation}}
Since each entry of $\mW^{(2)}$ is initialized independently from $\gN(0,\frac1n)$, by Central Limit Theorem we have $\sum_{j=1}^n\mW_{ij}^{(2)}\sim \gN(0,\frac{1}{n})$. For any $\eps > 0$, fix $\eps$. By Chebyshev's inequality,
\begin{equation}
    \lim_{n\to\infty}\pr{\left|\sum_{j=1}^n\mW^{(2)}_{ij}\right|\geq\epsilon} < \lim_{n\to\infty}\frac{1}{n\eps^2} = 0. 
\end{equation}
\end{proofof}

\begin{lemma}
\label{lemma:chi2-tail}\citep{laurent2000adaptive} For $X\sim\chi_n^2$,\begin{equation}
    \pr{X-n\geq 2\sqrt{nt}+2t}\leq e^{-t},\qquad \pr{X-n\leq -2\sqrt{nt}}\leq e^{-t}.
\end{equation}

\end{lemma}

\begin{lemma}
\label{lemma:W-norm}
For all $\eps>0$, 
\begin{equation}
    \lim_{n\to\infty}\pr{\abs{\fns{\mW^{(2)}}-c}\geq\eps}=0.
\end{equation}
Beside, for all $i\in[c]$,
\begin{equation}
    \lim_{n\to\infty}\pr{\abs{\ns{\mW_i^{(2)}}-1}\geq\eps}=0.
\end{equation}
\end{lemma}
\begin{proofof}{\lemmaref{lemma:W-norm}}
For simplicity of notations, we will use $\mW$ to denote $\mW^{(2)}$ in this proof.
Since each entry of $\mW$ is initialized independently from $\gN(0,\frac1n)$, we know that $n\fns{\mW}=\sum_{i=1}^c\sum_{j=1}^nn{\mW_{i,j}}^2$ follows a $\chi_{cn}^2$-distribution. From \lemmaref{lemma:chi2-tail} we know that for large enough $n$,
\begin{equation}
\pr{|n\fns{\mW}-cn|\geq n\eps}\geq\pr{|n\fns{\mW}-cn|\geq 2\sqrt{c}n^{3/4}+2n^{1/2}}\leq 2\exp(-n^{1/2}).
\end{equation}
In other words,
\begin{equation}
\lim_{n\to\infty}\pr{|\fns{\mW}-c|\geq\eps} = \lim_{n\to\infty}\pr{|n\fns{\mW}-cn|\geq n\eps} = 0.
\end{equation}
Similarly, for any $i\in[c]$, $n\fns{\mW_i}$ follows a $\chi_n^2$-distribution, so for large enough $n$,
\begin{equation}
\pr{|n\fns{\mW_i}-n|\geq n\eps}\leq\pr{|n\fns{\mW}-n|\geq 2n^{3/4}+2n^{1/2}}\leq 2\exp(-n^{1/2}),
\end{equation}
which indicates that
\begin{equation}
\lim_{n\to\infty}\pr{|\ns{\mW_i}-1|\geq\eps} = \lim_{n\to\infty}\pr{|n\ns{\mW_i}-n|\geq n\eps} = 0.
\end{equation}
\end{proofof}

\begin{lemma}
\label{lemma:w1-col-norm}
Let $\vw_i$ denote the $i$-th column vector of $\mW^{(1)}$.
With probability 1 over $\mW^{(1)}$, \begin{equation}
    \max_{i=1}^d\norm{\vw_i}<5n^{-\frac\alpha2}.
\end{equation}

\end{lemma}
\begin{proofof}{\lemmaref{lemma:w1-col-norm}}
Since entries of $\mW^{(1)}$ are i.i.d. sampled from $\cN(0, \frac1n)$, each $\ns{\vw_i}$ obeys a $\chi_n^2$ scaled by $\frac1d = n^{-(1+\alpha)}$. Thus by the tail bound of \lemmaref{lemma:chi2-tail}, setting $t=n$ we have\begin{equation}
\pr{\ns{\vw_i}\geq 5n^{-\alpha}} =  \pr{d\ns{\vw_i}\geq n+2\sqrt{n^2}+2n}\leq e^{-n}.
\end{equation}
By a Union bound we have \begin{equation}
    \pr{ \max_{i=1}^d\norm{\vw_i}^2\geq 5n^{-\alpha}}\leq \sum_{i=1}^d\pr{ \norm{\vw_i}^2\geq 5n^{-\alpha}} = de^{-n} = n^{1+\alpha}e^{-n}.
\end{equation}
Since $\alpha$ is a constant, RHS converges to 0. Thus with probability 1 over $\mW^{(1)}$, we have \begin{equation}
    \max_{i=1}^d\norm{\vw_i}^2<5n^{-\alpha}.
\end{equation}
Taking square root on both sides completes the proof.
\end{proofof}

\begin{lemma}
\label{lemma:WW-identity}
For any random matrix $\mW$
For all $\eps>0$,
\begin{equation}
    \lim_{n\to\infty}\pr{\norm{\mW^{(1)}\mW^{(1)\T}-\mI_c}\geq\eps}=0.
\end{equation}Besides, for all $i,j\in[c]$, 
\begin{equation}
    \lim_{n\to\infty}\pr{|(\mW^{(1)}\mW^{(1)\T})_{i,j}-\delta_{i,j}|\geq\eps}=0
\end{equation}
Here $\delta$ is the Kronecker delta function, i.e., $\delta_{i,j} = \ind[i=j]$.
\end{lemma}
\begin{proofof}{\lemmaref{lemma:WW-identity}}
To prove this lemma we need the following tail bound:
\begin{lemma}\citep{zhu2012short} If $S$ follows a Wishart distribution $\cW_d(n,C)$, with $r = \emph{tr}(C)/\norm{C}$, for $\theta\geq 0$ the following inequality holds that \begin{equation}
    \pr{\lnorm{\frac1n S-C}\geq \rbr{\sqrt{\frac{2\theta(r+1)}{n}}+\frac{2\theta r}{n}}\norm{C}}\leq 2d\exp(-\theta).
\end{equation}
\label{lemma:W-wishart}
\end{lemma}

Since each entry of $\mW^{(1)}$ is initialized independently from $\gN(0,\frac1d)$, we know that $\mW^{(1)}\mW^{(1)\T}$ follows Wishart distribution $\cW_d(d, \frac1d \mI_n)$. With $r = \tr(\frac1d\mI_n)/\norm{\frac1d\mI_n} = n$ and set $\theta = n^{\frac{\alpha}{2}}$, from \lemmaref{lemma:W-wishart}, for $n\geq 1$ we get
\begin{equation}
\begin{split}
2d\exp(-n^{\frac{\alpha}{2}})&\geq \pr{\lnorm{\frac1d\mW^{(1)}\mW^{(1)\T}-\frac1d\mI_n}\geq \rbr{\sqrt{\frac{2\theta(n+1)}{d}}+\frac{2\theta n}{d}}\lnorm{\frac1d\mI_n}}\\
&=\pr{\lnorm{\frac1d\mW^{(1)}\mW^{(1)\T}-\frac1d\mI_n}\geq \rbr{\sqrt{\frac{2n^{\frac{\alpha}{2}}(2n)}{n^{1+\alpha}}}+\frac{2n^{\frac{\alpha}{2}} n}{n^{1+\alpha}}}\lnorm{\frac1d\mI_n}}\\
&=\pr{\lnorm{\mW^{(1)}\mW^{(1)\T}-\mI_n}\geq 2(n^{-\frac\alpha4}+n^{-\frac\alpha2})}.\\
\end{split}
\end{equation}
Fix any $\eps>0$, we may find $N\in \NN$ such that for all $n>N$, $2(n^{-\frac\alpha4}+n^{-\frac\alpha2})<\eps$. For any $\eps'>0$, we may find $N'$ such that $2d\exp(-n^{\frac\alpha2}) = 2n^{1+\alpha}\exp(-n^{\frac\alpha2}) < \eps'$. Passing $n$ to infinity we get
\begin{equation}
    \lim_{n\to\infty}\pr{\lnorm{\mW^{(1)}\mW^{(1)\T}-\mI_n}>\eps} = 0.
\end{equation}
Then we proceed to analyze the entries. For all $i,j\in[n]$, we have
\begin{equation}
\begin{split}
\pr{|(\mW^{(1)}\mW^{(1)\T})_{i,j}-\delta_{i,j}|\geq\eps} &\leq\pr{\sum_{i,j=1}^n\pr{(\mW^{(1)}\mW^{(1)\T})_{i,j}-\delta_{i,j}}^2\geq\eps^2}\\
&=\pr{\fns{\mW^{(1)}\mW^{(1)\T}-\mI_n}\geq\eps^2}\\
&\leq\pr{\norm{\mW^{(1)}\mW^{(1)\T}-\mI_n}\geq\frac{\eps}{\sqrt{n}}},
\end{split}
\end{equation}
which implies that for all $i,j\in[n]$,
\begin{equation}
\lim_{n\to\infty}\pr{|(\mW^{(1)}\mW^{(1)\T})_{i,j}-\delta_{i,j}|\geq\eps}=0.
\end{equation}
\end{proofof}
For the second weight matrix $\mW^{(2)}$, where $\mW^{(2)}\mW^{(2)\T}\sim \cW_c(n, \frac1n\mI_c)$, we may prove an identical statement as shown in the corollary below. The proof proceeds identical as above since we only need the ratio between the width and the height of $\mW$, which is $n/c$ in this case, to go to infinity.

\begin{corollary}
\label{cor:WW-Identity}
For all $\eps>0$,
\begin{equation}
    \lim_{n\to\infty}\pr{\norm{\mW^{(2)}\mW^{(2)\T}-\mI_n}\geq\eps}=0.
\end{equation}
\end{corollary}

Next we establish the approximate equivalence between the scatter matrix $\mW^{(2)\T}\mW^{(2)}$ and the projection matrix $P_{\mW^{(2)}}$.
\begin{lemma}
\label{lemma:W-projection}
Let $P_{\mW^{(2)}}$ be the projection matrix onto the row space of $\mW^{(2)}$, then for all $\eps>0$,
\begin{equation}
    \lim_{n\to\infty}\Pr\left[\fns{\mW^{(2)\T}\mW^{(2)}-P_{\mW^{(2)}}}>\eps\right]=0.
\end{equation}
\end{lemma}
\begin{proofof}{\lemmaref{lemma:W-projection}}
For simplicity of notations, in this proof we will neglect the layer index superscript and use $\mW$ to denote $\mW^{(2)}$. Recall that $\mW\in\R^{n\times c}$.

Fix $\eps\in(0,1)$ without loss of generality. Let $\mW_i(i\in[c])$ be the $i$-th row of $\mW$, and we will do the Gram–Schmidt process for the rows of $\mW$. Specifically, the Gram–Schmidt process is as following: Assume that the basis $\{\overline{\mW}_i\}_{i=1}^k$ are already normalized, we set $\mW_{k+1}'\triangleq \mW_{k+1} - \sum_{i=1}^k\langle \mW_{k+1}, \overline{\mW}_i\rangle$ and $\overline{\mW}_{k+1}\triangleq\mW_{k+1}'/\norm{\mW_{k+1}'}$. Finally, from the definition of projection matrix, we know that $P_\mW=\overline{\mW}^\T\overline{\mW}$.

From \lemmaref{lemma:W-norm} we have for all $i\in[c]$,
\begin{equation}
    \lim_{n\to\infty}\pr{|\ns{\mW_i}-1|\geq\eps}=0.
\end{equation}
Let $\eps'\triangleq \eps^2/\rbr{c^3\cdot 16^{2c+1}}$, from \lemmaref{lemma:WW-identity} we know that for all $i,j\in[c]$, 
\begin{equation}
    \lim_{n\to\infty}\pr{|\mW_i\mW_j^\T-\delta_{i,j}|\geq\eps'}=0.
\end{equation}

Then we use induction to bound the difference between $\mW$ and $\overline{\mW}$. Specifically, we will show that for all $i\in[c], \norm{\overline{\mW}_i-\mW_i}\leq 8^{i}\eps'$. For simplicity of notations, in the following proof we will not repeat the probability argument and assume that for all $i,j\in[c]$, $|\mW_i\mW_j^\T-\delta_{i,j}|\leq\eps'$ and for all $i\in[c]$, $|\ns{\mW_i}-1|\leq\eps'$. We will only use these inequalities finite times so applying a union bound will give the probability result.

For $i=1$, we know that $\overline{\mW}_1 = \mW_1/\norm{\mW_1}$ and $|\norm{\mW_1}-1|\leq\eps'$, so $\norm{\overline{\mW}_i-\mW_i}\leq \eps'$.

If our inductive hypothesis holds for $i\leq k$, then for $i=k+1$, we have for all $j\leq k$,
\begin{equation}\begin{split}
    |\langle \mW_i, \overline{\mW}_j\rangle|&\leq |\langle \mW_i, \mW_j\rangle| + |\langle \mW_i, \overline{\mW}_j-\mW_j\rangle|\\
    &\leq \eps'+ \norm{\mW_i}\cdot\norm{\overline{\mW}_j-\mW_j}
    \\
    &\leq \eps'+ (1+\eps')8^j\eps'\\
    &\leq (2^{3j+1}+1)\eps'.
\end{split}\end{equation}
Therefore,
\begin{equation}
    \norm{\mW_i'-\mW_i}\leq \sum_{j\in[k]} |\langle \mW_i, \overline{\mW}_j\rangle|\leq \eps'+\sum_{j\in[k]}(2^{3j+1}+1)\eps'\leq (2^{3k+2} - 1)\eps',
\end{equation}
and
\begin{equation}
    |\norm{\mW_i'}-1|\leq |\norm{\mW_i}-1| + \norm{\mW_i'-\mW_i}\leq 2^{3k+2}\eps'.
\end{equation}
Thus,
\begin{equation}\begin{split}
    \norm{\overline{\mW}_i-\mW_i}&\leq \norm{\overline{\mW}_i-\mW_i'} + \norm{\mW_i'-\mW_i}\\
                          &\leq |\norm{\mW_i'}-1| + \norm{\mW_i'-\mW_i}\\
                          &\leq 8^{k+1}\eps',
\end{split}\end{equation}
which finishes the induction and implies that for all $\eps>0$, for all $i\in[c], \norm{\overline{\mW}_i-\mW_i}\leq 8^{i}\eps'$. Thus,
\begin{equation}
\label{eqn:W-equal-W-bar}
    \fns{\overline{\mW}-\mW} = \sum_{i\in[c]} \ns{\overline{\mW}_i-\mW_i}\leq c\cdot 16^c\eps'.
\end{equation}
This means that
\begin{equation}\begin{split}
    \fn{\mW^\T\mW-P_\mW} &= \fn{\mW^\T\mW-\overline{\mW}^\T\overline{\mW}}\\
                  &\leq 2\fn{\mW-\overline{\mW}}\fn{\overline{\mW}} + \fns{\mW-\overline{\mW}}\\
                  &\leq 2c\cdot\sqrt{c}\cdot 8^c\sqrt{\eps'} + c\cdot 16^c\eps'\leq \eps.
\end{split}
\end{equation}
\end{proofof}

For the final property of the weight matrices, we show that the maximum among all entry of the weight matrices are reasonably small with high probability.
\begin{lemma}
\label{lemma:w-not-too-large}Fix any $\alpha>0$, consider $\mW\in\R^{a\times b}$ for some $b>a^{1+\alpha}$ such that each entry is sampled from a zero mean Gaussian $\cN(0, \frac1b)$. The largest entry of $\mW$ is reasonably small with high probability as $b$ goes to infinity, namely,
\begin{equation}
\lim_{b\to\infty}\Pr\left[\max_{(i,j)\in[a]\times[b]}|\mW^{(2)}_{ij}| > 2b^{-\frac13}\right] = 0
\end{equation}
\end{lemma}

\begin{proofof}{\lemmaref{lemma:w-not-too-large}}
For i.i.d. random variables $\rvx_1,\dots, \rvx_b\sim \gN(0,1)$,
by concentration inequality on maximum of Gaussian random variables, for any $t>0$, we have
\begin{equation}
    \Pr\left[\max_{i\in[b]}\rvx_i > \sqrt{2\log (2b)} + t\right] < 2e^{-\frac{t^2}{2}}.
\end{equation}
For any $i,j\in[a]\times [b]$, since $\mW_{ij}$ are i.i.d. sampled from $\gN(0,\frac1b)$, with rescaling of $1/\sqrt{b}$ we may substitute $\rvx_j$ with $\mW_{ij}$. It follows that
\begin{equation}
    \Pr\left[\max_{(i,j)\in[a]\times[b]}\mW^{(2)}_{ij} > \frac{\sqrt{2\log (2ab)} + t}{\sqrt{b}}\right] < 2e^{-\frac{t^2}{2}}.
\end{equation}
Taking $t=b^{\frac16}$, since $a<b$, for large $b$ we have $\sqrt{2\log (2ab)} < \sqrt{2\log (2b^2)} < b^{\frac16}$. Thus for large $b$,
\begin{equation}
\begin{split}
    \Pr\left[\max_{(i,j)\in[a]\times[b]}\mW_{ij} > 2b^{-\frac13}\right] &= \Pr\left[\max_{(i,j)\in[a]\times[b]}\mW_{ij} > \frac{b^{\frac16} + b^{\frac16}}{\sqrt{n}}\right]\\
    & < \Pr\left[\max_{(i,j)\in[a]\times[b]}\mW_{ij} > \frac{\sqrt{2\log (2b)} + b^{\frac16}}{\sqrt{b}}\right] < 2e^{-\frac{b^{\frac13}}{2}}.
\end{split}
\end{equation}
With the same argument, we have
\begin{equation}
    \Pr\left[\min_{(i,j)\in[a]\times[b]}\mW_{ij} < -2b^{-\frac13}\right] < 2e^{-\frac{b^{\frac13}}{2}}.
\end{equation}
Passing $b$ to infinity completes the proof.
\end{proofof}
From the above lemma, we can bound the maximum entry of $\mW^{(1)}$ and $\mW^{(2)}$ as follows:

\begin{corollary}
\label{cor:w-not-too-large}
With probability 1 over $\mW^{(1)}$ and $\mW^{(2)}$,
\begin{equation}
\begin{split}
    \lim_{n\to\infty}\Pr\left[\max_{(i,j)\in[n]\times[d]}|\mW^{(1)}_{ij}| > 2d^{-\frac13}\right] &= 0,\\
    \lim_{n\to\infty}\Pr\left[\max_{(i,j)\in[c]\times[n]}|\mW^{(2)}_{ij}| > 2n^{-\frac13}\right] &= 0.\\
\end{split}
\end{equation}
\end{corollary}
