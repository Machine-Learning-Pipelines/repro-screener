\section{Downstream Tasks}
\label{sec:tasks}

We further evaluate the benefits of incorporating AR autoencoders into two downstream tasks: style transfer and image denoising. To assess the benefits of AR autoencoders, in each task, we simply replace the standard autoencoders by the AR versions without incorporating any additional task-specific priors or tuning. Despite not tailoring our architecture to each scenario, it obtains on-par or better results than well-established methods. Refer to \secref{sec:supp_additional} and \secref{sec:supp_proposed_method} for more results and full implementation details.

\subsection{Style Transfer via Robust Feature Alignment}
\input{main_data/tab_style}
\input{main_data/fig_style}
Motivated by the perceptual properties of AR features \cite{engstrom_2019_adversarial}, we analyze their impact on style transfer using our AR AlexNet autoencoder as backbone and measure their improvement in both structure and texture preservation.

\textbf{Experimental Setup.} Stylization is evaluated on $75$ random content images and $100$ random style images, leading to $7,500$ image pairs. Content and style preservation is evaluated via the SSIM between content and stylized images and the VGG-19 Gram loss between style and stylized images, respectively. \layer{Conv1} and \layer{conv2} models use nearest neighbor interpolation instead of transposed convolution layers to improve reconstruction and avoid checkerboard effects, while the \layer{conv5} model remains unaltered. We also include results using Universal Style Transfer's (UST) official implementation, using a VGG-19 backbone.

\textbf{Results.} Our AR autoencoder improves both texture and structure preservation over its standard version (\tabref{tab:style_quantitative}). Stylization via AR features removes artifacts in flat areas, reducing blurry outputs and degraded structure (\figref{fig:style01}). Besides, our AR model gets a lower Gram loss with respect to UST. This implies that, despite matching less feature maps than the VGG-19 model (three instead of five), stylizing via our AR AlexNet autoencoder better preserves the style.

As expected, UST obtains a better SSIM since VGG-19 has more complexity and uses less contracted feature maps than our AlexNet model (e.g. $14 \times 14 \times 512$ vs. $6 \times 6 \times 256$). Also, UST \textit{blends} stylized and content features to better preserve shapes. Overall, a comparison between our AR model and UST shows a tradeoff between content and style preservation.

\subsection{Image Denoising via AR Autoencoder}
Similarly to the robustness imposed by regularized autoencoders \cite{vincent_2010_stacked,rifai2011contractive,kingma_2013_auto}, we harness the manifold learned by AR models to obtain noise-free reconstructions. We evaluate our AR AlexNet denoising model and compare its restoration properties with alternative learn-based methods.
\input{main_data/tab_denoising}
\input{main_data/fig_denoising}

\textbf{Experimental Setup.} Our image denoising model consists of an AR autoencoder equipped with skip connections in \layer{conv1}, \layer{conv2} and \layer{conv5} layers to better preserve image details. Skip connections follow the Wavelet Pooling approach \cite{yoo_2019_photorealistic}. Generators are trained on ImageNet via pixel and feature losses.

Accuracy is evaluated on the Kodak24, McMaster \cite{zhang_2011_color} and Color Berkeley Segmentation Dataset 68 (CBSD68) \cite{MartinFTM01} for clipped additive Gaussian noise ($\sigma=50/255$). We compare our AR model against two learn-based methods, Trainable Nonlinear Reaction Diffusion (TNRD) \cite{chen_2016_trainable} and Multi Layer Perceptron-based model (MLP) \cite{burger_2012_image}, often included in real-noise denoising benchmarks \cite{anwar_2019_real,guo_2019_toward}.

\textbf{Results.}  Our AR model improves over its standard version in all metrics across all datasets (\tabref{tab:denoising}). While standard predictions include color distorsions and texture artifacts, AR predictions show a better texture preservation and significantly reduce the distorsions introduced by the denoising process (\figref{fig:denoising}).

Our AR model obtains the best PSNR, SSIM and LPIPS scores on CBSD68, the most diverse of all datasets. While it is outperformed in PSNR by MLP in the two remaining datasets, it improves in SSIM and LPIPS, getting best or second best performance. For the McMaster dataset, SSIM and LPIPS values obtained by our model are slightly below the best values. Overall, our model consistently preserves the perceptual and structural similarity across all datasets, showing competitive results with alternative data-driven approaches.
