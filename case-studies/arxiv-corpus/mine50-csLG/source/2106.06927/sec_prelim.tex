\section{Preliminaries}
\label{sec:framework}
Our model exploits AR representations to reconstruct high-quality images, which is related to the feature inversion framework. Specifically, we explore AR features as a strong prior to obtain photorealism. For a clear understanding of our proposal, we review fundamental concepts of feature inversion and AR training.

{\bf Feature Inversion.}
Consider a target image $x_{0}\in \mathbb{R}^{W \times H \times C}$ and its contracted representation $f_{0}\triangleq F_{\theta}(x_{0})\in \mathbb{R}^{W'\times H'\times C'}$. Here, $F_{\theta}$ denotes the target model, \eg AlexNet, with parameters $\theta \in \mathbb{R}^{T}$ and $W'H'C'\ll WHC$. Features extracted by $F_{\theta}$ encapsulate rich input information that can either be used for the task it was trained on, transferred to a related domain \cite{pan_2009_survey} or  used for applications such as image enhancement and manipulation \cite{johnson_2016_perceptual,gatys_2016_image}.

An effective way to leverage these representations is by training a second model, a generator, to map them to the pixel domain. This way, deep features can be manipulated and transformed into images \cite{dosovitskiy_2015_inverting,dosovitskiy_2016_generating}. Also, since deep features preserve partial input information, inverting them elucidates what kind of attributes they encode. Based on these, \textit{feature inversion} \cite{simonyan_2013_deep,mahendran_2015_understanding,dosovitskiy_2015_inverting} has been extensively studied for visualization and understanding purposes as well as for synthesis and manipulation tasks. Typically, feature inversion is formulated as an optimization problem:
\begin{align}
    \hat{x}=&\ \text{arg } \underset{x}{\text{min}}\ \mathcal{F}(F_{\theta}(x), f_{0}) + \lambda \mathcal{R}(x),
\end{align}
where $\hat{x}$ is the estimated image and $\mathcal{F}(F_{\theta}(x), f_{0})$ the fidelity term between estimated and target representations, $F_{\theta}(x)$ and $f_{0}$ respectively. $\mathcal{R}(x)$ denotes the regularization term imposing \textit{apriori} constraints in the pixel domain and $\lambda \in \mathbb{R}_{++}$ balances between fidelity and regularization terms.

{\bf Adversarial Robustness.}
Adversarial training adds perturbations to the input data and lets the network learn how to classify in the presence of such adversarial attacks \cite{goodfellow_2014_explaining,madry_2018_towards,athalye_2018_obfuscated}. Consider the image classification task with annotated dataset $\mathcal{K}$. Let an annotated pair correspond to image $x \in \mathbb{R}^{W\times H\times C}$ and its one-hot encoded label $y\in \{0,1\}^{|\mathcal{C}|}$, where $\mathcal{C}$ is the set of possible classes. From the definition by Madry et al.~\cite{madry_2018_towards}, a perturbed input is denoted by $x'=x+\delta$, where $x'$ is the perturbed sample and $\delta$ the perturbation. Let the set of perturbations be bounded by the $\ell_{p}$ ball for $p\in \{2,\infty\},\ \mathcal{S}:\{\delta,\ \|\delta\|_{p}\leq\varepsilon\}$. Then, the AR training corresponds to an optimization problem:
\vspace{-0.1cm}
\begin{align}
    \label{eq:robust01}
    \tilde{\theta}=&\ \text{arg }\underset{\theta}{\text{min}}\ \mathbb{E}_{(x,y)\sim \mathcal{K}}\bigg[\underset{\delta \in \mathcal{S}}{\text{max}}\ \mathcal{L}_{x', y}(\theta) \bigg],
    \vspace{-0.1cm}
\end{align}
where $\tilde{\theta}\in \mathbb{R}^{T}$ are the optimal weights and $\mathcal{L}_{x',y}(\theta)$ the negative log-likelihood. The goal is to minimize $\mathcal{L}_{x',y}(\theta)$ in the presence of the worst possible adversary.
