\section{Experiments on Feature Inversion}
\label{sec:experimental}

\input{main_data/tab_inversion_alexnet}
\input{main_data/fig_inversion_alexnet}

We begin analyzing the reconstruction accuracy achieved by inverting features from different classifiers and empirically show that learning how to invert AR features via our proposed generator improves over standard feature inversion. Refer to \secref{sec:supp_results} and \secref{sec:supp_proposed_method} for additional inversion results and training details.

\subsection{Reconstruction Accuracy of AR Autoencoders}
\label{sec:experimental_inverting}
\textbf{Inverting AlexNet features.} Standard and AR AlexNet autoencoders are trained as described in \secref{sec:proposed_inversion} on ImageNet for comparison purposes. The AR AlexNet classifier is trained via $\ell_{2}$-PGD attacks \cite{madry_2018_towards} of radius $\varepsilon=\frac{3}{255}$ and $7$ steps of size $0.5$. Training is performed using $90$ epochs via SGD with a learning rate of $0.1$ reduced $10$ times every $30$ epochs. On the other hand, the standard AlexNet classifier is trained on natural images via cross-entropy (CE) loss with the same SGD setup as in the AR case.

Next, generators are trained  using pixel, feature and GAN losses to invert AlexNet \layer{conv5} features (size $6\times6\times256$). Both AR and standard models use the same generator architecture, which corresponds to the mirror network of the encoder. We deliberately use a simple architecture to highlight the reconstruction improvement is due to inverting AR features and not the generator capacity. We also train generators using (i) pixel and (ii) pixel and feature losses to ablate their effect. Reconstruction quality is evaluated using PSNR, SSIM and LPIPS.

Under all three loss combinations, reconstructions from AR AlexNet features obtain better PSNR and SSIM than their standard counterparts (\tabref{tab:inversion_alexnet}). Specifically, inverting AR AlexNet features gives an average PSNR improvement of over $2$ dB in all three cases. LPIPS scores also improve, except when using pixel, feature and GAN losses. Nevertheless, inverting AR features obtain a strong PSNR and SSIM improvement in this case as well. Qualitatively, inverting AR features better preserves the natural appearance in all cases, reducing the checkerboard effect and retaining sharp edges (\figref{fig:inversion_alexnet}).

\textbf{Inverting VGG features.} We extend the analysis to VGG-16 trained on ImageNet-143 and evaluate the reconstruction improvement achieved by inverting its AR features. We use the AR pre-trained classifier from the recent work by Liu et al.~\cite{liu_2018_adv} trained using $\ell_{\infty}$-PGD attacks of radius $\varepsilon=0.01$ and $10$ steps of size $\frac{1}{50}$. Training is performed using $80$ epochs via SGD with a learning rate of $0.1$ reduced $10$ times every $30$, $20$, $20$ and $10$ epochs. On the other hand, its standard version is trained on natural images via CE loss with the same SGD setup as in the AR case.

Generators are trained on pixel and feature losses to invert VGG-16 \layer{conv5\_1} features (size $14 \times 14 \times 512$). Similarly to the AlexNet analysis, generators inverting both standard and AR features correspond to the mirror network of the encoder. We evaluate the reconstruction accuracy of both models and report their level of adversarial robustness (\tabref{tab:inversion_vgg16} and \figref{fig:inversion_vgg16}).

\input{main_data/tab_rec_vgg16_combined}

Quantitatively, reconstructions from AR VGG-16 features are more accurate than those of standard features in PSNR, SSIM and LPIPS by a large margin. Specifically, inverting AR VGG-16 features gives an average PSNR improvement of $2.7$ dB. Qualitatively, reconstructions from AR VGG-16 features are more similar to the original images, reducing artifacts and preserving object boundaries.

Furthermore, the reconstruction accuracy attained by the AR VGG-16 autoencoder improves over that of the AR AlexNet model. This suggests that the benefits of inverting AR features are not constrained to shallow models such as AlexNet, but generalize to models with larger capacity.

\textbf{Inverting ResNet features.} To analyze the effect of inverting AR features from classifiers trained on different datasets, we evaluate the reconstruction accuracy obtained by inverting WideResNet-28-10 trained on CIFAR-10. We use the AR pre-trained classifier from the recent work by Zhang et al.~\cite{zhang_2020_geometry}. This model obtains State-of-the-art AR classification accuracy via a novel weighted adversarial training regime. Specifically, the model is adversarially trained via PGD by ranking the importance of each sample based on how close it is to the decision boundary (how \textit{attackable} the sample is).

AR training is performed using $\ell_{\infty}$ attacks of radius $\varepsilon=\frac{8}{255}$ and $10$ steps of size $\frac{2}{255}$. Classification training is performed using $100$ epochs (with a burn-in period of $30$ epochs) via SGD with a learning rate of $0.1$ reduced $10$ times every $30$ epochs. On the other hand, its standard version is trained on natural images via CE loss using the same SGD setup as in the AR case.

Generators for standard and AR WideResNet-28-10 models are trained to invert features from its 3rd residual block (size $8\times 8 \times 640$) via pixel and feature losses. Similarly to our previous analysis, both generators correspond to the mirror architecture of the encoder. We evaluate their reconstruction via PSNR, SSIM and LPIPS, and their robustness via AutoAttack \cite{croce_2020_reliable} (\tabref{tab:inversion_resnet28} and \figref{fig:inversion_resnet28}).

Similarly to previous scenarios, inverting WideResNet-28-10 AR features shows a large improvement over standard ones in all metrics. Specifically, inverting AR features increases PSNR in $4.8$ dB on average over standard features. Visually, the AR WideResNet-28-10 autoencoder reduces bogus components and preserves object contours on CIFAR-10 test samples.

Overall, results enforce our claim that the \textbf{benefits of inverting AR features extend to different models, datasets and training strategies}.
\input{main_data/tab_rec_resnet28_combined}

\subsection{Robustness Level vs. Reconstruction Accuracy}
\label{relationship_adversarial}
We complement the reconstruction analysis by exploring the relation between adversarial robustness and inversion quality. We train five AlexNet classifiers on ImageNet, one on natural images (standard) and four via $\ell_{2}$-PGD attacks with $\varepsilon\in \{0.5,2,3,4\}/255$. All other training parameters are identical across models.

For each classifier, an image generator is trained on an ImageNet subset via pixel, feature and GAN losses to invert \layer{conv5} features. Similar to \secref{sec:experimental_inverting}, all five generators correspond to the mirror network of the encoder. To realiably measure the impact of adversarial robustness, reconstruction accuracy is evaluated in terms of PSNR, SSIM and LPIPS. We also report the effective robustness level achieved by each model via AutoAttack (\tabref{tab:rob_vs_acc}).
\input{main_data/tab_robustness_vs_accuracy}
\input{main_data/tab_scale_combined}

Results show LPIPS and SSIM improve almost monotonically until a maximum value is reached at $\varepsilon=2$, while PSNR keeps increasing. This implies that just by changing $\varepsilon$ from $0.5$ to $4$ while keeping the exact same architecture and training regime, a reconstruction improvement of $1.2$ dB PSNR is obtained.

Based on this, we use an AR AlexNet model trained with $\varepsilon=3$ in our experiments, which gives the best tradeoff between PSNR, SSIM and LPIPS. Overall, our analysis suggests that, while all four AR models outperform the inversion accuracy of the standard model, the reconstruction improvement is not proportional to the robustness level. Instead, it is maximized at a particular level. Please refer to \secref{sec:supp_inverting_alternative} for additional robustness level vs. reconstruction accuracy experiments on ResNet-18 pointing to the same conclusion.

\subsection{Reconstructing Images at Unseen Resolutions}
\label{sec:experimental_scale}

Unlike extracting shift-invariant representations, image scaling is difficult to handle for standard CNN-based models \cite{sosnovik_2019_scale,fan_2020_scale}. Following previous work suggesting AR features are more generic and transferable than standard ones \cite{chen2020shape,salman_2020_adversarially}, we test whether our proposed AR autoencoder generalizes better to scale changes. We explore this property and show that our model trained on low-resolution samples improves reconstruction of images at unseen scales without any fine-tuning.

\textbf{Scenario 1: Reconstructing Upscaled Images.} Upscaled ImageNet samples are reconstructed from their AR AlexNet \layer{conv5} representations. For a fair comparison across scales, each image is normalized to $224 \times 224$ px. and then enlarged by an integer factor $L>1$. Experiments show a higher accuracy obtained from AR features in terms of PSNR, SSIM and LPIPS (\tabref{tab:eval_scale}). All metrics improve almost monotonically with $L$. In contrast, accuracy using standard features degrades with $L$.
Inversion from AR features show almost perfect reconstruction for large scales, while those of standard features show severe distorsions (\figref{fig:eval_scale}).

\input{main_data/tab_hires}
\input{main_data/fig_hires}

\textbf{Scenario 2: Reconstructing High-Resolution Images.} Standard and AR feature inversion is performed on the DIVerse 2K resolution dataset (DIV2K) \cite{agustsson_2017_ntire}, containing objects at multiple scales. AR feature reconstructions show a significant PSNR, SSIM and LPIPS improvement over standard ones, despite not being explicitly trained to handle such large-scale objects (\tabref{tab:hires}).

Qualitatively, reconstructions from AR AlexNet features preserve sharp edges, reduces color degradation and diminishes checkerboard effects induced by standard inversion (\figref{fig:hires}). Thus, for unseen scales and without finetuning, AR features better preserve structure without penalizing the perceptual similarity.

\subsection{Comparison against State-of-the-Art Inversion Techniques}
\label{sec:experimental_comparison}
The inversion accuracy of our AR autoencoder is compared against two alternative techniques: Optimization-based robust representation inversion (RI) \cite{engstrom_2019_adversarial} and DeePSiM \cite{dosovitskiy_2015_inverting}. For a fair comparison, all methods reconstruct images from AlexNet features. We begin by highlighting the differences between them.

While RI is a model-based approach that searches in the pixel domain for an image that matches a set of target AR features, we use a CNN-based generator trained on a combination of natural-image priors (\secref{sec:opt_crit}). On the other hand, while DeePSiM is also a CNN-based technique trained under multiple priors, its generator has approximately $63\%$ more trainable parameters than ours (\tabref{tab:inversion_comparison}).

\textbf{Experimental Setup.} All inversion methods are evaluated on ImageNet. Our standard and AR models are trained using pixel, feature and GAN losses using the training setup described in \secref{sec:supp_proposed_method}. DeePSiM is evaluated using its official Caffe implementation without any changes. RI is evaluated using its official PyTorch implementation modified to invert \layer{conv5} AR features. Input samples are rescaled to $224 \times 224$ px. ($227 \times 227$ px. for DeepSiM).

\textbf{Results.} Our AR AlexNet autoencoder obtains the best accuracy in terms of PSNR and the second best in terms of SSIM (\tabref{tab:inversion_comparison}). While it outperforms its standard version in PSNR and SSIM, it gets a marginally worse LPIPS. Moreover, our AR model outperforms RI in all metrics. Also, despite DeePSiM having more layers and using larger inputs, our model achieves a large PSNR improvement over it. Results highlight the improvement obtained by inverting AR features and how this fundamental change allows competitive reconstruction quality using three times less trainable parameters.

\input{main_data/tab_inversion_comparison}
