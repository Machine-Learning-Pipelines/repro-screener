
\section{Comparison with DP-SGD}
\label{sec-app:comparison-with-dp}

In this section, we provide more details on the arguments of
\Cref{sec:comparison-with-dp-sgd}, where we suppose that $\ell$ is $L$-component-Lipschitz
and $\Lambda$-Lipschitz.
To ease the comparison, we assume that $R_M = \norm{w^0 - w^*}_M$, which is
notably the case in the smooth setting with $\psi = 0$
(see Remark~\ref{rmq:expec-first-upper-bound-without-non-smooth}).

\paragraph{Balanced.}
We start by the scenario where coordinate-wise smoothness constants are balanced
and all equal to $M = M_1 = \cdots = M_p$.
We observe that
\begin{align}
  \norm{L}_{M^{-1}}
  = \sqrt{\sum_{j=1}^p \frac{1}{M_j} L_j^2}
  = \sqrt{\frac{1}{M} \sum_{j=1}^p L_j^2}
  = \frac{1}{\sqrt{M}} \norm{L}_2\enspace.
\end{align}
We then consider the convex and strongly-convex functions separately:
\begin{itemize}
  \item \textit{Convex functions:}
        it holds that $R_M = \sqrt{M} R_I$, which yields the equality
        $\norm{L}_{M^{-1}} R_{M} = \norm{L}_2 R_I$.
  \item \textit{Strongly convex functions:}
        if $f$ is $\mu_M$-strongly-convex with respect to $\norm{\cdot}_M$, then for
        any $x, y \in \RR^p$,
        \begin{align}
          f(y)
          \ge f(x) + \scalar{\nabla f(x)}{y - x} + \frac{\mu_M}{2} \norm{y - x}_M^2
          = f(x) + \scalar{\nabla f(x)}{y - x} + \frac{M\mu_M}{2} \norm{y - x}^2_2\enspace,
        \end{align}
        which means that $f$ is $M\mu_M$-strongly-convex with respect to $\norm{\cdot}_2$.
        This gives
        $\frac{\norm{L}_{M^{-1}}^2}{\mu_M}
          = \frac{\norm{L}_2^2 / M}{\mu_I / M}
          = \frac{\norm{L}_2^2}{\mu_I}$.
\end{itemize}

In light of the results summarized in \Cref{table:utility-cd-sgd}, it remains to
compare $\norm{L}_2 = \sqrt{\sum_{j=1}^p L_j^2}$ with $\Lambda$, for which it
holds that $\Lambda \le \sqrt{\sum_{j=1}^p L_j^2} \le \sqrt{p}\Lambda$, which is
our result.

\paragraph{Unbalanced.}
When smoothness constants are disparate, we discuss the case where
\begin{itemize}
  \item \textit{one coordinate of the gradient dominates the others:}
        we assume without loss of generality that the dominating coordinate is the first one.
        It holds that $M_1 =: M_{\max} \gg M_{\min} =: M_j$, for all $j \neq 1$ and
        $L_1 =: L_{\max} \gg L_{\min} =: L_j$, for all $j \neq 1$ such that
        $\frac{L_1^2}{M_1} \gg \sum_{j\neq 1} \frac{L_j^2}{M_j}$.
        As $L_1$ dominates the other component-Lipschitz constants, most of the variation of the loss comes from its first coordinate.
        This implies that $L_1$ is close to the global Lipschitz constant $\Lambda$ of $\ell$.
        As such, it holds that
        \begin{align}
          \norm{L}_{M^{-1}}^2
          =
          \sum_{j=1}^p \frac{L_j^2}{M_j}
          \approx \frac{L_1^2}{M_1}
          \approx \frac{\Lambda^2}{M_{\max}}\enspace.
        \end{align}
  \item \textit{the first coordinate of $\bar w^0$ is already very close to
          its optimal value}
        so that $M_1\abs{\bar w^0_1 - w^*_1} \ll \sum_{j \neq 1} M_j \abs{\bar w^0_j - w^*_j}$.
        Under this hypothesis,
        \begin{align}
          R_M^2
          \approx \sum_{j\neq 1}M_j\abs{w_j^0 - w_j^*}^2
          = M_{\min} \sum_{j\neq 1}\abs{w_j^0 - w_j^*}^2
          \approx M_{\min} R_I^2\enspace.
        \end{align}
\end{itemize}
We can now easily compare DP-CD with DP-SGD in this scenario.
First, if $\ell$ is convex, then
$\norm{L}_{M^{-1}} R_M \approx \sqrt{\frac{M_{\min}}{M_{\max}}} \Lambda R_I$.
Second, when $\ell$ is strongly-convex, we observe that for $x, y \in \RR^p$,
\begin{align}
  f(y)
  \ge f(x) + \scalar{\nabla f(x)}{y - x} + \frac{\mu_M}{2} \norm{y - x}_M^2
  \ge f(x) + \scalar{\nabla f(x)}{y - x} + \frac{M_{\min} \mu_M}{2} \norm{y - x}_2^2\enspace,
\end{align}
which implies that when $f$ is $\mu_M$ strongly-convex with respect to
$\norm{\cdot}_M$, it is $M_{\min}\mu_M$ strongly-convex with respect
to $\norm{\cdot}_2$.  This yields, under our hypotheses,
$\frac{\norm{L}_{M^{-1}}^2}{\mu_M} \approx \frac{\Lambda^2 /
  M_{\max}}{\mu_I / M_{\min}} = \frac{M_{\min}}{M_{\max}}
\frac{\Lambda^2}{\mu_I}$.
In both cases, DP-CD can get arbitrarily better than DP-SGD, and gets
better as the ratio ${M_{\max}}/{M_{\min}}$ increases.




The two hypotheses we describe above are of course very restrictive.
However, it gives some insight about when and why DP-CD can outperform
DP-SGD.
Our numerical experiments in \Cref{sec:numerical-experiments} confirm this
analysis, even in less favorable
cases.













