\begin{thebibliography}{10}

\bibitem{advbox}
{Advbox}.
\newblock \url{https://github.com/advboxes/AdvBox/}.

\bibitem{cleverhans}
{CleverHans Adversarial Examples Library}.
\newblock \url{https://github.com/tensorflow/cleverhans/}.

\bibitem{ibm-art}
{IBM Adversarial Robustness Toolbox (ART)}.
\newblock \url{https://github.com/Trusted-AI/adversarial-robustness-toolbox/}.

\bibitem{trojai}
Trojai.
\newblock \url{https://trojai.readthedocs.io}.

\bibitem{blind-backdoor}
Eugene {Bagdasaryan} and Vitaly {Shmatikov}.
\newblock {Blind Backdoors in Deep Learning Models}.
\newblock {\em Proceedings of USENIX Security Symposium (SEC)}, 2021.

\bibitem{federated-backdoor}
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly
  Shmatikov.
\newblock {How To Backdoor Federated Learning}.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2020.

\bibitem{Biggio:2012:spr}
Battista Biggio, Giorgio Fumera, Fabio Roli, and Luca Didaci.
\newblock {Poisoning Adaptive Biometric Systems}.
\newblock In {\em Proceedings of Joint {IAPR} International Workshop on
  Structural, Syntactic, and Statistical Pattern Recognition (SSPR{\&}SPR)},
  2012.

\bibitem{modelzoo}
{BVLC}.
\newblock Model zoo.
\newblock \url{https://github.com/BVLC/caffe/wiki/Model-Zoo}, 2017.

\bibitem{vggface2}
Qiong Cao, Li~Shen, Weidi Xie, Omkar~M Parkhi, and Andrew Zisserman.
\newblock {Vggface2: A dataset for recognising faces across pose and age}.
\newblock In {\em 13th IEEE International Conference on Automatic Face \&
  Gesture Recognition}, 2018.

\bibitem{active-clustering}
Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin
  Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava.
\newblock {Detecting Backdoor Attacks on Deep Neural Networks by Activation
  Clustering}.
\newblock In {\em ArXiv e-prints}, 2018.

\bibitem{deep-inspect}
Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar.
\newblock {DeepInspect: A Black-box Trojan Detection and Mitigation Framework
  for Deep Neural Networks}.
\newblock In {\em Proceedings of International Joint Conference on Artificial
  Intelligence}, 2019.

\bibitem{targeted-backdoor}
Xinyun {Chen}, Chang {Liu}, Bo~{Li}, Kimberly {Lu}, and Dawn {Song}.
\newblock {Targeted Backdoor Attacks on Deep Learning Systems Using Data
  Poisoning}.
\newblock {\em ArXiv e-prints}, 2017.

\bibitem{sentinet}
Edward Chou, Florian Tramer, Giancarlo Pellegrino, and Dan Boneh.
\newblock {SentiNet: Detecting Physical Attacks Against Deep Learning Systems}.
\newblock In {\em ArXiv e-prints}, 2018.

\bibitem{randomized-smoothing}
Jeremy~M {Cohen}, Elan {Rosenfeld}, and J.~{Zico Kolter}.
\newblock {Certified Adversarial Robustness via Randomized Smoothing}.
\newblock In {\em Proceedings of IEEE Conference on Machine Learning (ICML)},
  2019.

\bibitem{Cooper:2014:news}
Paul Cooper.
\newblock {Meet AISight: The scary CCTV network completely run by AI}.
\newblock \url{http://www.itproportal.com/}, 2014.

\bibitem{imagenet}
J.~Deng, W.~Dong, R.~Socher, L.~Li, Kai Li, and Li~Fei-Fei.
\newblock {ImageNet: A Large-scale Hierarchical Image Database}.
\newblock In {\em Proceedings of IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2009.

\bibitem{Esteva:2017:nature}
Andre Esteva, Brett Kuprel, Roberto~A. Novoa, Justin Ko, Susan~M. Swetter,
  Helen~M. Blau, and Sebastian Thrun.
\newblock Dermatologist-level classification of skin cancer with deep neural
  networks.
\newblock {\em Nature}, 542(7639):115--118, 2017.

\bibitem{fong:mask}
Ruth~C Fong and Andrea Vedaldi.
\newblock {Interpretable Explanations of Black Boxes by Meaningful
  Perturbation}.
\newblock In {\em Proceedings of IEEE International Conference on Computer
  Vision (ICCV)}, 2017.

\bibitem{strip}
Yansong Gao, Chang Xu, Derui Wang, Shiping Chen, Damith Ranasinghe, and Surya
  Nepal.
\newblock {STRIP: A Defence Against Trojan Attacks on Deep Neural Networks}.
\newblock In {\em Proceedings of Annual Computer Security Applications
  Conference (ACSAC)}, 2019.

\bibitem{Goodfellow:2014:nips}
Ian~J. {Goodfellow}, Jean {Pouget-Abadie}, Mehdi {Mirza}, Bing {Xu}, David
  {Warde-Farley}, Sherjil {Ozair}, Aaron {Courville}, and Yoshua {Bengio}.
\newblock {Generative Adversarial Networks}.
\newblock In {\em Proceedings of Advances in Neural Information Processing
  Systems (NeurIPS)}, 2014.

\bibitem{badnet}
Tianyu {Gu}, Brendan {Dolan-Gavitt}, and Siddharth {Garg}.
\newblock {BadNets: Identifying Vulnerabilities in the Machine Learning Model
  Supply Chain}.
\newblock {\em ArXiv e-prints}, 2017.

\bibitem{Guo:2018:ccs}
Wenbo Guo, Dongliang Mu, Jun Xu, Purui Su, Gang Wang, and Xinyu Xing.
\newblock {LEMNA: Explaining Deep Learning Based Security Applications}.
\newblock In {\em Proceedings of ACM Conference on Computer and Communications
  (CCS)}, 2018.

\bibitem{tabor}
Wenbo {Guo}, Lun {Wang}, Xinyu {Xing}, Min {Du}, and Dawn {Song}.
\newblock {TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan
  Backdoors in AI Systems}.
\newblock In {\em Proceedings of IEEE International Conference on Data Mining
  (ICDM)}, 2019.

\bibitem{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock {Deep Residual Learning for Image Recognition}.
\newblock In {\em Proceedings of IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2016.

\bibitem{densenet}
Gao {Huang}, Zhuang {Liu}, Laurens {van der Maaten}, and Kilian~Q.
  {Weinberger}.
\newblock {Densely Connected Convolutional Networks}.
\newblock In {\em Proceedings of IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2017.

\bibitem{neuron-inspect}
Xijie {Huang}, Moustafa {Alzantot}, and Mani {Srivastava}.
\newblock {NeuronInspect: Detecting Backdoors in Neural Networks via Output
  Explanations}.
\newblock In {\em Proceedings of AAAI Conference on Artificial Intelligence
  (AAAI)}, 2019.

\bibitem{Ji:2018:ccsa}
Yujie Ji, Xinyang Zhang, Shouling Ji, Xiapu Luo, and Ting Wang.
\newblock {Model-Reuse Attacks on Deep Learning Systems}.
\newblock In {\em Proceedings of ACM SAC Conference on Computer and
  Communications (CCS)}, 2018.

\bibitem{trojdrl}
Panagiota {Kiourti}, Kacper {Wardega}, Susmit {Jha}, and Wenchao {Li}.
\newblock {TrojDRL: Trojan Attacks on Deep Reinforcement Learning Agents}.
\newblock {\em ArXiv e-prints}, 2019.

\bibitem{cifar}
Alex Krizhevsky and Geoffrey Hinton.
\newblock {Learning Multiple Layers of Features from Tiny Images}.
\newblock {\em Technical report, University of Toronto}, 2009.

\bibitem{acl-backdoor}
Keita {Kurita}, Paul {Michel}, and Graham {Neubig}.
\newblock {Weight Poisoning Attacks on Pre-trained Models}.
\newblock In {\em Proceedings of Annual Meeting of the Association for
  Computational Linguistics (ACL)}, 2020.

\bibitem{adv-backdoor}
Te~{Lester Juin Tan} and Reza {Shokri}.
\newblock {Bypassing Backdoor Detection Algorithms in Deep Learning}.
\newblock In {\em Proceedings of IEEE European Symposium on Security and
  Privacy (Euro S\&P)}, 2020.

\bibitem{invisible-backdoor}
Shaofeng {Li}, Benjamin~Zi {Hao Zhao}, Jiahao {Yu}, Minhui {Xue}, Dali
  {Kaafar}, and Haojin {Zhu}.
\newblock {Invisible Backdoor Attacks Against Deep Neural Networks}.
\newblock {\em ArXiv e-prints}, 2019.

\bibitem{survey1}
Yiming {Li}, Baoyuan {Wu}, Yong {Jiang}, Zhifeng {Li}, and Shu-Tao {Xia}.
\newblock {Backdoor Learning: A Survey}.
\newblock {\em ArXiv e-prints}, 2020.

\bibitem{benign-feature-backdoor}
Junyu Lin, Lei Xu, Yingqi Liu, and Xiangyu Zhang.
\newblock {Composite Backdoor Attack for Deep Neural Network by Mixing Existing
  Benign Features}.
\newblock In {\em Proceedings of ACM SAC Conference on Computer and
  Communications (CCS)}, 2020.

\bibitem{Ling:2019:sp}
X.~Ling, S.~Ji, J.~Zou, J.~Wang, C.~Wu, B.~Li, and T.~Wang.
\newblock {DEEPSEC: A Uniform Platform for Security Analysis of Deep Learning
  Model}.
\newblock In {\em Proceedings of IEEE Symposium on Security and Privacy
  (S\&P)}, 2019.

\bibitem{fine-pruning}
Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg.
\newblock {Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural
  Networks}.
\newblock In {\em Proceedings of Symposium on Research in Attacks, Intrusions
  and Defenses (RAID)}, 2018.

\bibitem{abs}
Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and Xiangyu
  Zhang.
\newblock {ABS: Scanning Neural Networks for Back-Doors by Artificial Brain
  Stimulation}.
\newblock In {\em Proceedings of ACM SAC Conference on Computer and
  Communications (CCS)}, 2019.

\bibitem{trojannn}
Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang,
  and Xiangyu Zhang.
\newblock Trojaning attack on neural networks.
\newblock In {\em Proceedings of Network and Distributed System Security
  Symposium (NDSS)}, 2018.

\bibitem{reflection-backdoor}
Yunfei {Liu}, Xingjun {Ma}, James {Bailey}, and Feng {Lu}.
\newblock {Reflection Backdoor: A Natural Backdoor Attack on Deep Neural
  Networks}.
\newblock In {\em Proceedings of European Conference on Computer Vision
  (ECCV)}, 2020.

\bibitem{madry:iclr:2018}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock {Towards Deep Learning Models Resistant to Adversarial Attacks}.
\newblock In {\em Proceedings of International Conference on Learning
  Representations (ICLR)}, 2018.

\bibitem{magnet}
Dongyu Meng and Hao Chen.
\newblock {MagNet: A Two-Pronged Defense Against Adversarial Examples}.
\newblock In {\em Proceedings of ACM SAC Conference on Computer and
  Communications (CCS)}, 2017.

\bibitem{universal-perturbation}
Seyed-Mohsen {Moosavi-Dezfooli}, Alhussein {Fawzi}, Omar {Fawzi}, Pascal
  {Frossard}, and Stefano {Soatto}.
\newblock {Analysis of Universal Adversarial Perturbations}.
\newblock {\em ArXiv e-prints}, 2017.

\bibitem{imc}
Ren Pang, Hua Shen, Xinyang Zhang, Shouling Ji, Yevgeniy Vorobeychik, Xiapu
  Luo, Alex Liu, and Ting Wang.
\newblock {A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models}.
\newblock In {\em Proceedings of ACM SAC Conference on Computer and
  Communications (CCS)}, 2020.

\bibitem{deepsweep}
Han Qiu, Yi~Zeng, Shangwei Guo, Tianwei Zhang, Meikang Qiu, and Bhavani
  Thuraisingham.
\newblock Deepsweep: An evaluation framework for mitigating dnn backdoor
  attacks using data augmentation.
\newblock In {\em Proceedings of ACM Symposium on Information, Computer and
  Communications Security (AsiaCCS)}, 2021.

\bibitem{adaptive-trigger}
Ahmed {Salem}, Rui {Wen}, Michael {Backes}, Shiqing {Ma}, and Yang {Zhang}.
\newblock {Dynamic Backdoor Attacks Against Machine Learning Models}.
\newblock {\em ArXiv e-prints}, 2020.

\bibitem{schuster-humpty}
Roei Schuster, Tal Schuster, Yoav Meri, and Vitaly Shmatikov.
\newblock {Humpty Dumpty: Controlling Word Meanings via Corpus Poisoning}.
\newblock In {\em Proceedings of IEEE Symposium on Security and Privacy
  (S\&P)}, 2020.

\bibitem{Sculley:2015:nips}
D.~Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar
  Ebner, Vinay Chaudhary, Michael Young, Jean-Francois Crespo, and Dan
  Dennison.
\newblock {Hidden Technical Debt in Machine Learning Systems}.
\newblock In {\em Proceedings of Advances in Neural Information Processing
  Systems (NeurIPS)}, 2015.

\bibitem{selvaraju:gradcam}
R.~R. Selvaraju, M.~Cogswell, A.~Das, R.~Vedantam, D.~Parikh, and D.~Batra.
\newblock {Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based
  Localization}.
\newblock In {\em Proceedings of IEEE International Conference on Computer
  Vision (ICCV)}, 2017.

\bibitem{explanation-guided-poisoning-attack}
Giorgio {Severi}, Jim {Meyer}, Scott {Coull}, and Alina {Oprea}.
\newblock {Explanation-Guided Backdoor Poisoning Attacks Against Malware
  Classifiers}.
\newblock 2021.

\bibitem{Shafahi:2018:nips}
Ali {Shafahi}, W.~{Ronny Huang}, Mahyar {Najibi}, Octavian {Suciu}, Christoph
  {Studer}, Tudor {Dumitras}, and Tom {Goldstein}.
\newblock {Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural
  Networks}.
\newblock In {\em Proceedings of Advances in Neural Information Processing
  Systems (NeurIPS)}, 2018.

\bibitem{vgg}
Karen Simonyan and Andrew Zisserman.
\newblock {Very Deep Convolutional Networks for Large-Scale Image Recognition}.
\newblock In {\em Proceedings of International Conference on Learning
  Representations (ICLR)}, 2014.

\bibitem{gtsrb}
Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel.
\newblock {Man vs. Computer: Benchmarking Machine Learning Algorithms for
  Traffic Sign Recognition}.
\newblock {\em Neural Metworks}, pages 323--32, 2012.

\bibitem{certified-purging}
Jacob Steinhardt, Pang~Wei Koh, and Percy Liang.
\newblock {Certified Defenses for Data Poisoning Attacks}.
\newblock In {\em Proceedings of Advances in Neural Information Processing
  Systems (NeurIPS)}, 2017.

\bibitem{Su:2019:tevc}
Jiawei Su, Danilo~Vasconcellos Vargas, and Kouichi Sakurai.
\newblock One pixel attack for fooling deep neural networks.
\newblock {\em IEEE Transactions on Evolutionary Computation}, 23(5):828--841,
  2019.

\bibitem{Suciu:2018:sec}
Octavian Suciu, Radu M\u{a}rginean, Yi\u{g}itcan Kaya, Hal Daum{\'e}, III, and
  Tudor Dumitra\c{s}.
\newblock {When Does Machine Learning FAIL? Generalized Transferability for
  Evasion and Poisoning Attacks}.
\newblock In {\em Proceedings of USENIX Security Symposium (SEC)}, 2018.

\bibitem{tact}
Di~Tang, XiaoFeng Wang, Haixu Tang, and Kehuan Zhang.
\newblock {Demon in the Variant: Statistical Analysis of DNNs for Robust
  Backdoor Contamination Detection}.
\newblock 2021.

\bibitem{embarassingly-simple-backdoor}
Ruixiang {Tang}, Mengnan {Du}, Ninghao {Liu}, Fan {Yang}, and Xia {Hu}.
\newblock {An Embarrassingly Simple Approach for Trojan Attack in Deep Neural
  Networks}.
\newblock In {\em Proceedings of ACM International Conference on Knowledge
  Discovery and Data Mining (KDD)}, 2020.

\bibitem{Tao:nips:2018}
Guanhong Tao, Shiqing Ma, Yingqi Liu, and Xiangyu Zhang.
\newblock {Attacks Meet Interpretability: Attribute-Steered Detection of
  Adversarial Samples}.
\newblock In {\em Proceedings of Advances in Neural Information Processing
  Systems (NeurIPS)}, 2018.

\bibitem{spectral-signature}
Brandon {Tran}, Jerry {Li}, and Aleksander {Madry}.
\newblock {Spectral Signatures in Backdoor Attacks}.
\newblock In {\em Proceedings of Advances in Neural Information Processing
  Systems (NeurIPS)}, 2018.

\bibitem{neo}
Sakshi {Udeshi}, Shanshan {Peng}, Gerald {Woo}, Lionell {Loh}, Louth {Rawshan},
  and Sudipta {Chattopadhyay}.
\newblock {Model Agnostic Defence against Backdoor Attacks in Machine
  Learning}.
\newblock {\em ArXiv e-prints}, 2019.

\bibitem{Versprille:2015:news}
Allyson Versprille.
\newblock {Researchers Hack Into Driverless Car System, Take Control of
  Vehicle}.
\newblock \url{http://www.nationaldefensemagazine.org/}, 2015.

\bibitem{neural-cleanse}
B.~Wang, Y.~Yao, S.~Shan, H.~Li, B.~Viswanath, H.~Zheng, and B.~Y. Zhao.
\newblock {Neural Cleanse: Identifying and Mitigating Backdoor Attacks in
  Neural Networks}.
\newblock In {\em Proceedings of IEEE Symposium on Security and Privacy
  (S\&P)}, 2019.

\bibitem{rab}
Maurice {Weber}, Xiaojun {Xu}, Bojan {Karlas}, Ce~{Zhang}, and Bo~{Li}.
\newblock {RAB: Provable Robustness Against Backdoor Attacks}.
\newblock {\em ArXiv e-prints}, 2020.

\bibitem{skip-connection}
Dongxian {Wu}, Yisen {Wang}, Shu-Tao {Xia}, James {Bailey}, and Xingjun {Ma}.
\newblock {Skip Connections Matter: On the Transferability of Adversarial
  Examples Generated with ResNets}.
\newblock In {\em Proceedings of International Conference on Learning
  Representations (ICLR)}, 2020.

\bibitem{dba}
Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo~Li.
\newblock {DBA: Distributed Backdoor Attacks against Federated Learning}.
\newblock In {\em Proceedings of International Conference on Learning
  Representations (ICLR)}, 2020.

\bibitem{feature-squeeze}
W.~{Xu}, D.~{Evans}, and Y.~{Qi}.
\newblock {Feature Squeezing: Detecting Adversarial Examples in Deep Neural
  Networks}.
\newblock In {\em Proceedings of Network and Distributed System Security
  Symposium (NDSS)}, 2018.

\bibitem{mnt}
Xiaojun {Xu}, Qi~{Wang}, Huichen {Li}, Nikita {Borisov}, Carl~A. {Gunter}, and
  Bo~{Li}.
\newblock {Detecting AI Trojans Using Meta Neural Analysis}.
\newblock In {\em Proceedings of IEEE Symposium on Security and Privacy
  (S\&P)}, 2020.

\bibitem{latent-backdoor}
Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben~Y. Zhao.
\newblock {Latent Backdoor Attacks on Deep Neural Networks}.
\newblock In {\em Proceedings of ACM SAC Conference on Computer and
  Communications (CCS)}, 2019.

\bibitem{distillation-backdoor}
Kota Yoshida and Takeshi Fujino.
\newblock {Disabling Backdoor and Identifying Poison Data by Using Knowledge
  Distillation in Backdoor Attacks on Deep Neural Networks}.
\newblock In {\em Proceedings of ACM Workshop on Artificial Intelligence and
  Security (AISec)}, 2020.

\bibitem{transfer-learning}
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
\newblock {How Transferable Are Features in Deep Neural Networks?}
\newblock In {\em Proceedings of Advances in Neural Information Processing
  Systems (NeurIPS)}, 2014.

\bibitem{adv}
Xinyang {Zhang}, Ningfei {Wang}, Hua {Shen}, Shouling {Ji}, Xiapu {Luo}, and
  Ting {Wang}.
\newblock {Interpretable Deep Learning under Fire}.
\newblock In {\em Proceedings of USENIX Security Symposium (SEC)}, 2020.

\bibitem{lm-trojan}
Xinyang {Zhang}, Zheng {Zhang}, and Ting {Wang}.
\newblock {Trojaning Language Models for Fun and Profit}.
\newblock {\em ArXiv e-prints}, 2020.

\bibitem{Zhu:2019:icml}
Chen {Zhu}, W.~{Ronny Huang}, Ali {Shafahi}, Hengduo {Li}, Gavin {Taylor},
  Christoph {Studer}, and Tom {Goldstein}.
\newblock {Transferable Clean-Label Poisoning Attacks on Deep Neural Nets}.
\newblock In {\em Proceedings of IEEE Conference on Machine Learning (ICML)},
  2019.

\end{thebibliography}
