\begin{thebibliography}{10}

\bibitem{alemi2018fixing}
Alexander Alemi, Ben Poole, Ian Fischer, Joshua Dillon, Rif~A Saurous, and
  Kevin Murphy.
\newblock Fixing a broken {ELBO}.
\newblock In {\em ICML}, pages 159--168, 2018.

\bibitem{alemi2016deep}
Alexander~A Alemi, Ian Fischer, Joshua~V Dillon, and Kevin Murphy.
\newblock Deep variational information bottleneck.
\newblock In {\em ICLR}, 2016.

\bibitem{allen2017primer}
Linda~JS Allen.
\newblock A primer on stochastic epidemic models: Formulation, numerical
  simulation, and analysis.
\newblock {\em Infectious Disease Modelling}, 2(2):128--142, 2017.

\bibitem{allen2008mathematical}
Linda~JS Allen, Fred Brauer, Pauline Van~den Driessche, and Jianhong Wu.
\newblock {\em Mathematical epidemiology}, volume 1945.
\newblock Springer, 2008.

\bibitem{anonymous2022metaflo}
Anonymous.
\newblock Meta-flo: Principled simple fast few-shot learning with stochastic
  prompt encoding networks.
\newblock 2022.

\bibitem{arjovsky2017towards}
Martin Arjovsky and L{\'e}on Bottou.
\newblock Towards principled methods for training generative adversarial
  networks.
\newblock In {\em ICLR}, 2017.

\bibitem{asuncion2007uci}
Arthur Asuncion and David Newman.
\newblock Uci machine learning repository, 2007.

\bibitem{bach2002kernel}
Francis~R Bach and Michael~I Jordan.
\newblock Kernel independent component analysis.
\newblock {\em Journal of Machine Learning Research}, 3(Jul):1--48, 2002.

\bibitem{agakov2004algorithm}
David Barber and Felix Agakov.
\newblock The {IM} algorithm: a variational approach to information
  maximization.
\newblock {\em NIPS}, 16:201, 2004.

\bibitem{barber2003information}
David Barber and Felix~V Agakov.
\newblock Information maximization in noisy channels: A variational approach.
\newblock {\em NIPS}, 16, 2003.

\bibitem{battiti1994using}
Roberto Battiti.
\newblock Using mutual information for selecting features in supervised neural
  net learning.
\newblock {\em IEEE transactions on Neural Networks}, 5(4):537--550, 1994.

\bibitem{belghazi2018mutual}
Mohamed~Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair,
  Yoshua Bengio, Aaron Courville, and Devon Hjelm.
\newblock Mutual information neural estimation.
\newblock In {\em ICML}, 2018.

\bibitem{aif360-oct-2018}
Rachel K.~E. Bellamy, Kuntal Dey, Michael Hind, Samuel~C. Hoffman, Stephanie
  Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta,
  Aleksandra Mojsilovic, Seema Nagar, Karthikeyan~Natesan Ramamurthy, John
  Richards, Diptikalyan Saha, Prasanna Sattigeri, Moninder Singh, Kush~R.
  Varshney, and Yunfeng Zhang.
\newblock {AI Fairness} 360: An extensible toolkit for detecting,
  understanding, and mitigating unwanted algorithmic bias, October 2018.

\bibitem{bengio2013representation}
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
\newblock Representation learning: A review and new perspectives.
\newblock {\em IEEE transactions on Pattern Analysis and Machine Intelligence},
  35(8):1798--1828, 2013.

\bibitem{berger2013statistical}
James~O Berger.
\newblock {\em Statistical decision theory and Bayesian analysis}.
\newblock Springer Science \& Business Media, 2013.

\bibitem{brekelmans2021improving}
Rob Brekelmans, Sicong Huang, Marzyeh Ghassemi, Greg Ver~Steeg, Roger~Baker
  Grosse, and Alireza Makhzani.
\newblock Improving mutual information estimation with annealed and
  energy-based bounds.
\newblock In {\em ICLR}, 2021.

\bibitem{chaloner1995bayesian}
Kathryn Chaloner and Isabella Verdinelli.
\newblock Bayesian experimental design: A review.
\newblock {\em Statistical Science}, pages 273--304, 1995.

\bibitem{chen2021simpler}
Junya Chen, Zhe Gan, Xuan Li, Qing Guo, Liqun Chen, Shuyang Gao, Tagyoung
  Chung, Yi~Xu, Belinda Zeng, Wenlian Lu, et~al.
\newblock Simpler, faster, stronger: Breaking the log-{K} curse on contrastive
  learners with flatnce.
\newblock {\em arXiv preprint arXiv:2107.01152}, 2021.

\bibitem{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In {\em ICML}, 2020.

\bibitem{cheng2020club}
Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and Lawrence
  Carin.
\newblock {CLUB}: A contrastive log-ratio upper bound of mutual information.
\newblock In {\em ICML}, 2020.

\bibitem{donsker1983asymptotic}
Monroe~D Donsker and SR~Srinivasa Varadhan.
\newblock Asymptotic evaluation of certain markov process expectations for
  large time. iv.
\newblock {\em Communications on Pure and Applied Mathematics}, 36(2):183--212,
  1983.

\bibitem{dwork2012fairness}
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel.
\newblock Fairness through awareness.
\newblock In {\em Proceedings of the 3rd innovations in theoretical computer
  science conference}, 2012.

\bibitem{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em ICML}, 2017.

\bibitem{foster2021deep}
Adam Foster, Desi~R Ivanova, Ilyas Malik, and Tom Rainforth.
\newblock Deep adaptive design: Amortizing sequential bayesian experimental
  design.
\newblock In {\em ICML}, 2021.

\bibitem{foster2019variational}
Adam Foster, Martin Jankowiak, Eli Bingham, Paul Horsfall, Yee~Whye Teh, Tom
  Rainforth, and Noah Goodman.
\newblock Variational bayesian optimal experimental design.
\newblock In {\em NeurIPS}, 2019.

\bibitem{foster2020unified}
Adam Foster, Martin Jankowiak, Matthew Oâ€™Meara, Yee~Whye Teh, and Tom
  Rainforth.
\newblock A unified stochastic gradient approach to designing bayesian-optimal
  experiments.
\newblock In {\em AISTATS}, 2020.

\bibitem{gao2015efficient}
Shuyang Gao, Greg Ver~Steeg, and Aram Galstyan.
\newblock Efficient estimation of mutual information for strongly dependent
  variables.
\newblock In {\em AISTATS}, 2015.

\bibitem{gao2018demystifying}
Weihao Gao, Sewoong Oh, and Pramod Viswanath.
\newblock Demystifying fixed $ k $-nearest neighbor information estimators.
\newblock {\em IEEE transactions on Information Theory}, 64(8):5629--5661,
  2018.

\bibitem{geyer1994convergence}
Charles~J Geyer.
\newblock On the convergence of monte carlo maximum likelihood calculations.
\newblock {\em Journal of the Royal Statistical Society: Series B
  (Methodological)}, 56(1):261--274, 1994.

\bibitem{gretton2005kernel}
Arthur Gretton, Ralf Herbrich, Alexander Smola, Olivier Bousquet, Bernhard
  Sch{\"o}lkopf, et~al.
\newblock Kernel methods for measuring independence.
\newblock {\em Journal of Machine Learning Research}, 2005.

\bibitem{gretton2003kernel}
Arthur Gretton, Ralf Herbrich, and Alexander~J Smola.
\newblock The kernel mutual information.
\newblock In {\em ICASSP}, 2003.

\bibitem{grill2020bootstrap}
Jean-Bastien Grill, Florian Strub, Florent Altch{\'e}, Corentin Tallec,
  Pierre~H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo~Avila Pires,
  Zhaohan~Daniel Guo, Mohammad~Gheshlaghi Azar, et~al.
\newblock Bootstrap your own latent: A new approach to self-supervised
  learning.
\newblock In {\em NeurIPS}, 2020.

\bibitem{grover2016node2vec}
Aditya Grover and Jure Leskovec.
\newblock node2vec: Scalable feature learning for networks.
\newblock In {\em SIGKDD}, 2016.

\bibitem{gupta2021controllable}
Umang Gupta, Aaron Ferber, Bistra Dilkina, and Greg~Ver Steeg.
\newblock Controllable guarantees for fair outcomes via contrastive information
  estimation.
\newblock {\em arXiv preprint arXiv:2101.04108}, 2021.

\bibitem{gutmann2010noise}
Michael Gutmann and Aapo Hyv{\"a}rinen.
\newblock Noise-contrastive estimation: A new estimation principle for
  unnormalized statistical models.
\newblock In {\em AISTATS}, 2010.

\bibitem{haochen2021provable}
Jeff~Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma.
\newblock Provable guarantees for self-supervised deep learning with spectral
  contrastive loss.
\newblock {\em NeurIPS}, 2021.

\bibitem{hardt2016equality}
Moritz Hardt, Eric Price, and Nathan Srebro.
\newblock Equality of opportunity in supervised learning.
\newblock In {\em NIPS}, 2016.

\bibitem{he2020momentum}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In {\em CVPR}, 2020.

\bibitem{hernandez2014predictive}
Jos{\'e}~Miguel Hern{\'a}ndez-Lobato, Matthew~W Hoffman, and Zoubin Ghahramani.
\newblock Predictive entropy search for efficient global optimization of
  black-box functions.
\newblock In {\em NIPS}, 2014.

\bibitem{hinton2002training}
Geoffrey~E Hinton.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock {\em Neural Computation}, 14(8):1771--1800, 2002.

\bibitem{hiriart2012fundamentals}
Jean-Baptiste Hiriart-Urruty and Claude Lemar{\'e}chal.
\newblock {\em Fundamentals of convex analysis}.
\newblock Springer Science \& Business Media, 2012.

\bibitem{hjelm2019learning}
R~Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil
  Bachman, Adam Trischler, and Yoshua Bengio.
\newblock Learning deep representations by mutual information estimation and
  maximization.
\newblock In {\em ICLR}, 2019.

\bibitem{hyvarinen2005estimation}
Aapo Hyv{\"a}rinen.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock {\em Journal of Machine Learning Research}, 6(Apr):695--709, 2005.

\bibitem{ivanova2021implicit}
Desislava Ivanova, Adam Foster, Steven Kleinegesse, Michael~U Gutmann, and
  Thomas Rainforth.
\newblock Implicit deep adaptive design: Policy-based experimental design
  without likelihoods.
\newblock {\em NeurIPS}, 2021.

\bibitem{kingma2014auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational {B}ayes.
\newblock In {\em ICLR}, 2014.

\bibitem{kleinegesse2021sequential}
Steven Kleinegesse, Christopher Drovandi, and Michael~U Gutmann.
\newblock Sequential bayesian experimental design for implicit models via
  mutual information.
\newblock {\em Bayesian Analysis}, 1(1):1--30, 2021.

\bibitem{kleinegesse2020bayesian}
Steven Kleinegesse and Michael~U Gutmann.
\newblock Bayesian experimental design for implicit models by mutual
  information neural estimation.
\newblock In {\em ICML}, 2020.

\bibitem{kleinegesse2021gradient}
Steven Kleinegesse and Michael~U Gutmann.
\newblock Gradient-based bayesian experimental design for implicit models using
  mutual information lower bounds.
\newblock {\em arXiv preprint arXiv:2105.04379}, 2021.

\bibitem{kraskov2004estimating}
Alexander Kraskov, Harald St{\"o}gbauer, and Peter Grassberger.
\newblock Estimating mutual information.
\newblock {\em Physical review E}, 69(6):066138, 2004.

\bibitem{lee2022r}
Kyungmin Lee and Jinwoo Shin.
\newblock {R\'enyiCL}: Contrastive representation learning with skew r\'enyi
  divergence.
\newblock In {\em NeurIPS}, 2022.

\bibitem{linsker1988self}
Ralph Linsker.
\newblock Self-organization in a perceptual network.
\newblock {\em Computer}, 21(3):105--117, 1988.

\bibitem{ma2018noise}
Zhuang Ma and Michael Collins.
\newblock Noise contrastive estimation and negative sampling for conditional
  models: Consistency and statistical efficiency.
\newblock {\em arXiv preprint arXiv:1809.01812}, 2018.

\bibitem{mackay2003information}
David~JC MacKay.
\newblock {\em Information theory, inference and learning algorithms}.
\newblock Cambridge university press, 2003.

\bibitem{maes1997multimodality}
Frederik Maes, Andre Collignon, Dirk Vandermeulen, Guy Marchal, and Paul
  Suetens.
\newblock Multimodality image registration by maximization of mutual
  information.
\newblock {\em IEEE transactions on Medical Imaging}, 16(2):187--198, 1997.

\bibitem{mcallester2018formal}
David McAllester and Karl Stratos.
\newblock Formal limitations on the measurement of mutual information.
\newblock {\em arXiv preprint arXiv:1811.04251}, 2018.

\bibitem{mnih2013learning}
Andriy Mnih and Koray Kavukcuoglu.
\newblock Learning word embeddings efficiently with noise-contrastive
  estimation.
\newblock In {\em NIPS}, 2013.

\bibitem{neal2001annealed}
Radford~M Neal.
\newblock Annealed importance sampling.
\newblock {\em Statistics and computing}, 11(2):125--139, 2001.

\bibitem{nguyen2010estimating}
XuanLong Nguyen, Martin~J Wainwright, and Michael~I Jordan.
\newblock Estimating divergence functionals and the likelihood ratio by convex
  risk minimization.
\newblock {\em IEEE transactions on Information Theory}, 56(11):5847--5861,
  2010.

\bibitem{nowozin2016f}
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka.
\newblock f-{GAN}: Training generative neural samplers using variational
  divergence minimization.
\newblock In {\em NIPS}, 2016.

\bibitem{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock {\em arXiv preprint arXiv:1807.03748}, 2018.

\bibitem{palmer2015predictive}
Stephanie~E Palmer, Olivier Marre, Michael~J Berry, and William Bialek.
\newblock Predictive information in a sensory population.
\newblock {\em Proceedings of the National Academy of Sciences},
  112(22):6908--6913, 2015.

\bibitem{paninski2003estimation}
Liam Paninski.
\newblock Estimation of entropy and mutual information.
\newblock {\em Neural computation}, 15(6):1191--1253, 2003.

\bibitem{perez2008estimation}
Fernando P{\'e}rez-Cruz.
\newblock Estimation of information theoretic measures for continuous random
  variables.
\newblock In {\em NIPS}, 2008.

\bibitem{pluim2003mutual}
Josien~PW Pluim, JB~Antoine Maintz, and Max~A Viergever.
\newblock Mutual-information-based registration of medical images: a survey.
\newblock {\em IEEE transactions on Medical Imaging}, 22(8):986--1004, 2003.

\bibitem{poole2019variational}
Ben Poole, Sherjil Ozair, Aaron Van Den~Oord, Alex Alemi, and George Tucker.
\newblock On variational bounds of mutual information.
\newblock In {\em ICML}, 2019.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock {\em arXiv preprint arXiv:2103.00020}, 2021.

\bibitem{rainforth2018nesting}
Tom Rainforth, Rob Cornish, Hongseok Yang, Andrew Warrington, and Frank Wood.
\newblock On nesting monte carlo estimators.
\newblock In {\em International Conference on Machine Learning}, 2018.

\bibitem{reichl2016modern}
Linda~E Reichl.
\newblock {\em A modern course in statistical physics}.
\newblock John Wiley \& Sons, 2016.

\bibitem{reshef2011detecting}
David~N Reshef, Yakir~A Reshef, Hilary~K Finucane, Sharon~R Grossman, Gilean
  McVean, Peter~J Turnbaugh, Eric~S Lander, Michael Mitzenmacher, and Pardis~C
  Sabeti.
\newblock Detecting novel associations in large data sets.
\newblock {\em science}, 334(6062):1518--1524, 2011.

\bibitem{shannon1948mathematical}
Claude~E Shannon.
\newblock A mathematical theory of communication.
\newblock {\em The Bell system technical journal}, 27(3):379--423, 1948.

\bibitem{shwartz2017opening}
Ravid Shwartz-Ziv and Naftali Tishby.
\newblock Opening the black box of deep neural networks via information.
\newblock {\em arXiv preprint arXiv:1703.00810}, 2017.

\bibitem{song2020understanding}
Jiaming Song and Stefano Ermon.
\newblock Understanding the limitations of variational mutual information
  estimators.
\newblock In {\em ICLR}, 2020.

\bibitem{suzuki2008approximating}
Taiji Suzuki, Masashi Sugiyama, Jun Sese, and Takafumi Kanamori.
\newblock Approximating mutual information by maximum likelihood density ratio
  estimation.
\newblock In {\em New challenges for feature selection in data mining and
  knowledge discovery}, 2008.

\bibitem{tao2019fenchel}
Chenyang Tao, Liqun Chen, Shuyang Dai, Junya Chen, Ke~Bai, Dong Wang, Jianfeng
  Feng, Wenlian Lu, Georgiy Bobashev, and Lawrence Carin.
\newblock On {Fenchel} mini-max learning.
\newblock In {\em NeurIPS}, 2019.

\bibitem{tian2019contrastive}
Yonglong Tian, Dilip Krishnan, and Phillip Isola.
\newblock Contrastive multiview coding.
\newblock {\em arXiv preprint arXiv:1906.05849}, 2019.

\bibitem{tishby2015deep}
Naftali Tishby and Noga Zaslavsky.
\newblock Deep learning and the information bottleneck principle.
\newblock In {\em 2015 IEEE Information Theory Workshop (ITW)}, pages 1--5.
  IEEE, 2015.

\bibitem{torkkola2003feature}
Kari Torkkola.
\newblock Feature extraction by non-parametric mutual information maximization.
\newblock {\em Journal of machine learning research}, 2003.

\bibitem{towns2014xsede}
John Towns, Timothy Cockerill, Maytal Dahan, Ian Foster, Kelly Gaither, Andrew
  Grimshaw, Victor Hazlewood, Scott Lathrop, Dave Lifka, Gregory~D Peterson,
  et~al.
\newblock Xsede: accelerating scientific discovery.
\newblock {\em Computing in science \& engineering}, 16(5):62--74, 2014.

\bibitem{tschannen2020mutual}
Michael Tschannen, Josip Djolonga, Paul~K Rubenstein, Sylvain Gelly, and Mario
  Lucic.
\newblock On mutual information maximization for representation learning.
\newblock {\em ICLR}, 2020.

\bibitem{ver2013information}
Greg Ver~Steeg and Aram Galstyan.
\newblock Information-theoretic measures of influence based on content
  dynamics.
\newblock In {\em Proceedings of the sixth ACM international conference on Web
  search and data mining}, pages 3--12, 2013.

\bibitem{wen2020mutual}
Liangjian Wen, Yiji Zhou, Lirong He, Mingyuan Zhou, and Zenglin Xu.
\newblock Mutual information gradient estimation for representation learning.
\newblock In {\em ICLR}, 2020.

\bibitem{wu2011experiments}
CF~Jeff Wu and Michael~S Hamada.
\newblock {\em Experiments: planning, analysis, and optimization}, volume 552.
\newblock John Wiley \& Sons, 2011.

\bibitem{wu2018unsupervised}
Zhirong Wu, Yuanjun Xiong, Stella~X Yu, and Dahua Lin.
\newblock Unsupervised feature learning via non-parametric instance
  discrimination.
\newblock In {\em CVPR}, 2018.

\bibitem{xu2017information}
Aolin Xu and Maxim Raginsky.
\newblock Information-theoretic analysis of generalization capability of
  learning algorithms.
\newblock In {\em NIPS}, 2017.

\bibitem{xu2020theory}
Yilun Xu, Shengjia Zhao, Jiaming Song, Russell Stewart, and Stefano Ermon.
\newblock A theory of usable information under computational constraints.
\newblock In {\em ICLR}, 2020.

\bibitem{yuan2022provable}
Zhuoning Yuan, Yuexin Wu, Zi-Hao Qiu, Xianzhi Du, Lijun Zhang, Denny Zhou, and
  Tianbao Yang.
\newblock Provable stochastic optimization for global contrastive learning:
  Small batch does not harm performance.
\newblock In {\em ICML}, 2022.

\bibitem{zhang2018mitigating}
Brian~Hu Zhang, Blake Lemoine, and Margaret Mitchell.
\newblock Mitigating unwanted biases with adversarial learning.
\newblock In {\em Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics,
  and Society}, pages 335--340, 2018.

\bibitem{zheng2018robust}
Sue Zheng, Jason Pacheco, and John Fisher.
\newblock A robust approach to sequential information theoretic planning.
\newblock In {\em ICML}, 2018.

\end{thebibliography}
