@article{agrawalAdvancesBlackBoxVI2020,
  title = {Advances in {{Black}}-{{Box VI}}: {{Normalizing Flows}}, {{Importance Weighting}}, and {{Optimization}}},
  shorttitle = {Advances in {{Black}}-{{Box VI}}},
  author = {Agrawal, Abhinav and Sheldon, Daniel and Domke, Justin},
  year = {2020},
  month = oct,
  abstract = {Recent research has seen several advances relevant to black-box VI, but the current state of automatic posterior inference is unclear. One such advance is the use of normalizing flows to define flexible posterior densities for deep latent variable models. Another direction is the integration of Monte-Carlo methods to serve two purposes; first, to obtain tighter variational objectives for optimization, and second, to define enriched variational families through sampling. However, both flows and variational Monte-Carlo methods remain relatively unexplored for black-box VI. Moreover, on a pragmatic front, there are several optimization considerations like step-size scheme, parameter initialization, and choice of gradient estimators, for which there are no clear guidance in the existing literature. In this paper, we postulate that black-box VI is best addressed through a careful combination of numerous algorithmic components. We evaluate components relating to optimization, flows, and Monte-Carlo methods on a benchmark of 30 models from the Stan model library. The combination of these algorithmic components significantly advances the state-of-the-art "out of the box" variational inference.},
  archiveprefix = {arXiv},
  eprint = {2006.10343},
  eprinttype = {arxiv},
  file = {/home/derek/Zotero/storage/H2ZBUZSV/Agrawal et al. - 2020 - Advances in Black-Box VI Normalizing Flows, Impor.pdf;/home/derek/Zotero/storage/VABPJHXM/2006.html},
  journal = {arXiv:2006.10343},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Neural Information Processing Systems (NeurIPS) 2020},
  primaryclass = {cs, stat}
}

@article{batesMetropolizedKnockoffSampling2020,
  title = {Metropolized Knockoff Sampling},
  author = {Bates, Stephen and Cand{\`e}s, Emmanuel and Janson, Lucas and Wang, Wenshuo},
  year = {2020},
  month = mar,
  pages = {1--15},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2020.1729163},
  abstract = {Model-X knockoffs is a wrapper that transforms essentially any feature importance measure into a variable selection algorithm, which discovers true effects while rigorously controlling the expected fraction of false positives. A frequently discussed challenge to apply this method is to construct knockoff variables, which are synthetic variables obeying a crucial exchangeability property with the explanatory variables under study. This article introduces techniques for knockoff generation in great generality: we provide a sequential characterization of all possible knockoff distributions, which leads to a Metropolis\textendash Hastings formulation of an exact knockoff sampler. We further show how to use conditional independence structure to speed up computations. Combining these two threads, we introduce an explicit set of sequential algorithms and empirically demonstrate their effectiveness. Our theoretical analysis proves that our algorithms achieve near-optimal computational complexity in certain cases. The techniques we develop are sufficiently rich to enable knockoff sampling in challenging models including cases where the covariates are continuous and heavy-tailed, and follow a graphical model such as the Ising model. Supplementary materials for this article are available online.},
  file = {/home/derek/Zotero/storage/94KGURUF/Bates et al. - 2020 - Metropolized Knockoff Sampling.pdf},
  journal = {Journal of the American Statistical Association},
  language = {en}
}

@article{benjaminiControlFalseDiscovery2001,
  title = {The Control of the False Discovery Rate in Multiple Testing under Dependency},
  author = {Benjamini, Yoav and Yekutieli, Daniel},
  year = {2001},
  month = aug,
  volume = {29},
  pages = {1165--1188},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1013699998},
  abstract = {Benjamini and Hochberg suggest that the false discovery rate may be the appropriate error rate to control in many applied multiple testing problems. A simple procedure was given there as an FDR controlling procedure for independent test statistics and was shown to be much more powerful than comparable procedures which control the traditional familywise error rate. We prove that this same procedure also controls the false discovery rate when the test statistics have positive regression dependency on each of the test statistics corresponding to the true null hypotheses. This condition for positive dependency is general enough to cover many problems of practical interest, including the comparisons of many treatments with a single control, multivariate normal test statistics with positive correlation matrix and multivariate \$t\$. Furthermore, the test statistics may be discrete, and the tested hypotheses composite without posing special difficulties. For all other forms of dependency, a simple conservative modification of the procedure controls the false discovery rate. Thus the range of problems for which a procedure with proven FDR control can be offered is greatly increased.},
  file = {/home/derek/Zotero/storage/5MLI9RLH/Benjamini and Yekutieli - 2001 - The control of the false discovery rate in multipl.pdf;/home/derek/Zotero/storage/D3ZU5NR8/1013699998.html},
  journal = {The Annals of Statistics},
  keywords = {47N30,62G30,62J15,comparisons with control,discrete test statistics,FDR,Hochberg’s procedure,MTP2 densities,Multiple comparisons procedures,multiple endpoints many-to-one comparisons,positive regression dependency,Simes’equality,unidimensional latent variables},
  number = {4}
}

@article{berrettConditionalPermutationTest2020,
  title = {The Conditional Permutation Test for Independence While Controlling for Confounders},
  author = {Berrett, Thomas B. and Wang, Yi and Barber, Rina Foygel and Samworth, Richard J.},
  year = {2020},
  volume = {82},
  pages = {175--197},
  issn = {1467-9868},
  doi = {10.1111/rssb.12340},
  abstract = {We propose a general new method, the conditional permutation test, for testing the conditional independence of variables X and Y given a potentially high dimensional random vector Z that may contain confounding factors. The test permutes entries of X non-uniformly, to respect the existing dependence between X and Z and thus to account for the presence of these confounders. Like the conditional randomization test of Cand\`es and co-workers in 2018, our test relies on the availability of an approximation to the distribution of X|Z\textemdash whereas their test uses this estimate to draw new X-values, for our test we use this approximation to design an appropriate non-uniform distribution on permutations of the X-values already seen in the true data. We provide an efficient Markov chain Monte Carlo sampler for the implementation of our method and establish bounds on the type I error in terms of the error in the approximation of the conditional distribution of X|Z, finding that, for the worst-case test statistic, the inflation in type I error of the conditional permutation test is no larger than that of the conditional randomization test. We validate these theoretical results with experiments on simulated data and on the Capital Bikeshare data set.},
  journal = {Journal of the Royal Statistical Society: Series B},
  number = {1}
}

@book{bishopPatternRecognitionMachine2006,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  author = {Bishop, Christopher},
  year = {2006},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. Christopher M. Bishop is Deputy Director of Microsoft Research Cambridge, and holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, a Fellow of the Royal Academy of Engineering, and a Fellow of the Royal Society of Edinburgh. His previous textbook "Neural Networks for Pattern Recognition" has been widely adopted. Coming soon: *For students, worked solutions to a subset of exercises available on a public web site (for exercises marked "www" in the text) *For instructors, worked solutions to remaining exercises from the Springer web site *Lecture slides to accompany each chapter *Data sets available for download},
  file = {/home/derek/Zotero/storage/Y27227IY/9780387310732.html},
  isbn = {978-0-387-31073-2},
  language = {en},
  series = {Information {{Science}} and {{Statistics}}}
}

@article{bleiVariationalInferenceReview2017,
  title = {Variational {{Inference}}: {{A Review}} for {{Statisticians}}},
  shorttitle = {Variational {{Inference}}},
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  year = {2017},
  month = apr,
  volume = {112},
  pages = {859--877},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2017.1285773},
  file = {/home/derek/Zotero/storage/QZY9K2KZ/Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {518}
}

@article{bogachevTriangularTransformationsMeasures2005,
  title = {Triangular Transformations of Measures},
  author = {Bogachev, V. I. and Kolesnikov, A. V. and Medvedev, K. V.},
  year = {2005},
  month = apr,
  volume = {196},
  pages = {309},
  publisher = {{IOP Publishing}},
  issn = {1064-5616},
  doi = {10.1070/SM2005v196n03ABEH000882},
  file = {/home/derek/Zotero/storage/WGGGKPE4/Bogachev et al. - 2005 - Triangular transformations of measures.pdf;/home/derek/Zotero/storage/29UNYZ23/pdf.html},
  journal = {Sbornik: Mathematics},
  language = {en},
  number = {3}
}

@article{candesPanningGoldModelX2018,
  title = {Panning for Gold: `Model-{{X}}' Knockoffs for High Dimensional Controlled Variable Selection},
  shorttitle = {Panning for Gold},
  author = {Cand{\`e}s, Emmanuel and Fan, Yingying and Janson, Lucas and Lv, Jinchi},
  year = {2018},
  volume = {80},
  pages = {551--577},
  issn = {1467-9868},
  doi = {10.1111/rssb.12265},
  abstract = {Many contemporary large-scale applications involve building interpretable models linking a large set of potential covariates to a response in a non-linear fashion, such as when the response is binary. Although this modelling problem has been extensively studied, it remains unclear how to control the fraction of false discoveries effectively even in high dimensional logistic regression, not to mention general high dimensional non-linear models. To address such a practical problem, we propose a new framework of `model-X' knockoffs, which reads from a different perspective the knockoff procedure that was originally designed for controlling the false discovery rate in linear models. Whereas the knockoffs procedure is constrained to homoscedastic linear models with n{$\geqslant$}p, the key innovation here is that model-X knockoffs provide valid inference from finite samples in settings in which the conditional distribution of the response is arbitrary and completely unknown. Furthermore, this holds no matter the number of covariates. Correct inference in such a broad setting is achieved by constructing knockoff variables probabilistically instead of geometrically. To do this, our approach requires that the covariates are random (independent and identically distributed rows) with a distribution that is known, although we provide preliminary experimental evidence that our procedure is robust to unknown or estimated distributions. To our knowledge, no other procedure solves the controlled variable selection problem in such generality but, in the restricted settings where competitors exist, we demonstrate the superior power of knockoffs through simulations. Finally, we apply our procedure to data from a case\textendash control study of Crohn's disease in the UK, making twice as many discoveries as the original analysis of the same data.},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12265},
  file = {/home/derek/Zotero/storage/GFHNPTJ3/Candès et al. - 2018 - Panning for gold ‘model-X’ knockoffs for high dim.pdf;/home/derek/Zotero/storage/T9AVEYSY/downloadSupplement.pdf;/home/derek/Zotero/storage/RAPEPB6T/rssb.html},
  journal = {Journal of the Royal Statistical Society: Series B},
  keywords = {False discovery rate,Generalized linear models,Genomewide association study,Knockoff filter,Logistic regression,Markov blanket,Testing for conditional independence in non-linear models},
  language = {en},
  number = {3}
}

@inproceedings{chenGaussianization2000,
  title = {Gaussianization},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Chen, Scott Shaobing and Gopinath, Ramesh A.},
  year = {2000},
  month = jan,
  pages = {402--408},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  abstract = {High dimensional data modeling is difficult mainly because the so-called "curse of dimensionality". We propose a technique called "Gaussianization" for high dimensional density estimation, which alleviates the curse of dimensionality by exploiting the independence structures in the data. Gaussianization is motivated from recent developments in the statistics literature: projection pursuit, independent component analysis and Gaussian mixture models with semi-tied covariances. We propose an iterative Gaussianization procedure which converges weakly: at each iteration, the data is first transformed to the least dependent coordinates and then each coordinate is marginally Gaussianized by univariate techniques. Gaussianization offers density estimation sharper than traditional kernel methods and radial basis function methods. Gaussianization can be viewed as efficient solution of nonlinear independent component analysis and high dimensional projection pursuit.},
  file = {/home/derek/Zotero/storage/XP5EG36V/Chen and Gopinath - Gaussianization.pdf},
  series = {{{NIPS}}'00}
}

@inproceedings{daiKnockoffFilterFDR2016,
  title = {The Knockoff Filter for {{FDR}} Control in Group-Sparse and Multitask Regression},
  booktitle = {Proceedings of {{The}} 33rd {{International Conference}} on {{Machine Learning}}},
  author = {Dai, Ran and Barber, Rina},
  year = {2016},
  month = jun,
  pages = {1851--1859},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {We propose the group knockoff filter, a method for false discovery rate control in a linear regression setting where the features are grouped, and we would like to select a set of relevant groups which have a nonzero effect on the response. By considering the set of true and false discoveries at the group level, this method gains power relative to sparse regression methods. We also apply our method to the multitask regression problem where multiple response variables share similar sparsity patterns across the set of possible features. Empirically, the group knockoff filter successfully controls false discoveries at the group level in both settings, with substantially more discoveries made by leveraging the group structure.},
  langid = {english},
  file = {/var/home/derek/Dropbox/Zotero/Dai_Barber_2016_The knockoff filter for FDR control in group-sparse and multitask regression.pdf}
}



@article{dinhDensityEstimationUsing2016,
  title = {Density Estimation Using {{Real NVP}}},
  author = {Dinh, Laurent and {Sohl-Dickstein}, Jascha and Bengio, Samy},
  year = {2016},
  month = nov,
  abstract = {Efficient invertible neural networks for density estimation and generation},
  file = {/home/derek/Zotero/storage/M4QWX2CF/Dinh et al. - 2016 - Density estimation using Real NVP.pdf;/home/derek/Zotero/storage/69BNWU8B/forum.html},
  language = {en}
}

@article{doerschTutorialVariationalAutoencoders2016,
  title = {Tutorial on {{Variational Autoencoders}}},
  author = {Doersch, Carl},
  year = {2016},
  month = aug,
  abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
  archiveprefix = {arXiv},
  eprint = {1606.05908},
  eprinttype = {arxiv},
  file = {/home/derek/Zotero/storage/GJQQ2DU3/Doersch - 2016 - Tutorial on Variational Autoencoders.pdf;/home/derek/Zotero/storage/9MLHJU8T/1606.html},
  journal = {arXiv:1606.05908},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{durkanNeuralSplineFlows2019,
  title = {Neural {{Spline Flows}}},
  author = {Durkan, Conor and Bekasov, Artur and Murray, Iain and Papamakarios, George},
  year = {2019},
  month = dec,
  abstract = {A normalizing flow models a complex probability density as an invertible transformation of a simple base density. Flows based on either coupling or autoregressive transforms both offer exact density evaluation and sampling, but rely on the parameterization of an easily invertible elementwise transformation, whose choice determines the flexibility of these models. Building upon recent work, we propose a fully-differentiable module based on monotonic rational-quadratic splines, which enhances the flexibility of both coupling and autoregressive transforms while retaining analytic invertibility. We demonstrate that neural spline flows improve density estimation, variational inference, and generative modeling of images.},
  archiveprefix = {arXiv},
  eprint = {1906.04032},
  eprinttype = {arxiv},
  file = {/home/derek/Zotero/storage/J28KUJ8P/Durkan et al. - 2019 - Neural Spline Flows.pdf;/home/derek/Zotero/storage/N4C3A4GC/1906.html},
  journal = {arXiv:1906.04032},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Published at the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada},
  primaryclass = {cs, stat}
}

@article{fanRANKLargeScaleInference2020,
  title = {{{RANK}}: {{Large-Scale Inference With Graphical Nonlinear Knockoffs}}},
  shorttitle = {{{RANK}}},
  author = {Fan, Yingying and Demirkaya, Emre and Li, Gaorong and Lv, Jinchi},
  year = {2020},
  month = jan,
  journal = {Journal of the American Statistical Association},
  volume = {115},
  number = {529},
  pages = {362--379},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2018.1546589},
  abstract = {Power and reproducibility are key to enabling refined scientific discoveries in contemporary big data applications with general high-dimensional nonlinear models. In this article, we provide theoretical foundations on the power and robustness for the model-X knockoffs procedure introduced recently in Cand\`es, Fan, Janson and Lv in high-dimensional setting when the covariate distribution is characterized by Gaussian graphical model. We establish that under mild regularity conditions, the power of the oracle knockoffs procedure with known covariate distribution in high-dimensional linear models is asymptotically one as sample size goes to infinity. When moving away from the ideal case, we suggest the modified model-X knockoffs method called graphical nonlinear knockoffs (RANK) to accommodate the unknown covariate distribution. We provide theoretical justifications on the robustness of our modified procedure by showing that the false discovery rate (FDR) is asymptotically controlled at the target level and the power is asymptotically one with the estimated covariate distribution. To the best of our knowledge, this is the first formal theoretical result on the power for the knockoffs procedure. Simulation results demonstrate that compared to existing approaches, our method performs competitively in both FDR control and power. A real dataset is analyzed to further assess the performance of the suggested knockoffs procedure. Supplementary materials for this article are available online.},
  keywords = {Big data,Graphical nonlinear knockoffs,High-dimensional nonlinear models,Large-scale inference and FDR,Power,Reproducibility,Robustness},
  file = {/var/home/derek/Dropbox/Zotero/Fan et al_2020_RANK.pdf}
}


@InProceedings{giminezKnockoffsMassNew2019,
  title = 	 {Knockoffs for the Mass: New Feature Importance Statistics with False Discovery Guarantees},
  author =       {Gimenez, Jaime Roquero and Ghorbani, Amirata and Zou, James},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  year = 	 {2019},
  month = 	 {16--18 Apr},
  pdf = 	 {http://proceedings.mlr.press/v89/gimenez19a/gimenez19a.pdf},
  url = 	 {http://proceedings.mlr.press/v89/gimenez19a.html},
  abstract = 	 {An important problem in machine learning and statistics is to identify features that causally affect the outcome. This is often impossible to do from purely observational data, and a natural relaxation is to identify features that are correlated with the outcome even conditioned on all other observed features. For example, we want to identify that smoking really is correlated with cancer conditioned on demographics. The knockoff procedure is a recent breakthrough in statistics that, in theory, can identify truly correlated features while guaranteeing that false discovery rate is controlled. The idea is to create synthetic data-knockoffs-that capture correlations among the features. However, there are substantial computational and practical challenges to generating and using knockoffs. This paper makes several key advances that enable knockoff application to be more efficient and powerful. We develop an efficient algorithm to generate valid knockoffs from Bayesian Networks. Then we systematically evaluate knockoff test statistics and develop new statistics with improved power. The paper combines new mathematical guarantees with systematic experiments on real and synthetic data.}
}


@book{golubMatrixComputations2013,
  title = {Matrix Computations},
  author = {Golub, Gene H. (Gene Howard) and Van Loan, Charles F. and Golub, Gene H. and Van Loan, Charles F.},
  year = {2013},
  edition = {4th edition.},
  publisher = {{Johns Hopkins University Press}},
  address = {{Baltimore}},
  file = {/home/derek/Zotero/storage/86YS4RNH/012347518.html},
  series = {Johns {{Hopkins}} Studies in the Mathematical Sciences}
}

@article{friedmanSparseInverseCovariance2008,
  title = {Sparse Inverse Covariance Estimation with the Graphical Lasso},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  year = {2008},
  month = jul,
  journal = {Biostatistics},
  volume = {9},
  number = {3},
  pages = {432--441},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxm045},
  abstract = {We consider the problem of estimating sparse graphs by a lasso penalty applied to the inverse covariance matrix. Using a coordinate descent procedure for the lasso, we develop a simple algorithm\textemdash the graphical lasso\textemdash that is remarkably fast: It solves a 1000-node problem ({$\sim$}500000 parameters) in at most a minute and is 30\textendash 4000 times faster than competing methods. It also provides a conceptual link between the exact problem and the approximation suggested by Meinshausen and B\"uhlmann (2006). We illustrate the method on some cell-signaling data from proteomics.},
  file = {/var/home/derek/Dropbox/Zotero/Friedman et al_2008_Sparse inverse covariance estimation with the graphical lasso.pdf;/var/home/derek/Zotero/storage/C8CHK4B3/224260.html}
}


@article{katsevichMultilayerKnockoffFilter2019,
  title = {Multilayer Knockoff Filter: {{Controlled}} Variable Selection at Multiple Resolutions},
  shorttitle = {Multilayer Knockoff Filter},
  author = {Katsevich, Eugene and Sabatti, Chiara},
  year = {2019},
  month = mar,
  journal = {The Annals of Applied Statistics},
  volume = {13},
  number = {1},
  pages = {1--33},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/18-AOAS1185},
  abstract = {We tackle the problem of selecting from among a large number of variables those that are ``important'' for an outcome. We consider situations where groups of variables are also of interest. For example, each variable might be a genetic polymorphism, and we might want to study how a trait depends on variability in genes, segments of DNA that typically contain multiple such polymorphisms. In this context, to discover that a variable is relevant for the outcome implies discovering that the larger entity it represents is also important. To guarantee meaningful results with high chance of replicability, we suggest controlling the rate of false discoveries for findings at the level of individual variables and at the level of groups. Building on the knockoff construction of Barber and Cand\`es [Ann. Statist. 43 (2015) 2055\textendash 2085] and the multilayer testing framework of Barber and Ramdas [J. Roy. Statist. Soc. Ser. B 79 (2017) 1247\textendash 1268], we introduce the multilayer knockoff filter (MKF). We prove that MKF simultaneously controls the FDR at each resolution and use simulations to show that it incurs little power loss compared to methods that provide guarantees only for the discoveries of individual variables. We apply MKF to analyze a genetic dataset and find that it successfully reduces the number of false gene discoveries without a significant reduction in power.},
  keywords = {$p$-filter,false discovery rate (FDR),genomewide association study (GWAS),group FDR,knockoff filter,multiresolution,Variable selection},
  file = {/var/home/derek/Dropbox/Zotero/Katsevich_Sabatti_2019_Multilayer knockoff filter.pdf;/var/home/derek/Zotero/storage/5RGEMRWF/18-AOAS1185.html}
}



@article{kellerSelfNormalizingFlows2020,
  title = {Self {{Normalizing Flows}}},
  author = {Keller, T. Anderson and Peters, Jorn W. T. and Jaini, Priyank and Hoogeboom, Emiel and Forr{\'e}, Patrick and Welling, Max},
  year = {2020},
  month = nov,
  abstract = {Efficient gradient computation of the Jacobian determinant term is a core problem of the normalizing flow framework. Thus, most proposed flow models either restrict to a function class with easy evaluation of the Jacobian determinant, or an efficient estimator thereof. However, these restrictions limit the performance of such density models, frequently requiring significant depth to reach desired performance levels. In this work, we propose Self Normalizing Flows, a flexible framework for training normalizing flows by replacing expensive terms in the gradient by learned approximate inverses at each layer. This reduces the computational complexity of each layer's exact update from \$\textbackslash mathcal\{O\}(D\^3)\$ to \$\textbackslash mathcal\{O\}(D\^2)\$, allowing for the training of flow architectures which were otherwise computationally infeasible, while also providing efficient sampling. We show experimentally that such models are remarkably stable and optimize to similar data likelihood values as their exact gradient counterparts, while surpassing the performance of their functionally constrained counterparts.},
  archiveprefix = {arXiv},
  eprint = {2011.07248},
  eprinttype = {arxiv},
  file = {/home/derek/Zotero/storage/F7BZV3JG/Keller et al. - 2020 - Self Normalizing Flows.pdf;/home/derek/Zotero/storage/H7FFBPNU/2011.html},
  journal = {arXiv:2011.07248},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{kimAttentiveNeuralProcesses2019,
  title = {Attentive {{Neural Processes}}},
  author = {Kim, Hyunjik and Mnih, Andriy and Schwarz, Jonathan and Garnelo, Marta and Eslami, Ali and Rosenbaum, Dan and Vinyals, Oriol and Teh, Yee Whye},
  year = {2019},
  month = jul,
  abstract = {Neural Processes (NPs) (Garnelo et al 2018a;b) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions. Each function models the distribution of the output given an input, conditioned on the context. NPs have the benefit of fitting observed data efficiently with linear complexity in the number of context input-output pairs, and can learn a wide family of conditional distributions; they learn predictive distributions conditioned on context sets of arbitrary size. Nonetheless, we show that NPs suffer a fundamental drawback of underfitting, giving inaccurate predictions at the inputs of the observed data they condition on. We address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction. We show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled.},
  archiveprefix = {arXiv},
  eprint = {1901.05761},
  eprinttype = {arxiv},
  file = {/home/derek/Zotero/storage/UGYGG26E/Kim et al. - 2019 - Attentive Neural Processes.pdf;/home/derek/Zotero/storage/ZBVBUPK4/1901.html},
  journal = {arXiv:1901.05761},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@inproceedings{kingmaAdamMethodStochastic2017,
  title = {Adam: A Method for Stochastic Optimization},
  author = {Kingma, Diederik and Ba, Jimmy},
  year = {2017},
  booktitle = {International Conference for Learning Representations},
  keywords = {Computer Science - Machine Learning},
  primaryclass = {cs}
}

@article{kingmaAutoEncodingVariationalBayes2014a,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik and Welling, Max},
  year = {2014},
  month = may,
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  file = {/home/derek/Zotero/storage/Q4SU7XEB/Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf;/home/derek/Zotero/storage/KXZ3FG7V/1312.html},
  journal = {arXiv:1312.6114},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{kingmaGlowGenerativeFlow,
  title = {Glow: {{Generative Flow}} with {{Invertible}} 1\texttimes 1 {{Convolutions}}},
  author = {Kingma, Diederik and Dhariwal, Prafulla},
  pages = {10},
  abstract = {Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1 \texttimes{} 1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a flow-based generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow.},
  file = {/home/derek/Zotero/storage/J9ZAW2UF/Kingma and Dhariwal - Glow Generative Flow with Invertible 1×1 Convolut.pdf},
  language = {en}
}

@article{kingmaImprovingVariationalInference2016,
  title = {Improving {{Variational Inference}} with {{Inverse Autoregressive Flow}}},
  author = {Kingma, Diederik P. and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
  year = {2016},
  month = jun,
  abstract = {The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.},
  file = {/home/derek/Zotero/storage/QKH2ZTFZ/Kingma et al. - 2016 - Improving Variational Inference with Inverse Autor.pdf;/home/derek/Zotero/storage/9VCUEN4J/1606.html},
  language = {en}
}

@article{kobyzevNormalizingFlowsIntroduction2020,
  title={Normalizing flows: An introduction and review of current methods},
  author={Kobyzev, Ivan and Prince, Simon and Brubaker, Marcus},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2020},
  publisher={IEEE}
}

@article{kolouriGeneralizedSlicedWasserstein,
  title = {Generalized {{Sliced Wasserstein Distances}}},
  author = {Kolouri, Soheil and Nadjahi, Kimia and Simsekli, Umut and Badeau, Roland and Rohde, Gustavo},
  pages = {12},
  abstract = {The Wasserstein distance and its variations, e.g., the sliced-Wasserstein (SW) distance, have recently drawn attention from the machine learning community. The SW distance, specifically, was shown to have similar properties to the Wasserstein distance, while being much simpler to compute, and is therefore used in various applications including generative modeling and general supervised/unsupervised learning. In this paper, we first clarify the mathematical connection between the SW distance and the Radon transform. We then utilize the generalized Radon transform to define a new family of distances for probability measures, which we call generalized sliced-Wasserstein (GSW) distances. We further show that, similar to the SW distance, the GSW distance can be extended to a maximum GSW (max-GSW) distance. We then provide the conditions under which GSW and max-GSW distances are indeed proper metrics. Finally, we compare the numerical performance of the proposed distances on the generative modeling task of SW flows and report favorable results.},
  file = {/home/derek/Zotero/storage/I93JRLHZ/Kolouri et al. - Generalized Sliced Wasserstein Distances.pdf},
  language = {en}
}

@article{kucukelbirAutomaticDifferentiationVariational,
  title = {Automatic {{Differentiation Variational Inference}}},
  author = {Kucukelbir, Alp},
  pages = {45},
  abstract = {Probabilistic modeling is iterative. A scientist posits a simple model, fits it to her data, refines it according to her analysis, and repeats. However, fitting complex models to large data is a bottleneck in this process. Deriving algorithms for new models can be both mathematically and computationally challenging, which makes it difficult to efficiently cycle through the steps. To this end, we develop automatic differentiation variational inference ( ). Using our method, the scientist only provides a probabilistic model and a dataset, nothing else. automatically derives an efficient variational inference algorithm, freeing the scientist to refine and explore many models.},
  file = {/home/derek/Zotero/storage/FCDW7E4E/Kucukelbir - Automatic Diﬀerentiation Variational Inference.pdf},
  language = {en}
}

@article{leeSetTransformerFramework2019,
  title = {Set {{Transformer}}: {{A Framework}} for {{Attention}}-Based {{Permutation}}-{{Invariant Neural Networks}}},
  shorttitle = {Set {{Transformer}}},
  author = {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam R. and Choi, Seungjin and Teh, Yee Whye},
  year = {2019},
  month = may,
  abstract = {Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.},
  archiveprefix = {arXiv},
  eprint = {1810.00825},
  eprinttype = {arxiv},
  file = {/home/derek/Zotero/storage/ZKK8HBC2/Lee et al. - 2019 - Set Transformer A Framework for Attention-based P.pdf;/home/derek/Zotero/storage/TZHT9N2B/1810.html},
  journal = {arXiv:1810.00825},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: ICML 2019},
  primaryclass = {cs, stat}
}

@article{liuAutoEncodingKnockoffGenerator2018,
  title = {Auto-{{Encoding Knockoff Generator}} for {{FDR Controlled Variable Selection}}},
  author = {Liu, Ying and Zheng, Cheng},
  year = {2018},
  month = sep,
  abstract = {A new statistical procedure (Model-X \textbackslash cite\{candes2018\}) has provided a way to identify important factors using any supervised learning method controlling for FDR. This line of research has shown great potential to expand the horizon of machine learning methods beyond the task of prediction, to serve the broader needs in scientific researches for interpretable findings. However, the lack of a practical and flexible method to generate knockoffs remains the major obstacle for wide application of Model-X procedure. This paper fills in the gap by proposing a model-free knockoff generator which approximates the correlation structure between features through latent variable representation. We demonstrate our proposed method can achieve FDR control and better power than two existing methods in various simulated settings and a real data example for finding mutations associated with drug resistance in HIV-1 patients.},
  archiveprefix = {arXiv},
  eprint = {1809.10765},
  eprinttype = {arxiv},
  file = {/home/derek/Zotero/storage/VDUTM4H9/Liu and Zheng - 2018 - Auto-Encoding Knockoff Generator for FDR Controlle.pdf;/home/derek/Zotero/storage/LH66BJ62/1809.html},
  journal = {arXiv:1809.10765},
  keywords = {Statistics - Methodology},
  primaryclass = {stat}
}

@article{liuFastPowerfulConditional2020,
  title = {Fast and {{Powerful Conditional Randomization Testing}} via {{Distillation}}},
  author = {Liu, Molei and Katsevich, Eugene and Janson, Lucas and Ramdas, Aaditya},
  year = {2020},
  month = jul,
  abstract = {We consider the problem of conditional independence testing, where given a response \$Y\$ and covariates \$(X,Z)\$, we test the null hypothesis that \$Y \textbackslash perp X \textbackslash mid Z\$. The conditional randomization test (CRT) was recently proposed as a way to use distributional information about \$X\textbackslash mid Z\$ to exactly (non-asymptotically) control Type-I error using \$any\$ test statistic in \$any\$ dimensionality without assuming anything about \$Y\textbackslash mid (X,Z)\$. This flexibility in principle allows one to derive powerful test statistics from complex state-of-the-art machine learning algorithms while maintaining statistical validity. Yet the direct use of such advanced test statistics in the CRT is prohibitively computationally expensive, especially with multiple testing, due to the CRT's requirement to recompute the test statistic many times on resampled data. We propose the \$distilled\textasciitilde CRT\$, a novel approach to using state-of-the-art machine learning algorithms in the CRT while drastically reducing the number of times those algorithms need to be run, thereby taking advantage of their power and the CRT's statistical guarantees without suffering the usual computational expense. In addition to distillation, we propose a number of other tricks like screening and recycling computations to further speed up the CRT without sacrificing its high power and exact validity. Indeed, we show in simulations that all our proposals combined lead to a test that has similar power to the CRT but requires orders of magnitude less computation, making it a practical tool even for large data sets. We demonstrate these benefits on a breast cancer dataset by identifying biomarkers related to cancer stage.},
  archiveprefix = {arXiv},
  eprint = {2006.03980},
  eprinttype = {arxiv},
  file = {/home/derek/Zotero/storage/N3BQ2AWC/Liu et al. - 2020 - Fast and Powerful Conditional Randomization Testin.pdf;/home/derek/Zotero/storage/CIPM94UF/2006.html},
  journal = {arXiv:2006.03980},
  keywords = {Statistics - Methodology},
  primaryclass = {stat}
}

@article{lopezDeepGenerativeModeling2018,
  title = {Deep Generative Modeling for Single-Cell Transcriptomics},
  author = {Lopez, Romain and Regier, Jeffrey and Cole, Michael B. and Jordan, Michael I. and Yosef, Nir},
  year = {2018},
  volume = {15},
  pages = {1053--1058},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-018-0229-2},
  file = {/home/derek/Zotero/storage/CXAKFAXE/Lopez et al. - 2018 - Deep generative modeling for single-cell transcrip.pdf;/home/derek/Zotero/storage/YISU889U/41592_2018_229_MOESM1_ESM.pdf},
  journal = {Nature Methods},
  language = {en},
  number = {12}
}

@InProceedings{mengGaussianizationFlows2020b, 
title = {Gaussianization Flows}, 
author = {Meng, Chenlin and Song, Yang and Song, Jiaming and Ermon, Stefano}, booktitle = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics}, 
year = {2020}, 
month = {26--28 Aug}, 
pdf = {http://proceedings.mlr.press/v108/meng20b/meng20b.pdf}, 
url = { http://proceedings.mlr.press/v108/meng20b.html }, 
abstract = {Iterative Gaussianization is a fixed-point iteration procedure that allows one to transform a continuous distribution to Gaussian distribution. Based on iterative Gaussianization, we propose a new type of normalizing flow models that grants both efficient computation of likelihoods and efficient inversion for sample generation. We demonstrate that this new family of flow models, named as Gaussianization flows, are universal approximators for continuous probability distributions under some regularity conditions. This guaranteed expressivity, enabling them to capture multimodal target distributions better without compromising the efficiency in sample generation. Experimentally, we show that Gaussianization flows achieve better or comparable performance on several tabular datasets, compared to other efficiently invertible flow models such as Real NVP, Glow and FFJORD. In particular, Gaussianization flows are easier to initialize, demonstrate better robustness with respect to different transformations of the training data, and generalize better on small training sets.} } 


@article{nielsenSurVAEFlowsSurjections2020,
  title = {{{SurVAE Flows}}: {{Surjections}} to {{Bridge}} the {{Gap}} between {{VAEs}} and {{Flows}}},
  shorttitle = {{{SurVAE Flows}}},
  author = {Nielsen, Didrik and Jaini, Priyank and Hoogeboom, Emiel and Winther, Ole and Welling, Max},
  year = {2020},
  month = jul,
  abstract = {Normalizing flows and variational autoencoders are powerful generative models that can represent complicated density functions. However, they both impose constraints on the models: Normalizing flows use bijective transformations to model densities whereas VAEs learn stochastic transformations that are non-invertible and thus typically do not provide tractable estimates of the marginal likelihood. In this paper, we introduce SurVAE Flows: A modular framework of composable transformations that encompasses VAEs and normalizing flows. SurVAE Flows bridge the gap between normalizing flows and VAEs with surjective transformations, wherein the transformations are deterministic in one direction -- thereby allowing exact likelihood computation, and stochastic in the reverse direction -- hence providing a lower bound on the corresponding likelihood. We show that several recently proposed methods, including dequantization and augmented normalizing flows, can be expressed as SurVAE Flows. Finally, we introduce common operations such as the max value, the absolute value, sorting and stochastic permutation as composable layers in SurVAE Flows.},
  archiveprefix = {arXiv},
  eprint = {2007.02731},
  eprinttype = {arxiv},
  file = {/home/derek/Zotero/storage/E5VACHSS/Nielsen et al. - 2020 - SurVAE Flows Surjections to Bridge the Gap betwee.pdf;/home/derek/Zotero/storage/XSKHRKK6/2007.html},
  journal = {arXiv:2007.02731},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@inproceedings{papamakariosMaskedAutoregressiveFlow2017,
 author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 title = {Masked Autoregressive Flow for Density Estimation},
 url = {https://proceedings.neurips.cc/paper/2017/file/6c1da886822c67822bcf3679d04369fa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{papamakariosNormalizingFlowsProbabilistic2019a,
  title={Normalizing Flows for Probabilistic Modeling and Inference},
  author={Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={57},
  pages={1--64},
  year={2021}
}

@article{rezendeVariationalInferenceNormalizing2016,
  title = {Variational {{Inference}} with {{Normalizing Flows}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  year = {2016},
  month = jun,
  abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
  archiveprefix = {arXiv},
  eprint = {1505.05770},
  eprinttype = {arxiv},
  file = {/home/derek/Zotero/storage/DUBN9YCK/Rezende and Mohamed - 2016 - Variational Inference with Normalizing Flows.pdf;/home/derek/Zotero/storage/CTDPUQWV/1505.html},
  journal = {arXiv:1505.05770},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  note = {Comment: Proceedings of the 32nd International Conference on Machine Learning},
  primaryclass = {cs, stat}
}

@article{roederStickingLandingSimple2017,
  title = {Sticking the {{Landing}}: {{Simple}}, {{Lower}}-{{Variance Gradient Estimators}} for {{Variational Inference}}},
  shorttitle = {Sticking the {{Landing}}},
  author = {Roeder, Geoffrey and Wu, Yuhuai and Duvenaud, David K.},
  year = {2017},
  volume = {30},
  pages = {6925--6934},
  file = {/home/derek/Zotero/storage/DF4NSCLD/Roeder et al. - 2017 - Sticking the Landing Simple, Lower-Variance Gradi.pdf;/home/derek/Zotero/storage/399GVAQQ/e91068fff3d7fa1594dfdf3b4308433a-Abstract.html},
  journal = {Advances in Neural Information Processing Systems},
  language = {en}
}

@article{romanoDeepKnockoffs2018,
author = { Yaniv   Romano  and  Matteo   Sesia  and  Emmanuel   Candès },
title = {Deep Knockoffs},
journal = {Journal of the American Statistical Association},
volume = {115},
number = {532},
pages = {1861-1872},
year  = {2020},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2019.1660174},

URL = { 
        https://doi.org/10.1080/01621459.2019.1660174
    
},
eprint = { 
        https://doi.org/10.1080/01621459.2019.1660174
    
}

}


@article{skrljFeatureImportanceEstimation2020,
  title = {Feature {{Importance Estimation}} with {{Self}}-{{Attention Networks}}},
  author = {{\v S}krlj, Bla{\v z} and D{\v z}eroski, Sa{\v s}o and Lavra{\v c}, Nada and Petkovi{\v c}, Matej},
  year = {2020},
  month = feb,
  abstract = {Black-box neural network models are widely used in industry and science, yet are hard to understand and interpret. Recently, the attention mechanism was introduced, offering insights into the inner workings of neural language models. This paper explores the use of attention-based neural networks mechanism for estimating feature importance, as means for explaining the models learned from propositional (tabular) data. Feature importance estimates, assessed by the proposed Self-Attention Network (SAN) architecture, are compared with the established ReliefF, Mutual Information and Random Forest-based estimates, which are widely used in practice for model interpretation. For the first time we conduct scale-free comparisons of feature importance estimates across algorithms on ten real and synthetic data sets to study the similarities and differences of the resulting feature importance estimates, showing that SANs identify similar high-ranked features as the other methods. We demonstrate that SANs identify feature interactions which in some cases yield better predictive performance than the baselines, suggesting that attention extends beyond interactions of just a few key features and detects larger feature subsets relevant for the considered learning task.},
  archiveprefix = {arXiv},
  eprint = {2002.04464},
  eprinttype = {arxiv},
  file = {/home/derek/Zotero/storage/A25M9GCJ/Škrlj et al. - 2020 - Feature Importance Estimation with Self-Attention .pdf;/home/derek/Zotero/storage/6P7Q8BQ9/2002.html},
  journal = {arXiv:2002.04464},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Accepted for publication in ECAI 2020},
  primaryclass = {cs, stat}
}

@inproceedings{sudarshanDeepDirectLikelihood2020,
 author = {Sudarshan, Mukund and Tansey, Wesley and Ranganath, Rajesh},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Deep Direct Likelihood Knockoffs},
 volume = {33},
 year = {2020}
}

@article{tanseyHoldoutRandomizationTest2019,
  title = {The Holdout Randomization Test for Feature Selection in Black Box Models},
  author = {Tansey, Wesley and Veitch, Victor and Zhang, Haoran and Rabadan, Raul and Blei, David M.},
  year = {2021},
  journal = {Journal of Computational and Graphical Statistics},
  note = {[In press; available on arXiv]}
}

@inproceedings{tranDiscreteFlowsInvertible2019,
 author = {Tran, Dustin and Vafa, Keyon and Agrawal, Kumar and Dinh, Laurent and Poole, Ben},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Discrete Flows: Invertible Generative Models of Discrete Data},
 volume = {32},
 year = {2019}
}


@article{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  file = {/home/derek/Zotero/storage/CWYCEG49/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/home/derek/Zotero/storage/544FADYC/1706.html},
  journal = {arXiv:1706.03762},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  primaryclass = {cs}
}

@article{wangPowerAnalysisConditional2020,
  title = {A {{Power Analysis}} of the {{Conditional Randomization Test}} and {{Knockoffs}}},
  author = {Wang, Wenshuo and Janson, Lucas},
  year = {2020},
  month = oct,
  abstract = {In many scientific problems, researchers try to relate a response variable \$Y\$ to a set of potential explanatory variables \$X = (X\_1,\textbackslash dots,X\_p)\$, and start by trying to identify variables that contribute to this relationship. In statistical terms, this goal can be posed as trying to identify \$X\_j\$'s upon which \$Y\$ is conditionally dependent. Sometimes it is of value to simultaneously test for each \$j\$, which is more commonly known as variable selection. The conditional randomization test (CRT) and model-X knockoffs are two recently proposed methods that respectively perform conditional independence testing and variable selection by, for each \$X\_j\$, computing any test statistic on the data and assessing that test statistic's significance by comparing it to test statistics computed on synthetic variables generated using knowledge of \$X\$'s distribution. Our main contribution is to analyze their power in a high-dimensional linear model where the ratio of the dimension \$p\$ and the sample size \$n\$ converge to a positive constant. We give explicit expressions of the asymptotic power of the CRT, variable selection with CRT \$p\$-values, and model-X knockoffs, each with a test statistic based on either the marginal covariance, the least squares coefficient, or the lasso. One useful application of our analysis is the direct theoretical comparison of the asymptotic powers of variable selection with CRT \$p\$-values and model-X knockoffs; in the instances with independent covariates that we consider, the CRT provably dominates knockoffs. We also analyze the power gain from using unlabeled data in the CRT when limited knowledge of \$X\$'s distribution is available, and the power of the CRT when samples are collected retrospectively.},
  archiveprefix = {arXiv},
  eprint = {2010.02304},
  eprinttype = {arxiv},
  file = {/home/derek/Zotero/storage/I9QEY3QD/Wang and Janson - 2020 - A Power Analysis of the Conditional Randomization .pdf;/home/derek/Zotero/storage/8E6QCI3Y/2010.html},
  journal = {arXiv:2010.02304},
  keywords = {Mathematics - Statistics Theory},
  primaryclass = {math, stat}
}

@article{wenDeepGenerativeQuantileCopula2019,
  title = {Deep {{Generative Quantile}}-{{Copula Models}} for {{Probabilistic Forecasting}}},
  author = {Wen, Ruofeng and Torkkola, Kari},
  year = {2019},
  month = jul,
  abstract = {We introduce a new category of multivariate conditional generative models and demonstrate its performance and versatility in probabilistic time series forecasting and simulation. Specifically, the output of quantile regression networks is expanded from a set of fixed quantiles to the whole Quantile Function by a univariate mapping from a latent uniform distribution to the target distribution. Then the multivariate case is solved by learning such quantile functions for each dimension's marginal distribution, followed by estimating a conditional Copula to associate these latent uniform random variables. The quantile functions and copula, together defining the joint predictive distribution, can be parameterized by a single implicit generative Deep Neural Network.},
  archiveprefix = {arXiv},
  eprint = {1907.10697},
  eprinttype = {arxiv},
  file = {/home/derek/Zotero/storage/MKDXER3T/Wen and Torkkola - 2019 - Deep Generative Quantile-Copula Models for Probabi.pdf;/home/derek/Zotero/storage/33N92W8J/1907.html},
  journal = {arXiv:1907.10697},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Published at the 36th International Conference on Machine Learning (ICML2019), Time Series Workshop, Long Beach, California, 2019},
  primaryclass = {cs, stat}
}

@article{zouSparsePrincipalComponent2006,
  title = {Sparse {{Principal Component Analysis}}},
  author = {Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
  year = {2006},
  month = jun,
  volume = {15},
  pages = {265--286},
  issn = {1061-8600, 1537-2715},
  doi = {10.1198/106186006X113430},
  file = {/home/derek/Zotero/storage/M72TKF6L/Zou et al. - 2006 - Sparse Principal Component Analysis.pdf},
  journal = {Journal of Computational and Graphical Statistics},
  language = {en},
  number = {2}
}

@inproceedings{
jordon2018knockoffgan,
title={Knockoff{GAN}: Generating Knockoffs for Feature Selection using Generative Adversarial Networks},
author={James Jordon and Jinsung Yoon and Mihaela van der Schaar},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=ByeZ5jC5YQ},
}

@article{Benjamini1995,
  doi = {10.1111/j.2517-6161.1995.tb02031.x},
  url = {https://doi.org/10.1111/j.2517-6161.1995.tb02031.x},
  year = {1995},
  publisher = {Wiley},
  volume = {57},
  number = {1},
  pages = {289--300},
  author = {Yoav Benjamini and Yosef Hochberg},
  title = {Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing},
  journal = {Journal of the Royal Statistical Society: Series B}
}


@inproceedings{huangNeuralAutoregressiveFlows2018a,
  title = {Neural Autoregressive Flows},
  booktitle = {International Conference on Machine Learning},
  author = {Huang, Chin-Wei and Krueger, David and Lacoste, Alexandre and Courville, Aaron},
  year = {2018},
  month = jul,
  issn = {2640-3498},
  abstract = {Normalizing flows and autoregressive models have been successfully combined to produce state-of-the-art results in density estimation, via Masked Autoregressive Flows (MAF) (Papamakarios et al., 20...},
  file = {/Users/dereklh/Zotero/storage/2QVVFYRZ/Huang et al. - 2018 - Neural Autoregressive Flows.pdf;/Users/dereklh/Zotero/storage/M9EKX8RC/huang18d.html},
  language = {en}
}

@article{robertsGeneralStateSpace2004,
  title = {General State Space {{Markov}} Chains and {{MCMC}} Algorithms},
  author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
  year = {2004},
  month = jan,
  volume = {1},
  pages = {20--71},
  publisher = {{Institute of Mathematical Statistics and Bernoulli Society}},
  issn = {1549-5787, 1549-5787},
  doi = {10.1214/154957804100000024},
  abstract = {This paper surveys various results about Markov chains on general (non-countable) state spaces. It begins with an introduction to Markov chain Monte Carlo (MCMC) algorithms, which provide the motivation and context for the theory which follows. Then, sufficient conditions for geometric and uniform ergodicity are presented, along with quantitative bounds on the rate of convergence to stationarity. Many of these results are proved using direct coupling constructions based on minorisation and drift conditions. Necessary and sufficient conditions for Central Limit Theorems (CLTs) are also presented, in some cases proved via the Poisson Equation or direct regeneration constructions. Finally, optimal scaling and weak convergence results for Metropolis-Hastings algorithms are discussed. None of the results presented is new, though many of the proofs are. We also describe some Open Problems.},
  file = {/Users/dereklh/Zotero/storage/2DFPYMA3/Roberts and Rosenthal - 2004 - General state space Markov chains and MCMC algorit.pdf;/Users/dereklh/Zotero/storage/MZ8HZFWW/154957804100000024.html},
  journal = {Probability Surveys},
  number = {none}
}


@misc{arjovsky2017principled,
      title={Towards Principled Methods for Training Generative Adversarial Networks}, 
      author={Martin Arjovsky and Léon Bottou},
      year={2017},
      eprint={1701.04862},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{liu2020flows,
      title={Flows Succeed Where {GANs} Fail: Lessons from Low-Dimensional Data}, 
      author={Tianci Liu and Jeffrey Regier},
      year={2020},
      month={June},
      journal={arXiv:2006.10175}
}


@misc{dosovitskiy2016generating,
      title={Generating Images with Perceptual Similarity Metrics based on Deep Networks}, 
      author={Alexey Dosovitskiy and Thomas Brox},
      year={2016},
      eprint={1602.02644},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{li2015generative,
author = {Li, Yujia and Swersky, Kevin and Zemel, Richard},
title = {Generative Moment Matching Networks},
year = {2015},
publisher = {JMLR.org},
abstract = {We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer perceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {1718–1727},
numpages = {10},
location = {Lille, France},
series = {ICML'15}
}

@misc{dua2019uci,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }



@article{tibshirani96regression,
  added-at = {2009-04-04T18:01:35.000+0200},
  author = {Tibshirani, Robert},
  biburl = {https://www.bibsonomy.org/bibtex/290e648276aa6cd3c601e7c0a54366233/dieudonnew},
  interhash = {334927808d42a9a6bf8eae717fed41b3},
  intrahash = {90e648276aa6cd3c601e7c0a54366233},
  journal = {Journal of the Royal Statistical Society: Series B},
  keywords = {imported},
  pages = {267-288},
  timestamp = {2009-04-04T18:01:35.000+0200},
  title = {Regression Shrinkage and Selection via the {Lasso}},
  volume = 58,
  year = 1996
}


@article{breiman2001random,
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to },
  added-at = {2015-04-15T08:57:31.000+0200},
  author = {Breiman, Leo},
  biburl = {https://www.bibsonomy.org/bibtex/2b8187107bf870043f2f93669958858f1/kdepublication},
  description = {Random Forests - Springer},
  doi = {10.1023/A:1010933404324},
  interhash = {4450d2e56555e7cb8f3817578e1dd4da},
  intrahash = {b8187107bf870043f2f93669958858f1},
  issn = {0885-6125},
  journal = {Machine Learning},
  keywords = {classification classifier dblp decision ensemble final forest forests imported kde learning machine ml mykopie origin random text-detection the_youtube_social_network thema:exploiting_place_features_in_link_prediction_on_location-based_social_networks trees uw_ss14_web2.0},
  language = {English},
  number = 1,
  pages = {5-32},
  publisher = {Kluwer Academic Publishers},
  timestamp = {2015-04-24T14:37:24.000+0200},
  title = {Random Forests},
  url = {http://dx.doi.org/10.1023/A%3A1010933404324},
  volume = 45,
  year = 2001
}


@techreport{bishop94MDN,
title = "Mixture density networks",
abstract = "Minimization of a sum-of-squares or cross-entropy error function leads to network outputs which approximate the conditional averages of the target data, conditioned on the input vector. For classifications problems, with a suitably chosen target coding scheme, these averages represent the posterior probabilities of class membership, and so can be regarded as optimal. For problems involving the prediction of continuous variables, however, the conditional averages provide only a very limited description of the properties of the target variables. This is particularly true for problems in which the mapping to be learned is multi-valued, as often arises in the solution of inverse problems, since the average of several correct target values is not necessarily itself a correct value. In order to obtain a complete description of the data, for the purposes of predicting the outputs corresponding to new input vectors, we must model the conditional probability distribution of the target data, again conditioned on the input vector. In this paper we introduce a new class of network models obtained by combining a conventional neural network with a mixture density model. The complete system is called a Mixture Density Network, and can in principle represent arbitrary conditional probability distributions in the same way that a conventional neural network can represent arbitrary functions. We demonstrate the effectiveness of Mixture Density Networks using both a toy problem and a problem involving robot inverse kinematics.",
keywords = "NCRG sum-of-squares cross-entropy error function classifications problems coding scheme conditional probability distribution network models neural network mixture density model Mixture Density Network inverse kinematics",
author = "Bishop, {Christopher M.}",
year = "1994",
language = "English",
isbn = "NCRG/94/004",
publisher = "Aston University",
institution = "Aston University",

}


@article{Tam2019,
  doi = {10.1038/s41576-019-0127-1},
  url = {https://doi.org/10.1038/s41576-019-0127-1},
  year = {2019},
  publisher = {Springer Science and Business Media {LLC}},
  volume = {20},
  number = {8},
  pages = {467--484},
  author = {Vivian Tam and Nikunj Patel and Michelle Turcotte and Yohan Boss{\'{e}} and Guillaume Par{\'{e}} and David Meyre},
  title = {Benefits and limitations of genome-wide association studies},
  journal = {Nature Reviews Genetics}
}

@Manual{soynam,
    title = {SoyNAM: Soybean Nested Association Mapping Dataset},
    author = {Alencar Xavier and William Beavis and James Specht and Brian Diers and Rouf Mian and Reka Howard and George Graef and Randall Nelson and William Schapaugh and Dechun Wang and Grover Shannon and Leah McHale and Perry Cregan and Qijian Song and Miguel Lopez and William Muir and Katy Rainey.},
    year = {2019},
    note = {R package version 1.6},
    url = {https://CRAN.R-project.org/package=SoyNAM},
  }


@Article{qqman,
    title = {qqman: an {R} package for visualizing {GWAS} results using {Q-Q} and {Manhattan} plots},
    author = {Stephen Turner},
    journal = {The Journal of Open Source Software},
    year = {2018},
    doi = {10.21105/joss.00731},
  }

@article{liu_phenotype_2019, title = {Phenotype Prediction and Genome-Wide Association Study Using Deep Convolutional Neural Network of Soybean}, volume = {10}, issn = {1664-8021}, url = {https://www.frontiersin.org/articles/10.3389/fgene.2019.01091/full}, doi = {10.3389/fgene.2019.01091}, abstract = {Genomic selection uses single-nucleotide polymorphisms (SNPs) to predict quantitative phenotypes for enhancing traits in breeding populations, and it has been widely used to increase breeding efficiency for plants and animals. Existing statistical methods rely on a prior distribution assumption of imputed genotype effects, which may not fit experimental datasets. Emerging deep learning could serve as a powerful machine learning tool to predict quantitative phenotypes without imputation and also to discover potential associated genotype markers efficiently. We propose a deep-learning framework using convolutional neural networks to predict the quantitative traits from SNPs and also to investigate genotype contributions to the trait using saliency maps. The missing values of SNPs are treated as a new genotype for the input of the deep-learning model. We tested our framework on simulation data and on experimental datasets of soybean. The results show that the deep learning model can bypass the imputation of missing values and achieve more accurate results for predicting quantitative phenotypes than well-known statistical methods. It can also effectively and efficiently identify significant markers of SNPs and SNP combinations associated in genome wide association study.}, language = {English}, urldate = {2021-01-19}, journal = {Frontiers in Genetics}, author = {Liu, Yang and Wang, Duolin and He, Fei and Wang, Juexin and Joshi, Trupti and Xu, Dong}, year = {2019}, keywords = {CNN - convolutional neural network, GWAS - genome-wide association study, Soybean, biomarker, phenotype, saliency map}, }

@article{song2017soynam,
author = {Song, Qijian and Yan, Long and Quigley, Charles and Jordan, Brandon D. and Fickus, Edward and Schroeder, Steve and Song, Bao-Hua and Charles An, Yong-Qiang and Hyten, David and Nelson, Randall and Rainey, Katy and Beavis, William D and Specht, Jim and Diers, Brian and Cregan, Perry},
title = {Genetic Characterization of the Soybean Nested Association Mapping Population},
journal = {The Plant Genome},
volume = {10},
number = {2},
doi = {https://doi.org/10.3835/plantgenome2016.10.0109},
url = {https://acsess.onlinelibrary.wiley.com/doi/abs/10.3835/plantgenome2016.10.0109},
eprint = {https://acsess.onlinelibrary.wiley.com/doi/pdf/10.3835/plantgenome2016.10.0109},
abstract = {Core Ideas 40 NAM families were developed and 5600 RILs in the families were characterized. The linkage maps for each family and a composite linkage map were constructed. More than a half million high-confidence SNPs were identified and annotated. Segregation distortion in most families favored alleles from the female parent. The REs in the soybean genome is low. A set of nested association mapping (NAM) families was developed by crossing 40 diverse soybean [Glycine max (L.) Merr.] genotypes to the common cultivar. The 41 parents were deeply sequenced for SNP discovery. Based on the polymorphism of the single-nucleotide polymorphisms (SNPs) and other selection criteria, a set of SNPs was selected to be included in the SoyNAM6K BeadChip for genotyping the parents and 5600 RILs from the 40 families. Analysis of the SNP profiles of the RILs showed a low average recombination rate. We constructed genetic linkage maps for each family and a composite linkage map based on recombinant inbred lines (RILs) across the families and identified and annotated 525,772 high confidence SNPs that were used to impute the SNP alleles in the RILs. The segregation distortion in most families significantly favored the alleles from the female parent, and there was no significant difference of residual heterozygosity in the euchromatic vs. heterochromatic regions. The genotypic datasets for the RILs and parents are publicly available and are anticipated to be useful to map quantitative trait loci (QTL) controlling important traits in soybean.},
year = {2017}
}

@article{Xavier2016,
  doi = {10.1186/s12859-016-0899-7},
  url = {https://doi.org/10.1186/s12859-016-0899-7},
  year = {2016},
  month = feb,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {17},
  number = {1},
  author = {Alencar Xavier and William Muir and Katy Rainey},
  title = {Impact of imputation methods on the amount of genetic variation captured by a single-nucleotide polymorphism panel in soybeans},
  journal = {{BMC} Bioinformatics}
}

@article{srivastava14a,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}


@article{wolf2018scanpy,
  title={SCANPY: large-scale single-cell gene expression data analysis},
  author={Wolf, F Alexander and Angerer, Philipp and Theis, Fabian J},
  journal={Genome biology},
  volume={19},
  number={1},
  pages={1--5},
  year={2018},
  publisher={BioMed Central}
}

@article{Cao2017,
  doi = {10.3389/fpls.2017.01222},
  url = {https://doi.org/10.3389/fpls.2017.01222},
  year = {2017},
  month = jul,
  publisher = {Frontiers Media {SA}},
  volume = {8},
  author = {Yongce Cao and Shuguang Li and Zili Wang and Fangguo Chang and Jiejie Kong and Junyi Gai and Tuanjie Zhao},
  title = {Identification of Major Quantitative Trait Loci for Seed Oil Content in Soybeans by Combining Linkage and Genome-Wide Association Mapping},
  journal = {Frontiers in Plant Science}
}

@article{Sonah2014,
  doi = {10.1111/pbi.12249},
  url = {https://doi.org/10.1111/pbi.12249},
  year = {2014},
  month = sep,
  publisher = {Wiley},
  volume = {13},
  number = {2},
  pages = {211--221},
  author = {Humira Sonah and Louise O{\textquotesingle}Donoughue and Elroy Cober and Istvan Rajcan and Fran{\c{c}}ois Belzile},
  title = {Identification of loci governing eight agronomic traits using a {GBS}-{GWAS} approach and validation by {QTL} mapping in soya bean},
  journal = {Plant Biotechnology Journal}
}

@misc{rnasq_tenx,
    author={10x Genomics},
    title={Our 1.3 million single cell dataset is ready to download},
    howpublished={{https://www.10xgenomics.com/blog/our-13-million-single-cell-dataset-is-ready-to-download}},
    year={2017}
}

@article{Gayoso2021scvitools,
	author = {Gayoso, Adam and Lopez, Romain and Xing, Galen and Boyeau, Pierre and Wu, Katherine and Jayasuriya, Michael and Mehlman, Edouard and Langevin, Maxime and Liu, Yining and Samaran, Jules and Misrachi, Gabriel and Nazaret, Achille and Clivio, Oscar and Xu, Chenling and Ashuach, Tal and Lotfollahi, Mohammad and Svensson, Valentine and da Veiga Beltrame, Eduardo and Talavera-Lopez, Carlos and Pachter, Lior and Theis, Fabian J and Streets, Aaron and Jordan, Michael I and Regier, Jeffrey and Yosef, Nir},
	title = {scvi-tools: a library for deep probabilistic analysis of single-cell omics data},
	year = {2021},
	doi = {10.1101/2021.04.28.441833},
	publisher = {Cold Spring Harbor Laboratory},
	URL = {https://www.biorxiv.org/content/early/2021/04/29/2021.04.28.441833},
	eprint = {https://www.biorxiv.org/content/early/2021/04/29/2021.04.28.441833.full.pdf},
	journal = {bioRxiv}
}

@article{agarwal2020data,
  title={Data denoising and post-denoising corrections in single cell RNA sequencing},
  author={Agarwal, Divyansh and Wang, Jingshu and Zhang, Nancy},
  journal={Statistical Science},
  volume={35},
  number={1},
  pages={112--128},
  year={2020},
  publisher={Institute of Mathematical Statistics}
}

@article{smithBayesianComputationGibbs1993,
  title = {Bayesian {{Computation Via}} the {{Gibbs Sampler}} and {{Related Markov Chain Monte Carlo Methods}}},
  author = {Smith, A. F. M. and Roberts, G. O.},
  year = {1993},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {55},
  number = {1},
  pages = {3--23},
  issn = {2517-6161},
  doi = {10.1111/j.2517-6161.1993.tb01466.x},
  abstract = {The use of the Gibbs sampler for Bayesian computation is reviewed and illustrated in the context of some canonical examples. Other Markov chain Monte Carlo simulation methods are also briefly described, and comments are made on the advantages of sample-based approaches for Bayesian inference summaries.},
  langid = {english},
  keywords = {bayesian statistics,censored data,constrained parameter models,generalized linear models,gibbs sampler,hastings algorithm,hierarchical models,markov chain monte carlo methods,missing data,time series models},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1993.tb01466.x},
  file = {/var/home/derek/Dropbox/Zotero/Smith_Roberts_1993_Bayesian Computation Via the Gibbs Sampler and Related Markov Chain Monte Carlo Methods.pdf;/var/home/derek/Zotero/storage/6EBASUMW/j.2517-6161.1993.tb01466.html}
}
