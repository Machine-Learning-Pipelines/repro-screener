\section{Discussion}

 In independent work, \cite[Theorem 9]{kallus2021causal} present an asymptotic Gaussian approximation result for a particular global functional: average treatment effect identified by negative controls. This functional fits within our framework because it is a mean square continuous functional of a nonparametric instrumental variable regression. To verify mean square continuity, see Example~\ref{ex:CATE} in Lemma~\ref{lemma:cont}.
 
 In their analysis, the authors write the sufficient condition
    $$\min(\tau_{\gamma,n},\tau_{\alpha,n}) \iota_{\gamma,n} \iota_{\alpha,n}=o(n^{-1/2}),\quad  \tau_{\gamma,n}=\sup_{\gamma \in \mathcal{G}_n}\frac{\{\mathcal{R}(\gamma)\}^{1/2}}{\{\mathcal{P}(\gamma)\}^{1/2}},\quad \tau_{\alpha,n}=\sup_{\alpha \in \mathcal{A}_n}\frac{\{\mathcal{R}(\alpha)\}^{1/2}}{\{\mathcal{P}(\alpha)\}^{1/2}}$$
    where
    $
   (\tau_{\gamma,n},\tau_{\alpha,n})
    $
    are ratio measures of ill posedness and $(\iota_{\gamma,n},\iota_{\alpha,n})$ are critical radii for the sequence of function classes $(\mathcal{G}_n,\mathcal{A}_n)$ used in adversarial estimation procedures for $(\hat{\gamma},\hat{\alpha})$. In particular $(\iota_{\gamma,n},\iota_{\alpha,n})$ appear in the authors' bounds for $\{\mathcal{P}(\hat{\gamma})\}^{1/2}$ and $\{\mathcal{P}(\hat{\alpha})\}^{1/2}$, respectively.
    
    For comparison, our analogous condition in Theorem~\ref{thm:dml} is that
    $$
   \min\left[\{\mathcal{P}(\hat{\gamma}_{\ell})\mathcal{R}(\hat{\alpha}_{\ell})\}^{1/2}, \{\mathcal{R}(\hat{\gamma}_{\ell})\mathcal{P}(\hat{\alpha}_{\ell})\}^{1/2}\right]=o(\sigma n^{-1/2}).
    $$
    By contrast, our result is (i) for the entire class of mean square continuous functionals, (ii) for black box estimators $(\hat{\gamma},\hat{\alpha})$, and (iii) finite sample, so it also handles local functionals in which $\sigma$ diverges. Critical radius arguments are one way to prove mean square rates for certain machine learning estimators, but not the only way. This distinction is important, since many existing statistical learning theory rates, whether for neural networks as in Example~\ref{ex:CATE}, random forest as in Example~\ref{ex:RDD}, kernel ridge regression as in  Example~\ref{ex:elasticity}, or lasso as in Example~\ref{ex:deriv}, are not in terms of a critical radius.