\section{Framework and examples}\label{sec:framework}

The general inference problem is to find a confidence interval for some scalar $\theta_0\text{ in }\mathbb{R}$ where
$
\theta_0=E\{m(W,\gamma_0)\}$, $\gamma_0$ is in $\Gamma$,
and $m:\mathcal{W}\times \mathbb{L}_2\rightarrow{\mathbb{R}}$ is an abstract formula. $W\text{ in }\mathcal{W}$ is a concatenation of random variables in the model excluding the outcome $Y\text{ in }\mathcal{Y}\subset\mathbb{R}$.  $\mathbb{L}_2$ is the space of functions of the form $\gamma:\mathcal{W}\rightarrow \mathbb{R}$ that are square integrable with respect to measure $\text{pr}$. $\Gamma$ is a linear subset of $\mathbb{L}_2$ known by the analyst, which may be $\mathbb{L}_2$ itself. 

Note that $\gamma_0$ may be the conditional expectation function $\gamma_0(w)=E(Y \mid W=w)$ or some other nonparametric quantity. For example, it could be the function defined as the solution to the ill posed inverse problem $E(Y \mid W_2=w_2)=E\{\gamma(W_1) \mid W_2=w_2\}$ where $W_1,W_2\subset W$. Such a function is called a nonparametric instrumental variable regression in econometrics \cite{newey2003instrumental}. We study the exactly identified case, which amounts to assuming completeness when $\Gamma=\mathbb{L}_2$ \cite{chen2018overidentification}. If $W_1=W_2$ then nonparametric instrumental variable regression simplifies into nonparametric regression. 

A local functional $\theta_0^{\lim}$ in $\mathbb{R}$ is a scalar that takes the form
$$
\theta^{\lim}_{0}=\lim_{h\rightarrow 0} \theta_0^h,\quad \theta_0^h=E\{m_h(W,\gamma_0)\}=E\{\ell_h(W_j) m(W,\gamma_0)\},\quad \gamma_0 \text{ in } \Gamma,
$$
where $\ell_h$ is a Nadaraya Watson weighting with bandwidth $h$ and $W_j$ is a scalar component of $W$. $\theta^{\lim}_{0}$ is a nonparametric quantity. However, it can be approximated by the sequence $(\theta_0^h)$. Each $\theta_0^h$ can be analyzed like $\theta_0$ above as long as we keep track of how certain quantities depend on $h$. By this logic, finite sample semiparametric theory for $\theta^h_0$ translates to finite sample nonparametric theory for $\theta_0^{\lim}$ up to some approximation error. In this sense, our analysis encompasses both global and local functionals.

To illustrate, we consider some classic functionals.

\begin{example}[Heterogeneous treatment effect estimated by neural network]\label{ex:CATE}
Let $Y$ be a health outcome. Let $W=(D,V,X)$ concatenate binary treatment $D$, covariate of interest $V$ such as age, and other covariates $X$ such as medical scans. Let $\gamma_0(d,v,x)=E(Y\mid D=d,V=v,X=x)$ be a function estimated by a neural network. Under the assumption of selection on observables, the heterogeneous treatment effect is 
$$
\textsc{CATE}(v)=E\{\gamma_0(1,V,X)-\gamma_0(0,V,X)\mid V=v\}=\lim_{h\rightarrow 0}E[\ell_h(V)\{\gamma_0(1,V,X)-\gamma_0(0,V,X)\}],$$ where
$
\ell_h(V)=(h\omega)^{-1}K\left\{(V-v)/h\right\}$, $\omega=E [h^{-1} K\left\{(V-v)/h\right\}]
$, and $K$ is a bounded and symmetric kernel that integrates to one.
\end{example}
The heterogeneous treatment effect is defined with respect to some interpretable, low dimensional characteristic $V$ such as age, race, or gender \cite{abrevaya2015estimating}. The same functional without the localization $\ell_h$ is the classic average treatment effect. See \cite{bibaut2017data} and \cite{colangelo2020double} for other meaningful localizations of average treatment effect.

\begin{example}[Regression discontinuity design estimated by random forest]\label{ex:RDD}
Let $Y$ be an educational outcome. Let $W=(D,X)$ concatenate test score variable $D$ and covariates $X$. Let $\gamma_0(d,x)=E(Y\mid D=d,X=x)$ be a function estimated by a random forest. Suppose the cutoff for a scholarship is the test score $D=0$. The regression discontinuity design parameter is
$$
\textsc{RDD}=\lim_{d\downarrow 0}E\{\gamma_0(d,X)\}-\lim_{d\uparrow 0}E\{\gamma_0(d,X)\}=\lim_{h\rightarrow 0}E\{\ell^{+}_{h}(D)\gamma_0(D,X)-\ell^{-}_{h}(D)\gamma_0(D,X)\},
$$
where
$
\ell^+_h(D)=(h\omega^+)^{-1} K\left\{(2D-h)/(2h)\right\}$, $\omega^+=E \left[h^{-1} K\left\{(2D-h)/(2h)\right\}\right]
$, 
$\ell^-_h(D)=(h\omega^-)^{-1} K\left\{(-2D-h)/(2h)\right\}$, $\omega^-=E \left[h^{-1} K\left\{(-2D-h)/(2h)\right\}\right],$ and $K$ vanishes outside of the interval $(-1/2,1/2)$.
\end{example}
The expressions for fuzzy regression discontinuity, exact kink, and fuzzy kink designs are similar. 

\begin{example}[Demand elasticity estimated by kernel instrumental variable regression]\label{ex:elasticity}
Let $Y$ be log quantity demanded of some good. Let $W=(D,X,Z)$ concatenate log price $D$, covariates $X$, and cost shifter $Z$. Let $\gamma_0(d,x)$ be defined as the solution to $E(Y \mid X=x, Z=z)=E\{\gamma(D,X) \mid X=x, Z=z\}$ estimated by a kernel instrumental variable regression \cite{singh2019kernel}. The demand elasticity is 
$$
\textsc{ELASTICITY}=E\left\{\frac{\partial }{\partial d} \gamma_0(D,X) \right\}.
$$
\end{example}
In Supplement~2, we present the additional example of heterogeneous average derivative estimated by lasso, which is useful when an analyst has access to data on household spending behavior. 

For our simple and general theorem, we require that the formula $m$ is mean square continuous.
\begin{assumption}[Linearity and mean square continuity]\label{assumption:cont}
Assume that the functional $\gamma\mapsto \mathbb{E}\{m(W,\gamma)\}$ is linear, and that there exist $\bar{Q}<\infty $ and $q>0$ such that
$
E\{m(W,\gamma)^2\}\leq \bar{Q} [E\{\gamma(W)^2\}]^q$ for all $\gamma\text{ in } \Gamma.
$
\end{assumption}
This condition will be key in Section~\ref{sec:dml}, where we reduce the problem of inference for $\theta_0$ into the problem of learning $(\gamma_0,\alpha^{\min}_0)$, where $\alpha^{\min}_0$ is introduced below. It is a powerful condition satisfied by many functionals of interest, or at least satisfied by their approximating sequences. Though the local functional $\theta_0^{\lim}$ does not satisfy Assumption~\ref{assumption:cont}, each approximating $\theta_0^h$ does. In particular, for each $m_h$ there exists some $\bar{Q}_h$ that depends on $h$. We keep track of $\bar{Q}$ in our analysis and subsequently consider $\bar{Q}=\bar{Q}_h$. See Theorem~\ref{thm:local} below for conditions that characterize $\bar{Q}_h$ in local functionals, including Examples~\ref{ex:CATE} and~\ref{ex:RDD}.

The restriction that $\gamma_0$ is in $\Gamma\subset \mathbb{L}_2$, where $\Gamma$ is some linear function space, is called a restricted model in semiparametric statistical theory. In learning theory, mean square rates are adaptive to the smoothness of $\gamma_0$, encoded by $\gamma_0\text{ in }\Gamma$. We quote a general Riesz representation theorem for restricted models.

\begin{lemma}[Riesz representation \cite{chernozhukov2018global}]\label{prop:RR}
Suppose Assumption~\ref{assumption:cont} holds. Further suppose $\gamma_0\text{ is in } \Gamma$. Then there exists a Riesz representer $\alpha_0\text{ in } \mathbb{L}_2$ such that for all $\gamma$ in $\Gamma$, 
$
E\{m(W,\gamma)\}=E\{\alpha_0(W)\gamma(W)\}.
$
There exists a unique minimal Riesz representer $\alpha_0^{\min}\text{ in } closure(\Gamma)$ that satisfies this equation, obtained by projecting any $\alpha_0$ onto $\Gamma$. Moreover, denoting by $\bar{M}$ the operator norm of $\gamma\mapsto E\{m(W,\gamma)\}$, we have that
$
[E\{\alpha_0^{\min}(W)^2\}]^{1/2}=\bar{M} \leq \bar{Q}^{1/2}<\infty.
$
\end{lemma}
The condition $\bar{M}<\infty$ is enough for the conclusions of Lemma~\ref{prop:RR} to hold. Since $\bar{M}\leq \bar{Q}^{1/2}$, $\bar{Q}<\infty$ in Assumption~\ref{assumption:cont} is a sufficient condition. Nonetheless, we assume $\bar{Q}<\infty$ because mean square continuity plays a central role in the main results of Section~\ref{sec:dml}. In Examples~\ref{ex:CATE} and~\ref{ex:RDD}, with propensity score $\pi_0(v,x)$,
    $$
    \alpha_0(d,v,x)=\ell_h(v)\left\{\frac{d}{\pi_0(v,x)}-\frac{1-d}{1-\pi_0(v,x)}\right\};\quad
    \alpha^+_0(d,x)=\ell_h^{+}(d),
    \quad 
     \alpha^-_0(d,x)=\ell_h^{-}(d).
    $$
Riesz representation delivers a doubly robust formulation of the target $\theta_0\text{ in }\mathbb{R}$. For the case where $\gamma_0(w)$ is defined as a nonparametric regression in $\Gamma$ or projection onto $\Gamma$, consider the estimating equation
$$
\theta_0=E[m(W,\gamma_0)+\alpha^{\min}_0(W)\{Y-\gamma_0(W)\}].
$$
This formulation is doubly robust since it remains valid if either $\gamma_0$ or $\alpha^{\min}_0$ is correct: for all $(\gamma,\alpha)$ in $\Gamma$,
$$
\theta_0=E[m(W,\gamma_0)+\alpha(W)\{Y-\gamma_0(W)\}]=E[m(W,\gamma)+\alpha^{\min}_0(W)\{Y-\gamma(W)\}].
$$
The term $\alpha(w)\{y-\gamma(w)\}$ serves as a bias correction for the term $m(w,\gamma)$. We view $(\gamma_0,\alpha^{\min}_0)$ as nuisance parameters that we must learn in order to learn and infer $\theta_0$. While any Riesz representer $\alpha_0$ will suffice for valid learning and inference of $\theta_0=E\{m(W,\gamma_0)\}$ under correct specification of $\gamma_0$ as the regression $E(Y \mid W=w)$ in $\Gamma$, the minimal Riesz representer $\alpha_0^{\min}$ confers specification robust inference and semiparametric efficiency for estimating $\theta_0=E\{m(W,\gamma_0)\}$ when $\gamma_0$ is only the projection of $E(Y \mid W=w)$ onto $\Gamma$; see \cite[Theorem 4.2]{chernozhukov2018global}.

If $\gamma_0(w)$ is defined as the solution to an ill posed inverse problem, then the appropriate Riesz representer is defined as the solution to another ill posed inverse problem \cite{severini2012efficiency,ichimura2021influence}. The relevant nuisance parameters are $(\gamma_0,\alpha^{\min}_0)$ defined as unique solutions $(\gamma,\alpha)$ to
$$
E(Y \mid W_2=w_2)=E\{\gamma(W_1) \mid W_2=w_2\},\quad \eta_0^{\min}(w_1)=E\{\alpha(W_2) \mid W_1=w_1\},
$$
where $\eta_0^{\min}$ is the minimal Riesz representer satisfying $E\{m(W_1,\gamma)\}=E\{\eta_0(W_1)\gamma(W_1)\}$ for all $\gamma$ in $\Gamma$ from Lemma~\ref{prop:RR}. Uniqueness is due to the assumption of exact identification, which amounts to completeness when $\Gamma=\mathbb{L}_2$. In Example~\ref{ex:elasticity}, $w_1=(d,x)$, $w_2=(z,x)$, and $\eta_0(d,x)=-\partial_d  
\log f(d\mid x)$ where $f(d \mid x)$ is a conditional density. This abuse of notation allows us to state unified results. The estimating equation is 
$$
\theta_0=E[m(W_1,\gamma_0)+\alpha^{\min}_0(W_2)\{Y-\gamma_0(W_1)\}].
$$ 
A new insight of this work is that, for any mean square continuous functional, $n^{-1/2}$ Gaussian approximation is still possible if either $\gamma_0$ or $\alpha^{\min}_0$ is the solution to a mildly, rather than severely, ill posed inverse problem; the doubly robust formulation confers double robustness to ill posedness.