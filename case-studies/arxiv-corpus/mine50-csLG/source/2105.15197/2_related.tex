\section{Related work}\label{sec:related_main}

By focusing on functionals of nonparametric quantities, this paper continues the tradition of classic semiparametric statistics \cite{hasminskii1979nonparametric,robinson1988root,bickel1993efficient,newey1994asymptotic,andrews1994asymptotics,robins1995semiparametric,ai2003efficient}. Whereas classic semiparametric theory studies functionals of densities or regressions over low dimensional domains, we study functionals of machine learning algorithms over arbitrary domains. %that may include low, high, or infinite dimensional covariates. 
In classic semiparametric theory, an object called the Riesz representer appears in efficient influence functions and asymptotic variance calculations \cite{newey1994asymptotic}. For the same reasons, it appears in debiased machine learning confidence intervals.

In asymptotic inference, the Riesz representer is inevitable. A growing literature directly incorporates the Riesz representer into estimation, which amounts to debiasing known estimators. Doubly robust estimating equations serve this purpose \cite{robins1995semiparametric}. A geometric perspective emphasizes Neyman orthogonality: by debiasing, the learning problem for the functional becomes orthogonal to the learning problem for the nonparametric object \cite{chernozhukov2016locally,chernozhukov2018original,foster2019orthogonal}. An analytic perspective emphasizes the mixed bias property: by debiasing, the functional has bias equal to the product of certain learning rates \cite{chernozhukov2018original,rotnitzky2021characterization}. In this work, we focus on debiased machine learning with doubly robust estimating equations.

With debiasing alone, a key challenge remains: for inference, the function class in which the nonparametric quantity is learned must be Donsker \cite{van2006targeted,luedtke2016statistical,van2018targeted,qiu2021universal}, or it must have slowly increasing entropy \cite{belloni2013inference,belloni2014uniform,zhang2014confidence,javanmard2014confidence,vandegeer2014asymptotically}. However, popular nonparametric settings in machine learning may not satisfy this property. A solution to this challenging issue is to combine debiasing with sample splitting \cite{klaassen1987consistent}. The targeted \cite{zheng2011cross}
and debiased \cite{belloni2012sparse,chernozhukov2016locally,chernozhukov2018original} machine learning literatures provide this insight. In particular, debiased machine learning delivers sufficient conditions for asymptotic inference on functionals in terms of learning rates of the underlying nonparametric quantity and the Riesz representer. We complement prior results with a finite sample analysis. 

This paper subsumes \cite[Section 4]{singh2021debiased}.