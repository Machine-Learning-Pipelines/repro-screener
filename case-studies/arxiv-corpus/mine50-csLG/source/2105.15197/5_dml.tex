\section{Validity of confidence interval}\label{sec:dml}

We write this section at a high level of generality so it can be used by analysts working on a variety of problems. We assume a few simple and interpretable conditions and consider black box estimators $(\hat{\gamma},\hat{\alpha})$. We prove by finite sample arguments that $\hat{\theta}$ defined by Algorithm~\ref{alg:target} is consistent, and that its confidence interval is valid and semiparametrically efficient. Towards this end, define the oracle moment function
$$
 \psi_0(w)=\psi(w,\theta_0,\gamma_0,\alpha^{\min}_0), \quad \psi(w,\theta,\gamma,\alpha)=m(w,\gamma)+\alpha(w)\{y-\gamma(w)\}-\theta.
$$
Its moments are $\sigma^2=E\{\psi_0(W)^2\}$, $\kappa^3=E\{|\psi_0(W)|^3\}$, and $\zeta^4=E\{\psi_0(W)^4\}$. Write the Berry Esseen constant as $c^{BE}=0.4748$ \cite{shevtsova2011absolute}. The result will be in terms of abstract mean square rates.% on the nuisances $(\hat{\gamma},\hat{\alpha})$. 

\begin{definition}[Mean square error]
Write the mean square error $\mathcal{R}(\hat{\gamma}_{\ell})$ and the projected mean square error $\mathcal{P}(\hat{\gamma}_{\ell})$ of $\hat{\gamma}_{\ell}$ trained on observations indexed by $I^c_{\ell}$ as
$$
    \mathcal{R}(\hat{\gamma}_{\ell})=E[\{\hat{\gamma}_{\ell}(W)-\gamma_0(W)\}^2\mid I^c_{\ell}],\quad 
     \mathcal{P}(\hat{\gamma}_{\ell})=E([ E\{\hat{\gamma}_{\ell}(W_1)-\gamma_0(W_1)\mid W_2, I^c_{\ell}\} ]^2\mid I^c_{\ell}).
$$
Likewise define $\mathcal{R}(\hat{\alpha}_{\ell})$ and $\mathcal{P}(\hat{\alpha}_{\ell})$.
\end{definition}
Statistical learning theory provides rates of this form, where $I^c_{\ell}$ is a training set and $W$ is a test point. In the case of nonparametric regression, $\mathcal{R}(\hat{\gamma}_{\ell})$ or $\mathcal{R}(\hat{\alpha}_{\ell})$ typically has a fast rate between $n^{-1/2}$ and $n^{-1}$. In the case of nonparametric instrumental variable regression, $\mathcal{R}(\hat{\gamma}_{\ell})$ and $\mathcal{R}(\hat{\alpha}_{\ell})$ typically have rates slower than $n^{-1/2}$ due to ill posedness, but $\mathcal{P}(\hat{\gamma}_{\ell})$ or $\mathcal{P}(\hat{\alpha}_{\ell})$ may have a fast rate \cite{blundell2007semi,singh2019kernel,dikkala2020minimax}. Our main result is a finite sample Gaussian approximation.

\begin{theorem}[Finite sample Gaussian approximation]\label{thm:dml}Suppose Assumption~\ref{assumption:cont} holds,
$
E[\{Y-\gamma_0(W)\}^2 \mid W]\leq \bar{\sigma}^2,$
and
$\|\alpha^{\min}_0\|_{\infty}\leq\bar{\alpha}.
$
Then with probability $1-\epsilon$,
$$
\sup_{z\in\mathbb{R}} \left| \text{\normalfont pr} \left\{\frac{n^{1/2}}{\sigma}(\hat{\theta}-\theta_0)\leq z\right\}-\Phi(z)\right|\leq c^{BE}\left(\frac{\kappa}{\sigma}\right)^3 n^{-1/2}+\frac{\Delta}{(2\pi)^{1/2}}+\epsilon,
$$
where $\Phi(z)$ is the standard Gaussian cumulative distribution function and
$$
\Delta=\frac{3 L}{\epsilon   \sigma}\left[(\bar{Q}^{1/2}+\bar{\alpha})\{\mathcal{R}(\hat{\gamma}_{\ell})\}^{q/2}+\bar{\sigma}\{\mathcal{R}(\hat{\alpha}_{\ell})\}^{1/2}+\{n \mathcal{R}(\hat{\gamma}_{\ell}) \mathcal{R}(\hat{\alpha}_{\ell}) \}^{1/2}\right].
$$
If in addition $\|\hat{\alpha}_{\ell}\|_{\infty}\leq\bar{\alpha}'$ then the same result holds updating $\Delta$ to be
$$
\frac{4 L}{\epsilon^{1/2}  \sigma}\left[(\bar{Q}^{1/2}+\bar{\alpha}+\bar{\alpha}')\{\mathcal{R}(\hat{\gamma}_{\ell})\}^{q/2}
    +\bar{\sigma}\{\mathcal{R}(\hat{\alpha}_{\ell})\}^{1/2}\right]+\frac{1}{\sigma}[\{n\mathcal{P}(\hat{\gamma}_{\ell})\mathcal{R}(\hat{\alpha}_{\ell})\}^{1/2} \wedge \{n\mathcal{R}(\hat{\gamma}_{\ell})\mathcal{P}(\hat{\alpha}_{\ell})\}^{1/2}].
$$
For local functionals, further suppose approximation error of size $\Delta_h=
n^{1/2} \sigma_h^{-1}|\theta_0^h-\theta_0^{\lim}|$. Then the same result holds replacing $(\hat{\theta},\theta_0,\Delta)$ with $(\hat{\theta}^h,\theta_0^{\lim},\Delta+\Delta_h)$.
\end{theorem}

Theorem~\ref{thm:dml} is a finite sample Gaussian approximation for debiased machine learning with black box $(\hat{\gamma}_{\ell},\hat{\alpha}_{\ell})$. It degrades gracefully if the parameters $(\bar{Q},\bar{\sigma},\bar{\alpha},\bar{\alpha}')$ diverge relative to $n$ and the learning rates. Note that $\bar{\alpha}'$ is a bound on the chosen estimator $\hat{\alpha}_{\ell}$ that can be imposed by censoring extreme evaluations. Theorem~\ref{thm:dml} is a finite sample refinement of the asymptotic black box result in \cite{chernozhukov2016locally}. 

In the bound $\Delta$, the expression $\{n \mathcal{R}(\hat{\gamma}_{\ell}) \mathcal{R}(\hat{\alpha}_{\ell})\}^{1/2}$ allows a tradeoff: one of the learning rates may be slow, as long as the other is sufficiently fast to compensate. It is easily handled in the case of nonparametric regression, where $\mathcal{R}(\hat{\gamma}_{\ell})$ or $\mathcal{R}(\hat{\alpha}_{\ell})$ typically has a fast rate. However, the expression may diverge in the case of nonparametric instrumental variable regression, where both rates may be slow due to ill posedness. 

The refined bound provides an alternative path to Gaussian approximation, replacing $\{n \mathcal{R}(\hat{\gamma}_{\ell}) \mathcal{R}(\hat{\alpha}_{\ell}) \}^{1/2}$ with the minimum of $\{n\mathcal{P}(\hat{\gamma}_{\ell})\mathcal{R}(\hat{\alpha}_{\ell})\}^{1/2}$ and $\{n\mathcal{R}(\hat{\gamma}_{\ell})\mathcal{P}(\hat{\alpha}_{\ell})\}^{1/2}$. Importantly, the projected mean square error $\mathcal{P}(\hat{\gamma}_{\ell})$ can have a fast rate even when the mean square error $\mathcal{R}(\hat{\gamma}_{\ell})$ has a slow rate because its definition sidesteps ill posedness. Moreover, the analyst only needs $\mathcal{P}(\hat{\gamma}_{\ell})$ fast enough to compensate for the ill posedness encoded in $\mathcal{R}(\hat{\alpha}_{\ell})$, or  $\mathcal{P}(\hat{\alpha}_{\ell})$ fast enough to compensate for the ill posedness encoded in $\mathcal{R}(\hat{\gamma}_{\ell})$. This general and finite sample characterization of double robustness to ill posedness appears to be new. In independent work, \cite{kallus2021causal} document an asymptotic special case of this result for a specific global functional and specific nuisance estimators; see Supplement~3.

By Theorem~\ref{thm:dml}, the neighborhood of Gaussian approximation scales as $\sigma n^{-1/2}$. If $\sigma$ is a constant, then the rate of convergence is $n^{-1/2}$, i.e. the parametric rate. If $\sigma$ is a diverging sequence, then the rate of convergence degrades gracefully to nonparametric rates. A precise characterization of $\sigma$ is possible, which we provide in Supplement~2 and summarize here.
It turns out that global functionals have $\sigma$ that is constant, while local functionals have $\sigma=\sigma_h$ that is a diverging sequence. We emphasize which quantities are diverging sequences for local functionals by indexing with the bandwidth $h$.

\begin{theorem}[Characterization of key quantities]\label{thm:local}
If noise has finite variance then $\bar{\sigma}^2<\infty$. Suppose bounded moment and heteroscedasticity conditions defined in Supplement~2 hold. Then for global functionals
$
\kappa/\sigma \lesssim \sigma \asymp \bar{M} < \infty$; $\kappa, \zeta\lesssim \bar{M}^2\leq \bar{Q}<\infty$; and $\bar{\alpha}<\infty.$
Suppose bounded moment, heteroscedasticity, density, and derivative conditions defined in Supplement~2 hold. Then for local functionals
$
\kappa_h/\sigma_h \lesssim h^{-1/6}$, $\sigma_h \asymp \bar{M}_h \asymp h^{-1/2} $, $\kappa_h\lesssim h^{-2/3}$, $\zeta_h\lesssim h^{-3/4}$, 
$
\bar{Q}_h\lesssim h^{-2}$, $\bar{\alpha}_h\lesssim h^{-1}$, and $\Delta_h \lesssim n^{1/2} h^{\mathsf{v}+1/2}
$
where $\mathsf{v}$ is the order of differentiability defined in Supplement~2.
\end{theorem}
For global functionals, $(\bar{Q},\bar{\alpha})$ are finite constants that depend on the problem at hand. For example, for treatment effects a sufficient condition is that the propensity score is bounded away from zero and one. For derivatives, a sufficient condition is that $\Gamma$ satisfies Sobolev conditions. For local functionals, we handle $(\bar{Q}_h,\bar{\alpha}_h)$ on a case by case basis. See Supplement~2 for interpretable and complete characterizations.% of key quantities in each of the leading examples.

Observe that the finite sample Gaussian approximation in Theorem~\ref{thm:dml} is in terms of the true asymptotic variance $\sigma^2$. We now provide a guarantee for its estimator $\hat{\sigma}^2$.

\begin{theorem}[Variance estimation]\label{thm:var}
Suppose Assumption~\ref{assumption:cont} holds, $
E[\{Y-\gamma_0(W)\}^2 \mid W]\leq \bar{\sigma}^2,$ and $\|\hat{\alpha}_{\ell}\|_{\infty}\leq\bar{\alpha}'.
$
Then with probability $1-\epsilon'$,
$
|\hat{\sigma}^2-\sigma^2|\leq \Delta'+2(\Delta')^{1/2}\{(\Delta'')^{1/2}+\sigma\}+\Delta'',
$ where
$$
 \Delta'=4(\hat{\theta}-\theta_0)^2+\frac{24 L}{\epsilon'}\left[\{\bar{Q}+(\bar{\alpha}')^2\}\mathcal{R}(\hat{\gamma}_{\ell})^q+\bar{\sigma}^2\mathcal{R}(\hat{\alpha}_{\ell})\right],\quad 
    \Delta''=\left(\frac{2}{\epsilon'}\right)^{1/2}\zeta^2 n^{-1/2}.
$$
\end{theorem}
Theorem~\ref{thm:var} is a finite sample variance estimation guarantee. It degrades gracefully if the parameters $(\bar{Q},\bar{\sigma},\bar{\alpha}')$ diverge relative to $n$ and the learning rates. Theorems~\ref{thm:dml} and~\ref{thm:var} immediately imply simple, interpretable conditions for validity of the confidence interval. We conclude by summarizing these conditions.

\begin{corollary}[Confidence interval]\label{cor:CI}
Suppose Assumption~\ref{assumption:cont} holds as well as the following regularity and learning rate conditions, as $n\rightarrow \infty$ and as $h\rightarrow 0$:
$$
E[\{Y-\gamma_0(W)\}^2 \mid W]\leq \bar{\sigma}^2,\quad \|\alpha^{\min}_0\|_{\infty}\leq\bar{\alpha},\quad  \|\hat{\alpha}_{\ell}\|_{\infty}\leq\bar{\alpha}', \quad  \left\{\left(\kappa/\sigma\right)^3+\zeta^2\right\}n^{-1/2}\rightarrow0;
$$  
\begin{enumerate}
    \item $\left(\bar{Q}^{1/2}+\bar{\alpha}/\sigma+\bar{\alpha}'\right)\{\mathcal{R}(\hat{\gamma}_{\ell})\}^{q/2}=o_p(1)$;
    \item $\bar{\sigma}\{\mathcal{R}(\hat{\alpha}_{\ell})\}^{1/2}=o_p(1)$;
    \item $[\{n \mathcal{R}(\hat{\gamma}_{\ell}) \mathcal{R}(\hat{\alpha}_{\ell})\}^{1/2} \wedge \{n\mathcal{P}(\hat{\gamma}_{\ell})\mathcal{R}(\hat{\alpha}_{\ell})\}^{1/2} \wedge \{n\mathcal{R}(\hat{\gamma}_{\ell})\mathcal{P}(\hat{\alpha}_{\ell})\}^{1/2}]/\sigma =o_p(1)$.
\end{enumerate}
Then the estimator $\hat{\theta}$ in Algorithm~\ref{alg:target} is consistent and asymptotically Gaussian, and the confidence interval in Algorithm~\ref{alg:target} includes $\theta_0$ with probability approaching the nominal level. Formally,
$$
\hat{\theta}=\theta_0+o_p(1),\quad \sigma^{-1}n^{1/2}(\hat{\theta}-\theta_0)\leadsto\mathcal{N}(0,1),\quad \text{\normalfont pr} \left\{\theta_0 \text{ in }  \left(\hat{\theta}\pm c_a\hat{\sigma} n^{-1/2} \right)\right\}\rightarrow 1-a.
$$
For local functionals, if $\Delta_h \rightarrow 0$, then the same result holds replacing $(\hat{\theta},\theta_0)$ with $(\hat{\theta}^h,\theta_0^{\lim})$.
\end{corollary}