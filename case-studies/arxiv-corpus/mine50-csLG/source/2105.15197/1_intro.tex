\section{Introduction}\label{sec:intro}

The goal of this paper is to provide a useful technical result for analysts who desire confidence intervals for functionals, i.e. scalar summaries, of machine learning algorithms. For example, the functional of interest could be the average treatment effect of a medical intervention, and the machine learning algorithm could be a neural network trained on medical scans. Alternatively, the functional of interest could be the price elasticity of consumer demand, and the machine learning algorithm could be a kernel ridge regression trained on economic transactions. Treatment effects and price elasticities for a specific demographic are examples of localized functionals. In these various applications, confidence intervals are essential.

We provide a simple set of conditions that can be verified using the kind of rates provided by statistical learning theory. Unlike previous work, we provide a finite sample analysis for any global or local functional of any machine learning algorithm, without bootstrapping, subject to these simple and interpretable conditions. The machine learning algorithm may be estimating a nonparametric regression, a nonparametric instrumental variable regression, or some other nonparametric quantity. We provide conceptual and statistical contributions for the rapidly growing literature on debiased machine learning.

Conceptually, our result unifies, refines, and extends existing debiased machine learning theory for a broad audience. We unify finite sample results that are specific to particular functionals or machine learning algorithms. General asymptotic theory with abstract conditions already exists, which we refine to finite sample theory with simple conditions. In doing so, we uncover a new notion of double robustness for exactly identified ill posed inverse problems. A virtue of finite sample analysis is that it handles the case where the functional involves localization. We show how learning theory delivers inference.

Statistically, we provide results for the class of global functionals that are mean square continuous, and their local counterparts, using algorithms that have sufficiently fast finite sample learning rates. Formally, we prove (i) consistency, Gaussian approximation, and semiparametric efficiency for global functionals; and (ii) consistency and Gaussian approximation for local functionals. The analysis explicitly accounts for each source of error in any finite sample size. The rate of convergence is the parametric rate of $n^{-1/2}$ for global functionals, and it degrades gracefully to nonparametric rates for local functionals.