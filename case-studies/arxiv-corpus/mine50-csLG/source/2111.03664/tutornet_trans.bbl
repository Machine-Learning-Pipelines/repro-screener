\begin{thebibliography}{10}

\bibitem{pruning1:scheme}
S.~Han, H.~Mao, and W.~J. Dally.
\newblock {Deep compression: compressing deep neural network with pruning,
  trained quantization and huffman coding}.
\newblock In {\em Proc. ICLR}, 2016.

\bibitem{pruning2:scheme}
H.~Li, A.~Kadav, I.~Durdanovic, H.~Samet, and H.~P. Graf.
\newblock {Pruning filters for efficient convnets}.
\newblock In {\em Proc. ICLR}, 2017.

\bibitem{quantization:scheme}
J.~Wu, L.~Cong, Y.~Wang, Q.~Hu, and J.~Cheng.
\newblock {Quantized convolutional neural networks for mobile devices}.
\newblock In {\em Proc. CVPR}, page 4820–4828, 2016.

\bibitem{bucila-et-al:scheme}
C.~Bucila, R.~Caruana, and A.~Niculescu-Mizil.
\newblock {Model compression}.
\newblock In {\em Proc. ACM SIGKDD}, page 535–541, 2006.

\bibitem{hinton_kd-et-al:scheme}
G.~Hinton, O.~Vinyals, and J.~Dean.
\newblock {Distilling the knowledge in a neural network}.
\newblock In {\em Proc. NIPS Workshop Deep Learn.}, 2014.

\bibitem{graves-et-al:scheme}
A.~Graves, S.~Fern{\'a}ndez, F.~Gomez, and J.~Schmidhuber.
\newblock Connectionist temporal classification: labelling unsegmented sequence
  data with recurrent neural networks.
\newblock In {\em Proc. ICML}, pages 369--376, 2006.

\bibitem{acours_lin1}
G.~Zheng et~al.
\newblock Wav-bert: cooperative acoustic and linguistic representation learning
  for low-resource speech recognition.
\newblock In {\em Proc. EMNLP}, 2021.

\bibitem{acours_lin2}
B.~Sharma et~al.
\newblock Leveraging acoustic and linguistic embeddings from pretrained speech
  and language models for intent classification.
\newblock In {\em Proc. ICASSP}, 2021.

\bibitem{acours_lin3}
C.~Yi, S.~Zhou, and B.~Xu.
\newblock {Efficiently fusing pretrained acoustic and linguistic encoders for
  low-resource speech recognition}.
\newblock {\em IEEE Signal Processing Letters}, 2021.

\bibitem{acours_lin4}
F.~Yu and K.~Chen.
\newblock Non-autoregressive transformer-based end-to-end asr using bert.
\newblock {\em IEEE/ACM Transactions on Audio, Speech, and Language
  Processing}, 30:1474--1482, 2022.

\bibitem{acours_lin5}
G.~Winata et~al.
\newblock Adapt-and-adjust: overcoming the long-tail problem of multilingual
  speech recognition.
\newblock {\em arXiv preprint arXiv:2012.01687}, 2020.

\bibitem{dodeep:scheme}
J.~Ba and R.~Caruana.
\newblock Do deep nets really need to be deep?
\newblock In {\em Proc. NIPS}, pages 2654--2662, 2014.

\bibitem{firstasr:scheme}
J.~Li, R.~Zhao, T.~J. Huang, and Y.~Gong.
\newblock Learning small-size dnn with output-distribution-based criteria.
\newblock In {\em Proc. INTERSPEECH}, 2014.

\bibitem{seq:scheme}
J.~H.~M. Wong and M.~J.~F. Gales.
\newblock Sequence student-teacher training of deep neural networks.
\newblock In {\em Proc. INTERSPEECH}, pages 2761–--2765, 2016.

\bibitem{blending:scheme}
K.~J. Geras et~al.
\newblock Blending lstms into cnns.
\newblock In {\em Proc. ICLR Workshop}, 2016.

\bibitem{chebotar-et-al:scheme}
Y.~Chebotar and A.~Waters.
\newblock Distilling knowledge from ensembles of neural networks for speech
  recognition.
\newblock In {\em Proc. INTERSPEECH}, pages 3439--3443, 2016.

\bibitem{watanabe-et-al:scheme}
S.~Watanabe, T.~Hori, J.~L.~Roux, and J.~R. Hershey.
\newblock Student-teacher network learning with enhanced features.
\newblock In {\em Proc. ICASSP}, pages 5275--5279, 2017.

\bibitem{lu-et-al:scheme}
L.~Lu, M.~Guo, and S.~Renals.
\newblock Knowledge distillation for small-footprint highway networks.
\newblock In {\em Proc. ICASSP}, pages 4820--4824, 2017.

\bibitem{fukuda-et-al:scheme}
T.~Fukuda, M.~Suzuki, G.~Kurata, S.~Thomas, J.~Cui, and B.~Ramabhadran.
\newblock Efficient knowledge distillation from an ensemble of teachers.
\newblock In {\em Proc. INTERSPEECH}, pages 3697--3701, 2017.

\bibitem{senior-et-al:scheme}
A.~Senior, H.~Sak, F.~C.~Quitry, T.~Sainath, K.~Rao, et~al.
\newblock Acoustic modelling with cd-ctc-smbr lstm rnns.
\newblock In {\em Proc. ASRU}, pages 604--609, 2015.

\bibitem{takashima-et-al:scheme}
R.~Takashima, S.~Li, and H.~Kawai.
\newblock An investigation of a knowledge distillation method for ctc acoustic
  models.
\newblock In {\em Proc. ICASSP}, pages 5809--5813, 2018.

\bibitem{takashima-et-al2:scheme}
R.~Takashima, S.~Li, and H.~Kawai.
\newblock Investigation of sequence-level knowledge distillation methods for
  ctc acoustic models.
\newblock In {\em Proc. ICASSP}, pages 6156--6160, 2019.

\bibitem{kurata2-et-al:scheme}
G.~Kurata and K.~Audhkhasi.
\newblock Improved knowledge distillation from bi-directional to
  uni-directional lstm ctc for end-to-end speech recognition.
\newblock In {\em Proc. SLT}, pages 411--417, 2018.

\bibitem{kurata-et-al:scheme}
G.~Kurata and K.~Audhkhasi.
\newblock Guiding ctc posterior spike timings for improved posterior fusion and
  knowledge distillation.
\newblock In {\em Proc. INTERSPEECH}, pages 1616--1620, 2019.

\bibitem{tutornet:scheme}
J.~W. Yoon, H.~Lee, H.~Y. Kim, W.~I. Cho, and N.~S. Kim.
\newblock Tutornet: towards flexible knowledge distillation for end-to-end
  speech recognition.
\newblock {\em IEEE/ACM Transactions on Audio, Speech, and Language
  Processing}, 29:1626--1638, 2021.

\bibitem{compression:scheme}
R.~Pang, T.~Sainath, R.~Prabhavalkar, S.~Gupta, Y.~Wu, S.~Zhang, and C.~C.
  Chiu.
\newblock Compression of end-to-end models.
\newblock In {\em Proc. INTERSPEECH}, 2018.

\bibitem{wer_error:scheme}
H.~G. Kim, H.~Na, H.~Lee, J.~Lee, T.~G. Kang, M.~J. Lee, and Y.~S. Choi.
\newblock Knowledge distillation using output errors for self-attention
  end-to-end models.
\newblock In {\em Proc. ICASSP}, 2019.

\bibitem{entropy:scheme}
K.~Kwon, H.~Na, H.~Lee, and N.~S. Kim.
\newblock Adaptive knowledge distillation based on entropy.
\newblock In {\em Proc. ICASSP}, 2020.

\bibitem{romero-et-al:scheme}
A.~Romero, N.~Ballas, S.~E. Kahou, A.~Chassang, C.~Gatta, and Y.~Bengio.
\newblock Fitnets: hints for thin deep nets.
\newblock In {\em Proc. ICLR}, 2015.

\bibitem{at:scheme}
S.~Zagoruyko and N.~Komodakis.
\newblock Paying more attention to attention: improving the performance of
  convolutional neural networks via attention transfer.
\newblock In {\em Proc. ICLR}, 2017.

\bibitem{fsp:scheme}
J.~Yim, D.~Joo, J.~Bae, and J.~Kim.
\newblock A gift from knowledge distillation: fast optimization, network
  minimization and transfer learning.
\newblock In {\em Proc. CVPR}, 2017.

\bibitem{jacobian:scheme}
S.~Srinivas and F.~Fleuret.
\newblock Knowledge transfer with jacobian matching.
\newblock In {\em Proc. ICML}, 2018.

\bibitem{factor:scheme}
J.~Kim, S.~Park, and N.~Kwak.
\newblock Paraphrasing complex network: network compression via factor
  transfer.
\newblock In {\em Proc. NIPS}, page 2760–2769, 2018.

\bibitem{boundary:scheme}
B.~Heo, M.~Lee, S.~Yun, and J.~Y. Choi.
\newblock Knowledge transfer via distillation of activation boundaries formed
  by hidden neurons.
\newblock In {\em Proc. AAAI}, page 3779–3787, 2019.

\bibitem{relation:scheme}
W.~Park, D.~Kim, Yan. Lu, and M.~Cho.
\newblock Relational knowledge distillation.
\newblock In {\em Proc. CVPR}, pages 3967--3976, 2019.

\bibitem{showanddistill:scheme}
M.~Ji, B.~Heo, and S.~Park.
\newblock Show, attend and distill: knowledge distillation via attention-based
  feature matching.
\newblock In {\em Proc. AAAI}, pages 7945--7952, 2021.

\bibitem{distillhubert:scheme}
H.~Chang, S.~Yang, and H.~Lee.
\newblock Distilhubert: speech representation learning by layer-wise
  distillation of hidden-unit bert.
\newblock In {\em Proc. ICASSP}, 2022.

\bibitem{lighthubert:scheme}
R.~Wang and otheres.
\newblock Lighthubert: lightweight and configurable speech representation
  learning with once-for-all hidden-unit bert.
\newblock In {\em Proc. INTERSPEECH}, 2022.

\bibitem{fithubert}
Y.~Lee et~al.
\newblock Fithubert: going thinner and deeper for knowledge distillation of
  speech self-supervised learning.
\newblock In {\em Proc. INTERSPEECH}, 2022.

\bibitem{jasper:scheme}
J.~Li, V.~Lavrukhin, B.~Ginsburg, R.~Leary, O.~Kuchaiev, J.~M. Cohen,
  H.~Nguyen, and R.~T. Gadde.
\newblock Jasper: An end-to-end convolutional neural acoustic model.
\newblock In {\em Proc. INTERSPEECH}, 2019.

\bibitem{crnn:scheme}
B.~Shi et~al.
\newblock An end-to-end trainable neural network for image-based sequence
  recognition and its application to scene text recognition.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  39:2298–2304, 2017.

\bibitem{transformer:scheme}
A.~Vaswani et~al.
\newblock Attention is all you need.
\newblock In {\em Proc. NIPS}, pages 5998--6008, 2017.

\bibitem{librispeech:scheme}
V.~Panayotov, G.~Chen, D.~Povey, and S.~Khudanpur.
\newblock Librispeech: an asr corpus based on public domain audio books.
\newblock In {\em Proc. ICASSP}, pages 5206--5210, 2015.

\bibitem{asr1}
S.~Kim et~al.
\newblock Integer-only zero-shot quantization for efficient speech recognition.
\newblock In {\em Proc. ICASSP}, 2022.

\bibitem{asr2}
O.~Hrinchuk, M.~Popova, and B.~Ginsburg.
\newblock Correction of automatic speech recognition with transformer
  sequence-to-sequence model.
\newblock In {\em Proc. ICASSP}, 2020.

\bibitem{asr3}
N.~Zhang, J.~Wang, W.~Wei, x.~Qu, N.~Cheng, and J.~Xiao.
\newblock Cacnet: cube attentional cnn for automatic speech recognition.
\newblock In {\em Proc. IJCNN}, 2021.

\bibitem{transformer-xl}
Z.~Dai, Z.~Yang, Y.~Yang, J.~Carbonell, Q.~Le, and R.~Salakhutdinov.
\newblock Transformer-xl: attentive language models beyond a fixed-length
  context.
\newblock In {\em Proc. ACL}, pages 2978–--2988, 2019.

\bibitem{clova:scheme}
J.~Baek, G.~Kim, j.~Lee, S.~Park, D.~Han, S.~Yun, S.~J. Oh, and H.~Lee.
\newblock What is wrong with scene text recognition model comparisons?
\newblock In {\em Proc. ICCV}, 2019.

\bibitem{svt:scheme}
K.~Wang, B.~Babenko, and S.~Belongie.
\newblock End-to-end scenetext recognition.
\newblock In {\em Proc. ICCV}, page 1457–1464, 2011.

\bibitem{svtp:scheme}
T.~Q. Phan et~al.
\newblock Recognizing text with perspective distortion in natural scenes.
\newblock In {\em Proc. ICCV}, page 569–576, 2013.

\bibitem{iiit:scheme}
A.~Mishra, K.~Alahari, and C.~Jawahar.
\newblock Scene text recognition using higher order language priors.
\newblock In {\em Proc. BMVC}, 2012.

\bibitem{cute:scheme}
A.~Risnumawan et~al.
\newblock {A robust arbitrary text detection system for natural scene images}.
\newblock {\em Expert Systems with Applications}, 41:8027–8048, 2014.

\bibitem{ic03:scheme}
S.~M. Lucas et~al.
\newblock Icdar 2003 robust reading competitions.
\newblock In {\em Proc. ICDAR}, page 682–687, 2003.

\bibitem{ic13:scheme}
D.~Karatzas et~al.
\newblock Icdar 2013 robust reading competition.
\newblock In {\em Proc. ICDAR}, page 1484–1493, 2013.

\bibitem{ic15:scheme}
D.~Karatzas et~al.
\newblock Icdar 2015 competition on robust reading.
\newblock In {\em Proc. ICDAR}, page 1156–1160, 2015.

\bibitem{mj:scheme}
M.~Jaderberg, K.~Simonyan, A.~Vedaldi, and A.~Zisserman.
\newblock Synthetic data and artificial neural networks for natural scene text
  recognition.
\newblock In {\em Proc. NIPS}, 2014.

\bibitem{st:scheme}
A.~Gupta, A.~Vedaldi, and A.~Zisserman.
\newblock Synthetic data for text localisation in natural images.
\newblock In {\em Proc. CVPR}, 2016.

\bibitem{rosetta:scheme}
F.~Borisyuk et~al.
\newblock Rosetta: large scale system for text detection and recognition in
  images.
\newblock In {\em Proc. ACM SIGKDD}, page 71–79, 2018.

\bibitem{starnet:scheme}
W.~Liu, C.~Chen, K.~K. Wong, and Z.~Su.
\newblock Star-net: A spatial attention residue network for scene text
  recognition.
\newblock In {\em Proc. BMVC}, 2016.

\bibitem{textocr}
A.~Singh, G.~Pang, M.~Toh, J.~Huang, W.~Galuba, and T.~Hassner.
\newblock {Textocr: towards large-scale end-to-end reasoning for
  arbitrary-shaped scene text}.
\newblock In {\em Proc. CVPR}, page 8802–8812, 2021.

\bibitem{ocr2}
X.~Xu, J.~Chen, J.~Xiao, L.~Gao, F.~Shen, and H.~T. Shen.
\newblock {What machines see is not what they get: fooling scene text
  recognition models with adversarial text images}.
\newblock In {\em Proc. CVPR}, pages 12304--–12314, 2020.

\bibitem{openseq2seq}
O.~Kuchaiev, B.~Ginsburg, I.~Gitman, V.~Lavrukhin, J.~Li, H.~Nguyen, C.~Case,
  and P.~Micikevicius.
\newblock Mixed-precision training for nlp and speech recognition with
  openseq2seq.
\newblock {\em arXiv preprint arXiv:1805.10387}, 2018.

\bibitem{ginsburg-et-al:scheme}
B.~Ginsburg, P.~Castonguay, O.~Hrinchuk, O.~Kuchaiev, V.~Lavrukhin, R.~Leary,
  J.~Li, H.~Nguyen, and J.~M. Cohen.
\newblock Stochastic gradient methods with layer-wise adaptive moments for
  training of deep networks.
\newblock {\em arXiv preprint arXiv:1905.11286}, 2019.

\bibitem{Heafield2011KenLMFA}
K.~Heafield.
\newblock Kenlm: faster and smaller language model queries.
\newblock In {\em Proc. EMNLP}, 2011.

\bibitem{nemo:scheme}
O.~Kuchaiev et~al.
\newblock Nemo: a toolkit for building ai applications using neural modules.
\newblock {\em arXiv preprint arXiv:1909.09577}, 2019.

\bibitem{adadelta:scheme}
M.~D. Zeiler.
\newblock Adadelta: an adaptive learning rate method.
\newblock {\em arXiv preprint arXiv:1212.5701v1}, 2012.

\bibitem{quartznet:scheme}
S.~Kriman, K.~Beliaev, B.~Ginsburg, J.~Huang, O.~Kuchaiev, V.~Lavrukhin,
  R.~Leary, J.~Li, and Y.~Zhang.
\newblock Quartznet: deep automatic speech recognition with 1d time-channel
  separable convolutions.
\newblock {\em arXiv preprint arXiv:1910.10261}, 2019.

\bibitem{target1}
Y.~Xia et~al.
\newblock Deliberation networks: sequence generation deyond one-pass decoding.
\newblock 2017.

\bibitem{imputer}
W.~Chan, C.~Saharia, G.~Hinton, M.~Norouzi, and N.~Jaitly.
\newblock Imputer: sequence modelling via imputation and dynamic programming.
\newblock In {\em Proc. ICML}, 2020.

\bibitem{mask-ctc}
Y.~Higuchi, S.~Watanabe, N.~Chen, T.~Ogawa, and T.~Kobayashi.
\newblock Mask ctc: non-autoregressive end-to-end asr with ctc and mask
  predict.
\newblock In {\em Proc. INTERSPEECH}, 2020.

\end{thebibliography}
