\begin{table}[!htb]
    \begin{minipage}{.45\linewidth}
      \centering
        \ifeccv
        \caption{The impact of multi-view fusion architecture. Legend: TE - Transformer Encoder. MHA - Multi-head Attention. $l$ refer to no. of stacked layers. $h$ refer to no. of heads in attention layer.}
        \fi
        \input{./figures/ablation_mha_heads_reduced}
        \ifeccv
        \else
        % \vspace{-5.8pt}
        \caption{The impact of multi-view fusion architecture. Legend: TE - Transformer Encoder. MHA - Multi-head Attention. $l$ refer to no. of stacked layers. $h$ refer to no. of heads in attention layer.}
        \fi
        \label{tab:num_views}
    \end{minipage}%
    \hspace{.1\linewidth}
    \begin{minipage}{.45\linewidth}
      \centering
      \ifeccv
      \caption{The impact of various fusion methods. Note that processing all views together with an early fusion as performed in FLEX outperforms the other variations by a large margin.}
      \fi
      \input{./figures/ablation_fusion_layer_reduced}
      \ifeccv
      \else
      \vspace{-12pt}
      \caption{The impact of various fusion methods. Note that processing all views together with an early fusion as performed in FLEX outperforms the other variations by a large margin.}
      \fi
      \label{tab:2D_backbone}
    \end{minipage} 
\vspace{-13pt }
\end{table}