\sectiontinyvert{Related work}

\label{sec:related}
%\srm{For each of the following sections, state which algorithm is e2e, uses temporal data, predicts angles in a supervised or semi-supervised fashion.}
%We divide this section into three parts, describing single view pose estimation, multi-view pose estimation, and motion reconstruction.

\paragraphtinyvert{Pose Estimation using a Single View}
Pose estimation receives significant interest in computer vision. Before the deep era, it was approached using heuristics such as physical priors \cite{Sarafianos_2016}. %Sarafianos \etal~\shortcite{Sarafianos_2016} provide a comprehensive review of  algorithms of the pre-deep era.
The emergence of deep learning and large datasets~\cite{h36m_pami,CMU:mocap,3DPWvonMarcard2018,footballDS}, have led to significant advances. 
Pose estimation methods can generally be divided into two groups. The first  infers 3D locations directly from images or videos \cite{firstSingleViewCNN,Pavlakos_coarse_to_fine,zhu2020nbaplayers,tekin2016structured,sun2018integral,cheng20203d,habibie2019wild}. 
The second, aka \emph{lifting}, applies two steps: (i) estimating 2D poses and (ii) lifting them to 3D space~\cite{martinez2017simple,pavllo20193d,fang2018learning,sym12071116,liu2019improving,Hossain_2018,Shan2021ImprovingRA}.  
%Most methods in this group use state-of-the-art 2D pose predictors~\cite{chen2018cascaded,Cao:2018} to obtain joint locations in 2D. 
%Some would assert that input images are more descriptive, but on the other hand this approach enjoys intermediate supervision. 
The first group benefits from directly using images, which are more descriptive compared to 2D joint locations. The second gains from using intermediate supervision.
%
%Martinez \etal~\shortcite{martinez2017simple} show that given ground-truth 2D key-points, it is possible to achieve a satisfactory 3D pose prediction using simple convolutional networks. Pavllo \etal~\shortcite{pavllo20193d} %continue with the same approach and 
%extend this to show that using temporal information improves performance. 
%
%Recently, as part of the transformers hype, Llopart\shortcite{llopart2020liftformer} and Lin~\shortcite{lin2020endtoend} works demonstrates that these kind of architectures can also handle 3D human pose prediction task.
%\brchange{More recently, transformers based techniques have been presented~\mbox{~\cite{llopart2020liftformer,lin2020endtoend,liu2020attention}}.}{More recently, different techniques have been explored as transformers and graph convolutional have been presented ~\cite{llopart2020liftformer,lin2020endtoend,liu2020attention,li2021exploiting,wang2020motion,hu2021conditional}}
Recently, transformers and convolutional graph based methods were shown to improve performance~\cite{llopart2020liftformer,lin2020endtoend,liu2020attention,li2021exploiting,wang2020motion,hu2021conditional,Human_Mesh_Recovery_from_Multiple_Shots}.

\paragraphtinyvert{Pose Estimation using Multiple Views} %\label{rel_work_pos_multi_view}
The growing availability of synchronized video streams taken by multiple cameras has contributed to the emergence of multi-view algorithms. Such algorithms exploit the diversity in camera views to predict more accurate 3D poses. All works described below predict pose and many of them analyze each frame individually. On the other hand, our model, FLEX, reconstructs motion and exploits temporal information. 

Most works in the multi-view setting rely on lifting from 2D to 3D space.
Early works~\cite{Belagiannis_2016_IEEE,detection_complete_graphs,psm_2013} estimate the input 2D pose from single images, while later works~\cite{dong2019fast,qiu2019cross,iskakov2019learnable,he2020epipolar,kocabas2019selfsupervised,chen2019weaklysupervised,chu_and_pan_semisupervised,rhodin2018learning,kadkhodamohammadi2019generalizable,chu2021partaware} obtain the 2D pose by running a CNN over 2D poses given in multiple views; resulting in an increase in 2D pose prediction accuracy.
After estimating the 2D poses, most works apply heuristics such as triangulation or pictorial structure model (PSM). FLEX is one of the few works \cite{iskakov2019learnable,tu2020voxelpose} that present an end-to-end model. 

Several methods 
% have been explored to 
use multi-view data to improve the 2D pose estimation.
%in a multi-view setting.
% One strategy 
Some use the camera parameters to find the matching epipolar lines % between different views 
such that features gathered from several cameras are aggregated \cite{qiu2019cross,he2020epipolar}. 
% Also, one may use camera parameters to generalize to new camera setups that are not part of the train data \cite{he2020epipolar}.
Chen \etal~\shortcite{chen2019weaklysupervised} learn a geometric representation in latent space with an encoder-decoder.

Several works~\cite{kocabas2019selfsupervised,chu_and_pan_semisupervised,wandt2020canonpose,tome2018rethinking} use self-supervision, hence need no 3D ground-truth.
% to mitigate the scarcity of 
%and show that reasonable results can be achieved even when 3D ground-truth is scarce. 
Their main idea is to project the predicted 3D joints (using real or estimated camera parameters) and expect consistency with 2D input joints.
%
%Recent techniques \cite{huang2019deepfuse,zhang2020fusing} exploit more sensors during data capturing, such as IMU, to estimate a more accurate 3D pose.
Recent techniques \cite{huang2019deepfuse,zhang2020fusing} exploit more sensors, such as IMU, during data capturing.
 
%Kocabas \etal show a variation of their algorithm where camera parameters are estimated, and Chu and Pan introduce a network that estimates the camera parameters.

Current state-of-the-art results are attained by Iskakov \etal~\cite{iskakov2019learnable}, Tu \etal~\cite{tu2020voxelpose} and Reddy \etal~\cite{Reddy2021TesseTrackEL}. They use end-to-end networks, and present a volumetric approach, where 2D features are un-projected from individual views to a common 3D space, using camera parameters.
Sun \etal~\shortcite{sym12071116} show that synthetic generation of additional views helps produce more accurate lifting. 

At inference time, some of the aforementioned works expect monocular inputs \cite{sym12071116,he2020epipolar,chen2019weaklysupervised,chu_and_pan_semisupervised} and some, including FLEX, get multi-view inputs \cite{iskakov2019learnable,tu2020voxelpose,qiu2019cross}. The advantage of the first is the use of monocular data that is more common, and of the second is better results on multi-view settings. 

Epipolar Transformers~\cite{he2020epipolar} attend to spatial locations on an epipolar line in a \emph{single} view and 
query it using one joint in a query view. 
A concurrent work, TransFusion~\cite{ma2021transfusion},
% \sr{applies a transformer on concatenated spatial features from multiple views.}
\sr{applies a transformer on inter and intra-view features.}
% concatenates spatial features from multiple views, and applies a transformer on top of them.

In the absence of camera parameters,
% a setting where camera parameters are not given, 
most of the methods cannot be used. Some estimate rotation assuming the translation is given \cite{kocabas2019selfsupervised,bachmann2019motion} or engage an extra effort to estimate the camera parameters \cite{chu_and_pan_semisupervised,wandt2020canonpose,chen2021deductive,usman2021metapose,human_pose_calib_unsync}. 
Such an effort is not required by FLEX as it uses no camera parameters whatsoever. %In Section~\ref{sec:experiments}, we show that our strategy significantly outperforms works that estimate the camera parameters \cite{chu_and_pan_semisupervised,wandt2020canonpose,chen2021deductive}.
%\sr{\st{The relative transformation between cameras is usually calculated from the extrinsic parameters. However, some algorithms estimate it directly. To simplify the text of this work, we refer to such algorithms as estimating extrinsic parameters.}}


\paragraphtinyvert{Rotation and Motion Reconstruction}
%\dcc{This subsection seems to be focusing on "inverse kinematics, while from the title I expected to read about using the temporal axis}
Pose estimation may suffice for many applications; however, pose alone does not fully describe the motion and the rotations associated with the joints. \emph{Rotation reconstruction} relates to the prediction of joint rotation angles, while \emph{motion reconstruction} requires the prediction of bone lengths associated with them.
%
Many works explore the task of \emph{3D shape recovery}~\cite{SMPL:2015,Kanazawa:2018,Kolotouros:2019:ICCV,kocabas2020vibe,kissos2020weak,yoshiyasu2018skeleton,kanazawa2019learning,habermann2020deepcap,luo20203d,choi2021static}, focusing  on human mesh prediction along with joint rotations. Most of them do not guarantee temporal coherence, \eg, bone length may vary across time frames. 

Other works~\cite{pavllo2018quaternet,mao2020learning} 
focus on motion generation. Given a series of human motions, they predict future motions, using various techniques such as temporal supervision and graph convolutional networks (GCN).
Similar to us, human motion reconstruction methods~\cite{zhou2016deep,mehta2020xnect,shi2020motionet,video_motion_capture_cpm} focus on the temporal coherence of the body, where the bone lengths are fixed over time and rotations are smooth. 


