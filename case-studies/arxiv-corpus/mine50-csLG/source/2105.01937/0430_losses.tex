\iftog
\subsection{Losses}
\fi

%The losses used in our network follow the ones in MotioNet~\cite{shi2020motionet}, hence we describe them only briefly. % and emphasize the modifications we made in order to accommodate a multi-view setting. One main difference is that all loss terms now hold an additional index for the view.
%One main difference is that the dimension of the tensors in the loss terms is larger by one compared to MotioNet, as each tensor is a sequence of $K$ views.

We employ five loss functions. Our losses are inspired by Shi~\etal~\cite{shi2020motionet} and are enhanced to encompass the multitude of views.

\paragraphtinyvert{Joint Position Loss {\normalfont (the main loss)}} 
$\Loss_{\text{P}}$ ensures that joints in the extracted positions are in their correct 3D positions:

\setlength{\abovedisplayskip}{-7pt plus 2pt minus 2pt}
\setlength{\belowdisplayskip}{5pt plus 2pt minus 2pt}

\begin{align}
\Loss_{\text{P}} = \bbe_{\bP_{s,q,r}\sim \pp}\left[ \Vert FK(\tilde{\bss}, \tilde{\bq},\tilde{\br}_{pos_0})-\bP_{s,q,r_{pos_0}}\Vert^2 \right],
\end{align}

where $\bP_{s,q,r}\in \bbr^{T\times 3J\times K}$ denotes a 3D motion sequence, $\pp$ represents the distribution of 3D motion sequences in our dataset,
and $\tilde{r}_{pos_0}, r_{pos_0}$ stand for global position and rotation of the predicted and given root respectively, where the location is set to $(0,0,0)$, but the rotation is unchanged.

\paragraphtinyvert{Skeleton Loss}
$\Loss_{\text{S}}$ stimulates the skeleton branch of the network, $F_S$ and $E_S$, to correctly extract the skeleton $\bss$:
\ifeccv
    \setlength{\abovedisplayskip}{-7pt plus 2pt minus 2pt}
    \setlength{\belowdisplayskip}{5pt plus 2pt minus 2pt}
\fi

\begin{align}
\Loss_{\text{S}} = 
\bbe_{\bP_{s,q,r} \sim \pp} 
\left[ \Vert E_S(F_S(\bV_{s,q,r}))-\bss\Vert^2 \right].
\end{align}
%where $\bV_{s,q,r}\in\bbr^{T\times 3J\times K}$ denotes the 2D ground-truth of $\bP_{s,q,r}$.
\paragraphtinyvert{Adversarial Rotation Loss}
Our network learns to output rotations with natural velocity distribution using adversarial training. To achieve this, instead of focusing on rotation absolute values, like Kanazawa~\etal~\shortcite{Kanazawa:2018} we focus on the temporal differences of joint rotations. We create a discriminator $D_j$ for each joint. 
Note that the loss involving $D_{j\neq 0}$ takes the rotation values from $\tilde{q}$ while the loss involving $D_0$ takes the rotation values from $\tilde{r}$. It reads as
%Say the index of the root joint is zero. Then the loss involving $D_0$ is a bit different, since it takes the rotation values from $\tilde{r}$ rather than from $\tilde{q}$. It reads as
\setlength{\abovedisplayskip}{-5pt plus 2pt minus 2pt}
\setlength{\belowdisplayskip}{-5pt plus 2pt minus 2pt}

\begin{eqnarray*}
\Loss_{Q\_{GAN}_{j\ne 0}} \! & = \!&\bbe_{q \sim \qq}
\left[ \Vert D_j(\Delta_t q_j) \Vert^2 \right]+ \bbe_{\bP_{s,q,r}\sim \pp} \left[ \Vert 1-D_j(\Delta_t E_Q(F_Q((\bV_{s,q,r}))_{q_j}\Vert^2\right]
\end{eqnarray*}
% and
%
\ifeccv
    \setlength{\abovedisplayskip}{0pt plus 2pt minus 2pt}
    \setlength{\belowdisplayskip}{5pt plus 2pt minus 2pt}
\fi
% \begin{align}
\begin{eqnarray}
\Loss_{{Q\_{GAN}}_{j= 0, k}} & =  &\ \bbe_{q \sim \qq}
\left[ \Vert D_j(\Delta_t q_j) \Vert^2 \right] \\ 
\nonumber
 &  + & %\mkern-20mu 
 \ \bbe_{\bP_{s,q,r}\sim \pp} 
%  \mkern-7mu 
 \left[ \Vert 1-D_j(\Delta_t E_Q(F_Q((\bV_{s,q,r}))_{r_{{rot}_k}}\Vert^2\right],
% \end{align}
\end{eqnarray}
where $\qq$ stands for the distribution of natural joint angles in the dataset, $E_Q(F_Q(\cdot))_{q_j}$ denotes the predicted rotations of the $j$th joint, ${{E_Q(F_Q(\cdot))_r}_{rot_k}}$ represents the predicted rotation of the pelvis joint relative to camera $k$, and $\Delta_t$ denotes temporal differences. 

\paragraphtinyvert{Global Root Position Loss}
We estimate the depth parameter, $Z_r\in\bbr^{T\times K}$, by minimizing:
\ifeccv
    \setlength{\abovedisplayskip}{0pt plus 2pt minus 2pt}
    \setlength{\belowdisplayskip}{7pt plus 2pt minus 2pt}
\fi
\begin{align}
\Loss_{\text{R}} = \bbe_{\bP_{s,q,r}\sim \pp}\left[ \Vert E_Q(F_Q(\bV_{s,q,r}))_{r_{pos_z}} - Z_r\Vert^2 \right],
\end{align}
where $Z_r$ is the depth of the ground-truth root, and  $E_Q(F_Q(\cdot))_{r_{pos_z}}$ is the depth of the predicted root. Note that $Z_r$ consists of values for all views and all frames.

\paragraphtinyvert{Foot Contact Loss}
We predict whether each foot contacts the ground in each frame and train the network via

\setlength{\abovedisplayskip}{-2pt plus 2pt minus 2pt}
\setlength{\belowdisplayskip}{7pt plus 2pt minus 2pt}

\begin{align}
\Loss_{\text{F}} = \bbe_{\bP_{s,q,r}\sim \pp}\left[ \Vert E_Q(F_Q(\bV_{s,q,r}))_f - \bff\Vert^2 \right],
\end{align}
where $E_Q(F_Q((\cdot))_f$ denotes the predicted foot contact label part ($\tilde{\bff}\in\{0,1\}^{T\times 2}$).
We encourage the velocity of foot positions to be zero during contact frames, by
\begin{align}
\Loss_{\text{FC}} = \bbe_{\bP_{s,q,r}\sim \pp}\left[ \Vert \bff_i \sum_j \Delta_t FK(\tilde{\bss}, \tilde{\bq}, \tilde{\br})_{f_i} \Vert^2 \right],
\end{align}
where $FK(\cdot,\cdot,\cdot)_{f_i}\in\mathbb{R}^{T\times 3}$ and $\bff_i$ denote the positions and the contact labels of one of the feet joints ($i\in \mathit{left}, \mathit{right}$), and $\sum_j$ sums the components for all axes.

Altogether, we obtain a total loss of:
\begin{eqnarray}
\Loss \!&=\!& \Loss_{\text{P}} +
\lambda_{\text{S}} \Loss_{\text{S}}
%
\lambda_{\text{Q}} \left(
\sum_{j\ne 0}\Loss_{{\text{Q\_GAN}}_j} +
\sum_{j=0,k}\Loss_{{\text{Q\_GAN}}_{j,k}}
\right) \\ \nonumber \!&+\!&
%
\lambda_{\text{R}}\Loss_{\text{R}} +
\lambda_{\text{F}}\Loss_{\text{P}_{F}} +
\lambda_{\text{FC}}\Loss_{\text{P}_{FC}}.
\end{eqnarray}
In most experiments we use $\lambda_{\text{S}}=0.1$, $\lambda_{\text{Q}}=1$, %$\lambda_{P_{EE}}=1.2$, 
$\lambda_{\text{R}}=1.3$,  $\lambda_{\text{F}}=0.5$ and $\lambda_{\text{FC}}=0.5$. 

In the \ifarxiv{appendix  }\else{supplementary material }\fi we provide more implementation details, such as the description of each architectural block; in particular the novel fusion layers $F_S$ and $F_Q$. We discuss the advantages of early vs. middle and late fusion, and describe how we improve skeleton topology comparing to our single-view baseline. We also provide a detailed description of the datasets, a discussion of 2D pose estimators, and a description of the ground-truth we use.