\sectiontinyvert{Data}  \label{sec:data}
\subsectiontinyvert{Train and Evaluation} 
We train our model on the Human3.6M dataset~\cite{h36m_pami,IonescuSminchisescu11}. We evaluate FLEX on the Human3.6M and the KTH Multi-view Football II~\cite{footballDS} datasets, and on synthetic multi-person video streams captured by dynamic cameras.

Human3.6M~\cite{h36m_pami,IonescuSminchisescu11} is a dataset of 3.6 Million accurate 3D Human poses, acquired by recording the performance of 5 female and 6 male subjects, under 4 different viewpoints.
This dataset holds a diverse set of motions and poses encountered as part of 17 typical human activities such as talking on the phone, walking, and eating.
As recommended on the Human3.6M dataset page, we use subjects S1, S5, S6, S7, and S8 for training and subjects S9 and S11 for testing.
This dataset grants licenses free of charge that are limited to academic use only. More information and access to raw data are provided on the dataset webpage\footnote{\url{https://vision.imar.ro/human3.6m/}}.

KTH Multi-view Football II~\cite{footballDS} is a dataset of video streams from three synchronized cameras with 800-time frames per camera. The streams depict two different players (in separate streams), where each player has two sequences in varying levels of scene complexity.
%
This dataset is unique in the sense that the cameras are dynamic, hence the approximation of camera extrinsic parameters is very challenging.
%
We adjust the skeleton topology of the KTH dataset to match the topology of Human3.6M in the following way. KTH extracts 14 joints (top-head, mid-head, shoulders, hips, knees, feet, elbows, and hands). We create root (pelvis) and neck joints by averaging the hips and the shoulders respectively and then create a spine joint by averaging the root and the neck. Then we draw bones according to the Human3.6M skeleton topology.
The raw data can be accessed and downloaded from the dataset webpage\footnote{\url{https://www.csc.kth.se/cvap/cvg/?page=footballdataset2}}. This dataset may only be used for academic research and not for commercial use.

Ski-Pose PTZ-Camera~\cite{ski_ptz} is a 6 cameras' multi-view pant-tilt-zoom-camera (PTZ) dataset. It features competitive alpine skiers performing giant slalom runs.
It holds 20K images, with a single skier in each. 
The cameras can rotate, but their locations are fixed.
This dataset provides labels for the skiers’ 3D locations in each frame, their projected 2D locations, and per-frame calibration of the PTZ cameras.
In the dataset webpage\footnote{\url{https://www.epfl.ch/labs/cvlab/data/ski-poseptz-dataset/}} there are more descriptions of the dataset as well as download instructions.

Our synthetic videos are generated using Mixamo~\cite{mixamo} and Blender~\cite{blender}. We maintain two scenes with two interacting subjects in each. One scene illustrates a boxing arena, and one shows Macarena dancers. We create as many cameras as we wish, with full control on each camera's dynamic motion trajectory. Each synthetic camera outputs a video of the scene, taken from its view. To evaluate FLEX on these videos, we use a network that has been trained on the Human3.6M dataset, introducing satisfactory generalization abilities of our model.

\subsectiontinyvert{Input data} The input to our network is 2D joint locations per frame, accompanied by a confidence value. We train our network with several variations of input data.
\paragraphtinyvert{Ground truth 2D pose} Obviously, training with ground truth input data yields the best possible results. We use the 2D labeling provided by the Human3.6M dataset. 

\paragraphtinyvert{Estimated 2D pose} To simulate dynamic capture environments, where 2D labels are not available, we use several state-of-the-art 2D pose estimators as 2D backbones. In our ablation studies we demonstrate the dependency on a good estimator. The estimators that we use are OpenPose~\cite{Cao:2018}, CPN~\cite{chen2018cascaded}, and the one used by Iskakov \etal~\shortcite{iskakov2019learnable} (who base their 2D estimation on the ”simple baselines” architecture~\cite{xiao2018simple}). 
OpenPose and Iskakov \etal provide confidence values that we add to the network input. CPN does not provide these values, hence we assign identical confidence values for all joints when using it.
%
While OpenPose and CPN are dedicated 2D pose estimators, Iskakov \etal's 2D estimator is part of a 3D pose estimator. To extract the 2D pose we retrain their model using its given code and save intermediate values.
The 2D pose estimation computed by Iskakov \etal~\shortcite{iskakov2019learnable} uses camera distortion parameters. In addition to being free of extrinsic camera parameters, we are strict about not using the intrinsic ones as well (see Section 3 in the main paper); hence, we retrain Iskakov \etal~\shortcite{iskakov2019learnable} without those parameters.

Skeleton topology may vary between the aforementioned 3D datasets and 2D poses predicted by backbone algorithms. To mitigate this, we make adjustments to the predicted 2D joints. Openpose \cite{Cao:2018} extract 16 joints (root, neck, mid-head, top-head, shoulders, hips, knees, feet, elbows, and hands). These joints exist in the aforementioned datasets as well. In addition, a spine joint, which exists only in the 3D datasets, is artificially added (calculated as the 2D spatial average between the root and the neck joint).
For the CPN \cite{chen2018cascaded} 2D prediction, we simply use the values computed by Pavllo~\etal~\cite{pavllo20193d} and provided in their project page, which already possess the requested topology. Lastly, Iskakov \etal~\shortcite{iskakov2019learnable} predict the exact joints required by the aforementioned 3D datasets.

At inference time, when videos from the wild are used, we use a network that was trained using an estimated 2D pose and make sure that during inference, the exact 2D backbone that was used for training, is applied.

\subsectiontinyvert{Ground truth}
During train time we use 3D joint location ground truth per view, plus rotation ground truth for the discriminator. 
In contrast to location ground truth, rotation ground truth is required only once, no matter how many views we have.
%Rotation ground truth requires root rotation per view, and since all the rotations between joints are identical for all views, we need one GT value for each joint rotation, with no dependency on views number. 
During test time we need none of the above.

%\paragraph{Preprocessing}
%We apply two normalization steps. First, for each frame we subtract the root position from every joint location. Second, we calculate the mean and standard deviation of joint position per joint over the entire dataset. Then we subtract the empirical mean from each joint and divide it by the empirical standard deviation. Predicted motion is thus pelvis centered; We convert it to its relative 2D location by adding the 2D difference (aka velocity) of pelvis location where each frame is compared to the one preceding it.

% \subsection{Network Structure}
% \input{./sup_mat/sup_network_structure}




