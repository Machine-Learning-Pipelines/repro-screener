\section{Related Work}

\textbf{Time Series Forecasting}: While Convolutional Neural Network (CNN) and Recurrent Neural network (RNN) based architectures \cite{RangapuramDeepState,salinas2020deepar} outperform traditional methods like ARIMA \cite{BoxArima} and exponential smoothing methods \cite{hyndman2018forecasting}, the addition of attention layers \cite{vaswani2017attention} to model time series forecasting has proven to be very beneficial across different problem settings \cite{10.1145/3292500.3330662,lai2018modeling,qin2017dual,wu2021autoformer}. Attention allows direct pair-wise interaction with eccentric events (like holidays) and can model temporal dynamics inherently unlike RNNs and CNNs that fail to capture long-range dependencies directly. Recent work like Reformer \cite{kitaev2020reformer}, Linformer, \cite{wang2020linformer}, Triformer \cite{cirstea2022triformer} and Informer \cite{zhou2020informer} have focused on reducing the quadratic complexity of modeling pair-wise interactions to a lower complexity with the introduction of restricted attention layers. Consequently, they can predict for longer forecasting horizons but are hindered by their capability of aggregating features and maintaining the resolution required for far horizon forecasting. 

\textbf{U-Net}: The Yformer model is inspired by the famous U-Net architecture introduced in \cite{ronneberger2015u} originating from the field of medical image segmentation. The U-net architecture is capable of compressing information by aggregating over the inputs and up-sampling embeddings to the same resolutions as that of the inputs from their compressed latent features. While there exist U-Net based transformer architectures within the vision community \cite{petit2021u,zhou2021nnformer}, to the best of our knowledge U-Net based transformer architecture for time series forecasting remains unexplored. Current transformer architectures like the Informer \cite{zhou2020informer} do not utilize up-sampling techniques even though the network produces intermediate multi-resolution feature maps. Our work aims to capitalize on these multi-resolution feature maps and use the U-net shape effectively for the task of time series forecasting. In \cite{perslev2019u}, the authors have successfully applied U-Net architecture for the task of time series segmentation, illustrating superior results in the task. These motivate the use of a U-Net-inspired architecture for time series forecasting as current methods fail to couple sparse attention mechanism with the U-Net shaped architecture for time series forecasting.

\textbf{Reconstruction Loss}: Reconstruction loss is widely used in the domain of time series outlier detection \cite{tungbcc19} and is less popular within the Time Series Forecasting community. Although recent time series forecasting architecture like the N-Beats \cite{nbeats} tries to reconstruct part of the past time steps (backcasting) as an effective method to improve model performance, the majority of transformer-based time series forecasting architectures \cite{10.1145/3292500.3330662,lai2018modeling,tft} fail to utilize the reconstruction loss as an auxiliary target to improve performance. In \cite{jawed2019multi}, the authors demonstrate a multi-task approach for time series forecasting that couples an auxiliary task of predicting known channels along with the target channel for improved regularization. Additionally, recent studies \cite{le2018supervised} have shown that the addition of the reconstruction term to any loss function generally provides uniform stability and bounds on the generalization error, therefore leading to a more robust model overall with no negative effect on the performance. 


