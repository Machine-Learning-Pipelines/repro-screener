\section{Experiments}

\subsection{Datasets}
We compare the experimental results of our proposed YFormer architecture, with that of the Informer on three real-world public datasets.

\textbf{ETTh1 and ETTh2} (Electricity Transformer Temperature\footnote{https://
github.com/zhouhaoyi/ETDataset.}):
These real-world datasets for the electric power deployment introduced by \cite{zhou2020informer} combine short-term periodical patterns, long-term periodical patterns, long-term trends, and irregular patterns. The data consists of load and temperature readings from two transformers at two different stations with varying load conditions. The ETTm1 dataset is generated by splitting ETTh1 dataset into 15-minute intervals. The dataset has six features and 70,080 data points in total. For easy comparison, we kept the splits for train/val/test consistent with the published results in \cite{zhou2020informer}, where the available 20 months of data is split as 12/4/4. For the Univariate setting, 'OT' (Oil Temperature) was set as the target value.

\textbf{ECL} (Electricity Consuming Load\footnote{https://archive.ics.uci.edu/ml/
datasets/ElectricityLoadDiagrams20112014}): This electricity dataset represents the electricity consumption from 2011 to 2014 of 370 clients recorded in 15-minutes periods in Kilowatt (kW). We split the data into 15/3/4 months for train, validation, and test respectively as in \cite{zhou2020informer}. For the Univariate setting, 'MT 320' was set as the target value.

\subsection{Experimental Setup}
\textbf{Baseline}: Our main baseline is the Informer architecture. As a second baseline, we also compare the second-best performing model which is the Informer that uses canonical attention module \cite{zhou2020informer} represented as Informer$^{\dag}$. Furthermore, we also compare against DeepAR \cite{salinas2020deepar}, and LogTrans \cite{li2019enhancing} for the univariate setting, and LSTnet \cite{lai2018modeling} for the multivariate setting as they outperform the Informer baseline for certain forecasting horizons. For a quick analysis, we present the percent improvement achieved by the Yformer over the current best results as the final column in Tables \ref{tbl:univariate}, \ref{tbl:multivariate}.

For a fair comparison, we retain the design choices from the Informer baseline like the history input length $(T)$ for a particular forecast length ($\tau$), so that any performance improvement can exclusively be attributed to the architecture of the Yformer model and not to an increased history input length. We performed a grid search for learning rates of $\{0.001, 0.0001\}$, $\alpha$-values of $\{0, 0.3, 0.5, 0.7, 1\}$, number of encoder and decoder blocks  $I = \{2, 3, 4\}$ while keeping all the other hyperparameters the same as the Informer. Furthermore, Adam optimizer and an early stopping criterion with a patience of three epochs was used for all experiments. To counteract overfitting, we tried dropout with varying ratios but interestingly found the effect to be minimal in the results. Therefore, we adopt weight-decay for our experiments with factors $\{0, 0.02, 0.05\}$ for additional regularization. We select the optimal hyperparameters based on the lowest validation loss. 

\begin{table*}[t]
    \caption{Univariate results for three datasets (four cases) with different prediction lengths $\tau \in \{24,48,96,168,288,336,672,720,960\}$.}
    \centering
    \fontsize{9pt}{9pt}\selectfont
    \centering
    \resizebox{1\textwidth}{!}{%
    \begin{tabular}{c|c|c|c|c|c|c|SS|}
    \toprule[1.0pt]
    \multicolumn{2}{c|}{Methods}    & {Yformer}     & {Informer} & {Informer$^{\dag}$} & {LogTrans}  & {DeepAR} & \multicolumn{2}{c|}{Improvement\%} \\
    \midrule[0.5pt]
    \multicolumn{2}{c|}{Metric}     & MSE~~MAE     & MSE~~MAE               & MSE~~MAE                    & MSE~~MAE           & MSE~~MAE            & {MSE}            & {MAE}\\
    \midrule[1.0pt]
    \multirow{5}{*}{\rotatebox{90}{ETTh$_1$}}      
         &     24   &    \textbf{0.082}~~\textbf{0.230} &   0.098~~0.247    &   \underline{0.092}~~\underline{0.246}     &   0.103~~0.259    &   0.107~~0.280    &   10.87 & 6.50 \\
         &     48   &    \textbf{0.139}~~\textbf{0.308} &   \underline{0.158}~~\underline{0.319}     &   0.161~~0.322    &   0.167~~0.328    &   0.162~~0.327    &   12.03 & 3.45 \\
         &     168  &    \textbf{0.111}~~\textbf{0.268} &   \underline{0.183}~~\underline{0.346}     &   0.187~~0.355    &   0.207~~0.375    &   0.239~~0.422    &   39.34 & 22.54 \\
         &     336  &    \textbf{0.195}~~\textbf{0.365} &   0.222~~0.387    &   \underline{0.215}~~\underline{0.369}     &   0.230~~0.398    &   0.445~~0.552    &   09.30 & 1.08 \\
         &     720  &    \textbf{0.226}~~\textbf{0.394}&    0.269~~0.435    &   \underline{0.257}~~\underline{0.421}     &   0.273~~0.463    &   0.658~~0.707    &   12.06 & 6.41 \\
    \midrule[0.5pt]
    \multirow{5}{*}{\rotatebox{90}{ETTh$_2$}}      
         &     24   &    \textbf{0.082}~~\textbf{0.221} &   \underline{0.093}~~\underline{0.240}     &   0.099~~0.241    &   0.102~~0.255     &  0.098~~0.263    &   11.83   & 7.92 \\
         &     48    &   0.172~~0.334 &      \textbf{0.155}~~\textbf{0.314}      &   \underline{0.159}~~\underline{0.317}     &   0.169~~0.348     &  0.163~~0.341    &   -10.97  & -6.37 \\
         &     168   &   \textbf{0.174}~~\textbf{0.337} &   \underline{0.232}~~\underline{0.389}     &   0.235~~0.390    &   0.246~~0.422     &  0.255~~0.414    &   25.00   & 13.37 \\
         &     336   &   \textbf{0.224}~~\textbf{0.391} &   0.263~~\underline{0.417}  &   \underline{0.258}~~0.423  &   0.267~~0.437     &  0.604~~0.607    &   13.18   & 6.24 \\
         &     720   &   \textbf{0.211}~~\textbf{0.382} &   \underline{0.277}~~\underline{0.431}     &   0.285~~0.442    &   0.303~~0.493     &  0.429~~0.580    &   23.83   & 11.37 \\
    \midrule[0.5pt]
    \multirow{5}{*}{\rotatebox{90}{ETTm$_1$}}      
         &     24    &   \textbf{0.024}~~\textbf{0.118} &   \underline{0.030}~~\underline{0.137}     &   0.034~~0.160    &   0.065~~0.202    &   0.091~~0.243    &   20.00   & 13.87 \\
         &     48    &   \textbf{0.048}~~\textbf{0.173} &   0.069~~0.203    &   \underline{0.066}~~\underline{0.194}     &   0.078~~0.220    &   0.219~~0.362    &   27.27   & 10.82 \\
         &     96    &   \textbf{0.143}~~\textbf{0.311} &   0.194~~\underline{0.372}  &   \underline{0.187}~~0.384  &   0.199~~0.386    &   0.364~~0.496    &   23.53   & 16.40 \\
         &     288   &   \textbf{0.150}~~\textbf{0.316}&    \underline{0.401}~~0.554  &   0.409~~\underline{0.548}  &   0.411~~0.572    &   0.948~~0.795    &   62.59   & 42.34 \\
         &     672   &   \textbf{0.305}~~\textbf{0.476} &   \underline{0.512}~~\underline{0.644}     &   0.519~~0.665    &   0.598~~0.702    &   2.437~~1.352    &   40.43   & 26.09 \\
    \midrule[0.5pt]
    \multirow{5}{*}{\rotatebox{90}{ECL}}      
         &     48    &   \textbf{0.194}~~\textbf{0.322} &   0.239~~0.359    &   0.238~~0.368    &   0.280~~0.429    &   \underline{0.204}~~\underline{0.357}     &   4.90    & 9.80 \\
         &     168   &   \textbf{0.260}~~\textbf{0.361} &   0.447~~0.503    &   0.442~~0.514    &   0.454~~0.529    &   \underline{0.315}~~\underline{0.436}     &   17.46   & 17.20 \\
         &     336   &   \textbf{0.269}~~\textbf{0.375} &   0.489~~0.528    &   0.501~~0.552    &   0.514~~0.563    &   \underline{0.414}~~\underline{0.519}     &   35.02   & 27.75 \\
         &     720   &   \textbf{0.427}~~\textbf{0.479} &   \underline{0.540}~~\underline{0.571}     &   0.543~~0.578    &   4.891~~4.047    &   0.563~~0.595    &   20.93   & 19.50 \\
         &     960   &   0.595~~\textbf{0.573} &  \textbf{0.582}~~\underline{0.608}   &   0.594~~0.638    &   7.019~~5.105    &   0.657~~0.683    &   -2.23   & 16.11 \\
    \midrule[0.5pt]
    \multicolumn{2}{c|}{Count}      & {37}     & {3}                 & {0}                          & {0}                & {0}           & {}  & {}    \\
    \midrule[0.5pt]
    \multicolumn{2}{c|}{Average}      & \multicolumn{1}{c}{}     & \multicolumn{1}{c}{}                 & \multicolumn{1}{c}{}           & \multicolumn{1}{c}{}     & \multicolumn{1}{c|}{}           & {19.82}  & {13.62}    \\
    \bottomrule[1.0pt]
    
    \end{tabular}
    }
    \label{tbl:univariate}
    \end{table*}

For easy comparison, we choose two commonly used metrics for time series forecasting to evaluate the Yformer architecture, the MAE and MSE in Equation \ref{eqn:msemae}. We performed our experiments on GeForce RTX 2080 Ti GPU nodes with 32 GB ram and provide results as an average of three runs. The source code \footnote{https://github.com/18kiran12/Yformer-Time-Series-Forecasting} and optimal hyperparameter configurations are made public for reproducibility.


\subsection{Results and Analysis}

This section compares our results with the results reported in the Informer baseline both in uni- and multivariate settings for the multiple datasets and horizons. A direct comparison with the reported results \cite{zhou2020informer} is possible as the experimental setup and the problem settings are kept the same. The best-performing and the second-best models are highlighted in bold and in underline, respectively. 

\begin{table*}[t]
\caption{Multivariate results for three datasets (four cases) with different prediction lengths $\tau \in \{24,48,96,168,288,336,672,720,960\}$.}
    \centering
    \resizebox{1\textwidth}{!}{%
    \fontsize{9pt}{9pt}\selectfont
    \begin{tabular}{c|c|cc|cc|cc|cc|cc|SS|}
    \toprule[1.0pt]
    \multicolumn{2}{c}{Methods}   & \multicolumn{2}{|c}{Yformer}  & \multicolumn{2}{|c}{Informer} & \multicolumn{2}{|c}{Informer$^{\dag}$} & \multicolumn{2}{|c}{LogTrans}  & \multicolumn{2}{|c}{LSTnet}  & \multicolumn{2}{|c|}{Improvement\%}     \\
    \midrule[0.5pt]
    \multicolumn{2}{c|}{Metric}     & MSE                & MAE    & MSE                & MAE               & MSE                          & MAE              & MSE               & MAE             & MSE            & MAE      & {MSE}            & {MAE}        \\
    \midrule[1.0pt]
    \multirow{5}{*}{\rotatebox{90}{ETTh$_1$}} 
    & 24  &\textbf{0.485} & \textbf{0.492} & \underline{0.577}          & \underline{0.549}          & 0.620                   & 0.577                   & 0.686                   & 0.604                   & 1.293                   & 0.901         &15.94 & 10.38         \\
    & 48  &\textbf{0.530} & \textbf{0.537}  & \underline{0.685}          & \underline{0.625}          & 0.692                   & 0.671                   & 0.766                   & 0.757                   & 1.456                   & 0.960        &22.63 & 14.08         \\
    & 168 &\textbf{0.866} & \textbf{0.684} & \underline{0.931}          & \underline{0.752}          & 0.947                   & 0.797                   & 1.002                   & 0.846                   & 1.997                   & 1.214         &06.98 & 09.04         \\
    & 336 &\textbf{1.041} & \textbf{0.803} & 1.128                   & 0.873                   & \underline{1.094}          & \underline{0.813}          & 1.362                   & 0.952                   & 2.655                   & 1.369         &04.84 & 01.23         \\
    & 720 &\textbf{1.098} & \textbf{0.803} & \underline{1.215}          & \underline{0.896}          & 1.241                   & 0.917                   & 1.397                   & 1.291                   & 2.143                   & 1.380         &09.63 & 10.38        \\
    \midrule[0.5pt]
    \multirow{5}{*}{\rotatebox{90}{ETTh$_2$}} 
    & 24  &\textbf{0.412} & \textbf{0.498} & \underline{0.720}          & \underline{0.665}          & 0.753                   & 0.727                   & 0.828                   & 0.750                   & 2.742                   & 1.457        &42.78 &  25.11         \\
    & 48  &\textbf{1.171} & \textbf{0.865} & \underline{1.457}          & \underline{1.001}          & {1.461}                   & 1.077                   & 1.806                   & 1.034                   & 3.567                   & 1.687        &19.63 &  13.59        \\
    & 168 &\textbf{2.171} & \textbf{1.218} & 3.489                   & \underline{1.515}          & 3.485                   & 1.612                   & 4.070                   & 1.681                   & \underline{3.242}          & 2.513        &33.04 &  19.60        \\
    & 336 &\textbf{2.260} & \textbf{1.283} & 2.723                   & 1.340                   & 2.626                   & \underline{1.285}          & 3.875                   & 1.763                   & \underline{2.544}          & 2.591        &11.16 &  0.16        \\
    & 720 &\textbf{2.595} & \textbf{1.337} & \underline{3.467}          & \underline{1.473}          & 3.548                   & 1.495                   & 3.913                   & 1.552                   & 4.625                   & 3.709        &25.15 &  9.23       \\
    \midrule[0.5pt]
    \multirow{5}{*}{\rotatebox{90}{ETTm$_1$}} 
    & 24 &\textbf{0.289} &\textbf{0.363}  & 0.323                   & \underline{0.369}          & \underline{0.306}          & 0.371                   & 0.419                   & 0.412                   & 1.968                   & 1.170         &05.56  &  1.63        \\
    & 48 &\underline{0.486} &\textbf{0.457}  & 0.494                   & 0.503                   & \textbf{0.465}          & \underline{0.470}          & 0.507                   & 0.583                   & 1.999                   & 1.215         &-4.52 &   2.77       \\
    & 96  &\textbf{0.569} &\textbf{0.567} & \underline{0.678}          & 0.614                   & 0.681                   & \underline{0.612}          & 0.768                   & 0.792                   & 2.762                   & 1.542         &16.08 &   7.35       \\
    & 288 &\textbf{0.649} &\textbf{0.593} & \underline{1.056}                   & \underline{0.786}          & 1.162                   & 0.879                   & 1.462                   & 1.320                   & 1.257          & 2.076         &38.54 &   24.55       \\
    & 672 &\textbf{0.772} &\textbf{0.656} & \underline{1.192}          & \underline{0.926}          & 1.231                   & 1.103                   & 1.669                   & 1.461                   & 1.917                   & 2.941         &35.23 &   29.16       \\
    \midrule[0.5pt]
    \multirow{5}{*}{\rotatebox{90}{ECL}}      
    & 48  &\textbf{0.306} &\textbf{0.390} & 0.344                   & \underline{0.393}          & \underline{0.334}          & 0.399                   & 0.355                   & 0.418                   & 0.369                   & 0.445         &08.38 &   0.76       \\
    & 168 &\textbf{0.317} &\textbf{0.387} & 0.368                   & 0.424          & \underline{0.353}          & \underline{0.420}                   & 0.368                   & 0.432                   & 0.394                   & 0.476         &10.20 &   7.86       \\
    & 336 &\textbf{0.323} &\textbf{0.394} & 0.381                   & \underline{0.431}          & 0.381                   & 0.439                   & \underline{0.373}          & 0.439                   & 0.419                   & 0.477         &15.22 &   8.58       \\
    & 720 &\textbf{0.312} &\textbf{0.384} & 0.406                   & 0.443                   & \underline{0.391}          & \underline{0.438}          & 0.409                   & 0.454                   & 0.556                   & 0.565         &20.20 &   12.33       \\
    & 960 &\textbf{0.315} &\textbf{0.388} & \underline{0.460}          & \underline{0.548}          & 0.492                   & 0.550                   & 0.477                   & 0.589                   & 0.605                   & 0.599         &31.52 &   29.20      \\
    \midrule[1.0pt]
    \multicolumn{2}{c}{Count}       & \multicolumn{2}{|c}{39}  & \multicolumn{2}{|c}{0}                 & \multicolumn{2}{|c}{1}                          & \multicolumn{2}{|c}{0}           & \multicolumn{2}{|c}{0}  & \multicolumn{2}{|c|}{}   \\
    \midrule[0.5pt]
    \multicolumn{2}{c|}{Average}      & \multicolumn{1}{c}{}     & \multicolumn{1}{c}{}                 & \multicolumn{1}{c}{}                          & \multicolumn{1}{c}{}            & \multicolumn{1}{c}{}     & \multicolumn{1}{c}{}    & \multicolumn{1}{c}{}   & \multicolumn{1}{c}{}    & \multicolumn{1}{c}{}    & \multicolumn{1}{c|}{}   & {18.41}  & {11.85}    \\
    \bottomrule[1.0pt]
    \end{tabular}
    }
    
    \label{tbl:multivariate}
\end{table*}


\textbf{Univariate}: The proposed Yformer model is able to outperform the Informer baseline in 37 out of the 40 available tasks across different datasets and horizons by an average of 19.82\% MSE and 13.62 \% of MAE. Table \ref{tbl:univariate} illustrates that the superiority of the Yformer is not just limited to a far horizon but even for the shorter horizons and in general across datasets.  Considering the individual datasets, the Yformer surpasses the baselines by 8, 6.8, 21.9, and 18.1\% of MAE for the ETTh1, ETTh2, ETTm1, and ECL datasets respectively. MSE results illustrates an improvement of 16.7, 12.6, 34.8, and 15.2\% for the ETTh1, ETTh2, ETTm1, and ECL datasets respectively. We observe that the MAE for the model is greater at horizon 48 than the MAE at horizon 168 for the ETTh1 dataset. This may be a case where the reused hyperparameters from the Informer paper are far from optimal for the Yformer. The other results show consistent behavior of increasing error with increasing horizon length $\tau$. Additionally, this behavior is also observed in the Informer baseline for ETTh2 dataset (Table \ref{tbl:multivariate}), where the loss is 1.340 for horizon 336 and 1.515 for a horizon of 168. 

\textbf{Multivariate}: We observe a similar trend in the multivariate setting. Here the Yformer model outperforms the baseline method in almost all of the 40 tasks across the three datasets by a margin of 18.41 \% MSE and 11.85\% of MAE. There is a clear superiority of the proposed approach, especially for the longer horizons. Across the different datasets, the Yformer improves on the baseline results by 9, 13.5, 13.1, and 11.7\% of MAE, and 12, 26.3, 13.9, and 17.1\% of MSE  for the ETTh1, ETTh2, ETTm1, and ECL datasets respectively. We attribute the improvement in performance to superior architecture and the ability to approximate the data distribution due to the addition of auxiliary loss. 
