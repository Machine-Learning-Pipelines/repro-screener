\section{Transfer Analysis}
\label{sec_transfer}

Unlike finetuning, progressive nets do not destroy the features learned on
prior tasks. This enables us to study in detail which features and at which
depth transfer actually occurs. We explored two related methods: an intuitive,
but slow method based on a perturbation analysis, and a faster analytical method
derived from the Fisher Information \cite{amari98natural}.

\paragraph{Average Perturbation Sensitivity (APS).} To evaluate the degree to
which source columns contribute to the target task, we can inject
Gaussian noise at isolated points in the architecture (e.g. a given
layer of a single column) and measure the impact of this perturbation
on performance. A significant drop in performance indicates that the
final prediction is heavily reliant on the feature map or
layer. We find that this method yields similar results to the faster
Fisher-based method presented below. We thus relegate details and
results of the perturbation analysis to the appendix.

\paragraph{Average Fisher Sensitivity (AFS).}
We can get a local approximation to the perturbation sensitivity by using
the Fisher Information matrix
\cite{amari98natural}. While the Fisher matrix is typically computed with
respect to the model
parameters, we compute a modified diagonal Fisher $\hat{F}$ of the network policy $\pi$
with respect to the \textit{normalized activations}
\footnote{The Fisher of individual neurons (fully connected) and feature maps
(convolutional layers) are computed over $\rho_{\pi^{(k)}}(s,a)$. The use of a normalized
representation $\hat{h}$ is non-standard, but makes the scale of $\hat{F}$
comparable across layers and columns.}
at each layer $\hat{h}_i^{(k)}$. For convolutional layers, we define
$\hat{F}$ to implicitly perform a summation over pixel locations. $\hat{F}$ can
be interpreted as the sensitivity of the policy to small changes in the
representation. We define the diagonal matrix $\hat{F}$, having
elements $\hat{F}(m,m)$, and the derived Average Fisher
Sensitivity (AFS) of feature $m$ in layer $i$ of column $k$ as:
\begin{align*}
\hat{F}_i^{(k)} &=
     \mathbb{E}_{\rho(s, a)}
        \left[
          \frac {\partial \log \pi} {\partial \hat{h}_i^{(k)}} \,
          \frac {\partial \log \pi} {\partial \hat{h}_i^{(k)}}^T
        \right]
& \hspace{1cm} &
\text{AFS}(i,k,m) &=
    \frac{\hat{F}_i^{(k)}(m,m)}{\sum_k \hat{F}_i^{(k)}(m,m)}
\end{align*}
where the expectation is over the joint state-action distribution $\rho(s,a)$
induced by the progressive network
trained on the target task. In practice, it is often useful to consider the AFS
score per-layer $\text{AFS}(i,k) = \sum_m \text{AFS}(i,k,m)$, i.e. summing over
all features of layer $i$. The AFS and APS thus estimate how much
the network relies on each feature or column in a layer to compute its output.
