\section{Related Literature}

There exist many different paradigms for transfer and
multi-task reinforcement learning, as these have long been recognized 
as critical challenges in AI research
\cite{Ring1995,SilverYL13,AAAIMag11-Taylor}. Many methods for transfer
learning rely on linear and other simple models
(e.g. \citep{Ruvolo2013ELLA}), which is a limiting factor to their
applicability. Recently, there have been new methods proposed for
multi-task or transfer learning with deep RL:
\cite{tessler2016,Rusu15,ParisottoICLR16}. In this
work we present an architecture for deep reinforcement learning that
in sequential task regimes that enables learning without forgetting
while supporting individual feature transfer from previous
learned tasks.

Pretraining and finetuning was proposed in \citep{hinton2006science}
and applied to transfer learning in
\citep{Bengio12deeplearning,UTLC+LISA-2011}, generally
in unsupervised-to-supervised or supervised-to-supervised
settings. The actor-mimic approach
\cite{ParisottoICLR16} applied these principles to
reinforcement learning, by fine-tuning a DQN multi-task network on new
Atari games and showing that some responded with faster learning,
while others did not. Progressive networks differ from the
finetuning direction substantially, since capacity is added as new tasks are learned.

Progressive nets are related to the incremental and constructive
architectures proposed in neural network literature. The
cascade-correlation architecture was designed to eliminate forgetting
while incrementally adding and refining feature extractors
\cite{Fahlman1990}.  Auto-encoders such as \citep{ZhouAISTATS12} use
incremental feature augmentation to track concept drift, and deep
architectures such as \citep{RozantsevSF16} have been designed that
specifically support feature transfer.  More recently, in
\cite{Agostinelli_NIPS2013}, columns are separately trained on
individual noise types, then linearly combined, and 
\cite{CiresanMS12} use columns for image classification. The block-modular architecture of \cite{Terekhov2015} has many similarities to our approach but focuses on a visual discrimination task.
The progressive net approach, in contrast, uses lateral connections to access previously learned features for deep compositionality. It can be used in any sequential learning setting but is especially valuable in RL.
