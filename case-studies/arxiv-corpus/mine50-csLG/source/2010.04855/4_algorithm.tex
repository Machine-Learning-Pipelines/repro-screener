\section{Algorithm}\label{sec:algorithm}

\subsection{RKHS background}

To present the algorithm, we provide background on the RKHS. The essential property of a function $\gamma$ in an RKHS $\mathcal{H}$ is the eponymous reproducing property: $\gamma(w)=\langle \gamma,\phi(w)\rangle_{\mathcal{H}}$ where $\phi(w)$ are features, formally defined below, that serve as the basis functions for $\mathcal{H}$. Our key algorithmic insight is to interpret the reproducing property as a way to separate the function $\gamma$ from the features $\phi(w)$. We use this defining property of the RKHS to decouple the three steps of nonparametric causal estimation. After providing RKHS background material, we prove an inner product representation that formalizes the decoupling, then introduce the causal estimators.

A scalar-valued RKHS $\mathcal{H}$ is a Hilbert space with elements that are functions $\gamma:\mathcal{W}\rightarrow\mathbb{R}$, on which the operator of evaluation is bounded \cite{berlinet2011reproducing}. Polynomial, spline, and Sobolev spaces are widely used examples of RKHSs. $\mathcal{W}$ can be any Polish space, so a value $w\in \mathcal{W}$ can be discrete or continuous. An RKHS is fully characterized by its feature map, which takes a point $w$ in the original space $\mathcal{W}$ and maps it to a feature $\phi(w)$ in the RKHS $\mathcal{H}$. The closure of $span\{\phi(w)\}_{w\in \mathcal{W}}$ is the RKHS $\mathcal{H}$. In other words, $\{\phi(w)\}_{w\in\mathcal{W}}$ can be viewed as the dictionary of basis functions for the RKHS $\mathcal{H}$. The kernel $k:\mathcal{W}\times \mathcal{W}\rightarrow \mathbb{R}$ is the inner product of features $\phi(w)$ and $\phi(w')$: $
k(w,w')=\langle \phi(w),\phi(w') \rangle_{\mathcal{H}}
$. A real-valued kernel $k$ is continuous, symmetric, and positive definite. Though we have constructed the kernel from the feature map, the Moore--Aronszajn Theorem states that, for any positive definite kernel $k$, there exists a unique RKHS $\mathcal{H}$ with feature map $\phi:w\mapsto k(w,\cdot)$. We have already seen that if $\gamma\in\mathcal{H}$, then $\gamma:\mathcal{W}\rightarrow \mathbb{R}$. With the additional notation of the feature map, we write
$
\gamma(w)= \langle \gamma,\phi(w) \rangle_{\mathcal{H}}
$. If $\mathcal{W}$ is separable and $\phi$ is continuous, then $\mathcal{H}$ is separable and may be infinite dimensional.

The RKHS is a practical hypothesis space for nonparametric regression. Consider output $Y\in\mathbb{R}$, input $W\in\mathcal{W}$, and the goal of estimating the conditional expectation function $\gamma_0(w)=E(Y \mid W=w)$. A kernel ridge regression estimator of $\gamma_0$ is
\begin{equation}\label{eq:cef_loss}
\hat{\gamma}=\argmin_{\gamma \in\mathcal{H}} n^{-1}\sum_{i=1}^n \{Y_i-\langle\gamma,\phi(W_i)\rangle_{\mathcal{H}}\}^2 + \lambda \|\gamma\|^2_{\mathcal{H}}.
\end{equation}
$\lambda>0$ is a hyperparameter on the ridge penalty $\|\gamma\|^2_{\mathcal{H}}$, which imposes smoothness in estimation. The solution to the optimization problem has a well known closed form \cite{kimeldorf1971some}, which we exploit and generalize throughout this work:
\begin{equation}\label{eq:cef_form}
\hat{\gamma}(w)=Y^{\top}(K_{WW}+n\lambda  I )^{-1}K_{Ww}.
\end{equation}
The closed form solution involves the kernel matrix $K_{WW}\in\mathbb{R}^{n\times n}$ with $(i,j)$th entry $k(W_i,W_j)$, and the kernel vector $K_{Ww}\in\mathbb{R}^n$ with $i$th entry $k(W_i,w)$. To tune the ridge hyperparameter $\lambda$, both generalized cross validation and leave-one-out cross validation have closed form solutions, and the former is asymptotically optimal \cite{craven1978smoothing,li1986asymptotic}.

We have seen that the feature map takes a value in the original space $w\in\mathcal{W}$ and maps it to a feature in the RKHS $\phi(w)\in\mathcal{H}$. Now we generalize this idea, from the embedding of a value $w$ to the embedding of a distribution $\text{\normalfont q}$. Just as a value $w$ in the original space  is embedded as an element $\phi(w)$ in the RKHS, so too the distribution $\text{\normalfont q}$ over the original space can be embedded as an element  $\mu=E_{\text{\normalfont q}}\{\phi(W)\}$ in the RKHS \cite{smola2007hilbert,berlinet2011reproducing}. Boundedness of the kernel implies existence of the mean embedding as well as Bochner integrability, which permits us to exchange the expectation and inner product. Mean embeddings facilitate the evaluation of expectations of RKHS functions: for $\gamma \in \mathcal{H}$,
$E_{\text{\normalfont q}}\{\gamma(W)\}=E_{\text{\normalfont q}}\left\{\langle \gamma,\phi(W) \rangle_{\mathcal{H}}\right\}=\langle \gamma,\mu \rangle_{\mathcal{H}}$. The final expression foreshadows how we will use the technique of mean embeddings to decouple the nonparametric regression step from the nonparametric reweighting step in the estimation of causal functions. A natural question is whether the embedding $\text{\normalfont q}\mapsto E_{\text{\normalfont q}}\{\phi(W)\}$ is injective, i.e. whether the RKHS element representation is unique. This is called the characteristic property of the kernel $k$, and it holds for commonly used RKHSs e.g. the exponentiated quadratic kernel \cite{sriperumbudur2010relation}.

The tensor product RKHS is one way to construct an RKHS for functions with multiple arguments. Consider the RKHSs $\mathcal{H}_{1}$ and $\mathcal{H}_{2}$ with positive definite kernels $k_{1}:\mathcal{W}_1\times\mathcal{W}_1\rightarrow \mathbb{R}$ and  
$k_{2}:\mathcal{W}_2\times\mathcal{W}_2\rightarrow \mathbb{R}$, respectively. An element $\gamma_1\in \mathcal{H}_{1}$ is a function $\gamma_1:\mathcal{W}_1\rightarrow \mathbb{R}$ and an element $\gamma_2\in \mathcal{H}_{2}$ is a function $\gamma_2:\mathcal{W}_2\rightarrow \mathbb{R}$. The tensor product RKHS $\mathcal{H}=\mathcal{H}_{1}\otimes \mathcal{H}_{2}$ is the RKHS with the product kernel
$
k:(\mathcal{W}_1\times \mathcal{W}_2) \times (\mathcal{W}_1\times \mathcal{W}_2)\rightarrow \mathbb{R},\; \{(w_1,w_2),(w'_1,w'_2)\}\mapsto k_{1}(w_1,w_1')  k_{2}(w_2,w_2')
$. Equivalently, the tensor product RKHS $\mathcal{H}$ has feature map $\phi(w_1)\otimes \phi(w_2)$ such that $\|\phi(w_1)\otimes \phi(w_2)\|_{\mathcal{H}}=\|\phi(w_1)\|_{\mathcal{H}_1}\|\phi(w_2)\|_{\mathcal{H}_2}$. Formally, tensor product notation means $(a\otimes b)c=a \langle b,c\rangle$. An element of the tensor product RKHS $\gamma \in\mathcal{H}$ is a function $\gamma:\mathcal{W}_1\times \mathcal{W}_2 \rightarrow \mathbb{R}$. We assume that the regression function $\gamma_0(w_1,w_2)=E(Y \mid W=w_1,w_2=w_2)$ is an element of a tensor product RKHS, i.e. $\gamma_0\in \mathcal{H}$. As such, the different arguments of $\gamma_0$ are decoupled, which we exploit when calculating partial means. %We will estimate $\gamma_0$ by a kernel ridge regression in $\mathcal{H}$.

Finally, we introduce the RKHS $\mathcal{L}_2(\mathcal{H}_{1},\mathcal{H}_{2})$ that we employ for conditional expectation operators. Rather than being a space of real-valued functions, it is a space of Hilbert--Schmidt operators from one RKHS to another. If the operator $E$ is an element of $ \mathcal{L}_2(\mathcal{H}_{1},\mathcal{H}_{2})$, then $E:\mathcal{H}_{1} \rightarrow \mathcal{H}_{2}$. Formally, it can be shown that $\mathcal{L}_2(\mathcal{H}_{1},\mathcal{H}_{2})$ is an RKHS in its own right with an appropriately defined kernel and feature map. $\mathcal{L}_2(\mathcal{H}_{1},\mathcal{H}_{2})$ is an example of a vector-valued RKHS; see \cite{micchelli2005learning} for a more general discussion. In the present work, we assume the conditional expectation operator $E_0:\gamma_1(\cdot)\mapsto E\{\gamma_1(W_1) \mid W_2=\cdot\}$ is an element of this RKHS, i.e. $E_0\in \mathcal{L}_2(\mathcal{H}_{1},\mathcal{H}_{2})$. We estimate $E_0$ by a kernel ridge regression in $\mathcal{L}_2(\mathcal{H}_{1},\mathcal{H}_{2})$, which coincides with estimating the conditional mean embedding $\mu_{w_1}(w_2)=E\{\phi(W_1) \mid W_2=w_2\}$ via the kernel ridge regression of $\phi(W_1)$ on $\phi(W_2)$; see the derivation of Algorithm~\ref{algorithm:treatment} below.

\subsection{Decoupled representation}

Lemma~\ref{theorem:id_treatment} makes precise how each causal function is identified as a partial mean of the form $\int \gamma_0(d,x)\mathrm{d}\text{\normalfont q}$ for some distribution $\text{\normalfont q}$. To facilitate estimation, we now assume that $\gamma_0$ is an element of an RKHS. %, which is a dense subset of $L^2$. See Appendices~\ref{sec:gentle} and~\ref{sec:formal} for background on the RKHS, which is a canonical setting for machine learning.
In our construction, we define scalar valued RKHSs for treatment $D$ and covariates $(V,X)$, then assume that the regression is an element of the tensor product space. Let $k_{\mathcal{D}}:\mathcal{D}\times \mathcal{D}\rightarrow \mathbb{R}$, $k_{\mathcal{V}}:\mathcal{V}\times \mathcal{V}\rightarrow \mathbb{R}$, and $k_{\mathcal{X}}:\mathcal{X}\times \mathcal{X}\rightarrow \mathbb{R}$ be measurable positive definite kernels corresponding to scalar valued RKHSs $\mathcal{H}_{\mathcal{D}}$, $\mathcal{H}_{\mathcal{V}}$, and $\mathcal{H}_{\mathcal{X}}$. Denote the feature maps
$
\phi_{\mathcal{D}}:\mathcal{D}\rightarrow \mathcal{H}_{\mathcal{D}}, \; d\mapsto k_{\mathcal{D}}(d,\cdot ); \;
\phi_{\mathcal{V}}:\mathcal{V}\rightarrow \mathcal{H}_{\mathcal{V}}, \; v\mapsto k_{\mathcal{V}}(v,\cdot );
 \; \phi_{\mathcal{X}}:\mathcal{X}\rightarrow \mathcal{H}_{\mathcal{X}}, \; x\mapsto k_{\mathcal{X}}(x,\cdot )
$.
To lighten notation, we suppress subscripts when arguments are provided. %, e.g. $\phi(d)=\phi_{\mathcal{D}}(d)$.

For $\theta_0^{ATE}$, $\theta_0^{DS}$, and $\theta_0^{ATT}$, we assume the regression $\gamma_0$ is an element of the RKHS $\mathcal{H}$ with the kernel $k(d,x;d',x')=k_{\mathcal{D}}(d,d')k_{\mathcal{X}}(x,x')$. We appeal to the fact that the product of positive definite kernels for $\mathcal{H}_{\mathcal{D}}$ and $\mathcal{H}_{\mathcal{X}}$ defines a new positive definite kernel for $\mathcal{H}$. The product construction provides a rich composite basis; $\mathcal{H}$ has the tensor product feature map $\phi(d)\otimes \phi(x)$ and $\mathcal{H}=\mathcal{H}_{\mathcal{D}}\otimes \mathcal{H}_{\mathcal{X}}$. In this RKHS,
$
\gamma_0(d,x)=\langle \gamma_0, \phi(d)\otimes \phi(x)\rangle_{\mathcal{H}} 
$.
Likewise for $\theta_0^{CATE}$ we assume $\gamma_0\in \mathcal{H}=\mathcal{H}_{\mathcal{D}}\otimes\mathcal{H}_{\mathcal{V}}\otimes  \mathcal{H}_{\mathcal{X}}$. We place regularity conditions on this RKHS construction in order to represent causal functions as inner products in $\mathcal{H}$. In anticipation of counterfactual distributions in Supplement~\ref{sec:distribution}, we also include conditions for an outcome RKHS in parentheses.

\begin{assumption}[RKHS regularity conditions]\label{assumption:RKHS}
Assume 
\begin{enumerate}
    \item $k_{\mathcal{D}}$, $k_{\mathcal{V}}$, $k_{\mathcal{X}}$ (and $k_{\mathcal{Y}}$) are continuous and bounded. Formally,
    $
    \sup_{d\in\mathcal{D}}\| \phi(d)\|_{\mathcal{H}_{\mathcal{D}}}\leq \kappa_d$, $ \sup_{v\in\mathcal{V}}\|\phi(v)\|_{\mathcal{H}_{\mathcal{V}}}\leq \kappa_v$, $ \sup_{x\in\mathcal{X}}\|\phi(x)\|_{\mathcal{H}_{\mathcal{X}}}\leq \kappa_x
    $ \{and $ \sup_{y\in\mathcal{Y}}\|\phi(y)\|_{\mathcal{H}_{\mathcal{Y}}}\leq \kappa_y$\}.
    \item $\phi(d)$, $\phi(v)$, $\phi(x)$ \{and $\phi(y)$\} are measurable.
    \item $k_{\mathcal{X}}$ (and $k_{\mathcal{Y}}$) are characteristic.
\end{enumerate}
For incremental functions, further assume $\mathcal{D}\subset \mathbb{R}$ is an open set and $\nabla_d  \nabla_{d'} k_{\mathcal{D}}(d,d')$ exists and is continuous, hence $\sup_{d\in\mathcal{D}}\|\nabla_d\phi(d)\|_{\mathcal{H}}\leq \kappa_d'$.
\end{assumption}
Commonly used kernels are continuous and bounded. Measurability is a similarly weak condition. The characteristic property ensures injectivity of the mean embeddings.

\begin{theorem}[Decoupling via kernel mean embeddings]\label{theorem:representation_treatment}
Suppose the conditions of Lemma~\ref{theorem:id_treatment}, Assumption~\ref{assumption:RKHS}, and $\gamma_0\in\mathcal{H}$ hold. Then
\begin{enumerate}
    \item $\theta_0^{ATE}(d)=\langle \gamma_0, \phi(d)\otimes \mu_x\rangle_{\mathcal{H}} $ where $\mu_x=\int\phi(x) \mathrm{d}\text{\normalfont pr}(x) $;
    \item $\theta_0^{DS}(d,\tilde{\text{\normalfont pr}})=\langle \gamma_0, \phi(d)\otimes \nu_x\rangle_{\mathcal{H}} $ where $\nu_x=\int\phi(x) \mathrm{d}\tilde{\text{\normalfont pr}}(x) $;
    \item $\theta_0^{ATT}(d,d')=\langle \gamma_0, \phi(d')\otimes \mu_x(d)\rangle_{\mathcal{H}} $ where $\mu_x(d)=\int\phi(x) \mathrm{d}\text{\normalfont pr}(x \mid d)$;
    \item $\theta_0^{CATE}(d,v)=\langle \gamma_0, \phi(d)\otimes \phi(v)\otimes \mu_{x}(v)\rangle_{\mathcal{H}} $ where $\mu_{x}(v)= \int \phi(x) \mathrm{d}\text{\normalfont pr}(x \mid v)$.
\end{enumerate}
Likewise for incremental functions, e.g. $\theta_0^{\nabla:ATE}(d)=\langle \gamma_0, \nabla_d\phi(d)\otimes \mu_x\rangle_{\mathcal{H}} $.
\end{theorem}

\begin{proof}[Sketch] 
Consider $\theta_0^{CATE}(d,v)$. Boundedness of the kernel implies Bochner integrability, which allows us to exchange the integral and inner product:
\begin{align*}
    \int \gamma_0(d,v,x)\mathrm{d}\text{\normalfont pr}(x \mid v)=\int \langle \gamma_0, \phi(d)\otimes \phi(v)\otimes \phi(x)\rangle_{\mathcal{H}}  \mathrm{d}\text{\normalfont pr}(x \mid v) =\langle \gamma_0, \phi(d)\otimes \mu_x(v) \rangle_{\mathcal{H}}.
\end{align*}
\end{proof}
See Supplement~\ref{sec:derivation} for the full proof. $\mu_x(v)=\int\phi(x) \text{\normalfont pr}(x \mid v)$ is the mean embedding of the conditional distribution $\text{\normalfont pr}(x \mid v)$. It encodes the distribution $\text{\normalfont pr}(x \mid v)$ as a function $\mu_x(v)\in\mathcal{H}_{\mathcal{X}}$ such that the causal function $\theta_0^{CATE}(d,v)$ can be expressed as an inner product in $\mathcal{H}$.


\subsection{Closed form solution}

The representation in Theorem~\ref{theorem:representation_treatment}  is essential to the algorithm derivation. In particular, the representation cleanly separates the three steps necessary to estimate a causal function: estimating a nonlinear regression, which may involve many covariates; estimating the distribution for reweighting; and using the nonparametric distribution to integrate the nonparametric regression. For example, for $\theta_0^{CATE}(d,v)$, our estimator is $\hat{\theta}^{CATE}(d,v)=\langle \hat{\gamma}, \phi(d)\otimes \phi(v)\otimes \hat{\mu}_x(v)\rangle_{\mathcal{H}}$. The nonlinear regression estimator $\hat{\gamma}$ is a standard kernel ridge regression of $Y$ on $\phi(D)\otimes \phi(V)\otimes \phi(X)$; the reweighting distribution estimator $\hat{\mu}_x(v)$ is a generalized kernel ridge regression of $\phi(X)$ on $\phi(V)$; and the latter can be used to integrate the former by simply multiplying the two. This algorithmic insight is a key innovation of the present work, and the reason why our estimators have simple closed form solutions despite complicated causal integrals.
\begin{algorithm}[Estimation of causal functions]\label{algorithm:treatment}
Denote the empirical kernel matrices
$
K_{DD}, K_{VV}, K_{XX}\in\mathbb{R}^{n\times n}
$ calculated from observations drawn from population $\text{\normalfont pr}$. Let $\tilde{X}_i$ $(i=1,...,\tilde{n})$ be observations drawn from population $\tilde{\text{\normalfont pr}}$. Denote by $\odot$ the elementwise product. Causal function estimators have the closed form solutions
\begin{enumerate}
    \item $\hat{\theta}^{ATE}(d)=n^{-1}\sum_{i=1}^n Y^{\top}(K_{DD}\odot K_{XX}+n\lambda  I )^{-1}(K_{Dd}\odot K_{Xx_i})  $;
     \item $\hat{\theta}^{DS}(d,\tilde{\text{\normalfont pr}})=\tilde{n}^{-1}\sum_{i=1}^{\tilde{n}} Y^{\top}(K_{DD}\odot K_{XX}+n\lambda  I )^{-1}(K_{Dd}\odot K_{X\tilde{x}_i}) $;
    \item $\hat{\theta}^{ATT}(d,d')=Y^{\top}(K_{DD}\odot K_{XX}+n\lambda  I )^{-1}[K_{Dd'}\odot \{K_{XX}(K_{DD}+n\lambda_1  I )^{-1}K_{Dd}\}]$;
    \item $\hat{\theta}^{CATE}(d,v)=Y^{\top}(K_{DD}\odot K_{VV}\odot K_{XX} +n\lambda  I )^{-1}[K_{Dd}\odot K_{Vv}\odot \{K_{XX}(K_{VV}+n\lambda_2  I )^{-1}K_{Vv} \}] $;
\end{enumerate}
where $(\lambda,\lambda_1,\lambda_2)$ are ridge regression penalty hyperparameters.  Likewise for incremental functions, e.g. $\hat{\theta}^{\nabla:ATE}(d)=n^{-1}\sum_{i=1}^n Y^{\top}(K_{DD}\odot K_{XX}+n\lambda  I )^{-1}(\nabla_d K_{D{d}}\odot K_{Xx_i})  $ where $(\nabla_d  K_{D{d}})_i=\nabla_d k(D_i,d)$.
\end{algorithm}

\begin{proof}[Sketch]
Consider $\theta_0^{CATE}(d,v)$. Analogously to~\eqref{eq:cef_loss}, the kernel ridge regression estimators of the regression $\gamma_0$ and the conditional mean embedding $\mu_x(v)$ are given by
\begin{align*}
    \hat{\gamma}&=\argmin_{\gamma \in\mathcal{H}} n^{-1}\sum_{i=1}^n \{Y_i-\langle\gamma, \phi(D_i)\otimes \phi(V_i) \otimes\phi (X_i)\rangle_{\mathcal{H}}\}^2 + \lambda \|\gamma\|^2_{\mathcal{H}}, \\
    \hat{E}&=\argmin_{E\in\mathcal{L}_2(\mathcal{H}_{\mathcal{X}},\mathcal{H}_{\mathcal{V}})} n^{-1}\sum_{i=1}^n \{\phi(X_i)-E^*\phi(V_i)\}^2 + \lambda_2 \|E\|^2_{\mathcal{L}_2(\mathcal{H}_{\mathcal{X}},\mathcal{H}_{\mathcal{V}})},
\end{align*}
where $\hat{\mu}_x(v)=\hat{E}^*\phi(v)$ and $E^*$ is the adjoint of $E$.
%and $\mathcal{L}_2(\mathcal{H}_{\mathcal{X}},\mathcal{H}_{\mathcal{V}})$ is an RKHS with elements that are operators. 
Analogously to~\eqref{eq:cef_form}, the closed forms are
\begin{align*}
    \hat{\gamma}(d,v,\cdot)&=Y^{\top}(K_{DD}\odot K_{VV}\odot K_{XX}+n\lambda  I )^{-1}\{K_{Dd}\odot K_{Vv}\odot K_{X(\cdot)}\},\\
    [\hat{\mu}_x(v)](\cdot)&=K_{(\cdot) X}(K_{VV}+n\lambda_2  I )^{-1}K_{Vv}.
\end{align*}
To arrive at the main result, match the empty arguments $(\cdot)$ of the kernel ridge regressions.
\end{proof}
See Supplement~\ref{sec:derivation} for the full derivation and a comparison to series estimation. We give theoretical values for $(\lambda,\lambda_1,\lambda_2)$ that optimally balance bias and variance in Theorem~\ref{theorem:consistency_treatment} below. Supplement~\ref{sec:tuning} gives practical tuning procedures based on generalized and leave-one-out cross validation to empirically balance bias and variance, the former of which is asymptotically optimal. %$\hat{\theta}^{DS}$ requires observations of covariates from $\tilde{\text{\normalfont pr}}$.

