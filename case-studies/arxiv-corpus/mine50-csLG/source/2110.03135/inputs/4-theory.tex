% \section{Double Descent from an Implicit Label Noise Perspective}
% \section{Adversarial perturbation can cause label noise implicitly}
% \section{Explore Label Noise in Adversarial Training}
% \label{sect:reason}

% \chengyu{Make sure we show the point that adversarial training only \emph{magnifies} the label noise and thus makes the robust overfitting more evident.}

% \chengyu{Avoid mention double descent to the best}

% \chengyu{maybe also no need to define ``implicit label noise''. Then people will not require us to demonstrate such a new type of label noise in the reality.}

% In this section, we present a novel perspective to understand the double descent in adversarial training. The implicit label noise is originated from the improper labeling of the adversarial examples and can induce double descent in adversarial training.

% In this section, we first show that traditional adversarial perturbation does not cause label noise directly. We then argue that adversarial perturbation cause label noise implicitly, but significantly.

% To understand intriguing behaviors of adversarial training such as robust overfitting we focus on its training set. We first show that label noise does not explicitly exist in the adversarially augmented training set. We then argue that label noise will implicitly exist in the adversarially augmented training set due to the distribution mismatch and improper label construction in the common practice.


 




% \input{inputs/4-theory/4.1-direct-label-noise}

\input{inputs/4-theory/4.2-implicit-label-noise}

\input{inputs/4-theory/4.2.3-example}

\input{inputs/4-theory/4.2.4-dependence}

\input{inputs/4-theory/4.4-relax}




    



% ----------------------------------------------
% ----------------------------------------------
% \subsection{Implicit label noise increases variance in adversarial training}
% \subsection{Implicit label noise induces double descent}
% \label{sect: noise-variance}

% \note{replace this part with results on cifar-10h}
% \chengyu{This should be okay. Augmentation is a good way to show implicit label noise. We just probably need to show before that implicit label noise is nothing but label noise if in a large dataset.}



% % We now show the implicit label noise induces double descent in adversarial training. 
% \chengyu{Move to related work}
% In standard training, the effect of label noise on double descent has been rigorously studied based upon both analytical settings~\citep{Mei2019TheGE, Hastie2019SurprisesIH, Deng2019AMO, Belkin2020TwoMO} and bias-variance analyses~\citep{Jacot2020ImplicitRO, Yang2020RethinkingBT, dAscoli2020DoubleTI}.
% % , with an emphasis on model-wise double descent. 
% % Here, we follow a bias-variance understanding and empirically show that the implicit label noise can promote variance during training and thus produces the epoch-wise double descent.
% Since implicit label noise is just a special case of label noise (Remark~\ref{remark:label-noise}), 
% % \jingbo{do you mean that label flipping is a special case of implicit label noise?} \chengyu{See Remark 2.2}
% and adversarial training can be viewed as standard training on an augmented dataset (Equation~(\ref{eq:outer-minimization})), % it can be inferred that implicit label noise will cause double descent in adversarial training. 
% it can be inferred that implicit label noise will increase the variance and make an evident double descent in adversarial training.
% %  we will not repeat the analyses here, but instead demonstrate in a scenario other than adversarial training where implicit label noise causes double descent.
% To demonstrate this in a straightforward way, in Figure~\ref{fig:dependence-variance} \jingbo{we have a undefined reference here} we employ standard training on a dataset augmented by fixed adversarial perturbation and show it can indeed produce double descent.



% To clearly show the implicit label noise promotes the variance and induces double descent, we employ \emph{adversarial augmentation}, namely the adversarial perturbation is generated by a surrogate model and applied to the training set only once. 
% Standard training is then conducted on the augmented training set to simulate the effect of adversarial training. 
% Such experiment excludes the possibility that the double descent (or robust overfitting) results from the variation of the adversary strength during training.

% Figure~\ref{fig:dependence-variance} shows the adversarial augmentation induces the epoch-wise double descent similar to adversarial training. 
% We further conduct the training on multiple independent training subsets and perform a bias-variance decomposition of the $0$-$1$ loss (see Appendix~\ref{sect:bias-variance-0-1loss}). 
% One can find that the bias almost monotonically decreases throughout the training while the variance increase significantly when the overfitting happens and larger perturbation radius will induces higher variance. 

% \chengyu{Maybe just show figure (a), remove bias-variance analyses and move to appendix.}
% \begin{figure*}[htbp]
%   \centering
%   \includegraphics[width=0.95\textwidth]{figures/reason-variance.pdf}
%   \caption{``Risk'' (Average test error over independent training subsets) obtained when training on an adversarially augmented dataset, as well as the ``Bias'' and ``Variance'' following a bias-variance decomposition of the 0-1 loss. Detailed experiment settings can be found in Appendix~\ref{sect: exp-ad-augment}.
%   }
% \label{fig:dependence-variance}
% \end{figure*}




% Finally, we note that our above analyses echo the existing works. 
% Since implicit label noise modulates the double descent, and by Theorem~\ref{theo:label-noise-perturbation} it depends on the perturbation radius and data quality, the double descent in adversarial training should strongly correlate with the perturbation radius and data quality. Indeed, it has been observed respectively that small perturbation radius will not induce robust overfitting~\citep{Dong2021ExploringMI}, and high-quality data will not induce robust overfitting~\citep{Dong2021DataPF}.
% for small perturbation radius and high-quality dataset, the double descent may not be observed in adversarial training, which echos the recent empirical observation made in \citet{Dong2021ExploringMI} and \citet{Dong2021DataPF} respectively.