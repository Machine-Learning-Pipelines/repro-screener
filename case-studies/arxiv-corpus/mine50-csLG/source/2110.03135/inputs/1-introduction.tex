\section{Introduction}


% \chengyu{emphasize the focus of this work is to find out how does adversarial perturbation creates label noise, rather than to understand how label noise cause double descent}

% \chengyu{Introduce classic notion of label noise, new notion of label noise, then extend robust overfitting to double descent and join with effect of label noise in standard learning under this label noise perspective.}


Adversarial training~\citep{Goodfellow2015ExplainingAH, Huang2015LearningWA, Kurakin2017AdversarialML, Madry2018TowardsDL} is known as one of the most effective ways~\citep{Athalye2018ObfuscatedGG, Uesato2018AdversarialRA} to enhance the adversarial robustness of deep neural networks~\citep{Szegedy2014IntriguingPO, Goodfellow2015ExplainingAH}. 
It augments training data with adversarial perturbations to prepare the model for adversarial attacks. 
% can be viewed as a data augmentation technique that minimizes the empirical risk on an augmented set composed of adversarial training examples. 
Despite various efforts to generate more effective adversarial training examples~\citep{Ding2020MaxMarginA, Zhang2020AttacksWD},
% for adversarial training, 
the labels assigned to them attracts little attention. 
% In the common practice of adversarial training, 
As the common practice, the assigned labels of adversarial training examples are simply inherited from their clean counterparts. 

% In this paper, we argue that the existing labeling practice of the adversarial training examples may introduce label noise implicitly.
% It is known that adversarial perturbation can distort the data semantics~\citep{Tsipras2019RobustnessMB, Ilyas2019AdversarialEA}.
% \jingbo{since we are not going to say it's implicit label noise, shall we remove ``implicitly'' here?}
In this paper, we argue that the existing labeling practice of the adversarial training examples introduces label noise implicitly, since adversarial perturbation can distort the data semantics~\citep{Tsipras2019RobustnessMB, Ilyas2019AdversarialEA}.
% A small perturbation radius employed in adversarial training ensures that such semantic distortion is not significant enough to cause noisy labels. 
For example, as illustrated in Figure~\ref{fig:illustration}, even with a slight distortion of the data semantics (e.g., more ambiguous), the label distribution of the adversarially perturbed data may not match the label distribution of the clean counterparts.
Such distribution shift is neglected when assigning labels to adversarial examples, which are directly copied from the clean counterparts.
% From a statistical view of the annotation process, we define label noise as the empirical measure of label errors in a dataset. 
We observe that distribution mismatch caused by adversarial perturbation along with improper labeling practice will cause \emph{label noise
% \footnote{We define label noise as the empirical measure of label errors in a dataset.} 
in adversarial training}.


% \vspace{-4.0ex}
\begin{figure}[tp]
  \centering
  \includegraphics[width=1.00\textwidth]{figures/implicitLabelNoise.pdf}
%   \includegraphics[width=1.00\textwidth]{nips2022/figures/examples-low-quality-1.pdf}
  \vspace{-1ex}
  \caption{Illustration of the origin of label noise in adversarial training. 
  The adversarial perturbation causes a mismatch between the true label distributions of clean inputs $x$ and their adversarial examples $x'$. Such a distribution mismatch is however neglected by the labels assigned to adversarial examples in the common practice of adversarial training, resulting in label noise implicitly.
  % The traditional adversarial label introduces the implicit label noise by inducing a distribution mismatch between the assigned label distribution and true label distribution of the adversarial example. 
%   Using rectified model probabilities as assigned labels of adversarial examples can provably reduce this distribution mismatch.
%   \chengyu{Change pictures (needs to use 8/255 instead of 16/255, we don't want people to argue that this is exactly label noise), shrink, merge with Figure 1?}
  }
 \vspace{-2ex}
\label{fig:illustration}
\end{figure}

% Our label noise perspective brings a better understanding on robust overfitting, a prominent phenomenon in adversarial training~\citep{Rice2020OverfittingIA}.
It is a mysterious and prominent phenomenon that the robust test error would start to increase after conducting adversarial training for a certain number of epochs~\citep{Rice2020OverfittingIA}, and our label noise perspective provides an adequate explanation for this phenomenon. 
% A classic bias-variance view of model generalization suggests that overfitting can result from increased model complexity, while training longer can be viewed as one way to increase model complexity~\citep{Rice2020OverfittingIA}. Compared to standard training, label noise that implicitly exists in adversarial training can increase the model variance~\citep{Yang2020RethinkingBT} and thus make the overfitting much more evident.
Specifically, from a classic bias-variance view of model generalization, label noise that implicitly exists in adversarial training can increase the model variance~\citep{Yang2020RethinkingBT} and thus make the overfitting much more evident compared to standard training.
Further analyses of label noise in adversarial training also explain the intriguing dependence of robust overfitting on the perturbation radius~\citep{Dong2021ExploringMI} and data quality~\citep{Dong2021DataPF} presented in the literature. 


% \begin{figure} % {r}{0.9\linewidth} % [!ht]
\begin{wrapfigure}{r}{6cm} % [!ht]
  % \vspace{-4mm}
  \centering
  \includegraphics[width=1.0\linewidth]{figures/intro.pdf}
  \caption{Robust overfitting can be viewed as an early part of the epoch-wise double descent. 
  We employ PGD training~\citep{Madry2018TowardsDL} on CIFAR-10~\citep{Krizhevsky2009LearningML} with Wide ResNet (WRN)~\citep{Zagoruyko2016WideRN} and a fixed learning rate. 
%   We employ PGD training on CIFAR-10 with Wide ResNet (WRN) and a fixed learning rate. 
  WRN-$28$-$k$ refers to WRN with depth $28$ and widen factor $k$.
  }
\label{fig:intro}
\vspace{-3mm}
% \end{figure}
\end{wrapfigure}


% \jingbo{I wish to put the Figure 2 next to this paragraph. }
Providing the label noise in adversarial training, one can further expect the existence of double descent based on the modern generalization theory of deep neural networks.
% , which have never been reported in adversarial training. 
% Furthermore, modern generalization theory of deep neural networks has been shown to go beyond the classic bias-variance trade-off. 
\emph{Epoch-wise double descent} refers to the phenomenon that the test error will first decrease and then increase as predicted by the classic bias-variance trade-off, but it will decrease again as the training continues. Such phenomenon is only reported in standard training of deep neural networks, often requiring significant label noise in the training set~\citep{Nakkiran2020DeepDD}.
% When increasing model complexity by training longer, the resulting double descent is known as the epoch-wise double descent. 
% Joining the standard training, we show that implicit label noise also shares a similar effect in adversarial training. 
As the label noise intrinsically exists in adversarial training, such epoch-wise double descent phenomenon also emerges when the training goes longer.
% However, as robust overfitting is induced by the label noise implicitly exists in adversarial training, it should join such double descent phenomenon as the training goes longer. 
Indeed, as shown in Figure~\ref{fig:intro}, for a relatively large model such as WRN-28-5, on top of the existing robust overfitting phenomenon, the robust test error will eventually decrease again \emph{after $1,000$ epochs}. Following \cite{Nakkiran2020DeepDD}, we further experiment different model sizes. One can find that a medium-sized model will follow a classic U-curve, which means only overfitting is observed; and the robust test error for a small model will monotonically decrease. These are well aligned with the observations in standard training regime.
% This verifies our understanding of label noise in adversarial training.
This again consolidates our understanding of label noise in adversarial training.
% \jingbo{there are two more curves in the figure. we should also talk about them?}



In light of our analyses, we design a theoretically-grounded method to mitigate the label noise in adversarial training automatically.
The key idea is to resort to an alternative labeling of the adversarial examples. 
We show that the predictive label distribution of an adversarially trained probabilistic classifier can approximate the true label distribution with high probability. Thus it can be utilized as a better labeling of the adversarial examples and provably reduce the label noise. We also show that with proper temperature scaling and interpolation, such predictive label distribution can further reduce the label noise.
% We show that the predictive label distribution of a probabilistic classifier adversarially trained as usual, but after being properly scaled and interpolated, can be utilized as a better labeling of the adversarial examples and provably reduce the implicit label noise. 
This echoes the recent empirical practice of incorporating knowledge distillation~\citep{Hinton2015DistillingTK} into adversarial training~\citep{chen2021robust}. 
While previous works heuristically select fixed scaling and interpolation parameters for knowledge distillation, we show that it is possible to fully unleash the potential of knowledge distillation by automatically determining the set of parameters that maximally reduces the label noise, with a strategy similar to confidence calibration~\citep{Guo2017OnCO}. 
Such strategy can further mitigate robust overfitting to a minimal amount without additional human tuning effort.
Extensive experiments on different datasets, training methods, neural architectures and robustness evaluation metrics verify the effectiveness of our method.



In summary, our findings and contributions are: 1) we show that the labeling of adversarial examples in adversarial training practice introduces label noise implicitly; 2) we show that robust overfitting can be adequately explained by such label noise, and it is the early part of an epoch-wise double descent; 3) 2e show an alternative labeling of the adversarial examples can be established to provably reduce the label noise and mitigate the robust overfitting.  

% \begin{itemize}[leftmargin=2em, topsep=0pt,itemsep=-0.5ex,partopsep=1ex,parsep=1ex]
%     \item We show that the labeling of adversarial examples in adversarial training practice introduces label noise implicitly.
%     \item We show that robust overfitting can be adequately explained by such label noise, and it is the early part of an epoch-wise double descent.
%     % \item We show that robust overfitting shall be viewed as the early part of an epoch-wise double descent, extending the common belief in adversarial training. Robust overfitting can thus be incorporated into the existing understanding of double descent.
%     % \item We show that double descent in adversarial training may originate from the implicit label noise introduced by improper \jingbo{maybe say imperfect? we argued that the common practice is correct if it has to be one-hot labels} labeling of adversarial examples in adversarial training practice.
%     \item We show an alternative labeling of the adversarial examples can be established to provably reduce the label noise and mitigate the robust overfitting.  
%     % \jingbo{maybe mention minimal human effort or something similar? }
% \end{itemize}

% The remainder of this paper is organized as follows.
% In Section~\ref{sect:related}, we briefly review the existing works on robust overfitting.
% In Section~\ref{sect:reason}, we explore the origin of label noise in adversarial training and analyze its dependence.
% In Section~\ref{sect:mitigate-double-descent}, we propose to mitigate robust overfitting by alternative labeling.
% Section~\ref{sect:experiment} demonstrates the effectiveness of our method on realistic datasets. 
% Conclusions and further implications are discussed in Section~\ref{sect:conclusion}.









% Adversarial training is a popular technique for robust deep learning...

% Adversariral training can be viewed as training on an augmented dataset~\cite{Tsipras2019RobustnessMB}, where the assigned labels are directly copied from the assigned labels of clean inputs.
% We argue that such common practice introduce label noise implicitly. By a distribution mismatch, although no conventional definition of label noise is observed.

% Explains robust overfitting.

% Double descent confirms this label noise view. 

% Mitigate double descent.





% \chengyu{Here goes the old version.}

% In adversarial training, a typical phenomenon is that after a certain training epoch, the robust test error will start to increase constantly with further training, despite the robust training error continues to decrease~\citep{Rice2020OverfittingIA}. 
% \sout{This phenomenon, known as \emph{robust overfitting}, is believed to be separated from \emph{double descent}~\citep{Belkin2019ReconcilingMM} in the literature~\citep{Rice2020OverfittingIA}.} 

% % In contrast to the common belief that deep networks hardly overfit in standard training~\citep{Zhang2017UnderstandingDL, Neyshabur2017ExploringGI}, overfitting has been shown to be a dominant phenomenon in adversarially robust training~\citep{Rice2020OverfittingIA}.
% % After a certain point during training, the robust test error will start to increase constantly with further training, despite the robust training error continues to decrease. 
% % This phenomenon, known as ``\emph{robust overfitting}'', is believed to be separated from \emph{double descent}~\citep{Belkin2019ReconcilingMM} in previous works~\citep{Rice2020OverfittingIA}. 



% Here, we find that robust overfitting\jingbo{Robust overfitting not defined in intro} shall be viewed as the early part of an epoch-wise double descent~\citep{Nakkiran2020DeepDD}.
% % after conducting adversarial training for sufficiently many epochs. 
% As shown in Figure~\ref{fig:intro}, for relatively large models \jingbo{I think you want to refer to the medium size one in the figure? if so name it.}, the robust test error increases only transiently and will eventually decrease again after a considerable number of training epochs. 
% Modulating the model architecture size can shifting the epoch-wise double descent curve such that either the overfitting curve is shown \jingbo{probably refer to the model name here and also in the other part of ``or'' clause.}, or the entire double descent curve is revealed within the same number of training epochs.
% % as shown in Figure~\ref{fig:intro}. 
% % Therefore, Robust overfitting should be unified with double descent and 
% % Our analyses thus manage to unifying the two seemingly separately phenomena. % robust overfitting and double descent.
% In Appendix~\ref{sect:double-descent-reconcile}, we verify our observation across various training settings such as different sample sizes, optimizers, learning rate schedulers, and neural architectures.
% Therefore, robust overfitting will not go beyond modern generalization theory as an exception and should be incorporated into the existing understanding of \jingbo{I think some reviewers mentioned the issue of mixing model-wise and epoch-wise double descent. Probably we want to make it very clear this time.} double descent.
% % adequately explained by the origin of double descent such as label noise.
% % In Appendix~\ref{sect:double-descent-reconcile}, we observe consistent and similar patterns across various training settings (e.g., different sample sizes, optimizers, learning rate schedulers, and neural architectures)

% % , which further verifies our intuition 
% % that robust overfitting shall be viewed as the early part of an epoch-wise double descent (see Appendix~\ref{sect:double-descent-reconcile}).
% % \jingbo{the last sentence is a little repeated?}
% % \lucas{how about verifies the connection between the robust overfitting, and the double descent?}
% % on the connection between the robust overfitting and the double descent (See ). 

% % \chengyu{Previous works suggest that those hard examples in adversarial training might be label noise after adversarial perturbation (`Explore memorization`). However, we carefully inspect the hardest examples and find that they are at most more ambiguous. Nevertheless, we show that label noise can implicitly exist in adversarial training even though all assigned labels are correct. ...} 
% % \chengyu{Discuss that ``hard examples might be label noise'' mentioned in `exploring memorization...`, just show the hardest examples with $8/255$ perturbation and are at most ambiguous.} 


% % In our study, we aim to better understand the 
% % Motivated by our observations, we aim to adopt the double descent perspective to better understand the robust overfitting.  
% % Inspired by this observation, we further analyze the epoch-wise double descent to better understand the robust overfitting.
% % Inspired by the above observation
% We further explore the origin of the double descent in adversarial training to better understand robust overfitting.
% % propose to understand the double descent in adversarial training from an \emph{implicit label noise} perspective.
% In standard training, 
% % the double descent is shown to be related to the label noise~\citep{}. Specifically, 
% to observe the double descent for modern neural architectures it is often necessary to manually inject label noise~\citep{Nakkiran2020DeepDD, Yang2020RethinkingBT}.
% However, adversarial perturbations are believed to not change the underlying labels, \jingbo{shall we make it clear that it's the one-hot label?} % or introduce label noise.
% % We further empirically confirm that adversarial perturbation is unlikely to change the argmax label. \jingbo{this argmax label might be confusing.}
% which is further confirmed empirically from our manual inspection. 
% Therefore, label noise \jingbo{since we will implicit label noise later, we should define it clearly what is label noise or at least use something like (e.g., label flipped) to explain it along with some citation} should not explicitly exist in adversarial training.
% % Therefore, the popular label flipping noise explanation is not applicable here. 
% % We note that the common practice of directly copying labels from clean examples to their adversarial counterparts may introduce label noise, which is essential to produce double descent in model neural architectures~\citep{Nakkiran2020DeepDD, Yang2020RethinkingBT}.
% % However, based on our empirical checks and literature \jingbo{cite the work you mentioned later}, adversarial perturbation is unlikely to change the argmax label; therefore, the popular label flipping noise explanation \jingbo{maybe a citation here} is not applicable here. 
% On the other hand, it is reasonable to believe that adversarial perturbation distorts the label distribution since the data examples do become more ambiguous after perturbation. 
% Such perturbation thus induces a mismatch between the assigned label distribution \jingbo{What is assigned label distribution? not clear here.} and (unknown) true label distribution, which leads to \emph{implicit label noise}.
% % Thus, we focus on the \emph{implicit label noise} which measures the mismatch between the assigned label distribution and (unknown) true label distribution. 
% We show such implicit label noise can be interpreted as a specific type of label noise and can have a great impact in adversarial training, which thus provides a new perspective to understand the double descent in adversarial training. Indeed, extensive analyses of implicit label noise can explain the intriguing dependency of robust overfitting on perturbation size~\citep{Dong2021ExploringMI} and data quality~\citep{Dong2021DataPF}.
% % \lucas{maybe we start from the contradict between the label flipping noise, and the adversarial training practice?}

% % Our implicit label noise perspective can further inspire novel adversarial algorithms to mitigate the double descent.
% Guided by our analyses, we design a theoretically-grounded method to mitigate the robust overfitting.
% % A natural idea is to resort to an alternative labeling of the adversarial examples. 
% The key idea is to resort to an alternative labeling of the adversarial examples. 
% We show that the predictive label distribution of a probabilistic classifier adversarially trained as usual, but after being properly scaled and interpolated, can be utilized as a better labeling of the adversarial examples and provably reduce the implicit label noise. 
% This echoes the recent empirical practice of incorporating knowledge distillation~\citep{Hinton2015DistillingTK} into adversarial training~\citep{chen2021robust}. 
% While previous works heuristically select fixed scaling and interpolation parameters for knowledge distillation, we show that it is possible to fully unleash the potential of knowledge distillation by automatically determining the set of parameters that maximally reduces the implicit label noise, with a strategy similar to confidence calibration~\citep{Guo2017OnCO}. 
% Such strategy can further mitigate robust overfitting to a minimal amount without additional human tuning effort.
% % without additional tuning.
% % \jingbo{is this ``minimal amount'' supported by any of our exps? otherwise, let's remove this suffix.}
% % Extensive experiments on different datasets, training methods, neural architectures and robustness evaluation metrics demonstrate our approach can effectively mitigate robust overfitting.
% Extensive experiments on different datasets, training methods, neural architectures and robustness evaluation metrics verify the effectiveness of our method.

% In summary, our findings and contributions are as follows.
% \begin{itemize}[leftmargin=2em, topsep=0pt,itemsep=-0.5ex,partopsep=1ex,parsep=1ex]
%     % \item We challenge the common belief that the robust overfitting and double descent are separate phenomena and show that robust overfitting is an early part of epoch-wise double descent. % the in adversarial training with the  
%     \item We show that robust overfitting shall be viewed as the early part of an epoch-wise double descent, extending the common belief in adversarial training. Robust overfitting can thus be incorporated into the existing understanding of double descent.
%     \item We show that double descent in adversarial training may originate from the implicit label noise introduced by improper \jingbo{maybe say imperfect? we argued that the common practice is correct if it has to be one-hot labels} labeling of adversarial examples in adversarial training practice.
%     \item We show an alternative labeling of the adversarial examples can be established to provably reduce the implicit label noise and mitigate the robust overfitting. 
% \end{itemize}

% % \jingbo{Since our submission doesn't follow the conventional related work--method--exp--conclusion flow, I think it's a good idea to put a paragraph here to explain how the write-up is organized. }

% The remainder of this paper is organized as follows.
% In Section~\ref{sect:related}, we briefly review the existing works that explore robust overfitting and double descent.
% In Section~\ref{sect:reason}, we theoretically explore the origin of double descent in adversarial training from an implicit label noise perspective.
% In Section~\ref{sect:mitigate-double-descent}, we propose to mitigate the double descent in adversarial training by alternative labeling based on our understanding. Section~\ref{sect:experiment} demonstrates the effectiveness of our method on realistic datasets. 
% Conclusions and further implications are discussed in Section~\ref{sect:conclusion}.
