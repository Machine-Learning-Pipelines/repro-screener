\subsection{Adversarial perturbation generated by a realistic classifier}
\label{sect:reason-realistic}
We now consider a realistic case where the adversarial perturbation is generated by a probabilistic classifier $f_\theta$. 

% \begin{lemma}[Change of the predictive distribution after adversarial perturbation]
% Assume FGSM, cross-entropy loss.
% Assume $f_\theta(x)_y$ is $L$-locally Lipschitz. 
% \chengyu{copy from the proof of true probabilistic classifier Theorem 4.1 and Corollary 4.1}
% \begin{equation}
%     % \|P(Y|x) - P(Y'|x')\|_{\text{TV}} \ge
%         % \frac{\varepsilon}{2} (1 - q(x)) \frac{m}{L}  - \frac{\varepsilon^2}{4} M.
%     \|f_\theta(x') - f_\theta(x)\|_{\text{TV}} \ge
%         \frac{\varepsilon}{2} (1 - f_\theta(x)_y) \frac{m}{L}  - \frac{\varepsilon^2}{4} M.
% \end{equation}
% \end{lemma}

\smallsection{Approximation of the true label distribution}
% \chengyu{need to introduce some intuition here..}
We show that after sufficient adversarial training, the predictive label distribution of $f_\theta$ can approximate the true label distribution with high probability. 
% Therefore, an additional error term will be required to lower bound the label noise compared to the ideal case.
% We show that a probabilistic classifier trained on the conventional adversarially augmented training set can learn the true label distribution.
% Although we only have one-hot labels, but note that in the clean dataset one-hot label is an unbiased estimate of the true label distribution, namely $\mathbb{E}[\mathbf{1}_y] = P(Y|x)$.
% \chengyu{No, it is a biased-estimation... But it is close to true by Lipschitz}
% Thus given a collection of training inputs with the same true label distribution, the sample mean of their one-hot labels converges to the true label distribution.
% \chengyu{group and then relaxation}
\begin{lemma}% [Classifier by adversarial training learns the true label distribution]
\label{lemma:Learn-true-distribution}
% Assume the function generating the true label distribution $\mathbf{p}^*: \mathcal{X} \rightarrow \mathcal{Y}$ is $L^*$-locally Lipschitz continuous in a norm ball of radius $r$ around $x \in \mathcal{X}$.
% Select the loss function $\ell(\cdot, \cdot)$ to be a proper scoring rule.
% \chengyu{Only need to consider one particular adversarially augmented dataset here $D' = \{(x', \tilde{y})$, then any other points within metric ball will approximate true distribution with Lipschitz bound?}
Denote $\mathcal{S} = \{x: (x,y)\in \mathcal{D}\}$ as the collection of all training inputs.
Let $\rho\ge 1$ and $\mathcal{C}$ be an $\rho\varepsilon$-external covering of $\mathcal{S}$ with covering number $N_{\rho\varepsilon}$.
% Let $\{\mathcal{S}_i\}_{i\in [N_s]}$ be a disjoint partition of $\mathcal{S}$ induced by $\mathcal{C}$.
% Let $\mathcal{F}_{L, r}$ be a family of functions that are $L$-locally Lipschitz continuous in a norm ball of radius $r$ around $x \in \mathcal{C}^r$.  
Let $f_\theta$ be a probabilistic classifier that minimizes the adversarial empirical risk (\ref{eq:outer-minimization}).
% , namely
% This ensures all $x'$ in the norm ball is minimized, thus all $x'$ in the norm ball have true label distribution predicted.
% \begin{equation}
%     \label{eq:lip-minimization}
%     \theta = \argmin_\theta \frac{1}{|D|}\sum_{(x,y) \in D} \max_{x'\in \mathcal{B}_\varepsilon(x)}\ell (f_\theta(x'), y),
% \end{equation}
Assume $f_\theta$ is $L_\theta$-locally Lipschitz continuous in a norm ball of radius $\rho\varepsilon$ around $x \in \mathcal{C}$.
Let $\kappa \ge 1$ and $ \mathcal{\hat{S}}$ be a subset of $\mathcal{S}$ with cardinality at least $(1 - 1/\kappa + 1/(\kappa N_{\rho\varepsilon})) N$.
Let $\mathcal{N}_\varepsilon(\mathcal{\hat{S}})$ denote the neighborhood of the set $\mathcal{\hat{S}}$, i.e. $\mathcal{N}_\varepsilon(\mathcal{\hat{S}}) = \bigcup_{x\in\mathcal{\hat{S}}} \mathcal{B}_\varepsilon (x)$. 
Then for any $x \in \mathcal{N}_\varepsilon(\mathcal{\hat{S}})$, with probability at least $1-\delta$,
 
\begin{equation}
    \label{eq:main-theorem-bound}
    \|f_\theta(x) - P(Y|x)\|_{\text{TV}} \le \sqrt{\frac{\kappa N_{\rho\varepsilon} K}{2N}\log\frac{2}{\delta}}  + \left(\left(\frac{3}{2} - \frac{1}{K}\right) L_\theta + L\right) \rho \varepsilon,
\end{equation}
% where $\xi_K = \frac{1}{2}( 1 + K\|\mathbf{1}_{\circ} - K^{-1}\mathbf{1}\|_1)$.
% Here $\mathbf{1}_{\circ} \in \mathbb{R}^K$ denotes the one-hot vector where an arbitrary entry is $1$. 

% \chengyu{If it is anywhere robust, then it must be robust to manifold perturbation? But how large the perturbation is manifold perturbation that a model should be robust to? Is true model robust to manifold perturbation?}
% \chengyu{What's relation between $r$ and $\varepsilon$?}

% x' include x
% With probability $1-\delta'$, 
% $$
% |f_\theta(x') - f(x')| \le o(\delta')
% $$
\end{lemma}



% \chengyu{Talk about relation to the conventional robustness generalization bound (requires more stringent Lipschitz)}




\smallsection{Label noise in adversarial training with a realistic classifier}
% \chengyu{Or we prove that any perturbation that distort the output of a robust model is likely to at least partially distort the true label distribution?}
Adversarial perturbation generated by a realistic classifier $f_\theta$ will distort its predictive label distribution by gradient ascent. Subsequently, the true label distribution will also be distorted with high probability since the predictive label distribution of a realistic classifier $f_\theta$ can approximate the true label distribution. Specifically, by the triangle inequality we have
\begin{equation}
\|P(Y|x)  - P(Y'|x') \|_{\text{TV}} \ge \|f_\theta(x)  - f_\theta(x')\|_{\text{TV}} - (\|f_\theta(x) - P(Y|x)\|_{\text{TV}} + \|f_\theta(x') - P(Y'|x')\|_{\text{TV}}),
\end{equation}
where the last two terms are the approximation error of true label distribution on both clean and adversarial examples, which are guaranteed to be small. To conclude, we have the following result.
\begin{theorem}
\label{theo:realistic-classifier}
Instantiate the notations of Lemma~\ref{lemma:Learn-true-distribution}. For any $x \in \mathcal{N}_\varepsilon(\mathcal{\hat{S}})$, with probability at least $1 - 3\delta$, we have
\begin{equation}
    % \|P(\tilde{Y}'|x) - P(Y'|x)\|_{\text{TV}} \ge
    % \varepsilon \left[(1 - f_\theta(x)_y) \frac{m}{2L_\theta}- \rho\left(\left(\frac{3}{2} - \frac{1}{K}\right) L_\theta + L\right)\right]  - \varepsilon^2\frac{M}{4}  
    % %  \|f_\theta(x)  - f_\theta(x')\|_{\text{TV}}
    % - \sqrt{\frac{2\kappa N_s K}{N}\log\frac{2}{\delta}}
    p_e(\mathcal{D}') \ge
    \varepsilon \left[(1 - \mathbb{E}_x f_\theta(x)_y) \frac{\sigma_m}{2L_\theta}- 2\rho\left(\left(\frac{3}{2} - \frac{1}{K}\right) L_\theta + L\right)\right]  - \varepsilon^2\frac{\sigma_M}{4}  
    %  \|f_\theta(x)  - f_\theta(x')\|_{\text{TV}}
    - \xi \sqrt{\frac{1}{2N} \log\frac{2}{\delta}},
    %  - \sqrt{\frac{1}{2N} \log\frac{2}{\delta}}
    % - \sqrt{\frac{2\kappa N_s K}{N}\log\frac{2}{\delta}}
\end{equation}
% \chengyu{$\rho$ should be $2\rho$, see proof in the appendix.}
where $\xi = 1 + \sqrt{4\kappa N_{\rho\varepsilon} K}$.
% \chengyu{should be: $\xi = 1 + \sqrt{4\kappa N_{\rho\varepsilon} K}$}
\end{theorem}

    
    % Compare to Corollary 4.1, additional two terms associated with the errors of approximating the true distribution are introduced.
    
    % Necessary conditions are that the model has to be both accurate and robust. If a model is not robust, adversarial perturbation generated by it will not induce implicit label noise. We showcase this by performing the static adversarial perturbation, namely ...
    % \todo{Adversarial perturbation generated by non-robust models will not cause double descent}, while that generated by robust models will. We also include an extreme case (Gaussian noise, can be viewed as the adversarial perturbation generated by random initialized model). (for gaussian noise we use significantly larger perturbation radius to make the difference of the training curves evident.)
    % \chengyu{Only discuss this if the reduced probability is shown before.}
    % \chengyu{Interestingly, the above analysis implies that if the perturbation does not reduce the probability mass of the argmax true label, it will not cause implicit label noise even with an extremely large perturbation radius. 
    % We demonstrate this in detail and empirically verified it for Gaussian noise in Appendix~\ref{sect:exp-gaussian-noise}.}
    % We thus empirically verify Gaussian noise will not induce double descent even with an extremely large perturbation radius (See Appendix \ref{sect:exp-gaussian-noise}).
    
    
    % \sout{Such example also implies that even static adversarial perturbation~\footnote{namely the adversarial perturbation is added to the training set only once and the standard training is performed subsequently} can produce clear double descent as shown in Appendix~\ref{sect:exp-static}.} 
    % Therefore we believe implicit label noise can be an important source of label noise that makes double descent more evident in adversarial training.
    
