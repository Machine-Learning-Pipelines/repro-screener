
% -------------------------------------
% \section{Proof of Theorem~\ref{theorem:error}}
% \label{sect:proof_41}

% \smallsection{Step I: Partition the training inputs}
% \smallsection{Step I: Bound the difference between the true label distribution and the sample mean}
Let $\mathcal{D} = (x, y)$ be the adversarially augmented training set. Let $\mathcal{S} = \{x: (x,y)\in \mathcal{D}\}$ be the collection of all training inputs.
% Let $\rho\ge 1$ and $\mathcal{C}$ be an $\rho\varepsilon$-external covering of $\mathcal{S}$ with covering number $N_{\rho\varepsilon}$.
% Let $f_\theta$ be a probabilistic classifier that minimizes the adversarial empirical risk (\ref{eq:outer-minimization}).
% Assume $f_\theta$ is $L_\theta$-locally Lipschitz continuous in a norm ball of radius $\rho\varepsilon$ around $x \in \mathcal{C}$.
% Let $\kappa \ge 1$ and $ \mathcal{\hat{S}}$ be a subset of $\mathcal{S}$ with cardinality at least $(1 - 1/\kappa + 1/(\kappa N_{\rho\varepsilon})) N$.
% Let $\mathcal{N}_\varepsilon(\mathcal{\hat{S}})$ denote the neighborhood of the set $\mathcal{\hat{S}}$, i.e. $\mathcal{N}_\varepsilon(\mathcal{\hat{S}}) = \bigcup_{x\in\mathcal{\hat{S}}} \mathcal{B}_\varepsilon (x)$.
First we note that the set of all training inputs can be grouped into several subsets such that the inputs in each subset possess similar true label distribution. 
More formally, 
Let $\mathcal{C} = \{\bar{x}_j\}_{j=1}^{N_{\rho\varepsilon}}$ be an $\rho\varepsilon$-external covering of $\mathcal{S}$ with minimum cardinality, namely $\mathcal{S} \subseteq \bigcup_{x \in \mathcal{C}} \{x' ~|~ \|x ' - x\| \le \rho\varepsilon \}$,
where we refer $\bar{x}_j$ as the covering center, and $N_{\rho\varepsilon}$ is the covering number.

Let $\{\mathcal{S}_j\}_{j=1}^{N_{\rho\varepsilon}}$ be any disjoint partition of $\mathcal{S}$ such that $\mathcal{S}_j \subseteq \{x' | \|x ' - \bar{x}_j\| \le \rho\varepsilon \}$. 
% There are likely arbitrarily number of possible partitions, but we choose a way that distributes all the inputs as evenly as possible. Strictly speaking, we choose a partition such that it maximizes the cardinality of the subset with the minimum cardinality,
% $$
% \{\mathcal{{S}}_j^r\} = \arg\max_{\{\mathcal{{S}}_j^r\}} ~ \min_j | \mathcal{{S}}_j^r |
% $$
We show that $\mathcal{{S}}_j$ attains a property that the true label distribution of any input $x$ in this subset will not be too far from the sample mean of one-hot labels $\mathbf{\bar{y}}_j = {|\mathcal{{S}}_j|}^{-1} \sum_{x\in \mathcal{{S}}_j} \mathbf{1}_{y}$ in this subset. Specifically, let $p(x) = P(Y|x)$. We have with probability $1 - \delta$,
\begin{equation}
     \label{lemma:sample-mean-relax}
    \| p(x) - \bar{\mathbf{y}}_j \|_1 \le \sqrt{\frac{2K}{|\mathcal{{S}}_j|}\log\frac{2}{\delta}} + 2L\rho\varepsilon.
\end{equation}

To prove this property we first present two lemmas. 
\begin{lemma}[Lipschitz constraint of the true label distribution]
\label{lemma:Lipschitz-true-distribution}
Let $\mathcal{S}_j$ be a subset constructed above and $\mathbf{\bar{y}}_j = {|\mathcal{{S}}_j|}^{-1} \sum_{x\in \mathcal{{S}}_j} \mathbf{1}_{y}$. Then for any $x \in \mathcal{S}_j$ we have,
\begin{equation}
    \left\|p(x) - \mathbb{E}[\mathbf{\bar{y}}_j] \right\|_1 \le 2 L \rho\varepsilon.
\end{equation}
\end{lemma}
\begin{proof}
First, since $x \in \mathcal{S}_j$, we have $\|x - \bar{x}_j\|_1 \le \rho\varepsilon $, which implies $\|p(x) - p(\bar{x}_j)\|_1 \le L \rho\varepsilon$ by the locally Lipschitz continuity of $p$. Then for any $x, x' \in \mathcal{S}_j$, we will have $\|p(x) - p(x')\| \le 2L \rho\varepsilon$ by the triangle inequality. Let $N_{\mathcal{S}} = |\mathcal{S}_j|$. Therefore,
\begin{equation}
    \left\|p(x) - \frac{1}{N_{\mathcal{S}}} \sum_{x \in \mathcal{S}_j} p(x)\right\|_1 \le 2 \frac{N_{\mathcal{S}} - 1}{N_{\mathcal{S}}} L \rho\varepsilon \le 2 L \rho\varepsilon.
\end{equation}
Further, the linearity of the expectation implies
\begin{equation}
    \mathbb{E}[\mathbf{\bar{y}}] = {N_{\mathcal{S}}}^{-1} \sum_{x \in \mathcal{S}_j} \mathbb{E}[\mathbf{1}_{y(x)}] = {N_{\mathcal{S}}}^{-1} \sum_{x \in \mathcal{S}_j} p(x).
\end{equation}
Therefore $ \left\|p(x) - \mathbb{E}[\mathbf{\bar{y}}_j] \right\| \le 2 L \rho\varepsilon$.
\end{proof}


\begin{lemma}[Concentration inequality of the sample mean]
\label{lemma:sample-mean}
% Let $\mathbf{1}_y$ be the one-hot label of an input $x$. 
Let $\mathcal{S}$ be a set of $x$ with cardinality $N$.
Let $\mathbf{\bar{y}} = N^{-1} \sum_{x\in \mathcal{S}} \mathbf{1}_y$ be the sample mean.
Then for any $p$-norm $\|\cdot\|$ and any $\varepsilon > 0$,  we have with probability $1 - \delta$,
\begin{equation}
    \label{eq:concentration-sample-mean}
    \left\| \mathbf{\bar{y}} - \mathbb{E}\left[ \mathbf{\bar{y}} \right] \right\|_1 \le \sqrt{\frac{2K}{N}\log \frac{2}{\delta}}
\end{equation}
\end{lemma}

\begin{proof}
Note that $\mathbf{\bar{y}}$ obeys a multinomial distribution, \emph{i.e.} $\mathbf{\bar{y}} \sim N^{-1} multinomial(N, \mathbb{E}[\mathbf{\bar{y}}])$.
This lemma is thus the classic result on the concentration properties of multinomial distribution based on $\ell_1$ norm~\cite{Weissman2003InequalitiesFT, Qian2020ConcentrationIF}.
\end{proof}

One can see that Lemma~\ref{lemma:Lipschitz-true-distribution} bounds the difference between true label distribution of individual inputs and the mean true label distribution, while Lemma~\ref{lemma:sample-mean} bounds the difference between the sample mean and the mean true label distribution. Therefore the difference between the true label distribution and the sample mean is also bounded, since by the triangle inequality we have with probability $1 - \delta$,
\begin{equation}
    \begin{aligned}
    \|p(x) - \bar{\mathbf{y}} \| 
    & \le \|p(x) - \mathbb{E}[\bar{\mathbf{y}}] \| + \| \bar{\mathbf{y}} - \mathbb{E}[\bar{\mathbf{y}}] \| \\
    & \le \sqrt{\frac{2K}{N}\log \frac{2}{\delta}} + 2L\rho\varepsilon.
    \end{aligned}
\end{equation}

\vspace{1em}
% \smallsection{Step II: Bound the difference between the prediction given by the empirical risk minimizer and the sample mean}
% \smallsection{Step II: Find the minimizer of the empirical risk with Lipschitz constraint} 
We now show that given the locally Lipschitz constraint established in each disjoint partition we constructed above, the prediction given by the empirical risk minimizer will be close to the sample mean. As an example, we focus on the negative log-likelihood loss, namely $\ell(f_\theta(x), y) = - \mathbf{1}_y \cdot \log f_\theta (x) $. Other loss functions that are subject to the proper scoring rule can be investigated in a similar manner.
First, we regroup the sum in the empirical risk based on the partition constructed above, namely
\begin{equation}
    \hat{R}(f_\theta, \mathcal{S}) = \frac{1}{N_{\rho\varepsilon}}\sum_{j=1}^{N_{\rho\varepsilon}} \hat{R} (f_\theta, \mathcal{S}_j),
\end{equation}
where $\hat{R} (f_\theta, \mathcal{S}_j) = -{|\mathcal{S}_j|}^{-1} \sum_{i=1}^{|\mathcal{S}_j|} \mathbf{1}_{y_i} \cdot \log f_\theta(x_i)$ is the empirical risk in each partition. Since we are only concerned with the existence of a desired minimizer of the empirical risk, we can view $f_\theta$ as able to achieve any labeling of the training inputs that suffices the local Lipschitz constraint. 
% \chengyu{This is the only problem left, there are additional constraints imposed by those perimeter inputs that are close to other centers. In this case considering the Lipschitz continuity of true distribution at the scale of $2r$, and show the additional constraint would not affect the miniizer a lot?} 
Thus the empirical risk minimization is equivalent to the minimization of the empirical risk in each partition.
The problem can thus be defined as, for each $j = 1, \cdots, N_{\rho\varepsilon}$,
% The goal is that for each $j$, find $\mathbf{p}_i = f_\theta(x_i)$ and $\mathbf{\bar{p}} = f_\theta(\bar{x}_j)$ for each $i=1,\cdots, |\mathcal{S}_j(r)|$ that minimizes $\hat{R} (f_\theta, \mathcal{S}_j(r))$, given the constraint $\|\mathbf{p}_i - \mathbf{p}\| \le Lr $, which is imposed by the local Lipschitz continuity of the function $f_\theta$.
% \chengyu{The bound is due to all examples are within $r$ from $\bar{x}$}
% Now define the minimization problem as 
\begin{equation}
\label{eq:minimum-partition}
\begin{aligned}
& \min_{f_\theta} \hat{R} (f_\theta, \mathcal{S}_j)\\
& s.t.~~ \|f_\theta(x) - f_\theta(\bar{x}_j)\|_1 \le L\rho\varepsilon, 
% ~\forall~i=1,\cdots, |\mathcal{S}_j(r)|.
~\forall~x \in \mathcal{S}_j,
\end{aligned}
\end{equation}
where the constraint is imposed by the locally-Lipschitz continuity of $f_\theta$. By the following lemma, we show that the minimizer of such problem is achieved only if $f_\theta(\bar{x}_j)$ is close to the sample mean.

\begin{lemma}
\label{lemma:vector-minimization}
Let $\bar{\mathbf{y}} ={|\mathcal{S}_j|}^{-1} \sum_{x \in \mathcal{S}_j} \mathbf{1}_{y}$.  The minimum of the problem (\ref{eq:minimum-partition}) is achieved only if $f_\theta(\bar{x}_j) = \mathbf{\bar{y}_j}(1 + KL_\theta \rho\varepsilon) - L_\theta\rho\varepsilon$.
\end{lemma}


\begin{proof}
We note that since the loss function we choose is strongly convex, to minimize the empirical risk, the prediction of any input $x$ must be as close to the one-hot labeling as possible. Therefore the problem (\ref{eq:minimum-partition}) can be formulated into a vector minimization where we can employ Karush–Kuhn–Tucker (KKT) theorem to find the necessary conditions of the minimizer. 
% See detailed proof in Appendix~\ref{section:all-proof}.
% \end{proof}

% \subsection{Proof of Lemma~\ref{lemma:vector-minimization}}
% \begin{proof}
Let $\mathbf{p}_i := f_\theta(x_i)$ and $\tilde{\varepsilon} = L\rho\varepsilon$ for simplicity.
We rephrase the problem~(\ref{eq:minimum-partition}) as 

\begin{equation}
\label{eq:vector-minimization2}
\begin{aligned}
    \min_{\{\mathbf{p}_i\}_{i=1}^N} & - \frac{1}{N} \sum_{i} \mathbf{1}_{y_i} \cdot \log \mathbf{p}_i \\
    s.t.~~& \|\mathbf{p}_i - \mathbf{p}\|_1 \le \tilde{\varepsilon}, ~\sum_k \mathbf{p}_i^k = 1, ~\sum_k \mathbf{p}^k = 1, \mathbf{p}^k_i \ge 0, \mathbf{p}^k \ge 0. \\
\end{aligned}
\end{equation}

\smallsection{Case I}
We first discuss the case when $\mathbf{p}^k + \tilde{\varepsilon} < 1$ for all $k$. 
First, we observe that for any $\mathbf{p}$, the minimum of the above problem is achieved only if $\mathbf{p}_i^{y_i} = \mathbf{p}^{y_i} + \tilde{\varepsilon}$. Because by contradiction, if $\mathbf{p}_i^{y_i} < \mathbf{p}^{y_i} + \tilde{\varepsilon}$, we will have $-\log \mathbf{p}_i^{y_i} > -\log(\mathbf{p}^{y_i} + \tilde{\varepsilon}) $, and $\mathbf{p}^{y_i} + \tilde{\varepsilon}$ belongs to the feasible set, which means $\mathbf{p}_i^{y_i}$ does not attain the minimum.

The above problem can then be rephrased as
\begin{equation}
\begin{aligned}
    \min_{\mathbf{p}} - \frac{1}{N} \sum_{i} \log( {\mathbf{p}}^{y_i} + \tilde{\varepsilon}),~~ s.t.~~\sum_k \mathbf{p}^k = 1, \mathbf{p}^k \ge 0,
\end{aligned}
\end{equation}
where we have neglected the condition associated with $\mathbf{p}_i^{k\ne y_i}$, since they do not contribute to the objective, they can be chosen arbitrarily as long as the constraints are sufficed, and clearly the constraints are underdetermined.

Let $N_k = \sum_i 1(y_i = k)$, we have $\sum_i \log (\mathbf{p}^{y_i} + \tilde{\varepsilon}) = \sum_k N_k \log (\mathbf{p}^k + \tilde{\varepsilon})$. Therefore the above problem is equivalent to
\begin{equation}
\begin{aligned}
    \label{eq:convex-minimization}
    \min_{\mathbf{p}}- \sum_k \mathbf{\bar{y}}^k \log (\mathbf{p}^k + \tilde{\varepsilon}),~~ s.t.~~\sum_k \mathbf{p}^k = 1, \mathbf{p}^k \ge 0,
\end{aligned}
\end{equation}
where $\mathbf{\bar{y}} \equiv [N_1 / N, \cdots, N_k / N]^T $ is equal to the sample mean $N^{-1}\sum_i \mathbf{1}_{y_i}$. 

To solve the strongly convex minimization problem (\ref{eq:convex-minimization}) it is easy to employ KKT conditions to show that  
$$ \mathbf{p} = \mathbf{\bar{y}} (1 + K\tilde{\varepsilon}) - \tilde{\varepsilon}. 
$$

% \vspace{0.1em}
\smallsection{Case II} We now discuss the case when $\mathbf{\hat{p}}$ is the minimizer of (\ref{eq:vector-minimization2}) and there exists $k'$ such that $\mathbf{\hat{p}}^{k'} + \tilde{\varepsilon} \ge 1$. And $\mathbf{\hat{p}}\ne p$, where $p = \mathbf{\bar{y}}(1+K\tilde{\varepsilon}) - \tilde{\varepsilon}$ is the form of the minimizer in the previous case.

Considering a non-trivial case $\mathbf{p^*}^{k'} < 1 - \tilde{\varepsilon}$. Otherwise the true label distribution is already close to the one-hot labeling, which is the minimizer of the empirical risk. Therefore by $\sum_{k\ne k'} {p}^{k} > \tilde{\varepsilon}$ we have the condition
\begin{equation}
    \sum_{k\ne k'}\mathbf{\bar{y}}^k >  \frac{K\tilde{\varepsilon}}{1 + K\tilde{\varepsilon}}
\end{equation}


Now considering the minimization objective $R(p) = - {N}^{-1} \sum_i \mathbf{1}_{y_i} \cdot \log \mathbf{p}_i$. For all $i$ with $y_i = k'$, we must have $\mathbf{p}_i^{y_i} = 1$, otherwise the optimal cannot be attained by contradiction. Then the minimization problem can be rephrased as
\begin{equation}
    \min \sum_{k\ne k'} \mathbf{\bar{y}}^k \log (\mathbf{\hat{p}} + \tilde{\varepsilon}), ~s.t. \sum_{k'\ne k} \mathbf{\hat{p}}^{k} \ge \tilde{\varepsilon}, \mathbf{\hat{p}}^{k} \ge 0,
\end{equation}
where the first constraint is imposed by $\mathbf{\hat{p}}^{k'} \ge 1 - \tilde{\varepsilon}$.


Employ KKT conditions similarly we can have $\mathbf{\hat{p}}^k = \mathbf{\bar{y}}^k/\lambda - \tilde{\varepsilon}$ where $\lambda$ is a constant. By checking the constraint we can derive $\lambda \ge \sum_k \mathbf{\bar{y}}^k / (K\tilde{\varepsilon})$.

However, the minimization objective 
$$
\min_{\lambda} - \sum_{k\ne k'} \mathbf{\bar{y}}^k \log \frac{\mathbf{\bar{y}}^k}{\lambda},
$$
requires $\lambda$ to be minimized. Therefore $\lambda = \sum_{k\ne k'} \mathbf{\bar{y}}^k / (K\tilde{\varepsilon})$, which implies
\begin{equation}
\mathbf{\hat{p}}^k = K\tilde{\varepsilon} \frac{\mathbf{\bar{y}}^k}{\sum_{k\ne k'} \mathbf{\bar{y}}^k} - \tilde{\varepsilon}.
\end{equation}

Now since $\mathbf{\hat{p}} = \arg\min_p R(p)$ and $\mathbf{\hat{p}} \ne p$, we must have $R(\mathbf{\hat{p}}) < R(p)$. This means
\begin{equation}
 - \sum_{k\ne k'} \mathbf{\bar{y}}^k \log \frac{K\tilde{\varepsilon} \mathbf{\bar{y}}^k}{\sum_{k\ne k'} \mathbf{\bar{y}}^k} < - \sum_{k\ne k'} \mathbf{\bar{y}}^k \log [\mathbf{\bar{y}}^k(1+K\tilde{\varepsilon})],
\end{equation}
which is reduced to
\begin{equation}
\sum_{k\ne k'}\mathbf{\bar{y}}^{k} < \frac{K\tilde{\varepsilon}}{1+K\tilde{\varepsilon}}
\end{equation}
But this is contradict to our assumption.
\end{proof}





% \smallsection{Step III: Bound the error between the minimizer's prediction and the sample mean}
We are now be able to bound the difference between the predictions of the training inputs produced by the empirical risk minimizer and the sample mean in each $\mathcal{S}_j$. To see that we have for each $x \in \mathcal{S}_j$. % This is straightforward to show as for all training inputs in each partition $x \in \mathcal{S}_j(r)$, 
\begin{equation}
\begin{aligned}
    \|f_\theta(x) - \mathbf{\bar{y}}_j \|_1 
    & \le \|f_\theta(x) - f_\theta(\bar{x}_j) \|_1 + \|f_\theta(\bar{x}_j) - \mathbf{\bar{y}}_j \|_1  \\
    & \le L\rho\varepsilon ( 1 + K\|\mathbf{\bar{y}}_j -  K^{-1}\mathbf{1}\|_1)\\
    & \le L\rho\varepsilon ( 1 + K\|\mathbf{1}_{(\cdot)} - K^{-1}\mathbf{1}\|_1).\\
    & = L\rho\varepsilon\left(3 - \frac{2}{K}\right)
\end{aligned}
\end{equation}
% By Lemma~\ref{lemma:sample-mean-relax} we then have for any $x \in \mathcal{S}_j$, 
By Equation~(\ref{lemma:sample-mean-relax}) we then have for any $x \in \mathcal{S}_j$, with probability $1 - \delta$,
\begin{equation}
    \label{eq:final-bound-each-partition}
    \|f_\theta(x) - p(x)\|_1 \le \sqrt{\frac{2K}{|\mathcal{S}_j|}\log\frac{2}{\delta}} + L_\theta\rho\varepsilon\left(3 - \frac{2}{K}\right) + 2L\rho\varepsilon,\\ 
    % P\left(\|f_\theta(x) - p(x)\| \ge \tilde{\varepsilon}\right)  \le 2\exp\left(-\frac{|\mathcal{S}_j|}{2K}{\tilde{\tilde{\varepsilon}}}^2\right),
\end{equation}
which means the difference between the predictions and the true label distribution is also bounded.




\smallsection{Step III: Show the disjoint partition is non-trivial}
In (\ref{eq:final-bound-each-partition}), we have managed to bound the difference between the predictions yielded by an empirical risk minimizer and the true label distribution based on the cardinality of the subset $|\mathcal{S}_j|$, namely the number of inputs in $j$-partition. However $|\mathcal{S}_j|$ is critical to the bound here as if $|\mathcal{S}_j| = 1$, then (\ref{eq:final-bound-each-partition}) becomes a trivial bound. Here we show $|\mathcal{S}_j|$ is non-negligible based on simple combinatorics.

\begin{lemma}
\label{lemma:bound-partition}
Let $\{\mathcal{S}_j\}_{j=1}^{N_{\rho\varepsilon}}$ be a disjoint partition of the entire training set $\mathcal{S}$. Denote $\mathcal{S}(x)$ as the partition that includes $x$. Let $N(x) = |\mathcal{S}(x)|$ and $N = |\mathcal{S}|$.
 Then for any $\kappa \ge 1$,
 \begin{equation}
     \left| \left\{ x ~|~ N(x) \ge \frac{ N }{\kappa N_{\rho\varepsilon}}  \right\} \right| \ge \left(1 - \frac{1}{\kappa} + \frac{1}{\kappa N_{\rho\varepsilon}}\right)N.
 \end{equation}
\end{lemma}
\begin{proof}
We note that the problem is to show the minimum number of $x$ such that $N(x) \ge N / (\kappa N_{\rho\varepsilon})$. This is equivalent to find the maximum number of $x$ such that $N(x) \le N / (\kappa N_{\rho\varepsilon})$. Since we only have $N_{\rho\varepsilon}$ subsets, the maximum can be attained only if for $N_{\rho\varepsilon} - 1$ subsets $\mathcal{S}$, $|\mathcal{S}| = N / (\kappa N_{\rho\varepsilon})$. Otherwise, if for any one of these subsets $|\mathcal{S}| < N / (\kappa N_{\rho\varepsilon})$, then it is always feasible to let $|\mathcal{S}| = N / (\kappa N_{\rho\varepsilon})$ and the maximum increases. Similarly, if the number of such subsets is less than $N_{\rho\varepsilon} - 1$, then it is always feasible to let another subset subject to $|\mathcal{S}| = N / (\kappa N_{\rho\varepsilon})$ and the maximum increases. We can then conclude that at most $N(N_{\rho\varepsilon} - 1)/(\kappa N_{\rho\varepsilon})$ inputs can have the property $N(x) \le N / (\kappa N_{\rho\varepsilon})$.

\end{proof}
The above lemma basically implies when partitioning $N$ inputs into $N_{\rho\varepsilon}$ subsets, a large fraction of the inputs will be assigned to a subset with cardinality at least $N / (\kappa N_{\rho\varepsilon})$. Here $N_{\rho\varepsilon}$ is the covering number and is bounded above based on the property of the covering in the Euclidean space. Apply Lemma~\ref{lemma:bound-partition} to (\ref{eq:final-bound-each-partition}), and use the fact that $\|\cdot\|_{\text{TV}} = \|\cdot\|_1/2$ for category distributions, we then arrive at Lemma~\ref{lemma:Learn-true-distribution}.
% Theorem~\ref{theorem:error}. 






