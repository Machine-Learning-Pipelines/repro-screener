\subsection{Preliminaries}
\smallsection{Proof in Assumption~\ref{assumption:clean-dataset}}
Here we prove that if there no label error in the clean dataset, then $P(\tilde{Y}|x) = P(Y|x)$.

\begin{proof}
First, we note that 
$$
    P(\tilde{Y}=j'|x) = \sum_j P(\tilde{Y}=j'| Y=j, x) P(Y=j|x).
$$
Since $P(E=1| Y=j, x) = 0$ we have,
$$
    \begin{aligned}
    P(\tilde{Y}=j'| Y=j, x)
    & = \sum_e P(\tilde{Y}=j'| E=e, Y=j, x) P(E=e| Y=j, x)  \\
    & = P (\tilde{Y}=j'| E=0, Y=j, x) \\
    & = 
    \begin{cases}
    1, & ~\text{if}~j'=j,\\
    0, & ~\text{otherwise}.
    \end{cases}
    \end{aligned}
$$
Therefore we have for all $j'$,
$$
    P(\tilde{Y}=j'|x) = P(Y=j'|x). 
$$
\end{proof}



\smallsection{Total Variation distance for discrete probability distributions}
    For two discrete probability distributions $P(Y)$ and $P(Y')$ where $Y, Y'\in\mathcal{Y}$, the total variation distance between them can be equally defined as
    $$
     \begin{aligned}
       \|P(Y) - P(Y')\|_{\text{TV}} 
       & = \sup_{J\in \mathcal{A}} \left| P(Y\in J) - P(Y'\in J)\right| \\
       & = \sup_{J\in \mathcal{A}} \left| \sum_{j\in J} P(Y=j) - \sum_{j\in J}P(Y'=j)\right| \\
      & = \frac{1}{2}\sum_j |P(Y=j) - P(Y'=j|)| \\
     \end{aligned}
    $$
     