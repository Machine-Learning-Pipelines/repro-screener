\section{Study on a synthetic dataset with known true label distribution}
% \label{sect:synthetic}


% \note{2) change legends in figure 5 and table 1. Say $\rho=0$ is original dataset, $\rho=0.5$ is the traditional adversarial label, best model is the well-calirated model, last model is sth like the over-optimized model. 3) $\rho probably < 0.5$, like $\rho = 0.5 - \delta$.}

% \chengyu{Can move this section to appendix. or later part after the experiment results as some additional analysis of our method. }
% \chengyu{Or replace with similar experiments on cifar-10h}

    % Here, we empirically verify if the model probability can indeed approximate the true label distribution better than the traditional adversarial labels.
    
    \begin{wrapfigure}{r}{3cm} % [!ht]
    % \begin{figure} % {l}{3cm}% [!ht]
      \vspace{-3mm}
      \centering
      \includegraphics[width=0.2\textwidth]{figures/method-augment-image-example.pdf}
      \caption{Sample image by mixup augmentation.}
    \label{fig:method-augment-example}
    % \end{figure}
    \end{wrapfigure}
    
    \smallsection{Synthetic Dataset}
    Since the true label distribution is typically unknown for adversarial examples in real-world datasets, 
    we simulate the mechanism of implicit label noise in adversarial training from a feature learning perspective.
    Specifically, we adapt \emph{mixup}~\citep{Zhang2018mixupBE} for data augmentation on CIFAR-10. 
    For every example $x$ in the training set, we randomly select another example $x'$ in a different class and linearly interpolate them by a ratio $\rho$, namely $x:= \rho x + (1-\rho) x'$, which essentially perturbs $x$ with features from other classes. 
    Therefore, the true label distribution is arguably $y \sim \rho\cdot \mathbbm{1}(y) + (1-\rho)\cdot  \mathbbm{1}(y')$.
    Unlike mixup, we intentionally set the assigned label as $\hat{y} \sim \mathbbm{1}(y)$, thus deliberately create a mismatch between the true label distribution and the assigned label distribution.
    We refer this strategy as \emph{mixup augmentation} and only perform it once before the training. 
    % so we finalize the true distribution for every example.
    In this way, the true label distribution of every example in the synthetic dataset is fixed.
    % obtain a synthetic dataset with known true label distribution.
    
    % \begin{wrapfigure}{r}{5.5cm}% [!ht]
    %   \centering
    %   \includegraphics[width=1.0\linewidth]{figures/method-augment.pdf}
    %   \caption{Standard training on the synthetic dataset with $\rho=0.5$ and the original dataset (i.e., $\rho=0$). $(\cdot)$ in the legend indicates the distributions employed as the supervision. \jingbo{I don't think the legends here are clear enough. And it is not necessary to keep both best model and last model. We only need rho = 0, rho = 0.5 with assigned labels, rho = 0.5 with model probability. This will be enough to justify our points.}}%
    %   \label{fig:method-augment}
    % \end{wrapfigure}
    
    
%     \smallsection{Epoch-wise double descent can be observed on mixup augmentation}
%     In Figure~\ref{fig:method-augment}, we conduct standard training on the synthetic dataset with $\rho=0.5$ and the original dataset (i.e., $\rho = 0$) (See Appendix~\ref{sect: exp-mixup-augment} for experiment details). 
%     Compared with the training on the original dataset, the training on the synthetic dataset clearly exhibits an epoch-wise double descent, although no explicit label noise is injected into the dataset \jingbo{I'm a bit concerned here to say that no explicit label noise injected, because $\rho$ is 0.5. It will make me more comfortable if $\rho$ is strictly less than 0.5}, which further demonstrates the connection between the implicit label noise induced by distribution mismatch and the double descent phenomenon.


%     \smallsection{Model probability approximates the true label distribution}
%     Since we know the true label distribution in the synthetic dataset, we now quantitatively evaluate the approximation of model probability w.r.t. the true label distribution. 
%     We train a surrogate model on the same synthetic dataset to generate the probability. 
%     Since the true distribution is known, we can directly measure the distance between the model probability distribution and the true distribution. 
%     In addition to the total variation distance that measures the distribution mismatch, there are existing evaluation metrics in the literature such as \emph{calibration error}~\citep{Naeini2015ObtainingWC, Guo2017OnCO}, a measure specifically designed for classification algorithms, and \emph{proper scoring rules}~\citep{Gneiting2007StrictlyPS} such as negative log-likelihood (NLL) and Brier score~\citep{Brier1950VERIFICATIONOF} from a frequentist notion of uncertainty~\citep{Dawid1982TheWB, Degroot1983TheCA}. 
%     Instead of the label distribution, these metrics only requires the ground-truth hard label, which is accessible in an implicit label noise case since $\argmax_j p(\hat{y}=j|x) = \argmax_j p(y=j|x)$. 
%     Due to its simplicity and wide use in \emph{confidence calibration}~\citep{Guo2017OnCO}, we select the NLL loss on the validation set as a complementary metric to measure the quality of a approximate distribution. Smaller NLL loss means a better approximation to the true distribution. 
%     By Gibbs inequality, the NLL loss will be minimized when the model produces the true distribution.
%     Table~\ref{table: distance} shows the TV distance and the NLL loss to the true distribution of the model probability generated at the best checkpoint \jingbo{Do we have to emphasize the best vs. last? I think picking the best checkpoint is kind of common practice, assuming ``best'' is decided based on a dev set.} during training is significantly smaller than that of the assigned label.



% \begin{wraptable}{r}{7cm}
%   \caption{Average TV distance between the true label distribution and various approximate label distributions. The accuracy of the label distribution in terms of the argmax of the true label is also listed for reference.}
% \label{table: distance}
%   \small
%   \begin{tabular}{rlll}
%     \toprule
%     Distribution & Acc (\%) & TV & NLL (Val) \\
%     \midrule
%     Assigned label & 100  & 0.5 & - \\
%     Best Model & 80.24 & 0.4420 & 0.9504 \\
%     Last Model & 99.84 & 0.4996 & 2.7767 \\
%     \bottomrule
%   \end{tabular}
% \end{wraptable}

%     \smallsection{Employing model probability in training can mitigate double descent}
%     As suggested by Equation~(\ref{eq:loss-soft-label}), we simply substitute the one-hot label in the training objective by model probability with $T=1$ and $\rho=1$. 
%     Indeed, Figure~\ref{fig:method-augment} confirms that that employing the model at the best checkpoint as supervision can effectively mitigate the double descent. 
%     In contrast, the model at the last checkpoint cannot alleviate the double descent, which can also be reflected by the TV distance in Table~\ref{table: distance}, despite its high accuracy.
%     \jingbo{the following sentence may be not that important. Could be removed}
%     In Appendix~\ref{sect:better-approximate-mixup}, we show that the approximate label distribution modulated by the temperature and the interpolation ratio can further approach the true label distribution and mitigate the double descent more effectively.



% \input{appendix/4-synthetic-experiments/4.1-mixup}
\input{appendix/4-synthetic-experiments/4.2-concentration}