\section{More empirical analyses}

\subsection{Epoch-wise double descent is ubiquitous in adversarial training}
% \subsection{Reconcile robust overfitting and epoch-wise double descent}
\label{sect:double-descent-reconcile}

% In this section, we conduct extensive experiments with different optimizers, sample sizes, model architectures, and learning rate schedulers to verify the connection between robust overfitting and epoch-wise double descent. The default experiment settings are listed in Appendix~\ref{sect: exp-double-descent} in detail.

In this section, we conduct extensive experiments with different model architectures, and learning rate schedulers to verify the connection between robust overfitting and epoch-wise double descent. The default experiment settings are listed in Appendix~\ref{sect: exp-double-descent} in detail.


% We focus on PGD training as it is simple in nature and can produce robustness comparable to state-of-the-art methods~\cite{Rice2020OverfittingIA, pang2021bag, Gowal2020UncoveringTL}.

% \smallsection{Optimizer}
% Similar to the setting employed in \citet{Nakkiran2020DeepDD}, we conduct the adversarial training using both the Adam optimizer and SGD. As already shown in Figure~\ref{fig:intro}, double descent can be observed for both optimizers, although Adam may be inferior compared to SGD.


% \smallsection{Sample size}
% We randomly sample a desired number of examples without replacement from the original training set in CIFAR-10. As shown in Figure \ref{fig:reconcile-sample-size}, for both optimizers, increasing sample size will shrink the area under the double descent curve, but will not significantly distort its shape. 
% % reduce the overfitting gap, defined by the difference between the intermediate minimum and intermediate maximum of the robust test error.
% Similar observation is also made for double descent curve under standard training~\citep{Nakkiran2020DeepDD}.

% \begin{figure*}[!ht]
%   \centering
%   \includegraphics[width=0.9\textwidth]{figures/reconcile-sample-size.pdf}
%   \caption{Varying sample size will shrink the area under the epoch-wise double descent curve, but will not significantly distort its shape.
%   }
% \label{fig:reconcile-sample-size}
% \end{figure*}

% \subsection{Model, learning rate scheduler and regularization}

% \chengyu{Figure: Four subfigures side by side}


\smallsection{Model capacity}
We modulate the capacity of the deep model by varying the widening factor of the Wide ResNet. To extend the lower limit of the capacity, we allow the widening factor to be less than $1$. In such case, the number of channels in each residual block is scaled similarly but rounded, and the number of channels in the first convolutional layer will be reduced accordingly to ensure the width monotonically increasing through the forward propagation. 
% To accelerate the training with an extremely large model, we randomly sample a training set of size $5000$ and employ the Adam optimizer, since the sample size will not significantly distort the shape of the double descent as shown above. Figure~\ref{fig:reconcile-model-capacity} shows that the double descent will gradually become more complete as the model capacity increases and the model translates from under-parameterized to over-parameterized regime~\citep{Nakkiran2020DeepDD}.


\begin{figure*}[!ht]
\centering
\begin{subfigure}[t]{.48\textwidth}
  \centering
  \includegraphics[width=.95\textwidth]{figures/reconcile-model-architecture.pdf}
  \caption{
  % Different model architectures will affect the double descent curve. In particular, VGG-11 will have the second descent delayed due to its inferior performance compared to residual architectures.
  Epoch-wise double descent curves in adversarial training with various model architectures.
  }
  \label{fig:reconcile-model-architecture}
\end{subfigure}\hfill
\begin{subfigure}[t]{.48\textwidth}
  \centering
  \includegraphics[width=0.95\textwidth]{figures/reconcile-lr-scheduler.pdf}
  \caption{
  % The effect of the learning rate scheduler on the epoch-wise double descent curve in adversarial training. Modulating the model capacity can produce training curves with diverse behaviors. Different model architectures may produce slightly different double descent curves. 
   Epoch-wise double descent curves in adversarial training with various learning rate schedulers.
   The curves are smoothed by a moving average with a window of $5$ to avoid overlapping.
  }
\label{fig:reconcile-lr-scheduler}
\end{subfigure}
  \caption{Effect of model on the epoch-wise double descent curve}
 \label{fig:reconcile-model}
\end{figure*}

% \begin{figure*}[!ht]
%   \centering
%   \includegraphics[width=0.9\textwidth]{figures/reconcile-model.pdf}
%   \caption{The effect of the model architecture on the epoch-wise double descent curve in adversarial training. Modulating the model capacity can produce training curves with diverse behaviours. Different model architectures may produce slightly different double descent curves. The training curve is smoothed by moving average with a window of $5$.
%   }
% \label{fig:reconcile-model}
% \end{figure*}

% As proposed for double descent under standard training, both increasing the model capacity and training longer will increase the effective model complexity (EMC)~\citep{Nakkiran2020DeepDD}, thus driving the model from under-parameterized regime to over-parameterized regime. Indeed, as shown in Figure \ref{fig:reconcile-model}, modulating the capacity of the model can produce diverse behaviours of the training curve. For fairly small models, even the ascent interval will not be observed thus the robust test error will monotonically decrease throughout the training. For moderately large models, the ascent interval is observed thus the robust test error will increase after a certain point during training, which is the typical case present in adversarial training practice and known as robust overfitting. Finally, for sufficiently large model, a more complete picture of the double descent can be observed. Therefore the robust test error will decrease again after the robust overfitting. We note that due to computational constraints, we cannot capture the entire second descent interval, wherefore whether the minimum achieved in the second descent will be better than the first minimum is not clear. Nevertheless, under certain circumstances, such as with smaller perturbation radius or better data quality, we can indeed observe a better second minimum, which will be discussed in Section \ref{sect:factor}.

% In addition, increasing the capacity will also gradually shrink the overfitting gap. One may imagine that sufficiently large model will not undergo double descent and the robust overfitting will not happen likewise. We indeed observe this under certain circumstances as shown in Section \ref{sect:factor}.

\smallsection{Model architecture} We also experiment on model architectures other than Wide ResNet, including pre-activation ResNet-18~\citep{He2016IdentityMI} and VGG-11~\citep{Simonyan2015VeryDC}. We select these configurations to ensure comparable model capacities\footnote{WRN-28-5, pre-activation ResNet-18 and VGG-11 have $9.13\times 10^6$, $11.17\times 10^6$ and $9.23\times 10^6$ parameters, respectively.}. As shown in Figure \ref{fig:reconcile-model}, different model architectures may produce slightly different double descent curves. The second descent of VGG-11 in particular will be delayed due to its inferior performance compared to residual architectures.

% \smallsection{Optimizer}

% \smallsection{Method}
% \chengyu{Maybe method is more important that regularization}

\smallsection{Learning rate scheduler}
% \chengyu{maybe put the figure to appendix}
A specific learning rate scheduler may shape the robust overfitting differently as suggested by \citet{Rice2020OverfittingIA}. We consider the following learning rate schedulers in our experiments.
\begin{itemize}[leftmargin=*]
    \item \textbf{Piecewise decay}: The initial learning rate rate is set as $0.1$ and is decayed by a factor of $10$ at the $100$th and $500$th epochs within a total of $1000$ epochs.
    \item \textbf{Cyclic}: This scheduler was initially proposed by \citet{Smith2017CyclicalLR} and has been popular in adversarial training. We set the maximum learning rate to be $0.2$, and the learning rate will linearly increase from $0$ to $0.2$ for the initial $400$ epochs and decrease to $0$ for the later $600$ epochs.
    \item \textbf{Cosine}: This scheduler was initially proposed by \citet{Loshchilov2017SGDRSG}. The learning rate starts at $0.1$ and gradually decrease to $0$ following a cosine function for a total of $1000$ epochs.
\end{itemize}
Experiments on various learning rate schedulers show the second descent can be widely observed except the piecewise decay, where the appearance of second descent might be delayed due to extremely small learning rate in the late stage of training. 
% This further demonstrates the connection between robust overfitting and epoch-wise double descent.

% \label{sect: exp-lr-scheduler}
% \todo{merge with above}
% For SGD specifically, we also consider the following types of learning rate scheduler to align with \citet{Rice2020OverfittingIA}.


% \begin{figure*}[!ht]
%   \centering
%   \includegraphics[width=0.45\textwidth]{figures/reconcile-lr-scheduler.pdf}
%   \caption{The effect of the learning rate scheduler on the epoch-wise double descent curve in adversarial training. Modulating the model capacity can produce training curves with diverse behaviors. Different model architectures may produce slightly different double descent curves. The training curve is smoothed by moving average with a window of $5$.
%   }
% \label{fig:reconcile-lr-scheduler}
% \end{figure*}

% \smallsection{Regularization}
% \subsection{\chengyu{Bias-variance analysis of adversarial training}}


% \input{appendix/2-experiments/2.2-dependence}
% \input{appendix/2.3-modelwise}
% \input{appendix/2-experiments/2.5-gaussian-noise}
% \input{appendix/2-experiments/2.6-static-noise}

% \input{appendix/2-experiments/2.7-data-quality}
