\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andriushchenko et~al.(2020)Andriushchenko, Croce, Flammarion, and
  Hein]{Andriushchenko2020SquareAA}
Andriushchenko, M., Croce, F., Flammarion, N., and Hein, M.
\newblock Square attack: a query-efficient black-box adversarial attack via
  random search.
\newblock \emph{ArXiv}, abs/1912.00049, 2020.

\bibitem[Athalye et~al.(2018)Athalye, Carlini, and
  Wagner]{Athalye2018ObfuscatedGG}
Athalye, A., Carlini, N., and Wagner, D.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock \emph{ArXiv}, abs/1802.00420, 2018.

\bibitem[Belkin et~al.(2019)Belkin, Hsu, Ma, and
  Mandal]{Belkin2019ReconcilingMM}
Belkin, M., Hsu, D.~J., Ma, S., and Mandal, S.
\newblock Reconciling modern machine-learning practice and the classical
  biasâ€“variance trade-off.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116:\penalty0
  15849 -- 15854, 2019.

\bibitem[Carmon et~al.(2019)Carmon, Raghunathan, Schmidt, Liang, and
  Duchi]{Carmon2019UnlabeledDI}
Carmon, Y., Raghunathan, A., Schmidt, L., Liang, P., and Duchi, J.~C.
\newblock Unlabeled data improves adversarial robustness.
\newblock \emph{ArXiv}, abs/1905.13736, 2019.

\bibitem[Chen \& Gu(2020)Chen and Gu]{Chen2020RaySAR}
Chen, J. and Gu, Q.
\newblock Rays: A ray searching method for hard-label adversarial attack.
\newblock \emph{Proceedings of the 26th ACM SIGKDD International Conference on
  Knowledge Discovery \& Data Mining}, 2020.

\bibitem[Chen et~al.(2021)Chen, Zhang, Liu, Chang, and Wang]{chen2021robust}
Chen, T., Zhang, Z., Liu, S., Chang, S., and Wang, Z.
\newblock Robust overfitting may be mitigated by properly learned smoothening.
\newblock In \emph{International Conference on Learning Representations},
  volume~1, 2021.

\bibitem[Croce \& Hein(2020)Croce and Hein]{Croce2020ReliableEO}
Croce, F. and Hein, M.
\newblock Reliable evaluation of adversarial robustness with an ensemble of
  diverse parameter-free attacks.
\newblock In \emph{ICML}, 2020.

\bibitem[Devries \& Taylor(2017)Devries and Taylor]{Devries2017ImprovedRO}
Devries, T. and Taylor, G.~W.
\newblock Improved regularization of convolutional neural networks with cutout.
\newblock \emph{ArXiv}, abs/1708.04552, 2017.

\bibitem[Ding et~al.(2020)Ding, Sharma, Lui, and Huang]{Ding2020MaxMarginA}
Ding, G.~W., Sharma, Y., Lui, K. Y.~C., and Huang, R.
\newblock Max-margin adversarial (mma) training: Direct input space margin
  maximization through adversarial training.
\newblock \emph{ArXiv}, abs/1812.02637, 2020.

\bibitem[Dong et~al.(2021{\natexlab{a}})Dong, Liu, and Shang]{Dong2021DataPF}
Dong, C., Liu, L., and Shang, J.
\newblock Data profiling for adversarial training: On the ruin of problematic
  data.
\newblock \emph{ArXiv}, abs/2102.07437, 2021{\natexlab{a}}.

\bibitem[Dong et~al.(2021{\natexlab{b}})Dong, Xu, Yang, Pang, Deng, Su, and
  Zhu]{Dong2021ExploringMI}
Dong, Y., Xu, K., Yang, X., Pang, T., Deng, Z., Su, H., and Zhu, J.
\newblock Exploring memorization in adversarial training.
\newblock \emph{ArXiv}, abs/2106.01606, 2021{\natexlab{b}}.

\bibitem[Fr{\'e}nay \& Verleysen(2014)Fr{\'e}nay and
  Verleysen]{Frnay2014ClassificationIT}
Fr{\'e}nay, B. and Verleysen, M.
\newblock Classification in the presence of label noise: A survey.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  25:\penalty0 845--869, 2014.

\bibitem[Gneiting \& Raftery(2007)Gneiting and Raftery]{Gneiting2007StrictlyPS}
Gneiting, T. and Raftery, A.
\newblock Strictly proper scoring rules, prediction, and estimation.
\newblock \emph{Journal of the American Statistical Association}, 102:\penalty0
  359 -- 378, 2007.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{Goodfellow2015ExplainingAH}
Goodfellow, I.~J., Shlens, J., and Szegedy, C.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{CoRR}, abs/1412.6572, 2015.

\bibitem[Gowal et~al.(2020)Gowal, Qin, Uesato, Mann, and
  Kohli]{Gowal2020UncoveringTL}
Gowal, S., Qin, C., Uesato, J., Mann, T.~A., and Kohli, P.
\newblock Uncovering the limits of adversarial training against norm-bounded
  adversarial examples.
\newblock \emph{ArXiv}, abs/2010.03593, 2020.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{Guo2017OnCO}
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.~Q.
\newblock On calibration of modern neural networks.
\newblock \emph{ArXiv}, abs/1706.04599, 2017.

\bibitem[Hastie et~al.(2001)Hastie, Tibshirani, and Friedman]{Hastie2001TheEO}
Hastie, T.~J., Tibshirani, R., and Friedman, J.~H.
\newblock The elements of statistical learning.
\newblock 2001.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{He2016IdentityMI}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Identity mappings in deep residual networks.
\newblock \emph{ArXiv}, abs/1603.05027, 2016.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{Hinton2015DistillingTK}
Hinton, G.~E., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{ArXiv}, abs/1503.02531, 2015.

\bibitem[Huang et~al.(2015)Huang, Xu, Schuurmans, and
  Szepesvari]{Huang2015LearningWA}
Huang, R., Xu, B., Schuurmans, D., and Szepesvari, C.
\newblock Learning with a strong adversary.
\newblock \emph{ArXiv}, abs/1511.03034, 2015.

\bibitem[Ilyas et~al.(2019)Ilyas, Santurkar, Tsipras, Engstrom, Tran, and
  Madry]{Ilyas2019AdversarialEA}
Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran, B., and Madry, A.
\newblock Adversarial examples are not bugs, they are features.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{Izmailov2018AveragingWL}
Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., and Wilson, A.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock \emph{ArXiv}, abs/1803.05407, 2018.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{Keskar2017OnLT}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{ArXiv}, abs/1609.04836, 2017.

\bibitem[Krizhevsky(2009)]{Krizhevsky2009LearningML}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kurakin et~al.(2017)Kurakin, Goodfellow, and
  Bengio]{Kurakin2017AdversarialML}
Kurakin, A., Goodfellow, I.~J., and Bengio, S.
\newblock Adversarial machine learning at scale.
\newblock \emph{ArXiv}, abs/1611.01236, 2017.

\bibitem[Le \& Yang(2015)Le and Yang]{Le2015TinyIV}
Le, Y. and Yang, X.
\newblock Tiny imagenet visual recognition challenge.
\newblock 2015.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{Loshchilov2017SGDRSG}
Loshchilov, I. and Hutter, F.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock \emph{arXiv: Learning}, 2017.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{Madry2018TowardsDL}
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{ArXiv}, abs/1706.06083, 2018.

\bibitem[Moosavi-Dezfooli et~al.(2019)Moosavi-Dezfooli, Fawzi, Uesato, and
  Frossard]{MoosaviDezfooli2019RobustnessVC}
Moosavi-Dezfooli, S.-M., Fawzi, A., Uesato, J., and Frossard, P.
\newblock Robustness via curvature regularization, and vice versa.
\newblock \emph{2019 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  9070--9078, 2019.

\bibitem[Nakkiran et~al.(2020)Nakkiran, Kaplun, Bansal, Yang, Barak, and
  Sutskever]{Nakkiran2020DeepDD}
Nakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., and Sutskever, I.
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock \emph{ArXiv}, abs/1912.02292, 2020.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{Netzer2011ReadingDI}
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, McAllester, and
  Srebro]{Neyshabur2017ExploringGI}
Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N.
\newblock Exploring generalization in deep learning.
\newblock In \emph{NIPS}, 2017.

\bibitem[Papernot et~al.(2016)Papernot, McDaniel, Sinha, and
  Wellman]{Papernot2016TowardsTS}
Papernot, N., McDaniel, P., Sinha, A., and Wellman, M.~P.
\newblock Towards the science of security and privacy in machine learning.
\newblock \emph{ArXiv}, abs/1611.03814, 2016.

\bibitem[Qian et~al.(2020)Qian, Fruit, Pirotta, and
  Lazaric]{Qian2020ConcentrationIF}
Qian, J., Fruit, R., Pirotta, M., and Lazaric, A.
\newblock Concentration inequalities for multinoulli random variables.
\newblock \emph{ArXiv}, abs/2001.11595, 2020.

\bibitem[Rebuffi et~al.(2021)Rebuffi, Gowal, Calian, Stimberg, Wiles, and
  Mann]{Rebuffi2021FixingDA}
Rebuffi, S.-A., Gowal, S., Calian, D.~A., Stimberg, F., Wiles, O., and Mann,
  T.~A.
\newblock Fixing data augmentation to improve adversarial robustness.
\newblock \emph{ArXiv}, abs/2103.01946, 2021.

\bibitem[Rice et~al.(2020)Rice, Wong, and Kolter]{Rice2020OverfittingIA}
Rice, L., Wong, E., and Kolter, J.~Z.
\newblock Overfitting in adversarially robust deep learning.
\newblock \emph{ArXiv}, abs/2002.11569, 2020.

\bibitem[Simonyan \& Zisserman(2015)Simonyan and Zisserman]{Simonyan2015VeryDC}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{CoRR}, abs/1409.1556, 2015.

\bibitem[Singla et~al.(2021)Singla, Singla, Jacobs, and Feizi]{Singla2021LowCA}
Singla, V., Singla, S., Jacobs, D., and Feizi, S.
\newblock Low curvature activations reduce overfitting in adversarial training.
\newblock \emph{ArXiv}, abs/2102.07861, 2021.

\bibitem[Smith(2017)]{Smith2017CyclicalLR}
Smith, L.~N.
\newblock Cyclical learning rates for training neural networks.
\newblock \emph{2017 IEEE Winter Conference on Applications of Computer Vision
  (WACV)}, pp.\  464--472, 2017.

\bibitem[Stutz et~al.(2021)Stutz, Hein, and Schiele]{Stutz2021RelatingAR}
Stutz, D., Hein, M., and Schiele, B.
\newblock Relating adversarially robust generalization to flat minima.
\newblock \emph{ArXiv}, abs/2104.04448, 2021.

\bibitem[Su et~al.(2018)Su, Zhang, Chen, Yi, Chen, and Gao]{Su2018IsRT}
Su, D., Zhang, H., Chen, H., Yi, J., Chen, P., and Gao, Y.
\newblock Is robustness the cost of accuracy? - a comprehensive study on the
  robustness of 18 deep image classification models.
\newblock In \emph{ECCV}, 2018.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{Szegedy2014IntriguingPO}
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow,
  I.~J., and Fergus, R.
\newblock Intriguing properties of neural networks.
\newblock \emph{CoRR}, abs/1312.6199, 2014.

\bibitem[Tsipras et~al.(2019)Tsipras, Santurkar, Engstrom, Turner, and
  Madry]{Tsipras2019RobustnessMB}
Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and Madry, A.
\newblock Robustness may be at odds with accuracy.
\newblock \emph{arXiv: Machine Learning}, 2019.

\bibitem[Uesato et~al.(2018)Uesato, O'Donoghue, Oord, and
  Kohli]{Uesato2018AdversarialRA}
Uesato, J., O'Donoghue, B., Oord, A., and Kohli, P.
\newblock Adversarial risk and the dangers of evaluating against weak attacks.
\newblock \emph{ArXiv}, abs/1802.05666, 2018.

\bibitem[Uesato et~al.(2019)Uesato, Alayrac, Huang, Stanforth, Fawzi, and
  Kohli]{Uesato2019AreLR}
Uesato, J., Alayrac, J.-B., Huang, P.-S., Stanforth, R., Fawzi, A., and Kohli,
  P.
\newblock Are labels required for improving adversarial robustness?
\newblock \emph{ArXiv}, abs/1905.13725, 2019.

\bibitem[Weissman et~al.(2003)Weissman, Ordentlich, Seroussi, Verd{\'u}, and
  Weinberger]{Weissman2003InequalitiesFT}
Weissman, T., Ordentlich, E., Seroussi, G., Verd{\'u}, S., and Weinberger,
  M.~J.
\newblock Inequalities for the l1 deviation of the empirical distribution.
\newblock 2003.

\bibitem[Wu et~al.(2020)Wu, Xia, and Wang]{Wu2020AdversarialWP}
Wu, D., Xia, S., and Wang, Y.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock \emph{arXiv: Learning}, 2020.

\bibitem[Yang et~al.(2020)Yang, Yu, You, Steinhardt, and
  Ma]{Yang2020RethinkingBT}
Yang, Z., Yu, Y., You, C., Steinhardt, J., and Ma, Y.
\newblock Rethinking bias-variance trade-off for generalization of neural
  networks.
\newblock In \emph{ICML}, 2020.

\bibitem[Yu et~al.(2021)Yu, Yang, Dobriban, Steinhardt, and
  Ma]{Yu2021UnderstandingGI}
Yu, Y., Yang, Z., Dobriban, E., Steinhardt, J., and Ma, Y.
\newblock Understanding generalization in adversarial training via the
  bias-variance decomposition.
\newblock \emph{ArXiv}, abs/2103.09947, 2021.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{Zagoruyko2016WideRN}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock \emph{ArXiv}, abs/1605.07146, 2016.

\bibitem[Zhang et~al.(2018)Zhang, Ciss{\'e}, Dauphin, and
  Lopez-Paz]{Zhang2018mixupBE}
Zhang, H., Ciss{\'e}, M., Dauphin, Y., and Lopez-Paz, D.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{ArXiv}, abs/1710.09412, 2018.

\bibitem[Zhang et~al.(2019)Zhang, Yu, Jiao, Xing, Ghaoui, and
  Jordan]{Zhang2019TheoreticallyPT}
Zhang, H., Yu, Y., Jiao, J., Xing, E., Ghaoui, L., and Jordan, M.~I.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock \emph{ArXiv}, abs/1901.08573, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Xu, Han, Niu, zhen Cui, Sugiyama, and
  Kankanhalli]{Zhang2020AttacksWD}
Zhang, J., Xu, X., Han, B., Niu, G., zhen Cui, L., Sugiyama, M., and
  Kankanhalli, M.
\newblock Attacks which do not kill training make adversarial learning
  stronger.
\newblock \emph{ArXiv}, abs/2002.11242, 2020.

\end{thebibliography}
