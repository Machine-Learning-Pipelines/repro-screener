index,id,title,categories,abstract,doi,created,updated,authors
0,1606.04671,progressive neural networks,cs.lg,"learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. the progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. we evaluate this architecture extensively on a wide variety of reinforcement learning tasks (atari and 3d maze games), and show that it outperforms common baselines based on pretraining and finetuning. using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.",,2016-06-15,2022-10-22,"['andrei a. rusu', 'neil c. rabinowitz', 'guillaume desjardins', 'hubert soyer', 'james kirkpatrick', 'koray kavukcuoglu', 'razvan pascanu', 'raia hadsell']"
1,1903.09668,data augmentation for bayesian deep learning,stat.ml cs.lg stat.me,"deep learning (dl) methods have emerged as one of the most powerful tools for functional approximation and prediction. while the representation properties of dl have been well studied, uncertainty quantification remains challenging and largely unexplored. data augmentation techniques are a natural approach to provide uncertainty quantification and to incorporate stochastic monte carlo search into stochastic gradient descent (sgd) methods. the purpose of our paper is to show that training dl architectures with data augmentation leads to efficiency gains. we use the theory of scale mixtures of normals to derive data augmentation strategies for deep learning. this allows variants of the expectation-maximization and mcmc algorithms to be brought to bear on these high dimensional nonlinear deep learning models. to demonstrate our methodology, we develop data augmentation algorithms for a variety of commonly used activation functions: logit, relu, leaky relu and svm. our methodology is compared to traditional stochastic gradient descent with back-propagation. our optimization procedure leads to a version of iteratively re-weighted least squares and can be implemented at scale with accelerated linear algebra methods providing substantial improvement in speed. we illustrate our methodology on a number of standard datasets. finally, we conclude with directions for future research.",10.1214/22-ba1331,2019-03-22,2022-10-24,"['yuexi wang', 'nicholas g. polson', 'vadim o. sokolov']"
2,1904.10554,deep q-learning for nash equilibria: nash-dqn,cs.lg cs.gt q-fin.cp stat.ml,"model-free learning for multi-agent stochastic games is an active area of research. existing reinforcement learning algorithms, however, are often restricted to zero-sum games, and are applicable only in small state-action spaces or other simplified settings. here, we develop a new data efficient deep-q-learning methodology for model-free learning of nash equilibria for general-sum stochastic games. the algorithm uses a local linear-quadratic expansion of the stochastic game, which leads to analytically solvable optimal actions. the expansion is parametrized by deep neural networks to give it sufficient flexibility to learn the environment without the need to experience all state-action pairs. we study symmetry properties of the algorithm stemming from label-invariant stochastic games and as a proof of concept, apply our algorithm to learning optimal trading strategies in competitive electronic markets.",,2019-04-23,2022-10-23,"['philippe casgrain', 'brian ning', 'sebastian jaimungal']"
3,1908.05659,distributionally robust optimization: a review,math.oc cs.lg stat.ml,"the concepts of risk-aversion, chance-constrained optimization, and robust optimization have developed significantly over the last decade. statistical learning community has also witnessed a rapid theoretical and applied growth by relying on these concepts. a modeling framework, called distributionally robust optimization (dro), has recently received significant attention in both the operations research and statistical learning communities. this paper surveys main concepts and contributions to dro, and its relationships with robust optimization, risk-aversion, chance-constrained optimization, and function regularization.",10.5802/ojmo.15,2019-08-12,,"['hamed rahimian', 'sanjay mehrotra']"
4,1909.00931,transfer fine-tuning: a bert case study,cs.cl cs.lg,"a semantic equivalence assessment is defined as a task that assesses semantic equivalence in a sentence pair by binary judgment (i.e., paraphrase identification) or grading (i.e., semantic textual similarity measurement). it constitutes a set of tasks crucial for research on natural language understanding. recently, bert realized a breakthrough in sentence representation learning (devlin et al., 2019), which is broadly transferable to various nlp tasks. while bert's performance improves by increasing its model size, the required computational power is an obstacle preventing practical applications from adopting the technology. herein, we propose to inject phrasal paraphrase relations into bert in order to generate suitable representations for semantic equivalence assessment instead of increasing the model size. experiments on standard natural language understanding tasks confirm that our method effectively improves a smaller bert model while maintaining the model size. the generated model exhibits superior performance compared to a larger bert model on semantic equivalence assessment tasks. furthermore, it achieves larger performance gains on tasks with limited training datasets for fine-tuning, which is a property desirable for transfer learning.",10.18653/v1/d19-1542,2019-09-02,,"['yuki arase', 'junichi tsujii']"
5,1911.03867,a modular deep learning pipeline for galaxy-scale strong gravitational   lens detection and modeling,astro-ph.im astro-ph.co cs.lg,"upcoming large astronomical surveys are expected to capture an unprecedented number of strong gravitational lensing systems. deep learning is emerging as a promising practical tool for the detection and quantification of these galaxy-scale image distortions. the absence of large quantities of representative data from current astronomical surveys motivates the development of a robust forward-modeling approach using synthetic lensing images. using a mock sample of strong lenses created upon a state-of-the-art extragalactic catalogs, we train a modular deep learning pipeline for uncertainty-quantified detection and modeling with intermediate image processing components for denoising and deblending the lensing systems. we demonstrate a high degree of interpretability and controlled systematics due to domain-specific task modules trained with different stages of synthetic image generation. for lens detection and modeling, we obtain semantically meaningful latent spaces that separate classes of strong lens images and yield uncertainty estimates that explain the origin of misclassified images and provide probabilistic predictions for the lens parameters. validation of the inference pipeline has been carried out using images from the subaru telescope's hyper suprime-cam camera, and lsst desc simulated dc2 sky survey catalogues.",,2019-11-10,2022-10-21,"['sandeep madireddy', 'nesar ramachandra', 'nan li', 'james butler', 'prasanna balaprakash', 'salman habib', 'katrin heitmann', 'the lsst dark energy science collaboration']"
6,2002.05905,magneto: fingerprinting usb flash drives via unintentional magnetic   emissions,cs.cr cs.lg,"universal serial bus (usb) flash drives are nowadays one of the most convenient and diffused means to transfer files, especially when no internet connection is available. however, usb flash drives are also one of the most common attack vectors used to gain unauthorized access to host devices. for instance, it is possible to replace a usb drive so that when the usb key is connected, it would install passwords stealing tools, root-kit software, and other disrupting malware. in such a way, an attacker can steal sensitive information via the usb-connected devices, as well as inject any kind of malicious software into the host.   to thwart the above-cited raising threats, we propose magneto, an efficient, non-interactive, and privacy-preserving framework to verify the authenticity of a usb flash drive, rooted in the analysis of its unintentional magnetic emissions. we show that the magnetic emissions radiated during boot operations on a specific host are unique for each device, and sufficient to uniquely fingerprint both the brand and the model of the usb flash drive, or the specific usb device, depending on the used equipment. our investigation on 59 different usb flash drives---belonging to 17 brands, including the top brands purchased on amazon in mid-2019---, reveals a minimum classification accuracy of 98.2% in the identification of both brand and model, accompanied by a negligible time and computational overhead. magneto can also identify the specific usb flash drive, with a minimum classification accuracy of 91.2%. overall, magneto proves that unintentional magnetic emissions can be considered as a viable and reliable means to fingerprint read-only usb flash drives. finally, future research directions in this domain are also discussed.",10.1145/3422308,2020-02-14,2020-09-12,"['omar adel ibrahim', 'savio sciancalepore', 'gabriele oligeri', 'roberto di pietro']"
7,2004.05258,exploring optimal deep learning models for image-based malware variant   classification,cs.cr cs.lg,"analyzing a huge amount of malware is a major burden for security analysts. since emerging malware is often a variant of existing malware, automatically classifying malware into known families greatly reduces a part of their burden. image-based malware classification with deep learning is an attractive approach for its simplicity, versatility, and affinity with the latest technologies. however, the impact of differences in deep learning models and the degree of transfer learning on the classification accuracy of malware variants has not been fully studied. in this paper, we conducted an exhaustive survey of deep learning models using 24 imagenet pre-trained models and five fine-tuning parameters, totaling 120 combinations, on two platforms. as a result, we found that the highest classification accuracy was obtained by fine-tuning one of the latest deep learning models with a relatively low degree of transfer learning, and we achieved the highest classification accuracy ever in cross-validation on the malimg and drebin datasets. we also confirmed that this trend holds true for the recent malware variants using the virustotal 2020 windows and android datasets. the experimental results suggest that it is effective to periodically explore optimal deep learning models with the latest models and malware datasets by gradually reducing the degree of transfer learning from half.",10.1109/compsac54236.2022.00128,2020-04-10,2022-10-23,"['rikima mitsuhashi', 'takahiro shinagawa']"
8,2005.02607,towards quantum advantage via topological data analysis,quant-ph cs.cc cs.lg,"even after decades of quantum computing development, examples of generally useful quantum algorithms with exponential speedups over classical counterparts are scarce. recent progress in quantum algorithms for linear-algebra positioned quantum machine learning (qml) as a potential source of such useful exponential improvements. yet, in an unexpected development, a recent series of ""dequantization"" results has equally rapidly removed the promise of exponential speedups for several qml algorithms. this raises the critical question whether exponential speedups of other linear-algebraic qml algorithms persist. in this paper, we study the quantum-algorithmic methods behind the algorithm for topological data analysis of lloyd, garnerone and zanardi through this lens. we provide evidence that the problem solved by this algorithm is classically intractable by showing that its natural generalization is as hard as simulating the one clean qubit model -- which is widely believed to require superpolynomial time on a classical computer -- and is thus very likely immune to dequantizations. based on this result, we provide a number of new quantum algorithms for problems such as rank estimation and complex network analysis, along with complexity-theoretic evidence for their classical intractability. furthermore, we analyze the suitability of the proposed quantum algorithms for near-term implementations. our results provide a number of useful applications for full-blown, and restricted quantum computers with a guaranteed exponential speedup over classical methods, recovering some of the potential for linear-algebraic qml to become one of quantum computing's killer applications.",,2020-05-06,2022-10-21,"['casper gyurik', 'chris cade', 'vedran dunjko']"
9,2007.09855,wide boosting,cs.lg stat.ml,"gradient boosting (gb) is a popular methodology used to solve prediction problems by minimizing a differentiable loss function, $l$. gb performs very well on tabular machine learning (ml) problems; however, as a pure ml solver it lacks the ability to fit models with probabilistic but correlated multi-dimensional outputs, for example, multiple correlated bernoulli outputs. gb also does not form intermediate abstract data embeddings, one property of deep learning that gives greater flexibility and performance on other types of problems. this paper presents a simple adjustment to gb motivated in part by artificial neural networks. specifically, our adjustment inserts a matrix multiplication between the output of a gb model and the loss, $l$. this allows the output of a gb model to have increased dimension prior to being fed into the loss and is thus ``wider'' than standard gb implementations. we call our method wide boosting (wb) and show that wb outperforms gb on mult-dimesional output tasks and that the embeddings generated by wb contain are more useful in downstream prediction tasks than gb output predictions alone.",,2020-07-19,2022-10-22,['michael t. horrell']
10,2009.01947,practical and parallelizable algorithms for non-monotone submodular   maximization with size constraint,cs.ds cs.lg,"we present combinatorial and parallelizable algorithms for maximization of a submodular function, not necessarily monotone, with respect to a size constraint. we improve the best approximation factor achieved by an algorithm that has optimal adaptivity and nearly optimal query complexity to $0.193 - \varepsilon$. the conference version of this work mistakenly employed a subroutine that does not work for non-monotone, submodular functions. in this version, we propose a fixed and improved subroutine to add a set with high average marginal gain, \threseq, which returns a solution in $o( \log(n) )$ adaptive rounds with high probability. moreover, we provide two approximation algorithms. the first has approximation ratio $1/6 - \varepsilon$, adaptivity $o( \log (n) )$, and query complexity $o( n \log (k) )$, while the second has approximation ratio $0.193 - \varepsilon$, adaptivity $o( \log^2 (n) )$, and query complexity $o(n \log (k))$. our algorithms are empirically validated to use a low number of adaptive rounds and total queries while obtaining solutions with high objective value in comparison with state-of-the-art approximation algorithms, including continuous algorithms that use the multilinear extension.",,2020-09-03,2022-10-21,"['yixin chen', 'alan kuhnle']"
11,2010.04261,dissecting hessian: understanding common structure of hessian in neural   networks,cs.lg cs.ne stat.ml,"hessian captures important properties of the deep neural network loss landscape. previous works have observed low rank structure in the hessians of neural networks. in this paper, we propose a decoupling conjecture that decomposes the layer-wise hessians of a network as the kronecker product of two smaller matrices. we can analyze the properties of these smaller matrices and prove the structure of top eigenspace random 2-layer networks. the decoupling conjecture has several other interesting implications - top eigenspaces for different models have surprisingly high overlap, and top eigenvectors form low rank matrices when they are reshaped into the same shape as the corresponding weight matrix. all of these can be verified empirically for deeper networks. finally, we use the structure of layer-wise hessian to get better explicit generalization bounds for neural networks.",,2020-10-08,2022-10-21,"['yikai wu', 'xingyu zhu', 'chenwei wu', 'annie wang', 'rong ge']"
12,2010.04855,"kernel methods for causal functions: dose, heterogeneous, and   incremental response curves",econ.em cs.lg math.st stat.ml stat.th,"we propose estimators based on kernel ridge regression for nonparametric causal functions such as dose, heterogeneous, and incremental response curves. treatment and covariates may be discrete or continuous in general spaces. due to a decomposition property specific to the rkhs, our estimators have simple closed form solutions. we prove uniform consistency with finite sample rates via original analysis of generalized kernel ridge regression. we extend our main results to counterfactual distributions and to causal functions identified by front and back door criteria. we achieve state-of-the-art performance in nonlinear simulations with many covariates, and conduct a policy evaluation of the us job corps training program for disadvantaged youths.",,2020-10-09,2022-10-21,"['rahul singh', 'liyuan xu', 'arthur gretton']"
13,2011.11576,conjecturing-based computational discovery of patterns in data,cs.lg stat.ml,"we propose the use of a conjecturing machine that generates feature relationships in the form of bounds involving nonlinear terms for numerical features and boolean expressions for categorical features. the proposed \textsc{conjecturing} framework recovers known nonlinear and boolean relationships among features from data. in both settings, true underlying relationships are revealed. we then compare the method to a previously-proposed framework for symbolic regression and demonstrate that it can also be used to recover equations that are satisfied among features in a dataset. the framework is then applied to patient-level data regarding covid-19 outcomes to suggest possible risk factors that are confirmed in medical literature.",,2020-11-23,2022-10-23,"['j. p. brooks', 'd. j. edwards', 'c. e. larson', 'n. van cleemput']"
14,2012.09302,"trojanzoo: towards unified, holistic, and practical evaluation of neural   backdoors",cs.lg,"neural backdoors represent one primary threat to the security of deep learning systems. the intensive research has produced a plethora of backdoor attacks/defenses, resulting in a constant arms race. however, due to the lack of evaluation benchmarks, many critical questions remain under-explored: (i) what are the strengths and limitations of different attacks/defenses? (ii) what are the best practices to operate them? and (iii) how can the existing attacks/defenses be further improved?   to bridge this gap, we design and implement trojanzoo, the first open-source platform for evaluating neural backdoor attacks/defenses in a unified, holistic, and practical manner. thus far, focusing on the computer vision domain, it has incorporated 8 representative attacks, 14 state-of-the-art defenses, 6 attack performance metrics, 10 defense utility metrics, as well as rich tools for in-depth analysis of the attack-defense interactions. leveraging trojanzoo, we conduct a systematic study on the existing attacks/defenses, unveiling their complex design spectrum: both manifest intricate trade-offs among multiple desiderata (e.g., the effectiveness, evasiveness, and transferability of attacks). we further explore improving the existing attacks/defenses, leading to a number of interesting findings: (i) one-pixel triggers often suffice; (ii) training from scratch often outperforms perturbing benign models to craft trojan models; (iii) optimizing triggers and trojan models jointly greatly improves both attack effectiveness and evasiveness; (iv) individual defenses can often be evaded by adaptive attacks; and (v) exploiting model interpretability significantly improves defense robustness. we envision that trojanzoo will serve as a valuable platform to facilitate future research on neural backdoors.",10.1109/eurosp53844.2022.00048,2020-12-16,2022-10-21,"['ren pang', 'zheng zhang', 'xiangshan gao', 'zhaohan xi', 'shouling ji', 'peng cheng', 'xiapu luo', 'ting wang']"
15,2101.07354,exact recovery of community structures using deepwalk and node2vec,stat.ml cs.lg cs.si,"random-walk based network embedding algorithms like deepwalk and node2vec are widely used to obtain euclidean representation of the nodes in a network prior to performing downstream inference tasks. however, despite their impressive empirical performance, there is a lack of theoretical results explaining their large-sample behavior. in this paper, we study node2vec and deepwalk through the perspective of matrix factorization. in particular, we analyze these algorithms in the setting of community detection for stochastic blockmodel graphs (and their degree-corrected variants). by exploiting the row-wise uniform perturbation bound for leading singular vectors, we derive high-probability error bounds between the matrix factorization-based node2vec/deepwalk embeddings and their true counterparts, uniformly over all node embeddings. based on strong concentration results, we further show the perfect membership recovery by node2vec/deepwalk, followed by $k$-means/medians algorithms. specifically, as the network becomes sparser, our results guarantee that with large enough window size and vertices number, applying $k$-means/medians on the matrix factorization-based node2vec embeddings can, with high probability, correctly recover the memberships of all vertices in a network generated from the stochastic blockmodel (or its degree-corrected variants). the theoretical justifications are mirrored in the numerical experiments and real data applications, for both the original node2vec and its matrix factorization variant.",,2021-01-18,2022-10-23,"['yichi zhang', 'minh tang']"
16,2102.11887,quantum cross entropy and maximum likelihood principle,quant-ph cs.it cs.lg hep-th math.it stat.ml,"quantum machine learning is an emerging field at the intersection of machine learning and quantum computing. classical cross entropy plays a central role in machine learning. we define its quantum generalization, the quantum cross entropy, prove its lower bounds, and investigate its relation to quantum fidelity. in the classical case, minimizing cross entropy is equivalent to maximizing likelihood. in the quantum case, when the quantum cross entropy is constructed from quantum data undisturbed by quantum measurements, this relation holds. classical cross entropy is equal to negative log-likelihood. when we obtain quantum cross entropy through empirical density matrix based on measurement outcomes, the quantum cross entropy is lower-bounded by negative log-likelihood. these two different scenarios illustrate the information loss when making quantum measurements. we conclude that to achieve the goal of full quantum machine learning, it is crucial to utilize the deferred measurement principle.",,2021-02-23,2022-10-24,"['zhou shangnan', 'yixu wang']"
17,2104.11893,lgd-gcn: local and global disentangled graph convolutional networks,cs.lg,"disentangled graph convolutional network (disengcn) is an encouraging framework to disentangle the latent factors arising in a real-world graph. however, it relies on disentangling information heavily from a local range (i.e., a node and its 1-hop neighbors), while the local information in many cases can be uneven and incomplete, hindering the interpretabiliy power and model performance of disengcn. in this paper, we introduce a novel local and global disentangled graph convolutional network (lgd-gcn) to capture both local and global information for graph disentanglement. lgd-gcn performs a statistical mixture modeling to derive a factor-aware latent continuous space, and then constructs different structures w.r.t. different factors from the revealed space. in this way, the global factor-specific information can be efficiently and selectively encoded via a message passing along these built structures, strengthening the intra-factor consistency. we also propose a novel diversity promoting regularizer employed with the latent space modeling, to encourage inter-factor diversity. evaluations of the proposed lgd-gcn on the synthetic and real-world datasets show a better interpretability and improved performance in node classification over the existing competitive models.",,2021-04-24,2022-10-22,"['jingwei guo', 'kaizhu huang', 'xinping yi', 'rui zhang']"
18,2104.12546,the effects of air quality on the spread of the covid-19 pandemic in   italy: an artificial intelligence approach,cs.cy cs.ai cs.lg,"the covid-19 pandemic considerably affects public health systems around the world. the lack of knowledge about the virus, the extension of this phenomenon, and the speed of the evolution of the infection are all factors that highlight the necessity of employing new approaches to study these events. artificial intelligence techniques may be useful in analyzing data related to areas affected by the virus. the aim of this work is to investigate any possible relationships between air quality and confirmed cases of covid-19 in italian districts. specifically, we report an analysis of the correlation between daily covid-19 cases and environmental factors, such as temperature, relative humidity, and atmospheric pollutants. our analysis confirms a significant association of some environmental parameters with the spread of the virus. this suggests that machine learning models trained on the environmental parameters to predict the number of future infected cases may be accurate. predictive models may be useful for helping institutions in making decisions for protecting the population and contrasting the pandemic.",10.1016/j.procs.2022.09.112,2021-04-09,2021-08-31,"['andrea loreggia', 'anna passarelli', 'maria silvia pini']"
19,2105.01099,reinforcement learning for ridesharing: an extended survey,cs.lg cs.ai,"in this paper, we present a comprehensive, in-depth survey of the literature on reinforcement learning approaches to decision optimization problems in a typical ridesharing system. papers on the topics of rideshare matching, vehicle repositioning, ride-pooling, routing, and dynamic pricing are covered. most of the literature has appeared in the last few years, and several core challenges are to continue to be tackled: model complexity, agent coordination, and joint optimization of multiple levers. hence, we also introduce popular data sets and open simulation environments to facilitate further research and development. subsequently, we discuss a number of challenges and opportunities for reinforcement learning research on this important domain.",10.1016/j.trc.2022.103852,2021-05-03,2022-10-24,"['zhiwei qin', 'hongtu zhu', 'jieping ye']"
20,2105.01937,flex: extrinsic parameters-free multi-view 3d human motion   reconstruction,cs.cv cs.gr cs.lg,"the increasing availability of video recordings made by multiple cameras has offered new means for mitigating occlusion and depth ambiguities in pose and motion reconstruction methods. yet, multi-view algorithms strongly depend on camera parameters; particularly, the relative transformations between the cameras. such a dependency becomes a hurdle once shifting to dynamic capture in uncontrolled settings. we introduce flex (free multi-view reconstruxion), an end-to-end extrinsic parameter-free multi-view model. flex is extrinsic parameter-free (dubbed ep-free) in the sense that it does not require extrinsic camera parameters. our key idea is that the 3d angles between skeletal parts, as well as bone lengths, are invariant to the camera position. hence, learning 3d rotations and bone lengths rather than locations allows predicting common values for all camera views. our network takes multiple video streams, learns fused deep features through a novel multi-view fusion layer, and reconstructs a single consistent skeleton with temporally coherent joint rotations. we demonstrate quantitative and qualitative results on three public datasets, and on synthetic multi-person video streams captured by dynamic cameras. we compare our model to state-of-the-art methods that are not ep-free and show that in the absence of camera parameters, we outperform them by a large margin while obtaining comparable results when camera parameters are available. code, trained models, and other materials are available on our project page.",,2021-05-05,2022-10-21,"['brian gordon', 'sigal raab', 'guy azov', 'raja giryes', 'daniel cohen-or']"
21,2105.15197,a simple and general debiased machine learning theorem with finite   sample guarantees,stat.ml cs.lg econ.em math.st stat.th,"debiased machine learning is a meta algorithm based on bias correction and sample splitting to calculate confidence intervals for functionals, i.e. scalar summaries, of machine learning algorithms. for example, an analyst may desire the confidence interval for a treatment effect estimated with a neural network. we provide a nonasymptotic debiased machine learning theorem that encompasses any global or local functional of any machine learning algorithm that satisfies a few simple, interpretable conditions. formally, we prove consistency, gaussian approximation, and semiparametric efficiency by finite sample arguments. the rate of convergence is $n^{-1/2}$ for global functionals, and it degrades gracefully for local functionals. our results culminate in a simple set of conditions that an analyst can use to translate modern learning theory rates into traditional statistical inference. the conditions reveal a general double robustness property for ill posed inverse problems.",,2021-05-31,2022-10-21,"['victor chernozhukov', 'whitney k. newey', 'rahul singh']"
22,2106.01528,normalizing flows for knockoff-free controlled feature selection,stat.ml cs.lg,"controlled feature selection aims to discover the features a response depends on while limiting the false discovery rate (fdr) to a predefined level. recently, multiple deep-learning-based methods have been proposed to perform controlled feature selection through the model-x knockoff framework. we demonstrate, however, that these methods often fail to control the fdr for two reasons. first, these methods often learn inaccurate models of features. second, the ""swap"" property, which is required for knockoffs to be valid, is often not well enforced. we propose a new procedure called flowselect to perform controlled feature selection that does not suffer from either of these two problems. to more accurately model the features, flowselect uses normalizing flows, the state-of-the-art method for density estimation. instead of enforcing the ""swap"" property, flowselect uses a novel mcmc-based procedure to calculate p-values for each feature directly. asymptotically, flowselect computes valid p-values. empirically, flowselect consistently controls the fdr on both synthetic and semi-synthetic benchmarks, whereas competing knockoff-based approaches do not. flowselect also demonstrates greater power on these benchmarks. additionally, flowselect correctly infers the genetic variants associated with specific soybean traits from gwas data.",,2021-06-02,2022-10-21,"['derek hansen', 'brian manzo', 'jeffrey regier']"
23,2106.03157,self-supervision is all you need for solving rubik's cube,cs.lg,"existing combinatorial search methods are often complex and require some expertise. in this work, we propose a simple and performant deep learning method, especially for goal-predefined combinatorial problems represented by rubik's cube. we show that, for solving such problems with high optimality, it can be sufficient to train a deep neural network on random scrambles branching from the goal state. when tested on rubik's cube, our method outperformed the previous state-of-the-art method deepcubea in solution optimality, being 10 times more efficient in training and 2.0 times in inference. one key assumption is that, when viewed from scrambled states, random moves from the goal are biased to be optimal. we also demonstrate the proposed method on 15 puzzle and lights out.",,2021-06-06,2022-10-24,['kyo takano']
24,2106.03725,stability to deformations of manifold filters and manifold neural   networks,cs.lg,"the paper defines and studies manifold (m) convolutional filters and neural networks (nns). \emph{manifold} filters and mnns are defined in terms of the laplace-beltrami operator exponential and are such that \emph{graph} (g) filters and neural networks (nns) are recovered as discrete approximations when the manifold is sampled. these filters admit a spectral representation which is a generalization of both the spectral representation of graph filters and the frequency response of standard convolutional filters in continuous time. the main technical contribution of the paper is to analyze the stability of manifold filters and mnns to smooth deformations of the manifold. this analysis generalizes known stability properties of graph filters and gnns and it is also a generalization of known stability properties of standard convolutional filters and neural networks in continuous time. the most important observation that follows from this analysis is that manifold filters, same as graph filters and standard continuous time filters, have difficulty discriminating high frequency components in the presence of deformations. this is a challenge that can be ameliorated with the use of manifold, graph, or continuous time neural networks. the most important practical consequence of this analysis is to shed light on the behavior of graph filters and gnns in large scale graphs.",,2021-06-07,2022-10-21,"['zhiyang wang', 'luana ruiz', 'alejandro ribeiro']"
25,2106.06610,"scalars are universal: equivariant machine learning, structured like   classical physics",cs.lg math-ph math.mp stat.ml,"there has been enormous progress in the last few years in designing neural networks that respect the fundamental symmetries and coordinate freedoms of physical law. some of these frameworks make use of irreducible representations, some make use of high-order tensor objects, and some apply symmetry-enforcing constraints. different physical laws obey different combinations of fundamental symmetries, but a large fraction (possibly all) of classical physics is equivariant to translation, rotation, reflection (parity), boost (relativity), and permutations. here we show that it is simple to parameterize universally approximating polynomial functions that are equivariant under these symmetries, or under the euclidean, lorentz, and poincar\'e groups, at any dimensionality $d$. the key observation is that nonlinear o($d$)-equivariant (and related-group-equivariant) functions can be universally expressed in terms of a lightweight collection of scalars -- scalar products and scalar contractions of the scalar, vector, and tensor inputs. we complement our theory with numerical examples that show that the scalar-based method is simple, efficient, and scalable.",,2021-06-11,2022-10-20,"['soledad villar', 'david w. hogg', 'kate storey-fisher', 'weichi yao', 'ben blum-smith']"
26,2106.06927,inverting adversarially robust networks for image synthesis,cs.cv cs.lg cs.ne,"despite unconditional feature inversion being the foundation of many image synthesis applications, training an inverter demands a high computational budget, large decoding capacity and imposing conditions such as autoregressive priors. to address these limitations, we propose the use of adversarially robust representations as a perceptual primitive for feature inversion. we train an adversarially robust encoder to extract disentangled and perceptually-aligned image representations, making them easily invertible. by training a simple generator with the mirror architecture of the encoder, we achieve superior reconstruction quality and generalization over standard models. based on this, we propose an adversarially robust autoencoder and demonstrate its improved performance on style transfer, image denoising and anomaly detection tasks. compared to recent imagenet feature inversion methods, our model attains improved performance with significantly less complexity.",,2021-06-13,2022-10-21,"['renan a. rojas-gomez', 'raymond a. yeh', 'minh n. do', 'anh nguyen']"
27,2106.07704,efficient (soft) q-learning for text generation with limited good data,cs.cl cs.lg,"maximum likelihood estimation (mle) is the predominant algorithm for training text generation models. this paradigm relies on direct supervision examples, which is not applicable to many emerging applications, such as generating adversarial attacks or generating prompts to control language models. reinforcement learning (rl) on the other hand offers a more flexible solution by allowing users to plug in arbitrary task metrics as reward. yet previous rl algorithms for text generation, such as policy gradient (on-policy rl) and q-learning (off-policy rl), are often notoriously inefficient or unstable to train due to the large sequence space and the sparse reward received only at the end of sequences. in this paper, we introduce a new rl formulation for text generation from the soft q-learning (sql) perspective. it enables us to draw from the latest rl advances, such as path consistency learning, to combine the best of on-/off-policy updates, and learn effectively from sparse reward. we apply the approach to a wide range of novel text generation tasks, including learning from noisy/negative examples, adversarial attacks, and prompt generation. experiments show our approach consistently outperforms both task-specialized algorithms and the previous rl methods.",,2021-06-14,2022-10-22,"['han guo', 'bowen tan', 'zhengzhong liu', 'eric p. xing', 'zhiting hu']"
28,2106.10898,banditmf: multi-armed bandit based matrix factorization recommender   system,cs.ir cs.lg,"multi-armed bandits (mab) provide a principled online learning approach to attain the balance between exploration and exploitation. due to the superior performance and low feedback learning without the learning to act in multiple situations, multi-armed bandits drawing widespread attention in applications ranging such as recommender systems. likewise, within the recommender system, collaborative filtering (cf) is arguably the earliest and most influential method in the recommender system. crucially, new users and an ever-changing pool of recommended items are the challenges that recommender systems need to address. for collaborative filtering, the classical method is training the model offline, then perform the online testing, but this approach can no longer handle the dynamic changes in user preferences which is the so-called cold start. so how to effectively recommend items to users in the absence of effective information? to address the aforementioned problems, a multi-armed bandit based collaborative filtering recommender system has been proposed, named banditmf. banditmf is designed to address two challenges in the multi-armed bandits algorithm and collaborative filtering: (1) how to solve the cold start problem for collaborative filtering under the condition of scarcity of valid information, (2) how to solve the sub-optimal problem of bandit algorithms in strong social relations domains caused by independently estimating unknown parameters associated with each user and ignoring correlations between users.",,2021-06-21,2022-10-24,['shenghao xu']
29,2106.12177,"imitation learning: progress, taxonomies and challenges",cs.lg cs.ro,"imitation learning aims to extract knowledge from human experts' demonstrations or artificially created agents in order to replicate their behaviors. its success has been demonstrated in areas such as video games, autonomous driving, robotic simulations and object manipulation. however, this replicating process could be problematic, such as the performance is highly dependent on the demonstration quality, and most trained agents are limited to perform well in task-specific environments. in this survey, we provide a systematic review on imitation learning. we first introduce the background knowledge from development history and preliminaries, followed by presenting different taxonomies within imitation learning and key milestones of the field. we then detail challenges in learning strategies and present research opportunities with learning policy from suboptimal demonstration, voice instructions and other associated optimization schemes.",,2021-06-23,2022-10-20,"['boyuan zheng', 'sunny verma', 'jianlong zhou', 'ivor tsang', 'fang chen']"
30,2106.12936,fundamental limits for learning hidden markov model parameters,stat.ml cs.lg math.st stat.th,"we study the frontier between learnable and unlearnable hidden markov models (hmms). hmms are flexible tools for clustering dependent data coming from unknown populations. the model parameters are known to be fully identifiable (up to label-switching) without any modeling assumption on the distributions of the populations as soon as the clusters are distinct and the hidden chain is ergodic with a full rank transition matrix. in the limit as any one of these conditions fails, it becomes impossible in general to identify parameters. for a chain with two hidden states we prove nonasymptotic minimax upper and lower bounds, matching up to constants, which exhibit thresholds at which the parameters become learnable. we also provide an upper bound on the relative entropy rate for parameters in a neighbourhood of the unlearnable region which may have interest in itself.",10.1109/tit.2022.3213429,2021-06-24,2022-10-24,"['kweku abraham', 'zacharie naulet', 'elisabeth gassiat']"
31,2106.13823,quantum data compression and quantum cross entropy,quant-ph cond-mat.stat-mech cs.it cs.lg hep-th math.it,"quantum machine learning is an emerging field at the intersection of machine learning and quantum computing. a central quantity for the theoretical foundation of quantum machine learning is the quantum cross entropy. in this paper, we present one operational interpretation of this quantity, that the quantum cross entropy is the compression rate for sub-optimal quantum source coding. to do so, we give a simple, universal quantum data compression protocol, which is developed based on quantum generalization of variable-length coding, as well as quantum strong typicality.",,2021-06-25,2022-10-24,['zhou shangnan']
32,2107.01131,tight mutual information estimation with contrastive fenchel-legendre   optimization,stat.ml cs.ai cs.it cs.lg math.it,"successful applications of infonce and its variants have popularized the use of contrastive variational mutual information (mi) estimators in machine learning. while featuring superior stability, these estimators crucially depend on costly large-batch training, and they sacrifice bound tightness for variance reduction. to overcome these limitations, we revisit the mathematics of popular variational mi bounds from the lens of unnormalized statistical modeling and convex optimization. our investigation not only yields a new unified theoretical framework encompassing popular variational mi bounds but also leads to a novel, simple, and powerful contrastive mi estimator named as flo. theoretically, we show that the flo estimator is tight, and it provably converges under stochastic gradient descent. empirically, our flo estimator overcomes the limitations of its predecessors and learns more efficiently. the utility of flo is verified using an extensive set of benchmarks, which also reveals the trade-offs in practical mi estimation.",,2021-07-02,2022-10-24,"['qing guo', 'junya chen', 'dong wang', 'yuewei yang', 'xinwei deng', 'lawrence carin', 'fan li', 'jing huang', 'chenyang tao']"
33,2108.09779,transferring dexterous manipulation from gpu simulation to a remote   real-world trifinger,cs.ro cs.lg,"we present a system for learning a challenging dexterous manipulation task involving moving a cube to an arbitrary 6-dof pose with only 3-fingers trained with nvidia's isaacgym simulator. we show empirical benefits, both in simulation and sim-to-real transfer, of using keypoints as opposed to position+quaternion representations for the object pose in 6-dof for policy observations and in reward calculation to train a model-free reinforcement learning agent. by utilizing domain randomization strategies along with the keypoint representation of the pose of the manipulated object, we achieve a high success rate of 83% on a remote trifinger system maintained by the organizers of the real robot challenge. with the aim of assisting further research in learning in-hand manipulation, we make the codebase of our system, along with trained checkpoints that come with billions of steps of experience available, at https://s2r2-ig.github.io",,2021-08-22,2022-10-20,"['arthur allshire', 'mayank mittal', 'varun lodaya', 'viktor makoviychuk', 'denys makoviichuk', 'felix widmaier', 'manuel wüthrich', 'stefan bauer', 'ankur handa', 'animesh garg']"
34,2109.01372,sample noise impact on active learning,stat.ml cs.lg,"this work explores the effect of noisy sample selection in active learning strategies. we show on both synthetic problems and real-life use-cases that knowledge of the sample noise can significantly improve the performance of active learning strategies. building on prior work, we propose a robust sampler, incremental weighted k-means that brings significant improvement on the synthetic tasks but only a marginal uplift on real-life ones. we hope that the questions raised in this paper are of interest to the community and could open new paths for active learning research.",,2021-09-03,2022-10-21,"['alexandre abraham', 'léo dreyfus-schmidt']"
35,2109.12784,learning from few samples: transformation-invariant svms with   composition and locality at multiple scales,cs.lg stat.ml,"motivated by the problem of learning with small sample sizes, this paper shows how to incorporate into support-vector machines (svms) those properties that have made convolutional neural networks (cnns) successful. particularly important is the ability to incorporate domain knowledge of invariances, e.g., translational invariance of images. kernels based on the \textit{maximum} similarity over a group of transformations are not generally positive definite. perhaps it is for this reason that they have not been studied theoretically. we address this lacuna and show that positive definiteness indeed holds \textit{with high probability} for kernels based on the maximum similarity in the small training sample set regime of interest, and that they do yield the best results in that regime. we also show how additional properties such as their ability to incorporate local features at multiple spatial scales, e.g., as done in cnns through max pooling, and to provide the benefits of composition through the architecture of multiple layers, can also be embedded into svms. we verify through experiments on widely available image sets that the resulting svms do provide superior accuracy in comparison to well-established deep neural network benchmarks for small sample sizes.",,2021-09-27,2022-10-22,"['tao liu', 'p. r. kumar', 'ruida zhou', 'xi liu']"
36,2109.14855,sim2real for soft robotic fish via differentiable simulation,cs.ro cs.lg,"accurate simulation of soft mechanisms under dynamic actuation is critical for the design of soft robots. we address this gap with our differentiable simulation tool by learning the material parameters of our soft robotic fish. on the example of a soft robotic fish, we demonstrate an experimentally-verified, fast optimization pipeline for learning the material parameters from quasi-static data via differentiable simulation and apply it to the prediction of dynamic performance. our method identifies physically plausible young's moduli for various soft silicone elastomers and stiff acetal copolymers used in creation of our three different robotic fish tail designs. we show that our method is compatible with varying internal geometry of the actuators, such as the number of hollow cavities. our framework allows high fidelity prediction of dynamic behavior for composite bi-morph bending structures in real hardware to millimeter-accuracy and within 3 percent error normalized to actuator length. we provide a differentiable and robust estimate of the thrust force using a neural network thrust predictor; this estimate allows for accurate modeling of our experimental setup measuring bollard pull. this work presents a prototypical hardware and simulation problem solved using our differentiable framework; the framework can be applied to higher dimensional parameter inference, learning control policies, and computational design due to its differentiable character.",,2021-09-30,2022-10-20,"['john z. zhang', 'yu zhang', 'pingchuan ma', 'elvis nava', 'tao du', 'philip arm', 'wojciech matusik', 'robert k. katzschmann']"
37,2110.02343,quantum semi-supervised learning with quantum supremacy,quant-ph cond-mat.stat-mech cs.it cs.lg hep-th math.it,"quantum machine learning promises to efficiently solve important problems. there are two persistent challenges in classical machine learning: the lack of labeled data, and the limit of computational power. we propose a novel framework that resolves both issues: quantum semi-supervised learning. moreover, we provide a protocol in systematically designing quantum machine learning algorithms with quantum supremacy, which can be extended beyond quantum semi-supervised learning. in the meantime, we show that naive quantum matrix product estimation algorithm outperforms the best known classical matrix multiplication algorithm. we showcase two concrete quantum semi-supervised learning algorithms: a quantum self-training algorithm named the propagating nearest-neighbor classifier, and the quantum semi-supervised k-means clustering algorithm. by doing time complexity analysis, we conclude that they indeed possess quantum supremacy.",,2021-10-05,2022-10-24,['zhou shangnan']
38,2110.02474,can an ai agent hit a moving target?,econ.th cs.lg,"i model the belief formation and decision making processes of economic agents during a monetary policy regime change (an acceleration in the money supply) with a deep reinforcement learning algorithm in the ai literature. i show that when the money supply accelerates, the learning agents only adjust their actions, which include consumption and demand for real balance, after gathering learning experience for many periods. this delayed adjustments leads to low returns during transition periods. once they start adjusting to the new environment, their welfare improves. their changes in beliefs and actions lead to temporary inflation volatility. i also show that, 1. the ai agents who explores their environment more adapt to the policy regime change quicker, which leads to welfare improvements and less inflation volatility, and 2. the ai agents who have experienced a structural change adjust their beliefs and behaviours quicker than an inexperienced learning agent.",,2021-10-05,2022-10-24,"['n/a rui', 'n/a shi']"
39,2110.03135,label noise in adversarial training: a novel perspective to study robust   overfitting,cs.lg cs.ai stat.ml,"we show that label noise exists in adversarial training. such label noise is due to the mismatch between the true label distribution of adversarial examples and the label inherited from clean examples - the true label distribution is distorted by the adversarial perturbation, but is neglected by the common practice that inherits labels from clean examples. recognizing label noise sheds insights on the prevalence of robust overfitting in adversarial training, and explains its intriguing dependence on perturbation radius and data quality. also, our label noise perspective aligns well with our observations of the epoch-wise double descent in adversarial training. guided by our analyses, we proposed a method to automatically calibrate the label to address the label noise and robust overfitting. our method achieves consistent performance improvements across various models and datasets without introducing new hyper-parameters or additional tuning.",,2021-10-06,2022-10-19,"['chengyu dong', 'liyuan liu', 'jingbo shang']"
40,2110.05169,learning a subspace of policies for online adaptation in reinforcement   learning,cs.lg cs.ai,"deep reinforcement learning (rl) is mainly studied in a setting where the training and the testing environments are similar. but in many practical applications, these environments may differ. for instance, in control systems, the robot(s) on which a policy is learned might differ from the robot(s) on which a policy will run. it can be caused by different internal factors (e.g., calibration issues, system attrition, defective modules) or also by external changes (e.g., weather conditions). there is a need to develop rl methods that generalize well to variations of the training conditions. in this article, we consider the simplest yet hard to tackle generalization setting where the test environment is unknown at train time, forcing the agent to adapt to the system's new dynamics. this online adaptation process can be computationally expensive (e.g., fine-tuning) and cannot rely on meta-rl techniques since there is just a single train environment. to do so, we propose an approach where we learn a subspace of policies within the parameter space. this subspace contains an infinite number of policies that are trained to solve the training environment while having different parameter values. as a consequence, two policies in that subspace process information differently and exhibit different behaviors when facing variations of the train environment. our experiments carried out over a large variety of benchmarks compare our approach with baselines, including diversity-based methods. in comparison, our approach is simple to tune, does not need any extra component (e.g., discriminator) and learns policies able to gather a high reward on unseen environments.",,2021-10-11,2022-10-24,"['jean-baptiste gaya', 'laure soulier', 'ludovic denoyer']"
41,2110.08255,yformer: u-net inspired transformer architecture for far horizon time   series forecasting,cs.lg,"time series data is ubiquitous in research as well as in a wide variety of industrial applications. effectively analyzing the available historical data and providing insights into the far future allows us to make effective decisions. recent research has witnessed the superior performance of transformer-based architectures, especially in the regime of far horizon time series forecasting. however, the current state of the art sparse transformer architectures fail to couple down- and upsampling procedures to produce outputs in a similar resolution as the input. we propose the yformer model, based on a novel y-shaped encoder-decoder architecture that (1) uses direct connection from the downscaled encoder layer to the corresponding upsampled decoder layer in a u-net inspired architecture, (2) combines the downscaling/upsampling with sparse attention to capture long-range effects, and (3) stabilizes the encoder-decoder stacks with the addition of an auxiliary reconstruction loss. extensive experiments have been conducted with relevant baselines on four benchmark datasets, demonstrating an average improvement of 19.82, 18.41 percentage mse and 13.62, 11.85 percentage mae in comparison to the current state of the art for the univariate and the multivariate settings respectively.",,2021-10-13,2022-08-25,"['kiran madhusudhanan', 'johannes burchert', 'nghia duong-trung', 'stefan born', 'lars schmidt-thieme']"
42,2110.08432,meta-learning with adjoint methods,cs.lg,"model agnostic meta learning (maml) is widely used to find a good initialization for a family of tasks. despite its success, a critical challenge in maml is to calculate the gradient w.r.t. the initialization of a long training trajectory for the sampled tasks, because the computation graph can rapidly explode and the computational cost is very expensive. to address this problem, we propose adjoint maml (a-maml). we view gradient descent in the inner optimization as the evolution of an ordinary differential equation (ode). to efficiently compute the gradient of the validation loss w.r.t. the initialization, we use the adjoint method to construct a companion, backward ode. to obtain the gradient w.r.t. the initialization, we only need to run the standard ode solver twice -- one is forward in time that evolves a long trajectory of gradient flow for the sampled task; the other is backward and solves the adjoint ode. we need not create or expand any intermediate computational graphs, adopt aggressive approximations, or impose proximal regularizers in the training loss. our approach is cheap, accurate, and adaptable to different trajectory lengths. we demonstrate the advantage of our approach in both synthetic and real-world meta-learning tasks.",,2021-10-15,2022-10-23,"['shibo li', 'zheng wang', 'akil narayan', 'robert kirby', 'shandian zhe']"
43,2110.09902,toward understanding convolutional neural networks from volterra   convolution perspective,cs.lg,"we make an attempt to understanding convolutional neural network by exploring the relationship between (deep) convolutional neural networks and volterra convolutions. we propose a novel approach to explain and study the overall characteristics of neural networks without being disturbed by the horribly complex architectures. specifically, we attempt to convert the basic structures of a convolutional neural network (cnn) and their combinations to the form of volterra convolutions. the results show that most of convolutional neural networks can be approximated in the form of volterra convolution, where the approximated proxy kernels preserve the characteristics of the original network. analyzing these proxy kernels may give valuable insight about the original network. base on this setup, we presented methods to approximating the order-zero and order-one proxy kernels, and verified the correctness and effectiveness of our results.",,2021-10-19,2022-10-22,"['tenghui li', 'guoxu zhou', 'yuning qiu', 'qibin zhao']"
44,2110.11688,differentially private coordinate descent for composite empirical risk   minimization,cs.lg cs.cr stat.ml,"machine learning models can leak information about the data used to train them. to mitigate this issue, differentially private (dp) variants of optimization algorithms like stochastic gradient descent (dp-sgd) have been designed to trade-off utility for privacy in empirical risk minimization (erm) problems. in this paper, we propose differentially private proximal coordinate descent (dp-cd), a new method to solve composite dp-erm problems. we derive utility guarantees through a novel theoretical analysis of inexact coordinate descent. our results show that, thanks to larger step sizes, dp-cd can exploit imbalance in gradient coordinates to outperform dp-sgd. we also prove new lower bounds for composite dp-erm under coordinate-wise regularity assumptions, that are nearly matched by dp-cd. for practical implementations, we propose to clip gradients using coordinate-wise thresholds that emerge from our theory, avoiding costly hyperparameter tuning. experiments on real and synthetic data support our results, and show that dp-cd compares favorably with dp-sgd.",,2021-10-22,2022-10-21,"['paul mangold', 'aurélien bellet', 'joseph salmon', 'marc tommasi']"
45,2110.13624,technology fitness landscape for design innovation: a deep neural   embedding approach based on patent data,cs.lg,"technology is essential to innovation and economic prosperity. understanding technological changes can guide innovators to find new directions of design innovation and thus make breakthroughs. in this work, we construct a technology fitness landscape via deep neural embeddings of patent data. the landscape consists of 1,757 technology domains and their respective improvement rates. in the landscape, we found a high hill related to information and communication technologies (ict) and a vast low plain of the remaining domains. the landscape presents a bird's eye view of the structure of the total technology space, providing a new way for innovators to interpret technology evolution with a biological analogy, and a biologically-inspired inference to the next innovation.",,2021-10-21,2022-10-21,"['shuo jiang', 'jianxi luo']"
46,2110.14241,dynamic population-based meta-learning for multi-agent communication   with natural language,cs.lg cs.ai cs.cl cs.ma stat.ml,"in this work, our goal is to train agents that can coordinate with seen, unseen as well as human partners in a multi-agent communication environment involving natural language. previous work using a single set of agents has shown great progress in generalizing to known partners, however it struggles when coordinating with unfamiliar agents. to mitigate that, recent work explored the use of population-based approaches, where multiple agents interact with each other with the goal of learning more generic protocols. these methods, while able to result in good coordination between unseen partners, still only achieve so in cases of simple languages, thus failing to adapt to human partners using natural language. we attribute this to the use of static populations and instead propose a dynamic population-based meta-learning approach that builds such a population in an iterative manner. we perform a holistic evaluation of our method on two different referential games, and show that our agents outperform all prior work when communicating with seen partners and humans. furthermore, we analyze the natural language generation skills of our agents, where we find that our agents also outperform strong baselines. finally, we test the robustness of our agents when communicating with out-of-population agents and carefully test the importance of each component of our method through ablation studies.",,2021-10-27,,"['abhinav gupta', 'marc lanctot', 'angeliki lazaridou']"
47,2111.02997,global optimality and finite sample analysis of softmax off-policy actor   critic under state distribution mismatch,cs.lg,"in this paper, we establish the global optimality and convergence rate of an off-policy actor critic algorithm in the tabular setting without using density ratio to correct the discrepancy between the state distribution of the behavior policy and that of the target policy. our work goes beyond existing works on the optimality of policy gradient methods in that existing works use the exact policy gradient for updating the policy parameters while we use an approximate and stochastic update step. our update step is not a gradient update because we do not use a density ratio to correct the state distribution, which aligns well with what practitioners do. our update is approximate because we use a learned critic instead of the true value function. our update is stochastic because at each step the update is done for only the current state action pair. moreover, we remove several restrictive assumptions from existing works in our analysis. central to our work is the finite sample analysis of a generic stochastic approximation algorithm with time-inhomogeneous update operators on time-inhomogeneous markov chains, based on its uniform contraction properties.",,2021-11-04,2022-10-24,"['shangtong zhang', 'remi tachet', 'romain laroche']"
48,2111.03289,improved regret analysis for variance-adaptive linear bandits and   horizon-free linear mixture mdps,stat.ml cs.lg math.st stat.th,"in online learning problems, exploiting low variance plays an important role in obtaining tight performance guarantees yet is challenging because variances are often not known a priori. recently, considerable progress has been made by zhang et al. (2021) where they obtain a variance-adaptive regret bound for linear bandits without knowledge of the variances and a horizon-free regret bound for linear mixture markov decision processes (mdps). in this paper, we present novel analyses that improve their regret bounds significantly. for linear bandits, we achieve $\tilde o(\min\{d\sqrt{k}, d^{1.5}\sqrt{\sum_{k=1}^k \sigma_k^2}\} + d^2)$ where $d$ is the dimension of the features, $k$ is the time horizon, and $\sigma_k^2$ is the noise variance at time step $k$, and $\tilde o$ ignores polylogarithmic dependence, which is a factor of $d^3$ improvement. for linear mixture mdps with the assumption of maximum cumulative reward in an episode being in $[0,1]$, we achieve a horizon-free regret bound of $\tilde o(d \sqrt{k} + d^2)$ where $d$ is the number of base models and $k$ is the number of episodes. this is a factor of $d^{3.5}$ improvement in the leading term and $d^7$ in the lower order term. our analysis critically relies on a novel peeling-based regret analysis that leverages the elliptical potential `count' lemma.",,2021-11-05,2022-10-20,"['yeoneung kim', 'insoon yang', 'kwang-sung jun']"
49,2111.03664,oracle teacher: leveraging target information for better knowledge   distillation of ctc models,cs.lg eess.as eess.iv,"knowledge distillation (kd), best known as an effective method for model compression, aims at transferring the knowledge of a bigger network (teacher) to a much smaller network (student). conventional kd methods usually employ the teacher model trained in a supervised manner, where output labels are treated only as targets. extending this supervised scheme further, we introduce a new type of teacher model for connectionist temporal classification (ctc)-based sequence models, namely oracle teacher, that leverages both the source inputs and the output labels as the teacher model's input. since the oracle teacher learns a more accurate ctc alignment by referring to the target information, it can provide the student with more optimal guidance. one potential risk for the proposed approach is a trivial solution that the model's output directly copies the target input. based on a many-to-one mapping property of the ctc algorithm, we present a training strategy that can effectively prevent the trivial solution and thus enables utilizing both source and target inputs for model training. extensive experiments are conducted on two sequence learning tasks: speech recognition and scene text recognition. from the experimental results, we empirically show that the proposed model improves the students across these tasks while achieving a considerable speed-up in the teacher model's training time.",,2021-11-05,2022-10-24,"['ji won yoon', 'hyung yong kim', 'hyeonseung lee', 'sunghwan ahn', 'nam soo kim']"
