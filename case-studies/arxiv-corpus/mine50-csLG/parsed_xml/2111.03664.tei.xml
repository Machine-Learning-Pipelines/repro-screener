<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Oracle Teacher: Leveraging Target Information for Better Knowledge Distillation of CTC Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-24">24 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ji</forename><forename type="middle">Won</forename><surname>Yoon</surname></persName>
							<email>jwyoon@hi.snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of New Media and Communications</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nam</forename><surname>Soo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of New Media and Communications</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Oracle Teacher: Leveraging Target Information for Better Knowledge Distillation of CTC Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-24">24 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">B8B32C37429C749BF1F45F9985A95785</idno>
					<idno type="arXiv">arXiv:2111.03664v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-10-31T05:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Speech recognition</term>
					<term>scene text recognition</term>
					<term>connectionist temporal classification</term>
					<term>knowledge distillation</term>
					<term>teacherstudent learning</term>
					<term>transfer learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge distillation (KD), best known as an effective method for model compression, aims at transferring the knowledge of a bigger network (teacher) to a much smaller network (student). Conventional KD methods usually employ the teacher model trained in a supervised manner, where output labels are treated only as targets. Extending this supervised scheme further, we introduce a new type of teacher model for connectionist temporal classification (CTC)-based sequence models, namely Oracle Teacher, that leverages both the source inputs and the output labels as the teacher model's input. Since the Oracle Teacher learns a more accurate CTC alignment by referring to the target information, it can provide the student with more optimal guidance. One potential risk for the proposed approach is a trivial solution that the model's output directly copies the target input. Based on a many-to-one mapping property of the CTC algorithm, we present a training strategy that can effectively prevent the trivial solution and thus enables utilizing both source and target inputs for model training. Extensive experiments are conducted on two sequence learning tasks: speech recognition and scene text recognition. From the experimental results, we empirically show that the proposed model improves the students across these tasks while achieving a considerable speed-up in the teacher model's training time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A S deep neural networks bring a significant improvement in various fields such as speech recognition, computer vision, and natural language processing, they also become wider and deeper. However, as models grow in size and complexity, high-performing neural network models become either computationally expensive or consume a large amount of memory, hindering their wide deployment in resourcelimited scenarios. To mitigate this computational burden, several techniques such as model pruning <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, quantization <ref type="bibr" target="#b2">[3]</ref>, and knowledge distillation <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> have been suggested. Among these approaches, knowledge distillation (KD) is a popular compression scheme, which is the process of transferring knowledge from a deep and complex model (teacher) to a shallower and simpler model (student).</p><p>Conventional KD methods typically share a common feature; they require a teacher model with high capacity that has been trained in a supervised manner, where the ground-truth labels are required as a target. However, training the teacher from scratch can be costly since many of the current stateof-the-art (SOTA) models suffer from excessive training time and difficult hyper-parameters tuning. Thus, some existing approaches rely heavily on the pre-trained model, provided by other prior research, as the teacher to save the training time and resource cost. Even though making full use of the provided pre-trained models is one important motivation of KD, this dependency might limit the flexibility of our consideration. If we can train a better teacher model with fewer resources and training time, KD from various teachers will be possible on different tasks or databases.</p><p>We revisit the teacher model in KD from a different perspective. In a conventional KD scenario, there is no guarantee that the teacher can find the correct solution for every complicated problem in an optimal way, implying that the teacher model may provide suboptimal guidance for the student. The key idea of our framework is to derive a more accurate problemsolving process by referring to the existing solutions so that the teacher can provide better guidance to the student. On this basis, we introduce a new type of teacher for Connectionist Temporal Classification (CTC) <ref type="bibr" target="#b5">[6]</ref>-based sequence models, namely Oracle Teacher. The conventional teacher is typically built in a supervised manner whose goal is to predict the target output for a given source input data. In contrast, the proposed teacher model utilizes not only the source input but also the target value to estimate better CTC alignment.</p><p>However, it may be somewhat confusing to understand what it means to train a model using both the source inputs and the output labels as the model's input. Specifically, the Oracle Teacher is likely to heavily rely on the target input, i.e., the output label, while ignoring the embedding from the source input. To overcome this problem, we propose a training scheme that uses the many-to-one mapping property of the CTC algorithm. Since the relationship between the CTC alignment and the original target is many-to-one, we can prevent a trivial solution that the model's output directly copies the target input. To the best of our knowledge, this is the first attempt of using the target input to improve the ability of the teacher model. Utilizing the target input for training the teacher model brings several benefits for KD. Firstly, the proposed teacher model produces a more accurate CTC alignment by referring to the target information so that its knowledge can provide more optimal guidance to the student. Secondly, the representation of the proposed teacher contains target-related embedding that can be supportive for student training. For example, the Oracle Teacher for automatic speech recognition (ASR) is trained to use both speech and text as the model's input during training. Different from the typical ASR teachers that take only acoustic features into consideration, the Oracle Teacher performs a fusion of both acoustic (speech) and linguistic (text) features when generating the prediction. Since unifying acoustic and linguistic representation learning generally enhances the performance of the speech processing <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b10">[11]</ref>, the Oracle Teacher's representation, which considers not only the acoustic but also linguistic information, can be more effective for the ASR student. Also, the Oracle Teacher can boost up the speed of the training since the target input is used as the guidance to reduce the candidate scope of the prediction. Compared to the conventional teacher models that require tremendous time and GPU resources, our framework dramatically reduces the computational cost required to train the teacher model.</p><p>Extensive experiments are conducted on two different sequence learning tasks: ASR and scene text recognition (STR). Empirically, we verify that the student distilled from the Oracle Teacher achieves better performance compared to the case when it is distilled from the other pre-trained models, which yield the high performance for each task. Apart from performance, we also measure the computational cost for training teacher model and shows that a powerful teacher can be trained with a reduced computational burden via the proposed scheme.</p><p>Our main contributions are summarized as follows:</p><p>1) Our paper introduces a new type of teacher for CTCbased sequence models, namely Oracle Teacher, that utilizes the output labels as an additional input for model training. The proposed teacher model can estimate a more accurate CTC alignment, providing more optimal guidance to the student. To the best of our knowledge, this is the first attempt of using the target input to improve the performance of a teacher model. 2) Through extensive experiments on two sequence learning tasks, including ASR and STR, we verify the superiority of the Oracle Teacher compared to the conventional teacher models. Moreover, our framework dramatically reduces the computational cost of the teacher model in terms of the training time and required GPU resources. 3) In a detailed case study and analysis, we validate why the proposed method can result in better KD performance than the conventional teacher and check if the Oracle Teacher is correctly trained while preventing the trivial solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Knowledge Distillation</head><p>There has been a long line of research on KD, which aims at distilling knowledge from a big teacher model to a small student model. Bucila et al. <ref type="bibr" target="#b3">[4]</ref> proposed a method to compress an ensemble of models into a single model without significant accuracy loss. Later, Ba and Caruana <ref type="bibr" target="#b11">[12]</ref> extended it to deep learning by using the logits of the teacher model. Hinton et al. <ref type="bibr" target="#b4">[5]</ref> revived this idea under the name of KD that distills class probability by minimizing the Kullback-Leibler (KL)-divergence between the softmax outputs of the teacher and student. In the case of the ASR task, the most frequently employed KD approach is to train a student with the teacher's prediction as a target, in conjunction with the ground truth. For the conventional deep neural network (DNN)-hidden Markov model (HMM) hybrid systems, Li et al. <ref type="bibr" target="#b12">[13]</ref> first attempted to apply the teacher-student learning to a speech recognition task, and Wong et al. <ref type="bibr" target="#b13">[14]</ref> applied sequence-level KD to the acoustic model. Several researchers applied KD to improve the performance by minimizing the frame-level crossentropy loss between the output distributions of the teacher and student <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b18">[19]</ref>. For end-to-end speech recognition, KD has been successfully applied to CTC models <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b24">[25]</ref> and attention-based encoder-decoder models <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b28">[28]</ref>. However, as reported in previous KD studies <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref>, simply applying the frame-level CE to the CTC-based model can worsen the performance compared to the baseline. To cover this problem, Kurata and Audhkhasi <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> proposed KD approaches, where the CTC-based student can be trained using the frame-wise alignment of the teacher. Takashima et al. <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> explored sequence-level KD methods for training CTC models. Yoon et al. <ref type="bibr" target="#b24">[25]</ref> suggested that l 2 loss is more suitable than the conventional KL-divergence to distill framelevel posterior in the CTC framework.</p><p>The hidden representation from the teacher also has been proven to hold additional knowledge that can contribute to improving the student's performance. Recently, some KD methods <ref type="bibr" target="#b29">[29]</ref>- <ref type="bibr" target="#b36">[36]</ref>, particularly in computer vision, were proposed to minimize the mean squared error (MSE) between the representation-level knowledge of the two models. They address how to extract a better knowledge from the teacher model and transfer it to the student. Yoon et al. <ref type="bibr" target="#b24">[25]</ref> first attempted to transfer the the hidden representation across different structured neural networks for end-to-end speech recognition while using frame weighting that reflects which frames are important for KD. Recently, several KD approaches <ref type="bibr" target="#b37">[37]</ref>- <ref type="bibr" target="#b39">[39]</ref> suggested using the hidden representation-level knowledge to improve the self-supervised speech representation learningbased models, like Hidden-Unit BERT (HuBERT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Connectionist Temporal Classification</head><p>Generally, an end-to-end sequence model directly converts a sequence of input features x 1:T into a sequence of target labels y 1:L where y l ∈ I with I being the set of labels. T and L are respectively the length of x = x 1:T and y = y 1:L . To cope with the mapping problem when the two sequences have different lengths, the Connectionist Temporal Classification (CTC) framework <ref type="bibr" target="#b5">[6]</ref> introduces "blank" label and allows the repetition of each label to force the output and input sequences to have the same length. A CTC alignment π 1:T is a sequence of initial output labels, as every input x t is mapped to a certain label π t ∈ I where I = I ∪{blank}. A mapping function B, which is defined as y = B(π), maps the alignment sequence π into the final output sequence y after merging consecutive repeated characters and removing blank labels. The conditional Different from the conventional teacher, the target y is used as the additional input to the model. Note that the Oracle Teacher is a non-autoregressive model where the look-ahead mask is not included in the decoder. The architecture selection of the SourceNet depends on the task we are interested in. When the main task is ASR. the SourceNet corresponds to an acoustic model part of the conventional ASR model. In our experiment for ASR, the SourceNet is based on the architecture of Japser <ref type="bibr" target="#b40">[40]</ref>. For STR, we apply the CRNN <ref type="bibr" target="#b41">[41]</ref> as the SourceNet. probability of the target sequence y given the input sequence x is defined as</p><formula xml:id="formula_0">P (y|x) = π∈B −1 (y) P (π|x).<label>(1)</label></formula><p>where B −1 denotes the inverse mapping and returns all possible alignment sequences compatible with y. Given a target label sequence y, the loss function L CT C is defined as:</p><formula xml:id="formula_1">L CT C = − log P (y|x).<label>(2)</label></formula><p>III. ORACLE TEACHER This section introduces how to design the Oracle teacher that utilizes the output labels as an additional input. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, we let the Oracle Teacher model learn a function from the source x and the target y inputs to the CTC alignment π.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Oracle Teacher Training</head><p>Let x = x 1:T = {x 1 , ..., x T } be an input sequence of length T , and y = y 1:L = {y 1 , ..., y L } be a target sequence of length L. As mentioned in Section II-B, the CTC algorithm employs the intermediate CTC alignment π = π 1:T = {π 1 , ..., π T } to align variable-length input and output sequences. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the relationship between the alignment π and the target input y is many-to-one via the mapping B, and this many-to-one setting is the key to training the Oracle Teacher. Intuitively, predicting the CTC alignment π (many) from the target input y (one) should be difficult since many possible paths are compatible with y. Therefore, the proposed model can be trained to use the embeddings of both x and y while preventing a trivial solution where the model's output simply copies the input y. The Oracle Teacher can effectively estimate the most probable CTC alignment π since the target input y is used as guidance to reduce the candidate scope of π. The Oracle Teacher learns the parameters θ to minimize the following training loss:</p><formula xml:id="formula_2">L train = − log π∈B −1 (y) P (π|x, y; θ) (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where B is the many-to-one mapping function in (1) that maps the latent alignment π 1:T into the target y 1:L .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Knowledge Distillation with Oracle Teacher</head><p>We can interpret KD framework from a different perspective by applying the additional target input. Given a source x, the student model learns the parameter φ to maximize the following conditional probability: </p><p>where the inequality follows from Jensen's inequality, B represents the mapping function in (3), and D KL denotes the KL-divergence. In our framework, P (π|x, y; θ) and P (π|x; φ) correspond to the alignment probability derived from the Oracle Teacher and the student, respectively. By minimizing the KL-divergence between the Oracle Teacher and the student, we can maximize the conditional probability of the student model P (y|x; φ). Directly optimizing the KL-divergence in ( <ref type="formula" target="#formula_4">4</ref>) is intractable because the KL divergence involves the integral that is difficult to calculate. To sidestep this problem, we can minimize the CE between the softmax outputs of the Oracle Teacher and the student. However, as reported in previous KD studies <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>, simply applying the frame-level CE to the CTC-based model can worsen the performance compared to the baseline trained only with the ground truth. Instead, we adopt FitNets <ref type="bibr" target="#b29">[29]</ref> as the basic KD technique, which considers the hidden representation for distillation, and there are two reasons for this choice: (1) In the CTC framework, transferring the hidden representation is much more effective than the softmax-level KD approach <ref type="bibr" target="#b24">[25]</ref>; (2) Recent KD approaches for sequence learning <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b39">[39]</ref> are based on the Fitnets.</p><p>As depicted in Fig. <ref type="figure" target="#fig_3">3</ref>, w tea and w stu respectively denote the hidden representation obtained from the last layer of the teacher and student models. Since usually the hidden layer dimensions of w tea and w stu are different, we apply a fully connected layer g to bridge the dimension mismatch. The process of KD initializes the student by minimizing the distance between hidden representation of the teacher model w tea and student model w stu . The objective for KD is given by</p><formula xml:id="formula_5">L KD (w stu , w tea ) = w tea − g(w stu ) 2 2 .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model Structure</head><p>The Oracle Teacher mainly consists of three components: 1) an encoder to encode the target input, 2) a SourceNet to extract the features from the source input data, and 3) a decoder to predict the CTC alignment. Its architecture follows the encoder-decoder structure of the Transformer <ref type="bibr" target="#b42">[42]</ref>, which allows the model to attend to related target information when making a prediction. Note that the Oracle Teacher is a non-autoregressive model where the look-ahead mask is not included in the decoder. In Fig. <ref type="figure" target="#fig_0">1</ref>, we illustrate the schematics of the Oracle Teacher model which can also be summarized as follows:</p><formula xml:id="formula_6">h S = SourceNet(x; θ S ),<label>(6)</label></formula><formula xml:id="formula_7">h E = Encoder(y; θ E ),<label>(7)</label></formula><formula xml:id="formula_8">P (π|x, y) ∼ Decoder(h E , h S ; θ D ).<label>(8)</label></formula><p>Compared to the vanilla Transformer, the main architectural difference lies in the cross attention. The encoder takes inputs from the whole target y, and its resulting vectors h E are treated as key-value pairs of the cross attention. In addition, we do not employ a look-ahead mask, which is used in the vanilla Transformer to mask the future tokens, in the multihead attention layer of the decoder. 1) SourceNet: The SourceNet converts the source input x into high-level representations h S . Since h S serves as query for the decoder, the length of the decoder output has the same length as h S . The architecture of the SourceNet depends on the task we are interested in. When the main task is ASR. the SourceNet corresponds to an acoustic model part of the conventional ASR model. In our experiments, we apply CTC-based Jasper <ref type="bibr" target="#b40">[40]</ref> architecture as the SourceNet, and consequently |h S | ≥ |y|. In the case of the STR task, we adopt the architecture of the CRNN <ref type="bibr" target="#b41">[41]</ref> for the SourceNet.</p><p>2) Encoder: In the encoder, we adopt the same structure as the encoder of the original Transformer. The self-attention captures dependencies between different positions in y and outputs intermediate representations h E .</p><p>3) Decoder: The representations h S and h E are fed into the decoder, which follows the architecture of the conventional Transformer decoder. The self-attention layer, the first attention layer of the decoder, takes the representations h S as the input. Then, the output serves as queries for the cross attention, whose key-value pairs are the representations h E . The cross attention allows the decoder to look into the relevant target information when producing the prediction. Note that the lookahead mask is not included in the decoder. Different from the autoregressive model that only uses the past output tokens in producing the results, the Oracle Teacher can utilize more global output features when predicting the output.</p><p>In the proposed framework, the representation of the source x corresponds to the queries for the cross-attention. This is because, for KD, the length of the decoder output (= the length of the query h S ) should have the same length (T ) as the student's output, which is determined by the source x. If the decoder output has a different length from that of the student's output, the Oracle Teacher cannot transfer the knowledge to the student.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETTINGS</head><p>A. Datasets and Baselines 1) Speech Recogniton: For ASR, we evaluated the performance of the models on LibriSpeech <ref type="bibr" target="#b43">[43]</ref>. In the training phase, "train-clean-100", "train-clean-360", and "trainother-500" were used. For evaluation, "dev-clean", "devother", "test-clean", and "test-other" were applied. We adopted the current high-performing models for the conventional teacher in each task. In the case of ASR, we applied pre-trained Jasper Dense Residual (Jasper DR) <ref type="bibr" target="#b40">[40]</ref>, which consists of 54 convolutional layers. Recent ASR studies <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b44">[44]</ref>- <ref type="bibr" target="#b46">[46]</ref> utilized Jasper DR as the baseline. According to the previous study <ref type="bibr" target="#b40">[40]</ref>, Jasper DR produces word error rate (WER) 2.62 % with strong Transformer-XL <ref type="bibr" target="#b47">[47]</ref> LM, still SOTA performance on LibriSpeech. As for the ASR student model, we used Jasper Mini, composed of 33 depthwise separable 1D convolutional layers. The Oracle Teacher had the SourceNet with 8 convolutional layers, and both the encoder and decoder consisted of 2 layers.</p><p>2) Scene Text Recognition: We evaluated STR models on seven benchmark datasets 1 : Street View Text (SVT) <ref type="bibr" target="#b49">[49]</ref>, SVT Perspective (SVTP) <ref type="bibr" target="#b50">[50]</ref>, IIIT5K-Words (IIIT) <ref type="bibr" target="#b51">[51]</ref>, CUTE80 (CT) <ref type="bibr" target="#b52">[52]</ref>, ICDAR03 (IC03) <ref type="bibr" target="#b53">[53]</ref>, ICDAR13 (IC13) <ref type="bibr" target="#b54">[54]</ref>, and ICDAR15 (IC15) <ref type="bibr" target="#b55">[55]</ref>. For validation, IC13, IC15, IIIT, and SVT were applied. As training datasets, we used the two most popular datasets: MJSynth <ref type="bibr" target="#b56">[56]</ref> and SynthText <ref type="bibr" target="#b57">[57]</ref>. We adopted Rosetta <ref type="bibr" target="#b58">[58]</ref> and STAR-Net <ref type="bibr" target="#b59">[59]</ref>, considered as the benchmarking SOTA models in recent researches <ref type="bibr" target="#b60">[60]</ref>, <ref type="bibr" target="#b61">[61]</ref>. In the case of the student model, CRNN <ref type="bibr" target="#b41">[41]</ref> was adopted with a thin-plate spline (TPS), which normalizes curved and perspective texts into a standardized view. The SourceNet followed the TPS-CRNN structure, and both the encoder and decoder used 1 layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>1) Speech Recognition: For the LibriSpeech dataset, We used the OpenSeq2Seq <ref type="bibr" target="#b62">[62]</ref> toolkit for the implementation. ASR models were based on the character-level CTC models. The character set had 29 labels. In the case of Jasper DR, we used the pre-trained model provided by the OpenSeq2Seq toolkit. The student model was run on three Titan V GPUs, each with 12GB of memory. We used a NovoGrad optimizer <ref type="bibr" target="#b63">[63]</ref> whose initial learning rate started from 0.02 with a weight decay of 0.001. For KD, the student was initially trained with FitNets <ref type="bibr" target="#b29">[29]</ref> loss for 5 epochs. After initialization, 50 epochs were spent on CTC training. In addition, we trained the Oracle Teacher for 30 epochs on a single Titan V GPU using Noam learning rate scheduler with 4000 steps of warmup and a learning rate of 1.5. When applying beam-search decoding with language model (LM), we used KenLM <ref type="bibr" target="#b64">[64]</ref> for 4-gram LM, where the LM weight, the word insertion weight, and the beam width were experimentally set to 2.0, 1.5, and 512, respectively. For the Mandarin ASR dataset, the character set had a total of 5207 labels. Pre-trained Jasper DR, which was used as the conventional teacher, was provided by the NeMo <ref type="bibr" target="#b65">[65]</ref> toolkit. The student was trained in an identical way to LibriSpeech, but the initial learning rate was set to 0.005. Instead of WER, we measured the character error rate (CER) since a single character often represents a word for Mandarin.</p><p>2) Scene Text Recognition: When training the STR models, our experiments were conducted using the official implementation provided by Baek et al. 2  <ref type="bibr" target="#b48">[48]</ref>. STR models were based on the character-level CTC models. The character set had a total of 37 labels. All STR models, including the Oracle Teacher, were trained for 300k iterations on a single Titan V GPU (12GB) in the CTC framework. We employed the AdaDelta optimizer <ref type="bibr" target="#b66">[66]</ref> with a decay rate of 0.95, and the initial learning rate was 1.0. In FitNets <ref type="bibr" target="#b29">[29]</ref> training, we trained 300k iterations for the student.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>In the subsequent part of this paper, A → B means that teacher model A transfers knowledge to the student model B.</p><p>As mentioned in Section III-B, we employed FitNets <ref type="bibr" target="#b29">[29]</ref> as the basic KD technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Main Results: Performance Comparison</head><p>Since the Oracle Teacher is the teacher model for KD, not the baseline model performing the learning task, the evaluation results of the Oracle Teacher itself are not described in this section. Note that the model size and the performance of the Oracle Teacher will be additionally discussed in Section V-D and V-E3.</p><p>1) Speech Recognition: The results for LibriSpeech are shown in Table <ref type="table" target="#tab_0">I</ref>. We measured WER to quantify the performance. The best performance was achieved when training the student with the Oracle Teacher. In addition, to further check the effectiveness of the target input y, which is used as the additional input of the Oracle Teacher, we applied an incomplete Oracle Teacher model, called Oracle Teacher w/o target. In Oracle Teacher w/o target, zero arrays were treated as the additional input instead of the target input y during training and inference phases. Since the Oracle Teacher w/o target only consumed the source input, its architecture was similar to that of the conventional CTC model. When applying the Oracle Teacher w/o target, the distilled student achieved improvement over the baseline student, which indicates that the knowledge of the SourceNet contributed to improving the performance of the student. However, their performances were worse than the Oracle Teacher → Jasper Mini in all configurations, implying that the oracle guidance helped the Oracle Teacher extract a more supportive knowledge for the student.</p><p>As presented in Table <ref type="table" target="#tab_1">II</ref>, we can confirm that the Oracle Teacher still works well with KD on the Mandarin dataset. Interestingly, when the Oracle Teacher was applied, the distilled student (CER: 9.74 % on dev-iOS) performed similarly to the pre-trained Jasper DR (CER: 9.69 % on dev-iOS), notwithstanding its smaller parameter size (14 M parameters) than Jasper DR (333 M parameters). In some cases, including test-iOS and test-Android, the student distilled from the Oracle teacher outperformed the Jasper DR teacher. When transferring the knowledge from the Oracle Teacher w/o target, the results show that Oracle Teacher w/o target → Jasper Mini performed better than Jasper DR → Jasper Mini. It indicates that, even without the additional target information, the student can benefit from the knowledge of the SourceNet. However, our best performance was achieved when applying the Oracle Teacher as the teacher model.   2) Scene Text Recognition: For the STR task, we used accuracy, the success rate of word predictions per image, as a performance metric. As reported in Table <ref type="table" target="#tab_2">III</ref>, the student distilled from the Oracle Teacher showed better performance than those distilled from other teachers, and its total accuracy (82.21 %) was almost similar to that of the conventional teacher Star-Net (82.24 %) while having much fewer parameters (10 M parameters). On IC13 datasets, the performances of Star-Net → CRNN were slightly better than those of Oracle Teacher → CRNN. However, the differences were negligible since Oracle Teacher → CRNN showed better improvements in most cases, including the total accuracy. Oracle Teacher w/o target → CRNN performed better than Rosetta → CRNN in some cases. It means that, even without using the additional target input, the student can benefit from the knowledge of the SourceNet. However, the distilled student from the Oracle Teacher w/o target had worse achievements than Star-Net → CRNN and Oracle Teacher → CRNN, indicating that the target input played an important role in the effectiveness of the Oracle Teacher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Case Study: The Effect of Target Input</head><p>To validate why the proposed method could result in better KD performance than the conventional teacher, we conducted a case study for ASR on LibriSpeech test-other dataset. By comparing predictions between the conventional teacher and the Oracle Teacher, we verified the effect of using the target information and the behaviour of the Oracle Teacher.</p><p>In Fig. <ref type="figure" target="#fig_4">4</ref>, we visualized the softmax prediction (CTC alignment) of the conventional teacher and the Oracle Teacher. The x-axis refers to acoustic frames, and the y-axis refers to the character labels. As displayed in Fig. <ref type="figure" target="#fig_4">4</ref>, the conventional teacher converted a given speech into "but the king left him to scorn thou a sword equalle" and made erroneous predictions with "left" and "equalle". When conditioning on the speech voice only, it is hard to distinguish "left"/"laughed" and "equalle"/"he quoth". However, the Oracle Teacher gave accurate CTC alignment by utilizing the additional target (text) information, implying that a more optimal problem-solving could be derived by referring to both source (speech) and target (text) information.  Interestingly, as shown in Fig. <ref type="figure" target="#fig_4">4</ref>, most frames of the conventional teacher had the highest probability for the "blank" token. In contrast, the Oracle Teacher had fewer frames that were predicted as "blank" token. This is because the target input y was used as guidance to reduce the candidate scope of the CTC alignment π. By using the additional target (text) information, multiple frames of the Oracle Teacher were more likely to be predicted as non-blank character labels rather than the "blank" during the active speech periods, implying that the representation of the Oracle Teacher could contain much more information about non-blank labels corresponding to the characters (text).</p><p>In addition, we compared the ASR predictions of the conventional teacher and the Oracle Teacher, as shown in Fig. <ref type="figure" target="#fig_5">5</ref>. In Fig. <ref type="figure" target="#fig_5">5</ref>, the conventional teacher made erroneous predictions with "yuowe", "aight", and "a" using the greedy decoding. When considering the acoustic (speech) feature only, it is challenging to distinguish some words, such as "you owe"/"yuowe" and "wait"/"aight". The conventional teacher generated an accurate prediction when decoding with the external KenLM that provided additional linguistic information. However, the proposed Oracle Teacher could produce correct ASR prediction without using the external LM. This is because the Oracle Teacher leveraged both the source input (speech) and the output label (text) as the teacher model's input. Unlike the conventional teacher that only considered acoustic features, the Oracle Teacher performed a fusion of acoustic (speech) and linguistic (text) features when generating the prediction. Since unifying acoustic and linguistic representation learning generally enhances the performance of the speech processing <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b10">[11]</ref>, the Oracle Teacher that considered linguistic information could estimate better CTC prediction, and also its representation was a more supportive knowledge for the ASR student.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Comparison with Other KD Methods</head><p>In the previous results, we applied Fitnets <ref type="bibr" target="#b29">[29]</ref> as the basic KD method. To further validate the effectiveness of the Oracle Teacher, we used other KD methods for performance comparison.</p><p>Firstly, we applied RKD <ref type="bibr" target="#b24">[25]</ref> as the KD method, a recent KD approach in ASR task. It transfers the representation-level knowledge by considering a frame weighting, reflecting which frames were important for KD. In addition to the RKD, we adopted SKD <ref type="bibr" target="#b24">[25]</ref> for KD, which effectively transfers the softmax-level knowledge in the CTC framework. From the results in Table <ref type="table" target="#tab_3">IV</ref>, it is confirmed that Oracle Teacher was more supportive than the conventional Jasper DR teacher in all configurations. Also, we verified that RKD achieved better improvements than other KD methods, including FitNets and SKD. The best performance was achieved when using the Oracle Teacher with the RKD. We can observe the consistent performance gain of the Oracle Teacher over the conventional teacher for various KD methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Computational Cost Comparison</head><p>In addition to the previous experiments, we proceeded to verify the computational efficiency of the proposed teacher model. Computational resource consumption compared to the conventional teacher models are shown in Table <ref type="table" target="#tab_4">V</ref>.</p><p>1) Speech Recognition: Since it is difficult to reproduce the reported WER results of Jasper DR <ref type="bibr" target="#b40">[40]</ref> without a large number of resources, we used the checkpoint for LibriSpeech, provided by the OpenSeq2Seq <ref type="bibr" target="#b62">[62]</ref> toolkit. For LibriSpeech, the pre-trained Jasper DR model required eight 32GB GPUs  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Analysis</head><p>From the previous experimental results, we validate the superiority of the proposed teacher model. Therefore, it is necessary to test if the model has been correctly trained.</p><p>1) Visualization of Cross Attention: We trained another incomplete Oracle Teacher, called Oracle Teacher w/o source. The zero arrays, which had the same size of x, were treated as the input instead of the source input x during the training. Then, the Oracle Teacher w/o source only considered the target input y when making a prediction, similar to the aforementioned trivial solution. In Fig. <ref type="figure" target="#fig_6">6</ref>, we visualize the cross attention scores of the decoder for the ASR task, where  From this additional result, we can verify that the source input x is the necessary factor of the Oracle Teacher, and the proposed Oracle Teacher has been correctly trained.</p><p>3) Performance of Oracle Teacher: We also evaluated the performance of the Oracle Teacher itself compared to the conventional teachers, as shown in Table <ref type="table" target="#tab_5">VI</ref>. If the Oracle Teacher copies the target input y without utilizing the source input x, the performance of the Oracle Teacher should be perfect. We measured WER (%) results on LibriSpeech. The results show that the performance of the Oracle Teacher was more effective than that of the conventional teacher model, which seemed reasonable because the Oracle Teacher was trained with the guidance of y. Meanwhile, the predictions of the Oracle Teacher were not the same as each ground truth. This implies that the Oracle teacher's output did not simply copy the target input y, and the information from a properlytrained SourceNet was utilized to generate the prediction. Compared to the "clean" datasets, the difference of WER was huge in "other" sets. Since the "other" dataset represents a noisy dataset, the conventional ASR model (Jasper DR) showed low performance for the "other" dataset. However, the Oracle Teacher could result in high performance for the noisy dataset since it used text information from the target. This indicates that the Oracle Teacher could be a noisy robust teacher with small parameters. From the results, we can verify that the Oracle Teacher provided more accurate and better guidance to the student than the conventional ASR teacher model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSION</head><p>This work offered a powerful but efficient KD method in the context of CTC-based sequence learning tasks. However, the use of output labels as the additional input for the teacher model could limit its usage on unsupervised data. One possible way to apply the Oracle Teacher in the unsupervised setting is leveraging the prediction as the model's input, like <ref type="bibr" target="#b68">[68]</ref>- <ref type="bibr" target="#b70">[70]</ref>. However, in Section V-B, it is confirmed that the Oracle Teacher gave accurate CTC alignment by utilizing the groundtruth labels as the additional input. Since the target input was used as guidance to reduce the candidate scope of the alignment, using the erroneous prediction as the additional input might significantly degrade the performance of the Oracle Teacher. Extending the proposed framework further, using the Oracle Teacher on unsupervised data is our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>We introduced a novel teacher for CTC-based sequence models, namely Oracle Teacher, that leverages the output labels as the additional input to the model. Through a number of experiments, we confirmed that the student distilled from the Oracle Teacher performed better compared to the one distilled from the conventional teacher. Furthermore, our framework significantly reduced the computational cost of the teacher model in terms of the training time and required GPU resources. As the effective teacher can be trained with a reduced computational cost, the Oracle Teacher can be a new breakthrough in KD. We expect the application of the Oracle Teacher in various tasks, such as regression, ranking, etc., in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the Oracle Teacher. The proposed teacher model mainly consists of three components: the SourceNet, the encoder, and the decoder.Different from the conventional teacher, the target y is used as the additional input to the model. Note that the Oracle Teacher is a non-autoregressive model where the look-ahead mask is not included in the decoder. The architecture selection of the SourceNet depends on the task we are interested in. When the main task is ASR. the SourceNet corresponds to an acoustic model part of the conventional ASR model. In our experiment for ASR, the SourceNet is based on the architecture of Japser<ref type="bibr" target="#b40">[40]</ref>. For STR, we apply the CRNN<ref type="bibr" target="#b41">[41]</ref> as the SourceNet.</figDesc><graphic coords="3,82.38,56.07,184.22,283.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The relationship between the CTC alignment π and the target input y. A many-to-one mapping function B converts the alignment sequence π into the final output sequence y.</figDesc><graphic coords="3,117.88,456.46,113.22,87.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>log P (y|x; φ) = log π P (y, π|x; φ) dπ = log π P (y, π|x; φ) P (π|x, y; θ) P (π|x, y; θ) dπ = log π δ(y − B(π))P (π|x; φ) P (π|x, y; θ) P (π|x, y; θ) dπ = log π∈B −1 (y) P (π|x, y; θ) P (π|x; φ) P (π|x, y; θ) dπ ≥ π∈B −1 (y) P (π|x, y; θ) log P (π|x; φ) P (π|x, y; θ) dπ = −D KL (P (π|x, y; θ) Oracle Teacher P (π|x; φ) Student )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. KD procedure with the Oracle Teacher.</figDesc><graphic coords="4,67.68,56.07,213.63,133.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Frame-wise label probability example on LibriSpeech test-clean dataset. Conventional teacher denotes the Jasper DR model. The x-axis refers to acoustic frames, and the y-axis refers to the character labels. The last label index represents the "blank" label in the CTC framework.</figDesc><graphic coords="7,62.73,56.08,223.53,283.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Recognition example on LibriSpeech test-clean dataset.</figDesc><graphic coords="7,55.45,399.88,238.08,56.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Visualization of the attention weights of the Oracle Teacher with and without the source input</figDesc><graphic coords="8,58.58,263.59,228.35,62.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>the x-axis refers to acoustic frames and the y-axis refers to characters. For the Oracle Teacher w/o source, the attention had almost diagonal alignment along with the key position (text) while ignoring the length of query, as shown in Fig.6(b). In contrast, the Oracle Teacher considered both speech and text alignment in the cross attention, and the attention scores were correctly computed along with the acoustic frames (query), as shown in Fig.6(a). It means that the Oracle Teacher utilized the source x for model training while preventing the trivial solution. Therefore, we can confirm that the Oracle Teacher, including SourceNet, has been correctly trained.2) KD with Oracle Teacher w/o source: In addition to the previous experiments, we proceeded to train the student model with the knowledge of the Oracle Teacher w/o source. However, as we expected, the distilled student failed to converge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I WER</head><label>I</label><figDesc>(%) PERFORMANCE COMPARISON ACROSS CTC-BASED ASR MODELS ON LIBRISPEECH. THE BEST RESULT OF THE STUDENT IS IN BOLD.</figDesc><table><row><cell>ASR baseline model</cell><cell>Params.</cell><cell>clean</cell><cell>dev</cell><cell cols="2">w/o LM other clean</cell><cell>test</cell><cell>other</cell><cell>clean</cell><cell>dev</cell><cell cols="2">w/ LM other clean</cell><cell>test</cell><cell>other</cell></row><row><cell>Jasper DR [40]</cell><cell>333 M</cell><cell>3.61</cell><cell></cell><cell>11.37</cell><cell>3.77</cell><cell></cell><cell>11.08</cell><cell>2.99</cell><cell></cell><cell>9.40</cell><cell>3.62</cell><cell>9.33</cell></row><row><cell>Jasper Mini</cell><cell>8 M</cell><cell>8.66</cell><cell></cell><cell>23.28</cell><cell>8.85</cell><cell></cell><cell>24.26</cell><cell>4.78</cell><cell></cell><cell>15.14</cell><cell>5.15</cell><cell>15.77</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">w/o LM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">w/ LM</cell></row><row><cell>Student</cell><cell>Teacher</cell><cell></cell><cell>dev</cell><cell></cell><cell></cell><cell>test</cell><cell></cell><cell></cell><cell>dev</cell><cell></cell><cell></cell><cell>test</cell></row><row><cell></cell><cell></cell><cell>clean</cell><cell></cell><cell>other</cell><cell>clean</cell><cell></cell><cell>other</cell><cell>clean</cell><cell></cell><cell>other</cell><cell>clean</cell><cell>other</cell></row><row><cell></cell><cell>None</cell><cell>8.66</cell><cell></cell><cell>23.28</cell><cell>8.85</cell><cell></cell><cell>24.26</cell><cell>4.78</cell><cell></cell><cell>15.14</cell><cell>5.15</cell><cell>15.77</cell></row><row><cell>Jasper Mini</cell><cell>Jasper DR [40] Oracle Teacher (ours)</cell><cell>7.05 6.64</cell><cell></cell><cell>19.41 18.91</cell><cell>7.03 6.67</cell><cell></cell><cell>20.41 19.82</cell><cell>4.80 4.65</cell><cell></cell><cell>14.32 14.31</cell><cell>5.00 4.90</cell><cell>14.99 14.65</cell></row><row><cell></cell><cell>Oracle Teacher w/o target</cell><cell>7.22</cell><cell></cell><cell>20.39</cell><cell>7.32</cell><cell></cell><cell>21.10</cell><cell>4.72</cell><cell></cell><cell>14.67</cell><cell>4.91</cell><cell>15.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II CER</head><label>II</label><figDesc>(%) ON AISHELL-2 WHEN GREEDY DECODING WAS APPLIED. THE BEST RESULT OF THE STUDENT IS IN BOLD.</figDesc><table><row><cell>ASR baseline model</cell><cell>Params.</cell><cell>iOS</cell><cell>dev Android</cell><cell>Mic</cell><cell>iOS</cell><cell>test Android</cell><cell>Mic</cell></row><row><cell>Jasper DR [40]</cell><cell>338 M</cell><cell>9.69</cell><cell>11.48</cell><cell>12.23</cell><cell>9.37</cell><cell>10.84</cell><cell>11.84</cell></row><row><cell>Jasper Mini</cell><cell>14 M</cell><cell>11.77</cell><cell>14.23</cell><cell>15.03</cell><cell>11.38</cell><cell>12.71</cell><cell>14.27</cell></row><row><cell>Student</cell><cell>Teacher</cell><cell>iOS</cell><cell>dev Android</cell><cell>Mic</cell><cell>iOS</cell><cell>test Android</cell><cell>Mic</cell></row><row><cell></cell><cell>None</cell><cell>11.77</cell><cell>14.23</cell><cell>15.03</cell><cell>11.38</cell><cell>12.71</cell><cell>14.27</cell></row><row><cell>Jasper Mini</cell><cell>Jasper DR [40] Oracle Teacher (ours)</cell><cell>10.70 9.74</cell><cell>12.78 11.49</cell><cell>13.66 12.31</cell><cell>10.12 9.27</cell><cell>11.31 10.36</cell><cell>12.60 11.99</cell></row><row><cell></cell><cell>Oracle Teacher w/o target</cell><cell>10.45</cell><cell>12.42</cell><cell>13.13</cell><cell>9.76</cell><cell>10.92</cell><cell>12.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III PERFORMANCE</head><label>III</label><figDesc>OF CTC-BASED STR MODELS. THE BEST RESULT OF THE STUDENT IS IN BOLD.</figDesc><table><row><cell>STR baseline model</cell><cell>Params.</cell><cell>IIIT 3000</cell><cell>SVT</cell><cell>IC03 860</cell><cell>IC13 857</cell><cell>IC13 1015</cell><cell>IC15 1811</cell><cell>IC15 2077</cell><cell>SVTP</cell><cell>CT</cell><cell>Total accuracy</cell></row><row><cell>Rosetta [58]</cell><cell>46 M</cell><cell>85.53</cell><cell>84.85</cell><cell>94.19</cell><cell>91.95</cell><cell>90.74</cell><cell>73.22</cell><cell>70.55</cell><cell>76.12</cell><cell>68.99</cell><cell>82.45</cell></row><row><cell>Star-Net [59]</cell><cell>49 M</cell><cell>85.50</cell><cell>85.47</cell><cell>93.84</cell><cell>92.77</cell><cell>91.92</cell><cell>72.50</cell><cell>69.77</cell><cell>73.80</cell><cell>70.38</cell><cell>82.24</cell></row><row><cell>CRNN [41]</cell><cell>10 M</cell><cell>83.87</cell><cell>80.37</cell><cell>93.02</cell><cell>90.43</cell><cell>89.46</cell><cell>70.07</cell><cell>67.53</cell><cell>72.09</cell><cell>65.51</cell><cell>80.10</cell></row><row><cell>Student</cell><cell>Teacher</cell><cell>IIIT 3000</cell><cell>SVT</cell><cell>IC03 860</cell><cell>IC13 857</cell><cell>IC13 1015</cell><cell>IC15 1811</cell><cell>IC15 2077</cell><cell>SVTP</cell><cell>CT</cell><cell>Total accuracy</cell></row><row><cell></cell><cell>None</cell><cell>83.87</cell><cell>80.37</cell><cell>93.02</cell><cell>90.43</cell><cell>89.46</cell><cell>70.07</cell><cell>67.53</cell><cell>72.09</cell><cell>65.51</cell><cell>80.10</cell></row><row><cell></cell><cell>Rosetta [58]</cell><cell>84.70</cell><cell>83.46</cell><cell>92.91</cell><cell>91.02</cell><cell>90.15</cell><cell>71.89</cell><cell>69.20</cell><cell>71.16</cell><cell>65.85</cell><cell>81.04</cell></row><row><cell>CRNN [41]</cell><cell>Star-Net [59]</cell><cell>85.20</cell><cell>84.39</cell><cell>93.49</cell><cell>91.60</cell><cell>90.74</cell><cell>72.45</cell><cell>69.77</cell><cell>72.25</cell><cell>70.04</cell><cell>81.77</cell></row><row><cell></cell><cell>Oracle Teacher (ours)</cell><cell>85.77</cell><cell>84.54</cell><cell>93.61</cell><cell>91.48</cell><cell>90.54</cell><cell>73.11</cell><cell>70.40</cell><cell>74.26</cell><cell>70.38</cell><cell>82.21</cell></row><row><cell></cell><cell>Oracle Teacher w/o target</cell><cell>85.40</cell><cell>82.84</cell><cell>93.02</cell><cell>90.78</cell><cell>89.75</cell><cell>71.73</cell><cell>69.04</cell><cell>72.71</cell><cell>68.99</cell><cell>81.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV WER</head><label>IV</label><figDesc>(%) PERFORMANCE COMPARISON ACROSS CTC-BASED ASR MODELS ON LIBRISPEECH. THE BEST RESULT OF THE STUDENT IS IN BOLD.</figDesc><table><row><cell>Teacher</cell><cell>Student</cell><cell>KD method</cell><cell>clean</cell><cell>dev</cell><cell>other</cell><cell>clean</cell><cell>test</cell><cell>other</cell></row><row><cell>None</cell><cell></cell><cell>None</cell><cell>8.66</cell><cell></cell><cell>23.28</cell><cell>8.85</cell><cell></cell><cell>24.26</cell></row><row><cell>Jasper DR Oracle Teacher Jasper DR Oracle Teacher</cell><cell>Jasper Mini</cell><cell>RKD [25] SKD [25]</cell><cell>6.74 6.44 7.64 7.57</cell><cell></cell><cell>19.27 18.36 21.36 21.20</cell><cell>6.77 6.43 7.81 7.71</cell><cell></cell><cell>19.78 18.97 22.41 21.71</cell></row><row><cell>Jasper DR Oracle Teacher</cell><cell></cell><cell>FitNets [29]</cell><cell>7.05 6.64</cell><cell></cell><cell>19.41 18.91</cell><cell>7.03 6.67</cell><cell></cell><cell>20.41 19.82</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V COMPUTATIONAL</head><label>V</label><figDesc>RESOURCE CONSUMPTION COMPARISON ACROSS TEACHER MODELS.</figDesc><table><row><cell>Task</cell><cell>Training dataset</cell><cell>Teacher model</cell><cell>Params.</cell><cell>GPU</cell><cell>Batch</cell><cell>Times</cell><cell>Epochs</cell></row><row><cell>ASR</cell><cell>LibriSpeech</cell><cell>Jasper DR [40] Oracle Teacher (ours)</cell><cell>333 M 33 M</cell><cell>8 * 32GB 1 * 12GB</cell><cell>256 64</cell><cell>-22 h</cell><cell>400 epochs 30 epochs</cell></row><row><cell>ASR</cell><cell>AISHELL-2</cell><cell>Jasper DR [40] Oracle Teacher (ours)</cell><cell>338 M 34 M</cell><cell>8 * 32GB 1 * 12GB</cell><cell>128 64</cell><cell>-118 h</cell><cell>50 epochs 30 epochs</cell></row><row><cell></cell><cell></cell><cell>Star-Net [59]</cell><cell>49 M</cell><cell>1 * 12GB</cell><cell>192</cell><cell>27 h</cell><cell>300k iter.</cell></row><row><cell>STR</cell><cell>MJSynth + SynthText</cell><cell>Rosetta [58]</cell><cell>46 M</cell><cell>1 * 12GB</cell><cell>192</cell><cell>27 h</cell><cell>300k iter.</cell></row><row><cell></cell><cell></cell><cell>Oracle Teacher (ours)</cell><cell>12 M</cell><cell>1 * 12GB</cell><cell>192</cell><cell>10 h</cell><cell>300k iter.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI PERFORMANCE</head><label>VI</label><figDesc>AND PARAMETER COMPARISON BETWEEN THE ORACLE TEACHER AND THE CONVENTIONAL TEACHER.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">WER (%)</cell><cell></cell></row><row><cell>Task</cell><cell>Model</cell><cell>Param.</cell><cell>dev</cell><cell></cell><cell>test</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>clean</cell><cell>other</cell><cell>clean</cell><cell>other</cell></row><row><cell>ASR</cell><cell>Jasper DR [40] Oracle Teacher</cell><cell>333 M 33 M</cell><cell>3.61 2.87</cell><cell>11.37 3.10</cell><cell>3.77 4.03</cell><cell>11.08 3.29</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We applied the datasets used in the comparative study conducted by Baek et al.<ref type="bibr" target="#b48">[48]</ref>.2 https://github.com/clovaai/deep-text-recognition-benchmark</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep compression: compressing deep neural network with pruning, trained quantization and huffman coding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pruning filters for efficient convnets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quantized convolutional neural networks for mobile devices</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4820" to="4828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bucila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD</title>
				<meeting>ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS Workshop Deep Learn</title>
				<meeting>NIPS Workshop Deep Learn</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wav-bert: cooperative acoustic and linguistic representation learning for low-resource speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Leveraging acoustic and linguistic embeddings from pretrained speech and language models for intent classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficiently fusing pretrained acoustic and linguistic encoders for low-resource speech recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Non-autoregressive transformer-based end-to-end asr using bert</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1474" to="1482" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adapt-and-adjust: overcoming the long-tail problem of multilingual speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Winata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.01687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2654" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning small-size dnn with output-distribution-based criteria</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
				<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sequence student-teacher training of deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H M</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
				<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2761" to="2765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Blending lstms into cnns</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Geras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR Workshop</title>
				<meeting>ICLR Workshop</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distilling knowledge from ensembles of neural networks for speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
				<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3439" to="3443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Student-teacher network learning with enhanced features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5275" to="5279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowledge distillation for small-footprint highway networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4820" to="4824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient knowledge distillation from an ensemble of teachers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fukuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
				<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3697" to="3701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Acoustic modelling with cd-ctc-smbr lstm rnns</title>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Quitry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
				<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="604" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An investigation of a knowledge distillation method for ctc acoustic models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Takashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kawai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5809" to="5813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Investigation of sequence-level knowledge distillation methods for ctc acoustic models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Takashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kawai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6156" to="6160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improved knowledge distillation from bidirectional to uni-directional lstm ctc for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT</title>
				<meeting>SLT</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="411" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Guiding ctc posterior spike timings for improved posterior fusion and knowledge distillation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
				<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1616" to="1620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Tutornet: towards flexible knowledge distillation for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">I</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1626" to="1638" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Compression of end-to-end models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
				<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Knowledge distillation using output errors for self-attention end-to-end models</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adaptive knowledge distillation based on entropy</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fitnets: hints for thin deep nets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Paying more attention to attention: improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: fast optimization, network minimization and transfer learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Knowledge transfer with jacobian matching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Paraphrasing complex network: network compression via factor transfer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2760" to="2769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Knowledge transfer via distillation of activation boundaries formed by hidden neurons</title>
		<author>
			<persName><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3779" to="3787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3967" to="3976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Show, attend and distill: knowledge distillation via attention-based feature matching</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7945" to="7952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distilhubert: speech representation learning by layer-wise distillation of hidden-unit bert</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Lighthubert: lightweight and configurable speech representation learning with once-for-all hidden-unit bert</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Otheres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
				<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fithubert: going thinner and deeper for knowledge distillation of speech self-supervised learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
				<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Jasper: An end-to-end convolutional neural acoustic model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Gadde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
				<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for imagebased sequence recognition and its application to scene text recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Integer-only zero-shot quantization for efficient speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Correction of automatic speech recognition with transformer sequence-to-sequence model</title>
		<author>
			<persName><forename type="first">O</forename><surname>Hrinchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Popova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cacnet: cube attentional cnn for automatic speech recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN</title>
				<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Transformer-xl: attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">What is wrong with scene text recognition model comparisons?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">End-to-end scenetext recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Recognizing text with perspective distortion in natural scenes</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Phan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Scene text recognition using higher order language priors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
				<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A robust arbitrary text detection system for natural scene images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Risnumawan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="8027" to="8048" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Icdar 2003 robust reading competitions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR</title>
				<meeting>ICDAR</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="682" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Icdar 2013 robust reading competition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR</title>
				<meeting>ICDAR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR</title>
				<meeting>ICDAR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Rosetta: large scale system for text detection and recognition in images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Borisyuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD</title>
				<meeting>ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="71" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Star-net: A spatial attention residue network for scene text recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
				<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Textocr: towards large-scale end-to-end reasoning for arbitrary-shaped scene text</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Galuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8802" to="8812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">What machines see is not what they get: fooling scene text recognition models with adversarial text images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12304" to="12314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Mixed-precision training for nlp and speech recognition with openseq2seq</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10387</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Stochastic gradient methods with layer-wise adaptive moments for training of deep networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Castonguay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hrinchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11286</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Kenlm: faster and smaller language model queries</title>
		<author>
			<persName><forename type="first">K</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09577</idno>
		<title level="m">a toolkit for building ai applications using neural modules</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701v1</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Kriman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Beliaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10261</idno>
		<title level="m">Quartznet: deep automatic speech recognition with 1d time-channel separable convolutions</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Deliberation networks: sequence generation deyond onepass decoding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Imputer: sequence modelling via imputation and dynamic programming</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Mask ctc: non-autoregressive end-to-end asr with ctc and mask predict</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Higuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
				<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
