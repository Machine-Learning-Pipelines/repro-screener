<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalars are universal: Equivariant machine learning, structured like classical physics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-21">21 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Soledad</forename><surname>Villar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">W</forename><surname>Hogg</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kate</forename><surname>Storey-Fisher</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Weichi</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Blum-Smith</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Applied Mathematics and Statistics</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Center for Cosmology and Particle Physics Department of Physics</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Technology, Operations, and Statistics Stern School of Business</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Scalars are universal: Equivariant machine learning, structured like classical physics</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-21">21 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">5E8203B2815260D1C2EC3B256313BFE1</idno>
					<idno type="arXiv">arXiv:2106.06610v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-10-31T05:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There has been enormous progress in the last few years in designing neural networks that respect the fundamental symmetries and coordinate freedoms of physical law. Some of these frameworks make use of irreducible representations, some make use of high-order tensor objects, and some apply symmetry-enforcing constraints. Different physical laws obey different combinations of fundamental symmetries, but a large fraction (possibly all) of classical physics is equivariant to translation, rotation, reflection (parity), boost (relativity), and permutations. Here we show that it is simple to parameterize universally approximating polynomial functions that are equivariant under these symmetries, or under the Euclidean, Lorentz, and Poincaré groups, at any dimensionality d. The key observation is that nonlinear O(d)-equivariant (and related-group-equivariant) functions can be universally expressed in terms of a lightweight collection of scalars-scalar products and scalar contractions of the scalar, vector, and tensor inputs. We complement our theory with numerical examples that show that the scalar-based method is simple, efficient, and scalable.</p><p>35th Conference on Neural Information Processing Systems (NeurIPS 2021).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There is a great deal of current interest in building machine-learning methods that respect exact or approximate symmetries, such as translation, rotation, and physical gauge symmetries <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b49">50]</ref>. Some of this interest is inspired by the great success of convolutional neural networks (CNNs) <ref type="bibr" target="#b51">[52]</ref>, which are naturally translation equivariant. The implementation of convolutional layers in CNNs has been given significant credit for the success of deep learning, in a domain (natural images) in which the convolutional symmetry is only approximately valid. In many data-analysis problems in astronomy, physics, and chemistry there are exact symmetries that must be obeyed by any generalizable law or rule. Since the approximate symmetries introduced by convolutional networks help in the natural-image domain, then we have high hopes for the value of encoding exact symmetries for problems where these symmetries are known to hold exactly.</p><p>In detail, the symmetries of physics are legion. Translation symmetry (including conservation of linear momentum), rotation symmetry (including conservation of angular momentum), and time-translation symmetry (including conservation of energy) are the famous symmetries <ref type="bibr" target="#b69">[70]</ref>. But there are many more: there is a form of reflection symmetry (charge-parity-time or CPT); there is a symplectic symmetry that permits reinterpretations of positions and momenta; there are Lorentz and Poincaré symmetries (the fundamental symmetries of Galilean relativity and special relativity) that include velocity boosts; there is the generalization of these (general covariance) that applies in curved spacetime; there are symmetries associated with baryon number, lepton number, flavor, and color; and there are dimensional and units symmetries (not usually listed as symmetries, but they are) that restrict what kinds of quantities can be multiplied or added. If it were possible to parameterize a universal or universally approximating function space that is explicitly equivariant under a large set of non-trivial symmetries, that function space would-in some sense-contain within it every possible law of physics. It would also provide a basis for good new machine learning methods.</p><p>The most expressive approaches to equivariant machine learning make use of irreducible representations of the relevant symmetry groups. Implementing these approaches requires a way to explicitly decompose tensor products of known representations into their irreducible components. This is called the Clebsch-Gordan problem. The solution for SO <ref type="bibr" target="#b2">(3)</ref> has been implemented, and there is recent exciting progress for other Lie groups <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b40">41]</ref>. This is an area of current, active research.</p><p>Here we give an approach that bypasses the need for a solution to this problem. We find that, for a large class of problems relevant to classical physics, the space of equivariant functions can be constructed from functions only of a complete subset of the scalar products and scalar contractions of the input vectors and tensors. That is, invariant scalars are powerful objects, and point the way to building universally approximating functions that are constrained by exact, non-trivial symmetries.</p><p>Equivariance is the form of symmetry core to physical law: For an equivariant function, transforming the input results in an output representation transformed in the same way. An invariant function, on the other hand, produces the same output for transformed and non-transformed inputs.</p><p>Definition 1: Given a function f : X → Y and a group G acting on X and in Y as (possibly the action is defined differently in X and Y). We say that f is:</p><formula xml:id="formula_0">G-invariant: f (g x) = f (x) for all x ∈ X , g ∈ G ;<label>(1)</label></formula><p>G-equivariant: f (g x) = g f (x) for all x ∈ X , g ∈ G .</p><p>(</p><formula xml:id="formula_1">)<label>2</label></formula><p>Our contributions: In this work we provide a complete and computationally tractable characterization of all scalar functions f : (R d ) n → R, and of all vector functions h : (R d ) n → R d that satisfy all of the symmetries of classical physics. The groups corresponding to these symmetries are given in Table <ref type="table" target="#tab_0">1</ref>; they act according to the rules in Table <ref type="table">2</ref>. The characterization we provide is physically principled: It is based on invariant scalars. It is also connected to the symmetries encoded in the Einstein summation rules, a common notation in physics to write expressions compactly but that also allows only equivariant objects to be produced (see Appendix F).</p><p>Our characterization is based on simple mathematical observations. The first is the First Fundamental Theorem of Invariant Theory for O(d): a function of vector inputs returns an invariant scalar if and only if it can be written as a function only of the invariant scalar products of the input vectors [94, Section II.A.9]. There is a similar statement for the Lorentz group O <ref type="bibr">(1, d)</ref>. The second observation is that a function of vector inputs returns an equivariant vector if and only if it can be written as a linear combination of invariant scalar functions times the input vectors. In particular, if h</p><formula xml:id="formula_2">: (R d ) n → R d of inputs v 1 , . . , v n is O(d) or O(1, d)</formula><p>-equivariant, then it can be expressed as:</p><formula xml:id="formula_3">h(v 1 , v 2 , . . , v n ) = n t=1 f t v i , v j n i,j=1 v t ,<label>(3)</label></formula><p>where f t can be arbitrary functions, but if h is a polynomial function the f t can be chosen to be polynomials. In other words, the O(d) and O(1, d)-equivariant vector functions are generated as a module over the ring of invariant scalar functions by the projections to each input vector. In this expression, •, • denotes the invariant scalar product, which can be the usual Euclidean inner product, or the Minkowski inner product defined in terms of a metric Λ (see Table <ref type="table" target="#tab_0">1</ref>):</p><formula xml:id="formula_4">Euclidean: v i , v j = v i v j , Minkowski: v i , v j = v i Λ v j .<label>(4)</label></formula><p>Our mathematical observations lead to a simple characterization for a very general class of equivariant functions, simpler than any based on irreducible representations or the imposition of symmetries </p><formula xml:id="formula_5">Orthogonal O(d) = {Q ∈ R d×d : Q Q = Q Q = I d }, Rotation SO(d) = {Q ∈ R d×d : Q Q = Q Q = I d , det(Q) = 1} Translation T(d) = {w ∈ R d } Euclidean E(d) = T(d) O(d) Lorentz O(1, d) = {Q ∈ R (d+1)×(d+1) : Q Λ Q = Λ, Λ = diag([1, −1, . . , −1])} Poincaré IO(1, d) = T(d + 1) O(1, d) Permutation S n = {σ : [n] → [n] bijective function}</formula><formula xml:id="formula_6">Orthogonal; Lorentz Q (v 1 , . . , v n ) = (Q v 1 , . . , Q v n ) Translation w (v 1 , . . , v n ) = (v 1 + w, . . , v k + w, v k+1 , . . , v n ) (where the first k vectors are position vectors) Euclidean; Poincaré (w, Q) (v 1 , . . , v n ) = (Q v 1 + w, . . , Q v k + w, Q v k+1 , . . , Q v n ) Permutation σ (v 1 , . . , v n ) = (v σ(1) , . . , v σ(n) )</formula><p>Table <ref type="table">2</ref>: The actions of the groups on vectors. For the Euclidean group, the position vectors are positions of points; for the Poincaré group, the position vectors are positions of events.</p><p>through constraints (these methods are currently state of the art; see Section 2). This implies that very simple neural networks based on scalar products of input vectors-that enormously generalize those in <ref type="bibr" target="#b80">[81]</ref>-can universally approximate invariant and equivariant functions. This justifies the numerical success of the neural network model proposed in <ref type="bibr" target="#b80">[81]</ref>, and mathematically shows that their method can be extended to a universal equivariant architecture with respect to more general group actions. We note that the formulation in (3) might superficially resemble an attention mechanism <ref type="bibr" target="#b87">[88]</ref>, but it actually comes from the characterization of invariant functions of the group, in particular the picture for SO(d)-invariant functions is a little different (see <ref type="bibr">Proposition 5)</ref>.</p><p>In Section 2 (and Appendix A) we describe the state of the art for encoding symmetries, the expressive power of graph neural networks, and universal approximation of invariant and equivariant functions. In Section 3 we mathematically characterize the invariant and equivariant functions with respect to the groups in Table <ref type="table" target="#tab_0">1</ref>. In Section 4 we present some examples of physically meaningful equivariant functions, and show how to express them in the parameterization developed in Section 3.</p><p>In Section 5 and Appendix G we discuss which (of all possible) pairwise inner products one ought to provide, and in Section 6 we discuss some limitations of our approach. We present numerical experiments using our scalar-based approach compared to other methods in Section 7 (see also <ref type="bibr" target="#b96">[97]</ref>).</p><p>We also note that the symmetries considered in this work are all global symmetries, as they act on all points in the same way. Our characterization thus does not obviously generalize to all gauge symmetries, which are local symmetries that apply changes independently to points at different locations (see <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b92">93]</ref>). That said, we believe our model could be made general enough to encompass gauge symmetries if we replace the global metric by any position-dependent metric Λ x . In this case, spatially separated vectors would need to be propagated to the same point such that they can be input to locally invariant functions, and this can be done with parallel transport. The parallel transport operations would also have to obey our invariance characterization, but we believe this is possible to do; we will explore it in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Group invariant and equivariant neural networks: Symmetries have been used successfully for learning functions on images, sets, point clouds, and graphs. Neural networks can be designed to parameterize classes of functions satisfying different forms of symmetries, from the classical (approximately) translation-invariant convolutional neural networks <ref type="bibr" target="#b52">[53]</ref>-as well as new approaches that enforce additional symmetries (rotation, scale) on these networks <ref type="bibr" target="#b90">[91,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b85">86]</ref>-to more recent architectures that define permutation invariant and equivariant functions on point clouds pioneered by deep sets and pointnets <ref type="bibr" target="#b99">[100,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75]</ref>, to permutation-equivariant functions on graphs expressed as graph neural networks <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>For instance, deep sets and pointnets parameterize functions on (R d ) n that are invariant or equivariant with respect to the group of permutations S n acting as in Table <ref type="table">2</ref>. Invariant theory shows that all invariant and equivariant functions with respect to such actions can be approximated by easily characterized invariant polynomials <ref type="bibr" target="#b93">[94]</ref>. However, the permutation group can act in significantly more complicated ways. For instance, graph neural networks are equivariant with respect to a different action by permutations (conjugation) that is much harder to characterize (see Appendix A).</p><p>Since being introduced by <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b99">100]</ref>, deep learning on point clouds has been extremely fruitful, especially in computer vision <ref type="bibr" target="#b36">[37]</ref>. Recently, new symmetries and invariances have been incorporated into the design of neural networks on point clouds; especially invariances and equivariances with respect to rigid motions such as translations and rotations. Many architectures have been proposed to satisfy those symmetries such as <ref type="bibr" target="#b85">[86]</ref> based on irreducible representations (irreps), <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b100">101]</ref> based on convolutions, <ref type="bibr" target="#b29">[30]</ref> employing spherical harmonics and irreps, <ref type="bibr" target="#b101">[102]</ref> using quaternions, and <ref type="bibr" target="#b28">[29]</ref> applying a set of constraints to satisfy the symmetries. Most of the implementations of the approaches mentioned above (except for <ref type="bibr" target="#b28">[29]</ref>) are limited to 2D or 3D point clouds. We provide an overview of the main approaches below and in Appendix A.</p><p>Recently <ref type="bibr" target="#b75">[76]</ref> developed an approach to enforcing permutation equivariance in neural network layers using parameter sharing, which can then model other symmetry groups. The weight sharing approach is significantly simpler to implement than the ones described above, and it has been proven to be very successful in practice, with several applications <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b90">91]</ref>, including autonomous driving <ref type="bibr" target="#b38">[39]</ref>.</p><p>Characterizing the space of the functions this approach can express is an interesting open problem.</p><p>Universal approximation via linear invariant layers and irreducible representations: Universally approximating invariant functions can be obtained by taking universal non-invariant functions and averaging them over the group orbits <ref type="bibr" target="#b97">[98,</ref><ref type="bibr" target="#b68">69]</ref>. However, this approach is not practical for large groups like S n or infinite groups like O(d). A classical result in neural networks shows that feed-forward networks with non-polynomial activations can universally approximate continuous functions <ref type="bibr" target="#b54">[55]</ref>. This arguably inspired the use of neural networks that are the composition of linear invariant or equivariant layers with compatible non-linear activation functions to create expressive equivariant models <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b65">66]</ref>. Linear G-equivariant functions can be written in terms of the irreducible representations of G (we explain this and refer the reader to related literature in Appendix A). However, the explicit parameterization of the linear maps is only known for a few groups (for instance, SO(3)-equivariant linear maps are parameterized using the Clebsh-Gordan coefficients <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b8">9]</ref>). Moreover, very recent work <ref type="bibr" target="#b24">[25]</ref> shows that the classes of functions defined in terms of neural networks over irreducible representations in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b85">86]</ref> are universal. In particular, every continous SO(3)-equivariant function can be approximated uniformly in compacts sets by those neural networks. Despite universality, there is a limitation to this approach: Although decompositions into irreps are broadly studied in mathematics (also as plethysms), the explicit transformation to the irreps is not known or possible for general groups. This is in fact an area of current, active research, where there has been recent exciting progress for other Lie groups <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b40">41]</ref>, but the implementation is still limited.</p><p>Expressive power of (non-universal) neural networks on graphs: Graph neural networks express functions on graphs that are equivariant with respect to a certain action by permutations. However, the architectures that are used in practice, typically based on message passing <ref type="bibr" target="#b30">[31]</ref> or graph convolutions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b20">21]</ref>, are not universal in general. Implementing a universally approximating graph neural network using the formulation from the previous section would be prohibitively expensive. There is work characterizing the expressive power of message passing neural networks, mainly in terms of the graph isomorphism problem <ref type="bibr" target="#b94">[95,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b14">15]</ref>, and there is research on the design of graph networks that are expressive enough to perform specific tasks, like solving specific combinatorial optimization problems <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Machine learning for physics with symmetry preservation: Machine learning has been applied extensively to problems in physics. While many of these problems require certain symmetriesin many cases, exact symmetries-most applied projects to date rely on the data to encode the symmetry and hope that the model learns it. For instance, CNNs are commonly used to classify galaxy images; data augmentation is used to teach the model rotational symmetry <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32]</ref>. One well-known example is the Kaggle Galaxy Challenge, a classification competition based on the Galaxy Zoo project <ref type="bibr" target="#b58">[59]</ref>; the winning model improved performance by concatenating features from transformed images of each galaxy before further training <ref type="bibr" target="#b21">[22]</ref>.</p><p>There have been recent successes in enforcing physical symmetries in the architecture of the models themselves <ref type="bibr" target="#b98">[99]</ref>, for instance, in weather and climate modeling <ref type="bibr" target="#b45">[46]</ref> and in modeling chaotic dynamical systems such as turbulence <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b90">91]</ref>. In quantum many-body physics, recent work has shown that the symmetries of quantum energy levels on lattices can be enforced with gauge equivariant and invariant neural networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b61">62]</ref>. There is significant work on imposing permutation symmetry in jet assignment for high-energy particle collider experiments with selfattention networks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b53">54]</ref>. In molecular dynamics, rotationally invariant neural networks have been shown to better learn molecular properties <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b82">83]</ref>, and Hamiltonian neural networks have been constructed to better preserve molecular conformations <ref type="bibr" target="#b55">[56]</ref>. More broadly, Hamiltonian networks have been shown to improve physical characteristics, such as better conservation of energy, and to better generalize <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b102">103]</ref>, and Lagrangian neural networks can also enforce conservation laws <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Invariant theory as a basis for enforcing symmetry in neural networks: We are aware of two lines of prior work that develop approaches related to the one taken here: <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref> and <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>In <ref type="bibr" target="#b56">[57]</ref> certain tasks in turbulence modeling and materials science are considered, which have builtin O( <ref type="formula" target="#formula_3">3</ref>) and (in the latter case) octahedral symmetry. Each of these problems involves a specific representation of the given symmetry group, for which an explicit generating set for the invariant algebra (there called an "integrity basis") is known. The authors construct and test models that learn an invariant function, built on these generating sets. In <ref type="bibr" target="#b57">[58]</ref>, the turbulence example is taken up again, this time with the goal of learning an equivariant 2-tensor.</p><p>The idea of using the invariant algebra to enforce physical symmetries is also contemplated in <ref type="bibr" target="#b35">[36]</ref>.</p><p>In this work, the authors are focused on developing the underlying invariant theory in the case of simultaneous Lorentz and permutation invariance. In the followup work <ref type="bibr" target="#b37">[38]</ref>, this idea is developed into three models, using different generating sets, which are tested against each other.</p><p>The present work shares with these works the idea of using invariant-theoretic descriptions of invariant functions to hard-code physical symmetries into a neural network that models a physical system. We work in somewhat greater generality. While <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref> focus on specific (small) representations of O(3) and finite extensions, and while <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38]</ref> are focused on invariant functions and restricted to linearly reductive groups, we consider both invariant and equivariant functions for almost all groups relevant to physics, including groups (Euclidean and Poincaré) that are not reductive, although their invariant theory remains under control.</p><p>Inductive bias benefits of incorporating symmetries: The value of incorporating exact symmetries in machine learning has been recently established empirically in several applications (see for instance <ref type="bibr" target="#b90">[91]</ref>). Mathematical theory has been developed to explain how much one can improve in terms of sample complexity and generalization <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b26">27]</ref> but many questions remain open.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Equivariant maps</head><p>In this Section we provide a simple characterization of all invariant and equivariant functions with respect to the actions in Table <ref type="table">2</ref> by the groups in Table <ref type="table" target="#tab_0">1</ref>. The proof technique involves the characterization of equivariant maps from knowledge of the invariants, and it is explained in more generality in <ref type="bibr" target="#b7">[8]</ref>. In what follows, v 1 , v 2 , . . , v n will be vectors in R d , G will be a group acting in R d and (R d ) n as in Table <ref type="table">2</ref>, f : (R d ) n → R will be an invariant function with respect to the action, and h : (R d ) n → R d will be an equivariant function with respect to the same action. V will denote a d × n matrix whose columns are the v i vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>O(d) and SO(d) invariance and equivariance:</head><p>The following classical result in invariant theory (e.g., [94, Section II.A.9]) shows that O(d) invariant functions are functions of the scalars v i v j .</p><p>Lemma</p><formula xml:id="formula_7">1 (First Fundamental Theorem for O(d)): If f is an O(d)-invariant scalar function of vector inputs v 1 , . . , v n ∈ R d , then f (v 1 , v 2 , .</formula><p>. , v n ) can be written as a function of only the scalar products of the v i . That is, there is a function g(•) such that</p><formula xml:id="formula_8">f (v 1 , v 2 , . . , v n ) = g(V V ) = g (v i v j ) n i,j=1 .<label>(5)</label></formula><p>Proof. Given M = V V ∈ R n×n , we can reconstruct v 1 , . . , v n modulo the orthogonal group O(d) by computing the Cholesky decomposition of M (see for instance <ref type="bibr" target="#b86">[87]</ref> p. 174). Therefore, the function f is uniquely determined by the inner product matrix V V .</p><p>The classical theorem also includes the fact that if f is polynomial (in the entries of the v j 's), then g can be taken to be polynomial. In Section 5 we observe that the function g can be determined by a small subset of the scalars v i v j . There is an analogous statement for SO(d) (again see <ref type="bibr">[94,</ref> Section II.A.9]):</p><formula xml:id="formula_9">Lemma 2 (First Fundamental Theorem for SO(d)): If f is an SO(d)-invariant scalar function of vector inputs v 1 , v 2 , . . , v n ∈ R d , then f (v 1 , v 2 , .</formula><p>. , v n ) can be written as a function of the scalar products of the v i and the d × d subdeterminants of the d × n matrix V .</p><formula xml:id="formula_10">Lemma 3: If h is an O(d)-equivariant vector function of n vector inputs v 1 , v 2 , . . , v n , then h(v 1 , v 2 , . . , v n ) must lie in the subspace spanned by the input vectors v 1 , v 2 , . . , v n .</formula><p>Proof. Let {w 1 , . . , w r } ⊂ R d be an orthonormal basis of the orthogonal complement to span(v 1 , . . , v n ). Then we can write</p><formula xml:id="formula_11">h(v 1 , . . , v n ) = n t=1 α t v t + r t=1</formula><p>β t w t for some choice of α t , β t . We claim that the equivariance of h implies β t = 0 for t = 1, . . , r:</p><formula xml:id="formula_12">Consider Q ∈ O(d) such that Q(v) = v for all span(v 1 , . . , v n ), and Q(w) = −w for all w in the orthogonal complement. Since ( Qv 1 , . . , Qv n ) = (v 1 , . . , v n ) we have h( Qv 1 , . . , Qv n ) = d t=1 α t v t + r t=1 β t w t while Q(h(v 1 , . . , v n )) = d t=1 α t v t − r t=1 β t w t .</formula><p>Therefore equivariance implies that all β t = 0. Note that Lemma 3 doesn't hold for SO(d): although when the codimension of span(v 1 , . . . , v n ) is 0 or ≥ 2 a similar argument works, the situation dim R span(v 1 , . . . , v n ) = d − 1 breaks the proof, and there do exist equivariant vector functions that don't lie in the span when it has codimension 1. For instance, the cross product of two vectors in R 3 is an SO(3)-equivariant function that is not in the span of its inputs. Proposition 5 shows that generalized cross products are the only way that SO(d)equivariant vector functions can escape the span of the inputs. We further discuss this in Appendix F. Proposition 4 below gives a characterization of all O(d)-equivariant functions in terms of the scalars. We prove it in Appendix B. See <ref type="bibr" target="#b7">[8]</ref> for a more general explanation of the proof technique. </p><formula xml:id="formula_13">h(v 1 , v 2 , . . , v n ) = n t=1 f t (v 1 , v 2 , . . , v n ) v t .<label>(6)</label></formula><p>Moreover, if h is a polynomial function, the f t can be chosen to be polynomial.</p><p>The equivariant scalar functions form a module over the ring of invariant scalar functions. Proposition 4 states that this module is generated by the projections (v 1 , . . . , v n ) → v j . Proposition 5 extends this result to SO(d) in terms of the generalized cross product. The definition of the generalized cross product and the proof of Proposition 5 are in Appendix B.</p><p>Proposition 5: If h is an SO(d)-equivariant vector function of n vector inputs v 1 , v 2 , . . , v n then a similar characterization of Proposition 4 holds (with SO(d)-invariant scalar coefficients), except when v 1 , v 2 , . . , v n span a (d − 1)-dimensional space. In that case, there exist SO(d)-invariant scalar functions f t (•) and f S (•) such that</p><formula xml:id="formula_14">h(v 1 , v 2 , . . , v n ) = n t=1 f t (v 1 , v 2 , . . , v n ) v t + S∈( [n] d−1 ) f S (v 1 , v 2 , . . , v n ) v S ,<label>(7)</label></formula><p>where</p><formula xml:id="formula_15">[n] := {1, . . . , n}, [n] d−1 is the set of all (d − 1)-subsets of [n]</formula><p>, and v S represents the generalized cross product of vectors v j with j ∈ S (taken in ascending order). Moreover, if h is polynomial, the f t and f S can be taken to be polynomial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E(d) invariance and equivariance:</head><p>When modelling point clouds, we may want to express functions that are translation invariant or equivariant with respect to a subset of the input vectors (for instance, position vectors are translation equivariant). To this end, we consider the group of translations parametrized by w ∈ R d acting on the vectors in (R d ) n by translating the position vectors, and leaving everything else unchanged:</p><formula xml:id="formula_16">w (v 1 , . . , v k , v k+1 , . . , v n ) = (v 1 +w, . . , v k +w, v k+1 , . . , v n ).</formula><p>In this Section we characterize all functions that are translation and rotation invariant/equivariant. In the exposition below we assume for simplicity that all vectors are position vectors, but the results generalize trivially to a mix of vectors.</p><p>Lemma 6: Any translation-invariant function f : (R d ) n → R with inputs v 1 , . . , v n can be written uniquely as</p><formula xml:id="formula_17">f (v 1 , v 2 , . . , v n ) = f (v 2 − v 1 , . . , v n − v 1 )</formula><p>, where f : (R d ) n−1 → R is an arbitrary function. If f is polynomial, f is polynomial, and vice versa. If f is equivariant for the action of any given subgroup G ⊂ GL(n, R), then so is f , and vice versa.</p><p>The proof is given in Appendix C. There is nothing special about subtracting v 1 ; there exist more natural choices to express translation invariant functions. For instance, one classical way in physics to express translation invariance is to take class representatives of the form (v 1 , . . , v n ) where v i = 0 (for example, subtracting the center of mass). Proposition 7 characterizes the space of O(d)-equivariant functions that are also translation invariant or translation equivariant. The proof is in Appendix C.</p><formula xml:id="formula_18">Proposition 7: An O(d)-equivariant function h : (R d ) n → R d that</formula><p>is translation-invariant can be written as <ref type="bibr" target="#b5">(6)</ref> where the f t are O(d) and translation invariant and</p><formula xml:id="formula_19">n t=1 f t (v 1 , v 2 , . . , v n ) = 0. Similarly, if h is translation-equivariant then we can choose n t=1 f t (v 1 , v 2 , . . , v n ) = 1.</formula><p>Lorentz symmetry: The Lorentz group acts on Minkowski spacetime as Lorentz transformations, which keep the metric tensor invariant. Lorentz transformations relate space and time between inertial reference frames, which move at a constant relative velocity; spacetime intervals are invariant across frames. The group is made up of spatial rotations in the three space dimensions and linear velocity "boosts" along each dimension. This set of symmetries-required for special relativity-is not O(4), but rather the non-compact group O(1,3) defined in Table <ref type="table" target="#tab_0">1</ref>.</p><p>The characterization of invariant and equivariant functions we obtained for the orthogonal group can be extended to the Lorentz group, obtaining a very similar result, summarized in Proposition 8 and proven in Appendix D.</p><formula xml:id="formula_20">Proposition 8: A continuous function h : (R d+1 ) n → R d+1 is Lorentz-equivariant (with respect to the action in Table 2) if and only if h(v 1 , . . , v n ) = n t=1 f t (v 1 , . . , v n ) v t<label>(8)</label></formula><p>where f t : (R d+1 ) n → R are Lorentz-invariant scalar functions. Moreover, the functions f t are uniquely determined by the pairwise Minkowski inner product •, • (4):</p><formula xml:id="formula_21">f t (v 1 , . . , v n ) = g t ( v i , v j n i,j=1 ) . (<label>9</label></formula><formula xml:id="formula_22">)</formula><p>If h is polynomial, the f t and corresponding g t can be taken to be polynomial.</p><p>Poincaré symmetry: The Poincaré group combines translation symmetry with Lorentz symmetry. Together these complete the symmetries of special relativity, forming the full group of spacetime transformations that preserve the Minkowski metric. The generalization from a Lorentz-equivariant formulation to a Poincaré-equivariant formulation is similar to the generalization from O(d) to E(d):</p><p>The position vectors take on a special role, in which only differences of position can appear as vector inputs to the functions; all other vectors can act unchanged.</p><p>Proposition 9 generalizes the results above to the Poincaré group action. As before, we assume that all vectors are position vectors for simplicity. The proof is analogous to the proof of Proposition 7.</p><p>Proposition 9: A continuous function h : (R d+1 ) n → R d+1 is Poincaré-equivariant (with respect to the action in Table <ref type="table">2</ref>) if and only if</p><formula xml:id="formula_23">h(v 1 , . . , v n ) = n t=1 f t (v 1 , . . , v n ) v t (<label>10</label></formula><formula xml:id="formula_24">)</formula><p>where the f t : (R d+1 ) n → R are translation and Lorentz-invariant scalar functions, determined as in ( <ref type="formula" target="#formula_21">9</ref>) by the pairwise Minkowski inner products (4), but also satisfying</p><formula xml:id="formula_25">n t=1 f t (v 1 , v 2 , . . , v n ) = 1.</formula><p>Furthermore, if h is polynomial, the f t and corresponding g t (as in <ref type="bibr" target="#b8">(9)</ref>) can be taken to be polynomial.</p><p>Permutation invariance and equivariance: Most physics problems are permutation-invariant, in that once you know the masses, sizes, shapes, and so on of the objects in the problem, the physical predictions are invariant to labeling. In particle physics, this invariance is raised to a fundamental symmetry; fundamental particles (like electrons) are identical and exchangeable.</p><p>The characterization of permutation-invariant functions with respect to the action in Table <ref type="table">2</ref> is classical <ref type="bibr">[94, pp. 36-39]</ref>. Here we prove an extension that describes permutation-invariant vector functions that are also O(d)-equivariant. The proof is in Appendix E.</p><formula xml:id="formula_26">Proposition 10: Let h : (R d ) n → R d be O(d)-equivariant (or continuous O(1, d − 1)-equivariant)</formula><p>and also permutation-invariant with respect to the action in Table <ref type="table">2</ref>. Then h can be written as</p><formula xml:id="formula_27">h(v 1 , . . , v n ) = n t=1 f (v t , v 1 , . . , v t−1 , v t+1 , . . , v n ) v t ,<label>(11)</label></formula><p>where</p><formula xml:id="formula_28">f : (R d ) n → R is O(d)-invariant (or O(1, d − 1)</formula><p>-invariant) and permutation-invariant with respect to the last n − 1 inputs.</p><p>Proposition 11 proven in Appendix E extends the characterization above to permutation equivariant functions (see also <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b48">49]</ref>).</p><formula xml:id="formula_29">Proposition 11: Let h : (R d ) n → (R d ) n be O(d)-equivariant (or continuous O(1, d−1)-equivariant)</formula><p>and also permutation-equivariant with respect to the action in Table <ref type="table">2</ref>. Then h can be written as h = (h 1 , . . , h n ) where each</p><formula xml:id="formula_30">h i : (R d ) n → R d is O(d)-equivariant (or continuous O(1, d − 1)- equivariant)</formula><p>and</p><formula xml:id="formula_31">h i (v 1 , . . , v n ) = n t=1 f (i) t (v 1 , . . , v n ) v t ,<label>(12)</label></formula><p>where all the f</p><formula xml:id="formula_32">(i) j : (R d ) n → R are O(d)-invariant (or O(1, d−1)-invariant)</formula><p>, and for all i, j = 1, . . , n and all σ ∈ S n we have</p><formula xml:id="formula_33">f (i) σ −1 (j) (v σ(1) , . . , v σ(n) ) = f (σ(i)) j (v 1 , . . , . . , v n ). (<label>13</label></formula><formula xml:id="formula_34">)</formula><p>4 Examples</p><p>Here we briefly state two classical physics expressions that obey all the symmetries, and show how to formulate them in terms of invariant scalars.</p><p>Total mechanical energy: In Newtonian gravity, the total mechanical energy T of n particles with scalar masses m i , vector positions r i , and vector velocities v i is a scalar 1 function:</p><formula xml:id="formula_35">T = 1 2 n i=1 m i |v i | 2 − 1 2 n i=1 n j=1 j =i G mi mj |ri−rj | , (<label>14</label></formula><formula xml:id="formula_36">)</formula><p>where G is Newton's constant (a fundamental constant, and hence scalar). Since |a| ≡ (a a) 1/2 , this expression ( <ref type="formula" target="#formula_35">14</ref>) is manifestly constructed from functions only of scalars m i and scalar products of vectors. It is also worthy of note that the positions r i only appear in differences of position.</p><p>Electromagnetic force law: The total electromagnetic force F acting on a test particle of charge, 3-vector position, and 3-vector velocity (q, r, v) given a set of n other charges (q i , r i , v i ) is a vector function <ref type="bibr" target="#b41">[42]</ref>:</p><formula xml:id="formula_37">F = n i=1 k q q i (r−ri) |r−ri| 3 electrostatic force + n i=1 k q q i v×(vi×(r−ri)) c 2 |r−ri| 3 magnetic force , (<label>15</label></formula><formula xml:id="formula_38">)</formula><p>where k is an electromagnetic constant, c is the speed of light, and a × b represents the cross product that produces a pseudo-vector perpendicular to vectors a and b according to the right-hand rule. This doesn't obviously obey our equivariance requirements, because cross products deliver parityviolating pseudo-vectors; these can't be written in O(3)-equivariant form. However, a cross of a cross of vectors is a vector, so this expression is in fact O(3)-equivariant, as are all forces (because forces must be O(3)-equivariant vectors in order for the theory to be self-consistent).</p><p>We can expand the vector triple product using the identity a</p><formula xml:id="formula_39">× (b × c) = (a c) b − (a b) c: F = n i=1 k q q i (r − r i ) |r − r i | 3 + n i=1 k q q i (v (r − r i )) v i − (v v i ) (r − r i ) c 2 |r − r i | 3 = n i=1 k q q i 1 − v v i c 2 (r − r i ) |r − r i | 3 + n i=1 k q q i (v (r − r i )) v i c 2 |r − r i | 3 ,<label>(16)</label></formula><p>1 The energy is not a scalar in special relativity or general relativity, but it is a scalar in Newtonian physics.</p><p>where the quantity v v i is the scalar product of the velocities. All of the quantities are now straightforwardly functions of invariant scalar products times the input vectors.</p><p>5 How many scalars are needed?</p><p>Our analysis in Section 3 shows that the invariant and equivariant functions of interest (under actions in Table <ref type="table">2</ref> from groups in Table <ref type="table" target="#tab_0">1</ref>) with input vectors v 1 , . . , v n , can be expressed in terms of the scalars v i , v j n i≥j=1 (4). This greatly simplifies the parameterization of such functions, but it significantly increases the number of features if n d. In Appendix G we remark that the scalars can be uniquely determined by a small subset of size approximately (d + 1) n. This is related to the rigidity theory of Gram matrices <ref type="bibr" target="#b77">[78]</ref> that answers when there exists a unique set of vectors that realize a partial set of distances, and it is closely related to the low rank matrix completion problem <ref type="bibr" target="#b83">[84]</ref>. Furthermore, there is a vast literature studying high probability robust reconstruction of all scalars from a random subset via convex optimization techniques <ref type="bibr" target="#b12">[13]</ref>. Recently developed optimization techniques on Gram matrices could provide efficient algorithms to learn invariant and equivariant functions <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations and caveats</head><p>Although the principal results presented here work for many groups, and work naturally at all spatial dimensions d (unlike methods based on irreps, for example), they do not solve all problems for all use cases of equivariant machine learning. For one, there are myriad groups-and especially discrete groups-that apply to physical and chemical systems where invariant and equivariant functions do not have such a nice characterization. Such is the case for the GNNs discussed in Appendix A.</p><p>One example of a situation in which our formulation might not be practical is provided by multipole expansions (for example, those used in the fast multipole method <ref type="bibr" target="#b4">[5]</ref> and n-body networks <ref type="bibr" target="#b49">[50]</ref>). In the fast multipole method, a hierarchical spatial graph is constructed, and high-order tensors are used to aggregate information from lower-level nodes into higher-level nodes. This aggregation is concise and linear when it is performed using high-order tensors; this aggregation is hard (or maybe impossible) when only scalars can be transmitted, without the use of the irreps of the relevant symmetry groups. This is a research direction we are currently exploring.</p><p>Another example that suggests that our universal functions might be cumbersome is the current forms in which classical theories-such as electromagnetism and general relativity-are written. For example, in Section 4 we showed that the electromagnetic force law can be written in the form of functions of scalars and scalar products times vectors, but that is not how the theory is traditionally written. It is traditionally written in terms of the magnetic field (a pseudo-vector) or the electromagnetic tensor (an order-2 tensor). As another example, general relativity is traditionally written in terms of contractions of an order-4 curvature tensor. That is, although the theories can in principle be written in the forms we suggest, they will in general be much more concise or simple or clear in forms that make use of higher-order or non-equivariant forms.</p><p>Finally, although our results apply to many physically relevant groups, they do not encode all of the symmetries of classical physics. For example, one critical symmetry is the dimensional or units symmetry: You cannot add or subtract terms that have different units (positions and forces, for example). This symmetry or consideration has implications for the construction of valid polynomials. It also implies that only dimensionless (unitless) scalar quantities can be the arguments of large classes of nonlinear functions, including exponentials or sigmoids. These additional symmetries must be enforced at present with some additional considerations of network architecture or constraints.</p><p>We also note that in this work we have characterized global symmetries, which act on each point in the same way. Our characterization does not obviously generalize to gauge symmetries, which are local symmetries that apply changes independently to spatially separated points (see e.g. <ref type="bibr" target="#b11">[12]</ref>). That said, we believe our model could encompass general gauge symmetries if we replace the local metric by any position-dependent metric Λ x . Recent work has shown that equivariance under gauge symmetries is possible in the realm of convolutional neural networks by defining coordinateindependent kernels <ref type="bibr" target="#b92">[93]</ref>. In our case, we would have to propagate spatially separated vectors to the same location in order to pass them to a locally invariant function, requiring the operation of parallel Training Set Size Training Set Size  <ref type="formula" target="#formula_3">3</ref>)-equivariance and permutation invariance construction. MLP denotes a standard multilayer perceptron, and MLP+Aug denotes an MLP that has been trained with data augmentation to the given symmetry group. EMLP-G denotes the EMLP models from <ref type="bibr" target="#b28">[29]</ref> with different relevant symmetry groups G. For both tasks, the scalar method outperforms all other methods. The shaded regions depict 95% confidence intervals taken over 3 runs.</p><p>transport. This operation would then also have to obey our invariance characterization; we believe this is possible and we will detail it in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Numerical experiments</head><p>We demonstrate our approach using scalar-based multi-layer perceptrons (MLP) on two toy learning tasks from <ref type="bibr" target="#b28">[29]</ref>: an O(5)-invariant task and an O(3)-equivariant task. Further numerical experiments with these methods applied to dynamical systems appear in <ref type="bibr" target="#b96">[97]</ref>. The code is available on GitHub 2 , and it reuses much of the functionality provided by EMLP <ref type="bibr" target="#b28">[29]</ref>.</p><p>O(5)-invariant task: Given observations of the form</p><formula xml:id="formula_40">(x i 1 , x i 2 , f (x i 1 , x i 2 )) N i=1</formula><p>, where x i 1 , x i 2 ∈ R 5 and f is an O(5)-invariant scalar function, we aim to learn f . In this case f is the example from <ref type="bibr" target="#b28">[29]</ref>:</p><formula xml:id="formula_41">f (x 1 , x 2 ) = sin( x 1 ) − x 2 3 2 + x 1 x 2 x 1 x 2 , x 1 , x 2 ∈ R 5 .<label>(17)</label></formula><p>We model f using Lemma 1, namely f (x 1 , x 2 ) = g(x 1 x 1 , x 1 x 2 , x 2 x 2 ) where g : (R) 3 → R could be any function. We learn g by implementing it as an MLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>O(3)-equivariant task:</head><p>In this task, the data are n = 5 point masses and positions (m i , x i ) 5 i=1 , and the goal is to predict the matrix I = 5 i=1 m i (x i x i I − x i x i ). To this end we aim to learn h</p><formula xml:id="formula_42">: (R × R 3 ) 5 → R 3×3 (m i , x i ) 5 i=1 → I.<label>(18)</label></formula><p>The function h is O(3)-equivariant in positions, and S 5 -invariant, namely, for all</p><formula xml:id="formula_43">Q ∈ O(3) and σ ∈ S 5 we have h((m σ(i) , Qx σ(i) ) 5 i=1 ) = Qh((m i , x i ) 5 i=1 )</formula><p>. By Proposition 10 we know there exists functions f 0 , f 1 , and</p><formula xml:id="formula_44">f 2 such that h((m i , x i ) 5 i=1 ) = 5 i=1 f 0 (x i x i , m i , {x k x l , m k , m l } k,l =i ) x i x i + + 5 i&gt;j=1 f 1 (x i x j , m i , m j , {x k x l , m k , m l } k,l =i,j ) x i x j + f 2 ({x i x j , m i , m j } 5 i,j=1 )I.</formula><p>Here the set notation means that the function f i is permutation-invariant with respect to the inputs in the set. The function h is O(3)-equivariant by construction (see Section 3). We model f 0 , f 1 ,and f 2 with MLPs on deep sets [? ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Symmetry-enforcing universal neural networks and irreducible representations</head><p>In this Section we describe the general principles to parameterize invariant and equivariant universally approximating functions. As mentioned in Section 2, the most common approach is is to write the invariant/equivariant functions as the composition of linear invariant/equivariant layers and non-linear compatible pointwise activation functions. We start by describing the G-invariant graph networks from <ref type="bibr" target="#b64">[65]</ref>, which are invariant with respect to a group G (typically a subgroup of the symmetric group S n , since the work is in the context of graph neural networks) acting on R n as g x (g ∈ G, x ∈ R n ). They extend the action to tensors t ∈ R n k by acting in each of the k dimensions of the tensor.</p><formula xml:id="formula_45">Consider a graph X = (V, E) on n nodes. Let V = (v 1 , . . , v n ) ∈ (R d ) n</formula><p>, where v i ∈ R d are the node features, and assume for simplicity that the edge features are real numbers represented by the matrix E ∈ R n×n , then a graph neural network learns equivariant functions f : X → (R ) n , where the group of permutations S n acts in (R d ) n and (R ) n as in Table <ref type="table">2</ref>, and it acts on the graph X as:</p><formula xml:id="formula_46">Π (V, E) = (Π V, Π E) where Π E = ΠEΠ .<label>(19)</label></formula><p>The GNN learns an embedding of the form:</p><formula xml:id="formula_47">N (X) = θ • L T • . . . • L 2 • θ • L 1 (E) ,<label>(20)</label></formula><p>where θ is a point-wise non-linearity and L i : R</p><formula xml:id="formula_48">n ⊗k i → R n ⊗k i+1 is a linear equivariant map.</formula><p>When G is the group of permutations, the network (20) can universally approximate all continuous equivariant functions as long as the order of the intermediate tensors can be arbitrarily large <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b46">47]</ref>. The key insight is that the k i -tensors in the intermediate layers can express all (equivariant) polynomial functions of the input of degree k i . Universality follows from a generalization of the Stone-Weierstrass theorem that states that every continuous equivariant function defined on a compact set can be uniformly approximated as closely as desired by equivariant polynomial functions (for the specific action by permutations) <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b3">4]</ref>. The disavantage of this approach is that the space of linear equivariant functions L i , even though it is fully characterized, has dimension that grows super-exponentially with the order of the tensors <ref type="bibr" target="#b63">[64]</ref>.</p><p>The approach for general groups is very similar. Group-equivariant neural networks are written as the composition of linear equivariant functions going to higher order tensors, composed with nonlinear pointwise activation functions (for general groups there may be restrictions on the activation functions so that equivariance is preserved). Recent work proposes to enforce equivariance of these linear maps by imposing constraints <ref type="bibr" target="#b28">[29]</ref>. But in general, the most prevalent approach is to express intermediate layers as linear maps between (group) representations of G.</p><p>Let G be a group acting on R d as . In representation-theory language, a representation of G is a map ρ : G → GL(V ) that satisfies ρ(g 1 g 2 ) = ρ(g 1 )ρ(g 2 ) (where V is a vector space and GL(V ) denotes the automorphisms of V , that is, invertible linear maps V → V ). The group action of G on R d is equivalent to the group representation ρ :</p><formula xml:id="formula_49">G → GL(R d ) so that ρ(g)(v) = g v.</formula><p>We can extend the action to the tensor product (R d ) ⊗k so that the group acts independently in every tensor factor (i.e., in every dimension), namely</p><formula xml:id="formula_50">ρ k = ⊗ k r=1 ρ : G → GL((R d ) ⊗k</formula><p>). The first step is to note that a linear equivariant map L i : (R d ) ⊗ki → (R d ) ⊗ki+1 corresponds to a map between group representations such that L i • ρ ki (g) = ρ ki+1 (g) • L i for all g ∈ G. Homomorphisms between group representations are easily parametrizable if we decompose the representations in terms of irreps:</p><formula xml:id="formula_51">ρ ki = T k i =1 T .<label>(21)</label></formula><p>In particular, Schur's Lemma implies that a map between two irreps over C is either zero or a multiple of the identity.</p><p>The equivariant neural-network approach consists in decomposing the group representations in terms of irreps and explicitly parameterizing the maps <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b29">30]</ref>. In general it is not clear how to decompose an arbitrary group representation into irreps. However in the case where G = SO(3), the decomposition of a tensor representation as a sum of irreps is given by the Clebsh-Gordan decomposition:</p><formula xml:id="formula_52">⊗ k s=1 ρ s = ⊕ T =1 T<label>(22)</label></formula><p>The Clebsh-Gordan decomposition not only gives the decomposition of the RHS of ( <ref type="formula" target="#formula_52">22</ref>) but also it gives the explicit change of coordinates. This decomposition is fundamental for implementing the equivariant 3D point-cloud methods defined in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b8">9]</ref>. Moreover, very recent work <ref type="bibr" target="#b24">[25]</ref> shows that the classes of functions defined in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b85">86]</ref> are universal, meaning that every continous SO(3)equivariant function can be approximated uniformly in compacts sets by those neural networks. However, there exists a clear limitation to this approach: Even though decompositions into irreps are broadly studied in mathematics (a.k.a. plethysm), the explicit transformation that allows us to write the decomposition of tensor representations into irreps is a hard problem in general. It is called the Clebsch-Gordan problem. There is exciting, recent progress on this problem for large classes of groups <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b40">41]</ref>. The approach taken in the present work sidesteps this problem altogether.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Equivariant functions under rotations and the orthogonal group</head><p>Generalized cross-product: The generalized cross product of d − 1 vectors in R d is defined to be the Hodge dual of the exterior product of the d − 1 vectors. Namely, given v 1 , . . , v d−1 the cross product v 1 × . . × v d−1 is the unique vector that satisfies that for all y ∈ R d</p><formula xml:id="formula_53">v 1 × . . × v d−1 , y = det(v 1 , . . , v d−1 , y),<label>(23)</label></formula><p>where •, • denotes the usual inner product in R d , and det(v 1 , . . , v d ) corresponds to the determinant of the matrix with rows v 1 , . . , v d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Proposition 4:</head><p>The purely set-theoretic statement in Proposition 4 and the polynomial statement have independent proofs.</p><p>For the set-theoretic statement, let h : (R d ) n → R d be an arbitrary O(d)-equivariant vector function. Lemma 3 shows that for any fixed n-tuple of vectors (v 1 , . . , v n ), there exists an n-tuple of real numbers (a 1 , . . , a n ) such that</p><formula xml:id="formula_54">h(v 1 , . . . , v n ) = n t=1 a t v t .<label>(24)</label></formula><p>Pick one representative tuple (v 1 , . . . , v n ) from each O(d)-orbit on (R d ) n , and find a corresponding tuple (a 1 , . . . , a n ) satisfying this equation. (Note that the tuple (a 1 , . . . , a n ) thus depends on the orbit; however, this dependence is suppressed in the notation to prevent it from becoming cumbersome.) Define O(d)-invariant functions f t : (R d ) n → R by defining f t (v 1 , . . . , v n ) = a t for the a t corresponding to the chosen representative of (v 1 , . . . , v n )'s orbit. Then</p><formula xml:id="formula_55">h(v 1 , . . . , v n ) = f t (v 1 , . . . , v n )v t<label>(25)</label></formula><p>at the chosen orbit representatives, and the O(d)-equivariance of both sides then implies this equation is satisfied everywhere.</p><p>For the polynomial statement, now assume h</p><formula xml:id="formula_56">: (R d ) n → R d is a polynomial O(d)-equivariant map. Define a new map h : (R d ) n+1 → R by h(v 1 , . . . , v n , y) = h(v 1 , . . . , v n ), y ,<label>(26)</label></formula><p>where •, • is the usual inner product on R d . Then for any Q ∈ O(d), we have</p><formula xml:id="formula_57">h(Q v 1 , . . . , Q v n , Q y) = Q h(v 1 , . . . , v n ), Q y = h(v 1 , . . . , v n ), y = h(v 1 , . . . , v n , y),</formula><p>where the first equality is by the definition of h and the O(d)-equivariance of h and the second is the fact that the inner product is preserved by O(d). In other words, h is an O(d)-invariant scalar function with respect to O(d)'s natural action on (R d ) n+1 .</p><p>It follows from the First Fundamental Theorem for O(d) that h is a polynomial in the inner products v i , v j , v t , y , and y, y . On the other hand, by its definition, it is homogeneous of degree 1 in the coordinates of y. It follows that y, y does not appear this polynomial expression for h in terms of the dot products, and furthermore, each term contains some v t , y with degree 1, and is otherwise composed of v i , v j 's. Grouping the terms according to which v t , y each contains, we get</p><formula xml:id="formula_58">h(v 1 , . . . , v n , y) = n t=1 g t ( v i , v j n i,j=1 ) v t , y<label>(27)</label></formula><p>for all v 1 , . . . , v n , y.</p><formula xml:id="formula_59">Defining f t (v 1 , . . . , v n ) = g t ( v i , v j n i,j=1</formula><p>) for each t, we find the f t 's are invariant polynomials. Unspooling the definition of h on the left side, and using the linearity of the dot product on the right, this becomes</p><formula xml:id="formula_60">h(v 1 , . . . , v n ), y = n t=1 f t (v 1 , . . . , v n )v t , y .<label>(28)</label></formula><p>As this equation holds for all y, and dot product is a nondegenerate bilinear form, we can conclude that</p><formula xml:id="formula_61">h(v 1 , . . . , v n ) = n t=1 f t (v 1 , . . . , v n )v t ,<label>(29)</label></formula><p>with the f t invariant polynomials, as promised.</p><p>Proof of Proposition 5: The proof of this proposition runs parallel to the proof of Proposition 4.</p><p>For the set-theoretic part of the proposition, we need a substitute for Lemma 3 that applies to SO(d).</p><p>The needed statement is that if h From this lemma it follows that for any tuple (v 1 , . . . , v n ) with dim span(v 1 , . . . , v n ) = d − 1, there exists a solution in (a 1 , . . . , a n ) ∈ R n to the equation</p><formula xml:id="formula_62">: (R d ) n → R d is SO(d)-equivariant,</formula><formula xml:id="formula_63">h(v 1 , . . . , v n ) = a t v t .<label>(30)</label></formula><p>On the other hand, when dim span(v 1 , . . . , v n ) = d − 1, then there must exist d − 1 linearly independent vectors v i1 , . . . , v i d−1 . In this situation, the generalized cross product v i1 × • • • × v i d−1 is nonzero, and linearly independent from v i1 , . . . , v i d−1 (in fact it lies in the orthogonal complement of span(v 1 , . . . , v n )). Thus it and v 1 , . . . , v n span R d . So in all cases, i.e., for any tuple (v 1 , . . . , v n ), there exists a solution in the a t and a S to</p><formula xml:id="formula_64">h(v 1 , . . . , v n ) = n t=1 a t v t + S∈( [n] d−1 ) a S v S ,<label>(31)</label></formula><p>where the notation is as in the statement of Proposition 5.</p><p>The proof of the set-theoretic statement now proceeds exactly as for the proof of Proposition 4: from each SO(d)-orbit in (R d ) n , choose a representative tuple (v 1 , . . . , v n ); pick values of a t and a S that satisfy the above for this tuple; use them to define invariant functions f t (v 1 , . . . , v n ) and f S (v 1 , . . . , v n ); then the equation</p><formula xml:id="formula_65">h(v 1 , . . . , v n ) = n t=1 f t (v 1 , . . . , v n )v t + S∈( [n] d−1 ) f S (v 1 , . . . , v n )v S<label>(32)</label></formula><p>holds at the chosen representative tuples, and the SO(d)-equivariance of both sides shows it holds everywhere.</p><p>The proof of the polynomial statement is even more directly parallel to the proof of Proposition 4.</p><p>As in that proof, given an SO(d)-equivariant polynomial vector function h</p><formula xml:id="formula_66">: (R d ) n → R d , define a polynomial scalar function h(v 1 , . . . , v n , y) = h(v 1 , . . . , v n ), y .<label>(33)</label></formula><p>For the same reason as before, it is SO(d)-invariant. It follows from the First Fundamental Theorem for SO(d) that h is a polynomial in the dot products v i , v j , v t , y , y, y , and the d × d subdeterminants det(v i1 . . . v i d ) and det(v i1 . . . v i d−1 y). Because it is also homogeneous of degree 1 in the coordinates of y, y, y cannot occur, and every term must contain either a v t , y or a det(v i1 . . . v i d−1 y) exactly once, and is otherwise a product of v i , v j 's and det(v i1 . . . v i d )'s (which is to say, it is otherwise an SO(d)-invariant function of the v j 's). Grouping according to which v t , y or a det(v i1 . . . v i d−1 y) each term contains, we get</p><formula xml:id="formula_67">h(v 1 , . . . , v n ) = n t=1 f t (v 1 , . . . , v n ) v t , y + S∈( [n] d−1 ) f S (v 1 , . . . , v n ) det(ṽ S , y),<label>(34)</label></formula><p>where the f t 's and f S 's are invariant, and where we have abbreviated det(v i1 , . . . , v i d−1 , y) as det(ṽ S , y) with</p><formula xml:id="formula_68">S = {i 1 , i 2 , . . . , i d−1 }. (The tilde is to distinguish it from v S = v i1 × • • • × v i d−1 .) Now, det(ṽ S , y) = v S , y ,<label>(35)</label></formula><p>by definition of the generalized cross product v S , so by applying the linearity of the dot product on the right, and the definition of h on the left, we get</p><formula xml:id="formula_69">h(v 1 , . . . , v n ), y = n t=1 f t (v 1 , . . . , v n )v t + S∈( [n] d−1 ) f S (v 1 , . . . , v n )v S , y .<label>(36)</label></formula><p>As with O(d), we get the equality claimed in Proposition 5 because y is arbitrary and •, • is a nondegenerate bilinear form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Translation-invariant functions</head><p>Proof of Lemma 6: Consider the map Π : (R d ) n → (R d ) n−1 given by (v 1 , v 2 , . . , v n ) → (v 2 − v 1 , . . , v n − v 1 ). This map has a section i : (R d ) n−1 → (R d ) n given by (v 2 , . . . , v n ) → (0, v 2 , . . . , v n ), i.e., Π • i is the identity. The fibers of Π are exactly the orbits of the translation action. Thus a translation-invariant function f on (R d ) n descends via Π to a well-defined function f on (R d ) n−1 , so that</p><formula xml:id="formula_70">f = f • Π. (<label>37</label></formula><formula xml:id="formula_71">) Then f = f • Π • i = f • i.<label>(38</label></formula><p>) Because Π, i are polynomial, these equations show that if either f or f is polynomial then so is the other. Because i, Π are both equivariant for the action of GL(n, R), any equivariance property of either f or f with respect to any subgroup G ⊂ GL(n, R) is passed to the other. </p><p>This implies h can be written as</p><formula xml:id="formula_73">h(v 1 , v 2 , . . , v n ) = n t=2 f t (v 1 , . . , v n ) v t − ( n t=2 f t (v 1 , . . , v n )) v 1 (<label>40</label></formula><formula xml:id="formula_74">)</formula><p>where the functions f t are O(d)-and translation-invariant, which has the claimed form.</p><p>For the case where h is O(d)-and translation-equivariant we first observe that it suffices to take one representative per orbit, define the function on that representative, and extend it everywhere else by translations:</p><formula xml:id="formula_75">h(v 1 , . . , v n ) = h(v 2 − v 1 , . . , v n − v 1 ) + v 1 ,<label>(41)</label></formula><p>where h is O(d)-equivariant. Therefore any function h that is O(d)-and translation-equivariant can be written as</p><formula xml:id="formula_76">h(v 1 , . . , v n ) = n t=2 ft (v 2 − v 1 , . . , v n − v 1 )(v t − v 1 ) + v 1 (42) = n t=2 f t (v 1 , v 2 . . , v n ) v t + (1 − n t=2 f t (v 1 , v 2 . . , v n )) v 1 .<label>(43)</label></formula><p>1 Synthetic Experiments </p><p>We train a neural network to find a function g(•) such that:</p><formula xml:id="formula_78">f (x 1 , x 2 ) = g(x 1 x 1 , x 1 x 2 , x 2 x 2 ).<label>(2)</label></formula><p>See results in Figure <ref type="figure" target="#fig_3">1</ref>. Training Set Size   </p><p>The inputs X = {(m i , x i )} 5 i=1 are of type 5T 0 + 5T 1 and outputs are of type T 2 , both transform under the group. </p><p>where each corresponding f M is some multi-layer perceptron. See Scalars (E) in Figure <ref type="figure" target="#fig_13">2</ref>.</p><p>O(d)-equivariant + permutation invariant formulation See Proposition 10. The general idea is to approximate the sum of values of the two functions i,j f ij ({x i x j , m i , m j }, {x k x l , m k , m l , ∀k, l}) x i x j (5) and f 0 ({x i x j , m i , m j , ∀i, j})I,</p><p>where f ij and f 0 can be some multi-layer perceptrons. For example, we can apply a function on x i x j for all pairs of particles (i, j), i = j, and another function on x i x i for all i. To be more specific, </p><formula xml:id="formula_82">i =j f 1 x i x j , θ 1 (m i ) + θ 1 (m j ), k,l φ 1 (x k x l ), s ψ 1 (m s ) x i x j<label>(</label></formula><p>where each f i , i = 1, 2, 3, 4, θ j , j = 1, 2, 3, φ k , k = 1, 2, ψ l , l = 1, 2 is some multi-layer perceptron. See Scalars (E+P) in Figure <ref type="figure" target="#fig_13">2</ref>. Training Set Size Training Set Size  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Proposition 4 :</head><label>4</label><figDesc>If h is an O(d)-equivariant vector function of n vector inputs v 1 , v 2 , . . , v n , then there are n O(d)-invariant scalar functions f t (•) such that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Test error as a function of training set size for (Left) the O(5)-invariant task, and (Right) the O(3)equivariant task. Scalars (I) denotes the MLP model using the scalars method for O(5)-invariance construction, and Scalars (E+P) denotes the MLP model using the scalars method for O(3)-equivariance and permutation invariance construction. MLP denotes a standard multilayer perceptron, and MLP+Aug denotes an MLP that has been trained with data augmentation to the given symmetry group. EMLP-G denotes the EMLP models from<ref type="bibr" target="#b28">[29]</ref> with different relevant symmetry groups G. For both tasks, the scalar method outperforms all other methods. The shaded regions depict 95% confidence intervals taken over 3 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Proof of Proposition 7 :</head><label>7</label><figDesc>Let h : (R d ) n → R d be translation-invariant and O(d)-equivariant. Using Lemma 6 we can consider an O(d)-equivariant function h :(R d ) n−1 → R d so that h(v 1 , v 2 , . . , v n ) = h(v 2 − v 1 , . . , v n − v 1 ). Therefore there exists O(d) invariant functions ft h(v 2 − v 1 , . . , v n − v 1 ) = n t=2 ft (v 2 − v 1 , . . , v n − v 1 )(v t − v 1 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: O(5) invariant task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1. 2 OI = 5 i=1m</head><label>25</label><figDesc>(3) Equivariant TaskPredicting the moment of inertia matrix from n = 5 point masses and positions {(m i , x i )} 5 i=1 , i (x i x i I − x i x i ), m i ∈ R, x i ∈ R 5 , i = 1, 2, . . . , 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>O</head><label></label><figDesc>(d)-equivariant formulation See Proposition 4. Denote the set of 3 × 3 matrices M = {x i x j , ∀i, j} ∪ I (identity matrix). The idea is to approximate the followingM ∈M f M x 1 x 1 , x 1 x 2 , . . . , x 4 x 5 , x 5 x 5 , m 1 , m 2 , . . . , m 5 M,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: O(3) equivariant task. All perceptrons used in Scalars (E+P) and Scalars (E) are with 2 layers and 200 hidden nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The groups considered in this work.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>1.1 O(5) Invariant TaskA synthetic O(5) invariant regression problem given by the functionf (x 1 , x 2 ) = sin( x 1 ) − x 2 3 2 + x 1 x 2 x 1 x 2 , x 1 , x 2 ∈ R 5 .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>f 2 x i x i , θ 2 (m i ), k,l φ 2 (x k x l ), s ψ 2 (m s ) x i x i (8) i =j f 3 x i x j , θ 3 (m i ) + θ 3 (m j ) + i f 4 x i x i , θ 4 (m i ) I</figDesc><table /><note>7) 1 arXiv:2106.06610v3 [cs.LG] 21 Oct 2022 i</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/weichiyao/ScalarEMLP</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: It is a pleasure to thank Tim Carson (Google), Miles Cranmer (Princeton), Johannes Klicpera (TUM), Risi Kondor (Chicago), Lachlan Lancaster (Princeton), Yuri Tschinkel (NYU), Fedor Bogomolov (NYU), Gregor Kemper (TUM), and Rachel Ward (UT Austin) for valuable comments and discussions. We especially thank Gerald Schwartz (Brandeis) for indicating the line of the polynomial argument to us, and Marc Finzi (NYU) for their help with the EMLP codebase. SV was partially supported by NSF DMS 2044349, the NSF-Simons Research Collaboration on the Mathematical and Scientific Foundations of Deep Learning (MoDL) (NSF DMS 2031985), and the TRIPODS Institute for the Foundations of Graph and Deep Learning at Johns Hopkins University. KSF was supported by the Future Investigators in NASA Earth and Space Science and Technology (FINESST) award number 80NSSC20K1545.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Invariant and equivariant functions under the Lorentz group</head><p>Proof of Proposition 8: The proof follows the pattern of the proof of Proposition 4. The claim for polynomial h follows in exactly the same manner, because there is a First Fundamental Theorem for the Lorentz group precisely analogous to Lemma 1-in fact, both are consequences of the First Fundamental Theorem for O(d,C) [33, Proposition 5.2.2], because over C, the Lorentz group and the orthogonal group are related by a change of basis (namely, multiplying the space coordinates by i).</p><p>The claim for arbitrary continuous h has two added subtleties for the Lorentz group. First, unlike for O(d), the Minkowski inner products of the vectors v j do not distinguish every pair of distinct O(1,d− 1) orbits from each other: for example the orbit of (v, . . . , v), with v, v = 0, is indistinguishable from 0 by the inner products, even if v = 0. So there exist O(1, d − 1)-invariant set functions on (R d ) n that are not functions of the Minkowski inner products. However, they do distinguish every pair of closed orbits, and every orbit has a closed orbit in its closure (this is a general theorem about the invariant ring of a reductive group, see <ref type="bibr" target="#b72">[73,</ref><ref type="bibr">Theorems 4</ref>.6 and 4.7 and their corollaries]; the Lorentz group is reductive because it is a real form of the reductive group O(d, C)). Thus every continuous invariant function is indeed a function of the Minkowski inner products.</p><p>The second subtlety is in proving that a continuous, equivariant vector function h always lies in the span of v 1 , . . . , v n . Our approach in Section 3 to show that the invariant functions under the orthogonal group are restricted to the span of the input vectors is based on the following idea: Given v 1 , . . v n , if they span R d , then the result trivially holds. Otherwise let {w 1 , . . w m } be a basis of span(v 1 , . . , v n ) and extend it to a basis of R d . Then we do a full orthogonalization (via Gram Schmidt) to get orthogonal vectors u 1 , u 2 , . . , u d and then construct Q:</p><p>then for each j (2 ≤ j ≤ d) in order:</p><p>Then Q is an element of O(d) that fixes everything in span(v 1 , . . , v n ) and does not fix anything outside it. This can be used to show that equivariant functions are restricted to the span of the inputs (see Lemma 3).</p><p>This idea can be generalized to the Lorentz group O <ref type="bibr">(1, d)</ref>.</p><p>where êt is a timelike unit vector and the êxj are orthonormal space-like vectors (all orthogonal to êt ). Given {v 1 , . . v n } we consider {w 1 , . . w m } a basis of span(v 1 , . . , v n ) and we extend it to {w 1 , . . w d+1 } a basis of R d+1 and orthogonalize all the vectors according to the Lorentz generalization of orthogonalization given above to make orthogonal vectors u 1 , u 2 , . . , u d+1 and then construct Q:</p><p>then for each j (2 ≤ j ≤ d + 1) in order:</p><p>Note that ( <ref type="formula">49</ref>) and (50) require u j , u j = 0 for all j, however, the Minkowski inner product (4) is not positive definite, in particular there can be lightlike vectors u j = 0 in which u j , u j = 0. In our argument we first will require that if m = 1 then v 1 is not lightlike. If the procedure (49) generates a lightlike u j for any j in the range 1 ≤ j ≤ d + 1 we proceed in the following way:</p><p>• If m &gt; 1 and j ≤ m, replace the vectors w 1 , . . , w n by a linear combination of them with random coefficients and start again. The fact that the ligthlike vectors are a codimension 1 conic surface ensures that every linear subspace of dimension greater than or equal to 2 has an orthogonal basis where none of the basis elements are lightlike. • If j &gt; m and d ≥ m + 1 then the complement of the span of w 1 , . . , w m has dimension at least 2 and the same procedure can be applied: one can remove lightlike vectors without changing the subspace by replacing w m+1 , . . , w d+1 with a linear combination of them. • If m = d the procedure will not produce a lightlike vector as its last vector. This is because an orthogonal basis cannot have a lighlike vector. Each lightlike vector is orthogonal to d linearly independent vectors but one of those is itself. So if u d+1 is a lightlike vector w d+1 is in the span of w 1 , . . w d which contradicts our assumption that {w 1 , . . , w d+1 } is linearly independent.</p><p>This shows that given {v 1 , . . , v n } set of vectors spanning a linear space of dimension m &gt; 1,</p><p>If m = 1 and v 1 is lightlike, the continuity of h and the fact that the set of non-lightlike vectors are dense imply that h(v 1 ) ∈ span(v 1 ). Without the continuity of h this argument wouldn't work. This is because the set of lightlike vectors is invariant under the action of the Lorentz group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Permutation-invariant and equivariant functions that are also orthogonal or Lorentz-equivariant</head><p>Proof of Proposition 10: Let Π j→i = {σ ∈ S n : σ(j) = i}. We start by writing h as in <ref type="bibr" target="#b2">(3)</ref>. Using the invariance we average over the orbit of S n we obtain:</p><p>. For fixed j, the set of permutations Π j→i is stable (as a set) under post-composition with any permutation τ that fixes i. As a consequence, each summand σ∈Πj→i f j (v σ(1) , . . , v σ(n) ) is invariant under the action of any such permutation τ . As a consequence, fi is invariant with respect to the last n − 1 inputs, and h can be expressed as</p><p>using the notation v [−t] := (v 1 , . . , v t−1 , v t+1 , . . , v n ). We now show that all fi 's can be chosen to the same functions. In order to do so average over permutations again:</p><p>Therefore we define f such as</p><p>a O(d)-invariant function that is permutation invariant with respect to the last n − 1 inputs. The computation in <ref type="bibr" target="#b58">(59)</ref> shows that:</p><p>which proves the theorem.</p><p>Proof of Proposition 11: Let σ ∈ S n and (h 1 , . . ,</p><p>Since</p><p>-equivariant) we have that for all i there exists</p><p>Combining ( <ref type="formula">64</ref>) and ( <ref type="formula">65</ref>) we get</p><p>Taking t = σ −1 (j) in (66) we get t = j in (67) and matching coefficients we get</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Einstein summation notation</head><p>We can interpret the results from Section 3 as coming from the symmetries encoded in the Einstein summation rules.</p><p>An important early realization in differential geometry and in general relativity was that an enormous class of generally covariant forms can be written in a form that is commonly known (incorrectly perhaps) as "Einstein summation notation" <ref type="bibr" target="#b25">[26]</ref>; it is a subset of the prior Ricci calculus <ref type="bibr" target="#b76">[77]</ref>. This notation is a method for finding and checking equivariant quantities useful in physical laws: We imagine that we have O(d)-equivariant vectors u, v, w, and each has d components such that [u] i is the ith component of u. If we write products with repeated indices like [u] i [v] i , the sum rule is that the repeated index i is summed, so this corresponds to a O(d)-invariant scalar product (dot product or inner product) of the vectors u and v.</p><p>[</p><p>where the first := in (69) defines the summation notation and the second = relates this sum to the linear-algebra operation on two column-oriented vectors (d × 1 matrices). All indices appear either once (unsummed) or twice (summed) but never more than twice:</p><p>If there is one unsummed index in an expression, as in [u] i [v] i [w] j , then the result will be an O(d)equivariant vector:</p><p>If there are unsummed indices, then the expression is an O(d)-equivariant order-tensor.</p><p>This notation also reveals that the directionality of the output is entirely encoded in the input vectors and their combination. This is counter-intuitive given some physics expressions, such as the electromagnetic force law explained in <ref type="bibr" target="#b14">(15)</ref>; it contains a cross product that typically requires a particular coordinate system to determine the direction of the result. In this case, the direction of the force cannot depend on the coordinate system; the second cross product saves us, but it is not immediately clear how. With Einstein notation, we can express the vector triple product in a coordinate-free way. We first rewrite the d = 3 cross product (pseudo-vector product) a × b in terms of the maximally anti-symmetric rank-3 tensor in d = 3 (the Levi-Civita symbol) ijk :</p><p>The properties of the anti-symmetric tensor products are such that this product can be re-written as</p><p>where δ ij is the Kronecker delta. We thus see that the cross product can be expressed entirely in terms of scalar products (u w and v w) and the input vectors (v and u), and that this can only be done by employing the anti-symmetric tensor.</p><p>Einstein notation is often credited with making physical-law expressions more compact or brief. But what's important about the notation is that if the rules are obeyed, the notation can produce only equivariant objects in the theory.</p><p>Under these summation rules, all O(d)-invariant scalar expressions that can be made from polynomial expressions of vectors will include only terms that use even numbers of vectors. All these terms can be rearranged to be written as products of invariant scalar products. That is, any scalar function that can be written as a polynomial of vectors (or a function of such polynomials) can be written in terms only of available scalar products. This result corresponds very directly to Lemma 1. For concreteness, here is an example 4-vector scalar form, written in summation notation, reordered as an inner product of tensors or as a simple product of scalars:</p><p>On the other hand, all O(d)-equivariant vector expressions that can be made from polynomial expressions of vectors will include only terms that use odd numbers of vectors. These terms can be rearranged by the vector with the unsummed index. Once they are rearranged this way, the expression becomes the input vectors times polynomial expressions of scalar products. This demonstrates that any O(d)-equivariant vector expression constructible in the notation will lie in the subspace spanned by the input vectors. It also demonstrates that the O(d)-invariant scalar coefficients multiplying those vectors must themselves be constructible from polynomials (or functions of polynomials, it turns out) of scalar products. What is also true but to the best of our knowledge doesn't appear explicitly stated in the physics literature is that every O(d)-equivariant polynomial vector function can be expressed in Einstein notation. This result correspond very directly to Proposition 4.</p><p>When the metric is non-trivial, we must face the covariant/contravariant distinction, where vector components might be pre-multiplied by the metric or not. In this context, there are components [u] i of the covariant vectors and components [u] i of the contravariant vectors; the Einstein summation notation rules obtain the additional rule that repeated indices must belong to covariant-contravariant pairs:</p><p>where the [Λ] ij are the components of a d × d Hermitian metric tensor. In the case of the Lorentz group, d = 4 and the metric is diagonal, with elements (−1, 1, 1, 1) on the diagonal. In the case of the curved spacetime of general relativity, d = 4 and the metric is (in general) a function of spatial position and time.</p><p>We expect that these results will have generalizations for scalar, vector, and tensor functions of scalar, vector, and tensor inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Connections with low-rank matrix completion</head><p>Given a rank d, n × n matrix M , Example 4 in <ref type="bibr" target="#b71">[72]</ref> shows that M is almost always uniquely determined by the entries Ω(M ) := M i,i+s i = 1, . . , n, s = 0, . . , d, considering the indices "wrap around" (i.e., M i,n+s corresponds to M i,s = M i,n+s(mod n) ). In particular invariant functions f : (R d ) n → R , can be expressed as ( <ref type="formula">5</ref>)</p><p>where M is either V V or V Λ V . Therefore, any function g : Ω(M ) → R uniquely determines f on almost all possible inputs v 1 , . . , v n . This observation provides a parameterization for Lorentz and orthogonal-invariant and equivariant functions with a small number of scalars.</p><p>We note that the results from <ref type="bibr" target="#b71">[72]</ref> don't assume the matrix M is positive semi-definite, only that it is low rank. This is useful since M is not positive semi-definite in the Minkowski case. We also remark that the learning of the functions (3) and ( <ref type="formula">5</ref>) can be done from Ω(M ) without ever needing to compute all the scalars.</p><p>One disadvantage of the subset of scalars proposed in this section is that the sampling procedure Ω(M ) is not permutation invariant. One of our future goals is to find a set of permutation invariant scalars that are universally expressive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Model-building considerations</head><p>The results in Section 3 provide a simple characterization of all scalar functions and vector functions that satisfy important symmetries (see Table <ref type="table">1</ref>) for classical physics and special relativity. Here we discuss how we might use this to design and build a model.</p><p>First we need to identify the inputs, what groups are acting, and how the group action affects the inputs and outputs. For example, the translation group acts differently on position vectors than displacement vectors (see Table <ref type="table">2</ref>) than other kind of vectors (velocities, accelerations, fields, etc). Moreover, permutations don't typically act on the entire set of inputs but on tuples of inputs like the individual-particle charge, position, and velocity (q i , r i , v i ) in the electromagnetic example in Section 4.</p><p>The causal and group-action structure of the model must correspond to the structure of the physics problem. For example, suppose we are interested in solving an n-body problem in which we predict the trajectories of a set of interacting charged particles. The target functions h i (), which predict the position of particle i at time t, can be described as functions of that particle's charge, position, and velocity, and the set of charges, positions, and velocities of all the others. The group actions (rotation, translation, and permutation) can be written as:</p><p>h i (q i , Q r i , Q v i , (q j , Q r j , Q v j ) n j=1;j =i ) = Q h i (q i , r i , v i , (q j , r j , v j ) n j=1;j =i )</p><p>h i (q i , r i + w, v i , (q j , r j + w, v j ) n j=1;j =i ) = h i (q i , r i , v i , (q j , r j , v j ) n j=1;j =i ) + w (82)</p><p>h i (q i , r i , v i , (q σi(j) , r σi(j) , v σi(j) ) n j=1;σi(j) =i ) = h i (q i , r i , v i , (q j , r j , v j ) n j=1;j =i )</p><p>h σ(i) (q i , r i , v i , (q j , r j , v j ) n j=1;j =i ) = h i (q i , r i , v i , (q j , r j , v j ) n j=1;j =i ) ,</p><p>where σ is any permutation on n elements, and σ i is a permutation that fixes i, i.e., σ i (i) = i. The primary results of this paper imply that, in this n-body case, the rotation symmetries can be enforced by constructing the invariant scalar products, and building functions thereof. The translation symmetries can be enforced by taking positional differences with respect to r i prior to constructing the scalars. The first permutation symmetry <ref type="bibr" target="#b82">(83)</ref> can be enforced by working on a set-based neural network (or graph neural network), and the second permutation symmetry (84) can be enforced by taking all the functions h i = h to be identical. In general, graph-based or set-based methods are probably good frameworks for physics problems <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b79">80]</ref>, and the dynamics can probably be implemented with a form of message-passing.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A numerical algorithm for the explicit calculation of SU(N ) and SL(N, C) Clebsch-Gordan coefficients</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huckleberry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Von Delft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">23507</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cormorant: Covariant molecular neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Hy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Classifying radio galaxies with the convolutional neural network</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Aniyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Thorat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Astrophysical Journal Supplement Series</title>
		<imprint>
			<biblScope unit="volume">230</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2017-06">Jun 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Characterizing the expressive power of invariant and equivariant graph neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Azizian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lelarge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15646</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A short course on fast multipole methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Beatson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Greengard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wavelets, Multilevel Methods and Elliptic PDEs</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Machine learning for combinatorial optimization: a methodological tour d&apos;horizon</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prouvost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On the sample complexity of learning with geometric stability</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Venturi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07148</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Equivariant maps from invariant functions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Blum-Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lorentz group equivariant neural network for particle physics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bogatskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Offermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roussi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="992" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Bouritsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09252</idno>
		<title level="m">Improving graph neural network expressivity via subgraph isomorphism counting</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sampling using SU (N ) gauge equivariant flows</title>
		<author>
			<persName><forename type="first">D</forename><surname>Boyda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kanwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Racanière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Albergo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Hackett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Shanahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. D</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">74504</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. ArXiv, abs</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Velivckovi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2104.13478, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exact low-rank matrix completion via convex optimization</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">46th Annual Allerton Conference on Communication, Control, and Computing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="806" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Q</forename><surname>Cappart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chételat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09544</idno>
		<title level="m">Combinatorial optimization and reasoning with graph neural networks</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Can graph neural networks count substructures</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="10383" to="10395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Supervised community detection with line graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with gnns</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15868" to="15876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Symmetries and many-body excitations with neural-network quantum states</title>
		<author>
			<persName><forename type="first">K</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carleo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Regnault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Neupert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page">167204</biblScope>
			<date type="published" when="2018-10">Oct 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gauge equivariant convolutional networks and the icosahedral cnn</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kicanaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Greydanus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Spergel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04630</idno>
		<title level="m">Lagrangian neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rotation-invariant convolutional neural networks for galaxy morphology prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Willett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dambre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly Notices of the Royal Astronomical Society</title>
		<imprint>
			<biblScope unit="volume">450</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1441" to="1459" />
			<date type="published" when="2015-04">Apr 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving galaxy morphologies for sdss with deep learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huertas-Company</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuccillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly Notices of the Royal Astronomical Society</title>
		<imprint>
			<biblScope unit="volume">476</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="3661" to="3676" />
			<date type="published" when="2018-02">Feb 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">On the universality of rotation equivariant point cloud networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dym</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02449</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Die Grundlage der allgemeinen Relativitätstheorie</title>
		<author>
			<persName><forename type="first">A</forename><surname>Einstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annalen der Physik</title>
		<imprint>
			<biblScope unit="volume">354</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="769" to="822" />
			<date type="published" when="1916-01">Jan. 1916</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Provably strict generalisation benefit for equivariant models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Elesedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zaidi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10333</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Fenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shmakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-W</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09206</idno>
		<title level="m">Permutationless Many-Jet Event Reconstruction with Symmetry Preserving Attention Networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A practical method for constructing equivariant multilayer perceptrons for arbitrary matrix groups</title>
		<author>
			<persName><forename type="first">M</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09459</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Se (3)-transformers: 3d roto-translation equivariant attention networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Galaxy detection and identification using deep learning and data augmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Astronomy and Computing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="103" to="109" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Wallach</surname></persName>
		</author>
		<title level="m">Symmetry, representations, and invariants</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">255</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
				<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hamiltonian neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Greydanus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dzamba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Lorentz-and permutation-invariants of particles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gripaios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Haddadin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Lester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physics A: Mathematical and Theoretical</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page">155201</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep learning for 3d point clouds: A survey</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Haddadin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.12733</idno>
		<title level="m">Invariant polynomials and machine learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Traffic agent movement prediction using resnet-based model</title>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 6th International Conference on Intelligent Computing and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="136" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A catalog of visual-like morphologies in the 5 candels fields using deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Huertas-Company</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gravet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cabrera-Vives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Pérez-González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Kartaltepe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Barro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dimauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Astrophysical Journal Supplement Series</title>
		<imprint>
			<biblScope unit="volume">221</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015-10">Oct 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A new algorithm for computing branching rules and Clebsch-Gordan coefficients of unitary representations of compact groups</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ibort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">López</forename><surname>Yela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">101702</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Jackson</surname></persName>
		</author>
		<title level="m">Classical Electrodynamics</title>
				<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
	<note>2nd ed</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Variational gram functions: Convex analysis and optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jalali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fazel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2634" to="2661" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">An efficient graph convolutional network technique for the travelling salesman problem</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01227</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Erdos goes neural: an unsupervised learning framework for combinatorial optimization on graphs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Karalias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10643</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Physics-informed machine learning: case studies for weather and climate modelling</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kashinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society A</title>
		<imprint>
			<biblScope unit="volume">379</biblScope>
			<biblScope unit="page">20200093</biblScope>
			<date type="published" when="2021">2194. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Universal invariant and equivariant graph neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7092" to="7101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<author>
			<persName><surname>Gemnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08903</idno>
		<title level="m">Universal directional graph neural networks for molecules</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">N-body networks: a covariant hierarchical neural network architecture for learning atomic potentials</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01588</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Trivedi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02144</idno>
		<title level="m">Covariant compositional networks for learning graphs</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03542</idno>
		<title level="m">Zero-Permutation Jet-Parton Assignment using a Self-Attention Network</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks with a nonpolynomial activation function can approximate any function</title>
		<author>
			<persName><forename type="first">M</forename><surname>Leshno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schocken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="861" to="867" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Hamnet: Conformation-guided molecular representation with hamiltonian neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Machine learning strategies for systems with invariance properties</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Templeton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="page">318</biblScope>
			<date type="published" when="2016-05">05 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Reynolds averaged turbulence modelling using deep neural networks with embedded invariance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurzawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Templeton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Fluid Mechanics</title>
		<imprint>
			<biblScope unit="volume">807</biblScope>
			<biblScope unit="page" from="155" to="166" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Galaxy Zoo: morphologies derived from visual inspection of galaxies from the Sloan Digital Sky Survey*</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lintott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schawinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Slosar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Land</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Raddick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szalay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Andreescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly Notices of the Royal Astronomical Society</title>
		<imprint>
			<biblScope unit="volume">389</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1179" to="1189" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">What graph neural networks cannot learn: depth vs width</title>
		<author>
			<persName><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Gauge equivariant neural networks for quantum lattice gauge theories</title>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carleo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stokes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.07243</idno>
		<title level="m">Gauge Invariant Autoregressive Neural Networks for Quantum Lattice Models</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep lagrangian networks: Using physics as model prior for deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations (ICLR). ICLR</title>
				<imprint>
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Invariant and equivariant graph networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">On the universality of invariant networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Segol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4363" to="4371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">On learning sets of symmetric elements</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6734" to="6744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Misiakiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.13219</idno>
		<title level="m">Learning with invariances in random features and kernel models</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks. Association for the Advancement of Artificial Intelligence</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Relational pooling for graph representations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4663" to="4673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Invariant variation problems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Noether</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transport Theory and Statistical Physics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="186" to="207" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">A note on learning algorithms for quadratic assignment with graph neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Bandeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.07450</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A characterization of deterministic sampling patterns for low-rank matrix completion</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Pimentel-Alarcón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="623" to="636" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Invariant theory</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Vinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algebraic geometry IV</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="123" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Pointnet++ deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5105" to="5114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Equivariance through parameter-sharing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Méthodes de calcul différentiel absolu et leurs applications</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M G</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Levi-Civita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematische Annalen</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="125" to="201" />
			<date type="published" when="1900">1900</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Rigid and flexible frameworks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Mathematical Monthly</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6" to="21" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Macdonald</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05085</idno>
		<title level="m">Group Convolutional Neural Networks Improve Quantum State Accuracy</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12790</idno>
		<title level="m">Hamiltonian Graph Networks with ODE Integrators</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">E(n) equivariant graph neural networks</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09844</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Equivariant message passing for the prediction of tensorial properties and molecular spectra</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">T</forename><surname>Unke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gastegger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Uniqueness of low-rank matrix completion by rigidity theory</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cucuringu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1621" to="1641" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Tahmasebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03174</idno>
		<title level="m">Counting substructures with higher-order graph neural networks: Possibility and impossibility results</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08219</idno>
		<title level="m">Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Numerical linear algebra</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Trefethen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname><genName>III</genName></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">50</biblScope>
			<pubPlace>Siam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Restricted boltzmann machines for quantum states with non-abelian or anyonic symmetries</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vieijra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Casert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>De Neve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Haegeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ryckebusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Verstraete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2020-03">Mar 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Towards physics-informed deep learning for turbulent flow prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kashinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1457" to="1466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Incorporating symmetry into deep dynamics models for improved generalization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Walters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Duan</surname></persName>
		</author>
		<title level="m">Symmetry-adapted graph neural networks for constructing molecular dynamics force fields</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Forré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Verlinde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">Coordinate independent convolutional networks -isometry and gauge equivariant convolutions on riemannian manifolds</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">The Classical Groups</title>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1946">1946</date>
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Experimental performance of graph neural networks on random instances of max-cut</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Bandeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Optics and Photonics</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11138</biblScope>
			<biblScope unit="page">111380S</biblScope>
		</imprint>
	</monogr>
	<note>Wavelets and Sparsity XVIII</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">A simple equivariant machine learning method for dynamics based on scalars</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Storey-Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Hogg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Universal approximations of invariant maps by neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yarotsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10306</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Physics-guided ai for large-scale spatiotemporal data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpatne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;21</title>
				<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4088" to="4089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3394" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Rotation invariant convolutions for 3d point clouds deep learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="204" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Quaternion equivariant capsule networks for 3d point clouds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Menegatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Symplectic ode-net: Learning hamiltonian dynamics with control</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">D</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations (ICLR). ICLR</title>
				<imprint>
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
