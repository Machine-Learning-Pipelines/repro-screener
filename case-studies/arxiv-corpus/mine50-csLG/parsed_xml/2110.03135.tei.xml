<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Label Noise in Adversarial Training: A Novel Perspective to Study Robust Overfitting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chengyu</forename><surname>Dong</surname></persName>
							<email>cdong@eng.ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
							<email>lucliu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
							<email>jshang@eng.ucsd.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Label Noise in Adversarial Training: A Novel Perspective to Study Robust Overfitting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0C1703D47E2A8B1DEE60D8E6B0F556F3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-10-31T05:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show that label noise exists in adversarial training. Such label noise is due to the mismatch between the true label distribution of adversarial examples and the label inherited from clean examples -the true label distribution is distorted by the adversarial perturbation, but is neglected by the common practice that inherits labels from clean examples. Recognizing label noise sheds insights on the prevalence of robust overfitting in adversarial training, and explains its intriguing dependence on perturbation radius and data quality. Also, our label noise perspective aligns well with our observations of the epoch-wise double descent in adversarial training. Guided by our analyses, we proposed a method to automatically calibrate the label to address the label noise and robust overfitting. Our method achieves consistent performance improvements across various models and datasets without introducing new hyper-parameters or additional tuning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Adversarial training <ref type="bibr" target="#b13">(Goodfellow et al., 2015;</ref><ref type="bibr" target="#b19">Huang et al., 2015;</ref><ref type="bibr" target="#b24">Kurakin et al., 2017;</ref><ref type="bibr" target="#b27">Madry et al., 2018)</ref> is known as one of the most effective ways <ref type="bibr" target="#b1">(Athalye et al., 2018;</ref><ref type="bibr" target="#b43">Uesato et al., 2018)</ref> to enhance the adversarial robustness of deep neural networks <ref type="bibr" target="#b41">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b13">Goodfellow et al., 2015)</ref>. It augments training data with adversarial perturbations to prepare the model for adversarial attacks. Despite various efforts to generate more effective adversarial training examples <ref type="bibr" target="#b8">(Ding et al., 2020;</ref><ref type="bibr">Zhang et al., 2020)</ref>, the labels assigned to them attracts little attention. As the common practice, the assigned labels of adversarial training examples are simply inherited from their clean counterparts.</p><p>In this paper, we argue that the existing labeling practice of the adversarial training examples introduces label noise implicitly, since adversarial perturbation can distort the data semantics <ref type="bibr" target="#b42">(Tsipras et al., 2019;</ref><ref type="bibr" target="#b20">Ilyas et al., 2019)</ref>. For example, as illustrated in Figure <ref type="figure" target="#fig_1">1</ref>, even with a slight distortion of the data semantics (e.g., more ambiguous), the label distribution of the adversarially perturbed data may not match the label distribution of the clean counterparts. Such distribution shift is neglected when assigning labels to adversarial examples, which are directly copied from the clean counterparts. We observe that distribution mismatch caused by adversarial perturbation along with improper labeling practice will cause label noise in adversarial training.</p><p>It is a mysterious and prominent phenomenon that the robust test error would start to increase after conducting adversarial training for a certain number of epochs <ref type="bibr" target="#b35">(Rice et al., 2020)</ref>, and our label noise perspective provides an adequate explanation for this phenomenon. Specifically, from a classic bias-variance view of model generalization, label noise that implicitly exists in adversarial training 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2110.03135v2 <ref type="bibr">[cs.</ref>LG] 19 Oct 2022</p><p>x &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L E v 3 u H 6 8 + s x E 6 S s T F 8 E G G G G F g D w = " &gt; A A A B 6 H i c b Z D J S g N B E I Z r 4 h b j F p e b l 8 Y g e A o z I q g n A x 7 0 m I B Z I B l C T 6 c m a d O z 0 N 0 j x i F P 4 M W D I l 5 9 A E 8 + i T e P v o m d 5 a D R H x o + / r + K r i o v F l x p 2 / 6 0 M n P z C 4 t L 2 e X c y u r a + k Z + c 6 u m o k Q y r L J I R L L h U Y W C h 1 j V X A t s x B J p 4 A m s e / 3 z U V 6 / Q a l 4 F F 7 p Q Y x u Q L s h 9 z m j 2 l i V 2 3 a + Y B f t s c h f c K Z Q O H u / + 7 p 4 2 0 n L 7 f x H q x O x J M B Q M 0 G V a j p 2 r N 2 U S s 2 Z w G G u l S i M K e v T L j Y N h j R A 5 a b j Q Y d k 3 z g d 4 k f S v F C T s f u z I 6 W B U o P A M 5 U B 1 T 0 1 m 4 3 M / 7 J m o v 0 T N + V h n G g M 2 e Q j P x F E R 2 S 0 N e l w i U y L g Q H K J D e z E t a j k j J t b p M z R 3 B m V / 4 L t c O i c 1 Q 8 r d i F k g 0 T Z W E X 9 u A A H D i G E l x C G a r A A O E e H u H J u r Y e r G f r Z V K a s a Y 9 2 / B L 1 u s 3 C J + Q x A = = &lt; / l a t e x i t &gt;</p><p>x 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F 6 7 b y r 8 e l Q k x M 0 y 7 R X 7 3 I S 5 s d I 0 = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 6 K o k U 1 F v B i 8 c q 9 g P a U D b b T b t 0 s w m 7 E 7 G E / g M v H h T x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K Q w 6 L r f z s r q 2 v r G Z m G r u L 2 z u 7 d f O j h s m j j V j D d Y L G P d D q j h U i j e Q I G S t x P N a R R I 3 g p G N 1 O / 9 c i 1 E b F 6 w H H C / Y g O l A g F o 2 i l + 6 e z X q n s V t w Z y D L x c l K G H P V e 6 a v b j 1 k a c Y V M U m M 6 n p u g n 1 G N g k k + K X Z T w x P K R n T A O 5 Y q G n H j Z 7 N L J + T U K n 0 S x t q W Q j J T f 0 9 k N D J m H A W 2 M 6 I 4 N I v e V P z P 6 6 Q Y X v m Z U E m K X L H 5 o j C V B G M y f Z v 0 h e Y M 5 d g S y r S w t x I 2 p J o y t O E U b Q j e 4 s v L p H l R 8 a q V 6 7 t q u e b m c R T g G E 7 g H D y 4 h B r c Q h 0 a w C C E Z 3 i F N 2 f k v D j v z s e 8 d c X J Z 4 7 g D 5 z P H 0 U 2 j S c = &lt; / l a t e x i t &gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assigned Label True Label</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P (Y |x)</head><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E Y J E Z 2 a O b 0 w 9 F + j r x u Y + 7 2 U s 7 d M = " &gt; A A A B 7 X i c b V C 7 S g N B F L 0 b X z G + V u 2 0 G Q x C b M K u C G o X s L G w S M A 8 J F n C 7 G Q 2 G T M 7 u 8 z M i m H N F 9 j Y W C h i 6 4 f 4 B 3 b + h Z / g 5 F F o 9 M C F w z n 3 c u 8 9 f s y Z 0 o 7 z a W X m 5 h c W l 7 L L u Z X V t f U N e 3 O r p q J E E l o l E Y 9 k w 8 e K c i Z o V T P N a S O W F I c + p 3 W / f z b y 6 z d U K h a J S z 2 I q R f i r m A B I 1 g b q V Y u X N 3 d H r T t v F N 0 x k B / i T s l + Z J d + X q / 2 L k v t + 2 P V i c i S U i F J h w r 1 X S d W H s p l p o R T o e 5 V q J o j E k f d 2 n T U I F D q r x 0 f O 0 Q 7 R u l g 4 J I m h I a j d W f E y k O l R q E v u k M s e 6 p W W 8 k / u c 1 E V h F e B g B 9 p g A q n h f E 0 I F 0 7 d i 2 i W C U K W D y u k Q 7 O m X Z 0 n t s G g f F U 8 r O g 0 L j Z F F u 2 g P F Z C N j l E J n a M y q i K K B H p E z + j F u D W e j F f j b d y a M S Y z 2 + g P j P c f q I O V l Q = = &lt; / l a t e x i t &gt; P (Y 0 |x 0 ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p Q 7 w U Y q s 1 v x R Q O e B N m b 3 x w A Q T c U = " &gt; A A A B 7 3 i c b V C 7 S g N B F L 3 r M 8 b X q p 0 2 g 0 E S m 7 A r g</p><formula xml:id="formula_0">t o F b C w s E j A P S Z Y w O 5 k k Q 2 Z n 1 5 l Z M a z 5 A j s b C 0 V s / Q 7 / w M 6 / 8 B O c P A p N P H D h c M 6 9 3 H u P H 3 G m t O N 8 W X P z C 4 t L y 6 m V 9 O r a + s a m v b V d U W E s C S 2 T k I e y 5 m N F O R O 0 r J n m t B Z J i g O f 0 6 r f O x / 6 1 V s q F Q v F l e 5 H 1 A t w R 7 A 2 I 1 g b q V b M X W f v 7 7 K H T T v j 5 J 0 R 0 C x x J y R T s E v f H 5 e 7 D 8 W m / d l o h S Q O q N C E Y 6 X q r h N p L 8 F S M 8 L p I N 2 I F Y 0 w 6 e E O r R s q c E C V l 4 z u H a A D o 7 R Q O 5 S m h E Y j 9 f d E g g O l + o F v O g O s u 2 r a G 4 r / e f V Y t 0 + 9 h I k o 1 l S Q 8 a J 2 z J E O 0 f B 5 1 G K S E s 3 7 h m A i m b k V k S 6 W m G g T U d q E 4 E 6 / P E s q R 3 n 3 O H 9 W M m k 4 M E Y K 9 m A f c u D C C R T g A o p Q B g I c H u E Z X q w b 6 8 l 6 t d 7 G r X P W Z G Y H / s B 6 / w E A y Z J B &lt; / l a t e x i t &gt; P ( Ỹ 0 |x 0 ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A h q i c u Z y v R Z 3 d m j d 8 V B T e v D 9 h u M = " &gt; A A A B + X i c b V D L S s N A F J 3 4 r P U V d a e b Y J H W T U l E U H c F N y 5 c t G A f 0 o Y y m U z a o Z N J m L k p l t g v 8 B f c u F D E r Z / g H 7 j z L / w E p 4 + F t h 6 4 c D j n X u 6 9 x 4 s 5 U 2 D b X 8 b C 4 t L y y m p m L b u + s b m 1 b e 7 s 1 l S U S E K r J O K R b H h Y U c 4 E r Q I D T h u x p D j 0 O K 1 7 v c u R X + 9 T q V g k b m A Q U z f E H c E C R j B o q W 2 a 5 U I L G P d p e j v M 3 9 / l j 9 t m z i 7 a Y 1 j z x J m S X M m s f H 9 c 7 z + U 2 + Z n y 4 9 I E l I B h G O l m o 4 d g 5 t i C Y x w O s y 2 E k V j T H q 4 Q 5 u a C h x S 5 a b j y 4 f W k V Z 8 K 4 i k L g H W W P 0 9 k e J Q q U H o 6 c 4 Q Q 1 f N e i P x P 6 + Z Q H D u p k z E C V B B J o u C h F s Q W a M Y L J 9 J S o A P N M F E M n 2 r R b p Y Y g I 6 r K w O w Z l 9 e Z 7 U T o r O a f G i o t O w 0 Q Q Z d I A O U Q E 5 6 A y V 0 B U q o y</formula><p>o i q I 8 e 0 T N 6 M V L j y X g 1 3 i a t C 8 Z 0 Z g / 9 g f H + A + r B l i g = &lt; / l a t e x i t &gt; can increase the model variance <ref type="bibr" target="#b47">(Yang et al., 2020)</ref> and thus make the overfitting much more evident compared to standard training. Further analyses of label noise in adversarial training also explain the intriguing dependence of robust overfitting on the perturbation radius <ref type="bibr" target="#b10">(Dong et al., 2021b</ref>) and data quality <ref type="bibr" target="#b9">(Dong et al., 2021a)</ref> presented in the literature. Figure <ref type="figure">2</ref>: Robust overfitting can be viewed as an early part of the epochwise double descent. We employ PGD training <ref type="bibr" target="#b27">(Madry et al., 2018)</ref> on <ref type="bibr">CIFAR-10 (Krizhevsky, 2009)</ref> with Wide ResNet (WRN) <ref type="bibr" target="#b49">(Zagoruyko &amp; Komodakis, 2016)</ref> and a fixed learning rate. WRN-28-k refers to WRN with depth 28 and widen factor k.</p><p>Providing the label noise in adversarial training, one can further expect the existence of double descent based on the modern generalization theory of deep neural networks. Epoch-wise double descent refers to the phenomenon that the test error will first decrease and then increase as predicted by the classic bias-variance tradeoff, but it will decrease again as the training continues. Such phenomenon is only reported in standard training of deep neural networks, often requiring significant label noise in the training set <ref type="bibr" target="#b29">(Nakkiran et al., 2020)</ref>.</p><p>As the label noise intrinsically exists in adversarial training, such epoch-wise double descent phenomenon also emerges when the training goes longer. Indeed, as shown in Figure <ref type="figure">2</ref>, for a relatively large model such as WRN-28-5, on top of the existing robust overfitting phenomenon, the robust test error will eventually decrease again after 1, 000 epochs. Following <ref type="bibr" target="#b29">Nakkiran et al. (2020)</ref>, we further experiment different model sizes. One can find that a medium-sized model will follow a classic U-curve, which means only overfitting is observed; and the robust test error for a small model will monotonically decrease. These are well aligned with the observations in standard training regime. This again consolidates our understanding of label noise in adversarial training.</p><p>In light of our analyses, we design a theoretically-grounded method to mitigate the label noise in adversarial training automatically. The key idea is to resort to an alternative labeling of the adversarial examples. We show that the predictive label distribution of an adversarially trained probabilistic classifier can approximate the true label distribution with high probability. Thus it can be utilized as a better labeling of the adversarial examples and provably reduce the label noise. We also show that with proper temperature scaling and interpolation, such predictive label distribution can further reduce the label noise. This echoes the recent empirical practice of incorporating knowledge distillation <ref type="bibr" target="#b18">(Hinton et al., 2015)</ref> into adversarial training <ref type="bibr" target="#b5">(Chen et al., 2021)</ref>. While previous works heuristically select fixed scaling and interpolation parameters for knowledge distillation, we show that it is possible to fully unleash the potential of knowledge distillation by automatically determining the set of parameters that maximally reduces the label noise, with a strategy similar to confidence calibration <ref type="bibr" target="#b15">(Guo et al., 2017)</ref>. Such strategy can further mitigate robust overfitting to a minimal amount without additional human tuning effort. Extensive experiments on different datasets, training methods, neural architectures and robustness evaluation metrics verify the effectiveness of our method.</p><p>In summary, our findings and contributions are: 1) we show that the labeling of adversarial examples in adversarial training practice introduces label noise implicitly; 2) we show that robust overfitting can be adequately explained by such label noise, and it is the early part of an epoch-wise double descent; 3) 2e show an alternative labeling of the adversarial examples can be established to provably reduce the label noise and mitigate the robust overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Robust overfitting and double descent in adversarial training. Double descent refers to the phenomenon that overfitting by increasing model complexity will eventually improve test set performance <ref type="bibr" target="#b31">(Neyshabur et al., 2017;</ref><ref type="bibr" target="#b2">Belkin et al., 2019</ref>). This appears to conflict with the robust overfitting phenomenon in adversarial training, where increasing model complexity by training longer will impair test set performance constantly after a certain point during training. It is thus believed in the literature that robust overfitting and epoch-wise double descent are separate phenomena <ref type="bibr" target="#b35">(Rice et al., 2020)</ref>. In this work we show this is not the complete picture by conducting adversarial training for exponentially more epochs than the typical practice.</p><p>A recent work also considers a different notion of double descent that is defined with respect to the perturbation size <ref type="bibr" target="#b48">(Yu et al., 2021)</ref>. Such double descent might be more related to the robustnessaccuracy trade-off problem <ref type="bibr" target="#b32">(Papernot et al., 2016;</ref><ref type="bibr" target="#b40">Su et al., 2018;</ref><ref type="bibr" target="#b42">Tsipras et al., 2019;</ref><ref type="bibr" target="#b51">Zhang et al., 2019)</ref>, rather than the classic understanding of double descent based on model complexity.</p><p>Mitigate robust overfitting. Robust overfitting hinders the practical deployment of adversarial training methods as the final performance is often sub-optimal. Various regularization methods including classic approaches such as 1 and 2 regularization and modern approaches such as cutout <ref type="bibr" target="#b7">(Devries &amp; Taylor, 2017)</ref> and mixup <ref type="bibr" target="#b50">(Zhang et al., 2018)</ref> have been attempted to tackle robust overfitting, whereas they are shown to perform no better than simply early stopping the training on a validation set <ref type="bibr" target="#b35">(Rice et al., 2020)</ref>. However, early stopping raises additional concern as the best checkpoint of the robust test accuracy and that of the standard accuracy often do not coincide <ref type="bibr" target="#b5">(Chen et al., 2021)</ref>, thus inevitably sacrificing the performance on either criterion. Various regularization methods specifically designed for adversarial training are thus proposed to outperform early stopping, including regularization the flatness of the weight loss landscape <ref type="bibr" target="#b46">(Wu et al., 2020;</ref><ref type="bibr" target="#b39">Stutz et al., 2021)</ref>, introducing low-curvature activation functions <ref type="bibr" target="#b37">(Singla et al., 2021)</ref>, data-driven augmentations that adds high-quality additional data into the training <ref type="bibr" target="#b34">(Rebuffi et al., 2021)</ref> and adopting stochastic weight averaging <ref type="bibr" target="#b21">(Izmailov et al., 2018)</ref> and knowledge distillation <ref type="bibr" target="#b18">(Hinton et al., 2015)</ref>  <ref type="bibr" target="#b5">(Chen et al., 2021)</ref>. These methods are likely to suppress the label noise in adversarial training, with the self-distillation framework (i.e. the teacher shares the same architecture as the student model) introduced by <ref type="bibr" target="#b5">(Chen et al., 2021)</ref> as a particular example since introducing teacher's outputs as supervision is almost equivalent to the alternative labeling inspired by our understanding of the origin of label noise in adversarial training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>A statistic model of label noise <ref type="bibr" target="#b11">(Frénay &amp; Verleysen, 2014)</ref>. Let X ⊂ R d define the input space equipped with a norm • : X → R + and Y = [K] := {1, 2, . . . , K} define the label space. We introduce four random variables to describe noisy labeling process. Let X ∈ X denote the input, Y ∈ Y denote the true label of the input, Ỹ ∈ Y denote the assigned label of an input provided by an annotator, and finally E denote the occurrence of a label error by this annotator. E is a binary random variable with value 1 indicating that the assigned label is different from the true label for a given input, i.e., E = 1( Ỹ = Y ). We study the case where the label error depends on both the input X and the true label Y . For a classification problem, a training set consists of a set of examples that are sampled as</p><formula xml:id="formula_1">D = {(x i , ỹi )} i∈[N ] .</formula><p>Definition 3.1 (Label noise). We define label noise p e in a training set D as the empirical measure of the label error, namely p e (D) = 1/N i∈[N ] 1(ỹ i = y i ).</p><p>Assumption 3.1. We assume the annotation of a clean dataset involves no label error, namely P (E = 1|Y = y, x) = 0. This directly implies P ( Ỹ |x) = P (Y |x) (see proof in the Appendix). Definition 3.2 (Data quality). Given a training set D, we define its data quality as q(D) = E (x,y)∈D P (Y = y|x) Adversarially augmented training set.</p><p>Let f : X → Y be a probabilistic classifier and f (•) j be its predictive probability at class j. The adversarial example of x generated by f is obtained by solving the maximization problem x = arg max z∈Bε(x) (f (z), y). Here can be a typical loss function such as cross-entropy. And B ε (x) denotes the norm ball centered at x with radius ε, i.e., B ε (x) = {z ∈ X : z − x ≤ ε}.</p><p>Following previous notations, we denote Y as a random variable representing the true label of x and Ỹ as a random variable representing the assigned label of x . We refer D = {(x , ỹ )} as the adversarially augmented training set.</p><p>Adversarial training. Adversarial training can be viewed as a data augmentation technique that trains the parametric classifier f θ on the adversarially augmented training set <ref type="bibr" target="#b42">(Tsipras et al., 2019)</ref>, namely</p><formula xml:id="formula_2">θ * = arg min θ 1 |D | (x ,ỹ )∈D (f θ (x ), ỹ ).</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Label noise implicitly exists in adversarial training</head><p>In this section, we demonstrate the implicit existence of label noise in the adversarially augmented training set. We first consider a simple case where the adversarial perturbation is generated based on an ideal classifier that predicts the true label distribution. Under such a case we prove that the label noise in the adversarially augmented training set is lower-bounded. We then show that in realistic cases an adversarially trained classifier can approximate the true label distribution with high probability. Therefore, additional error terms will be required to lower bound the label noise. All proofs for the remainder of this paper are provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">When adversarial perturbation is generated by the true probabilistic classifier</head><p>We first consider an ideal case where the adversarial perturbation is generated by the true probabilistic classifier f (x) := P (Y |x), namely the classifier producing the true label distribution on any input x.</p><p>The true label distribution is distorted by adversarial perturbation. We quantify the mismatch between two probability distributions using the total variation (TV) distance.</p><p>Definition 4.1 (TV distance). Let A be a collection of the subsets of the label sample space Y. The TV distance between two probability distributions P (Y ) and P (Y ) can be defined as</p><formula xml:id="formula_3">P (Y ) − P (Y ) TV = sup J∈A |P (Y ∈ J) − P (Y ∈ J)|.</formula><p>We now show that adversarial perturbation generated by the true probabilistic classifier can induce a mismatch between the true label distributions of clean inputs and their adversarial examples.</p><p>For simplicity we consider adversarial perturbation based on FGSM and cross-entropy loss, namely</p><formula xml:id="formula_4">x = x−ε ∇ f (x) y −1 ∇ f (x) y .</formula><p>The distribution mismatch induced by such adversarial perturbation can be lower bounded.</p><formula xml:id="formula_5">Lemma 4.1. Assume f (x) y is L-locally Lipschitz around x with bounded Hessian. Let σ m = inf z∈Bε(x) σ min (∇ 2 f (z) y ) &gt; 0 and σ M = sup z∈Bε(x) σ max (∇ 2 f (z) y ) &gt; 0.</formula><p>Here σ min and σ max denote the minimum and maximum eigenvalues of the Hessian, respectively. We then have</p><formula xml:id="formula_6">P (Y |x) − P (Y |x ) TV ≥ ε 2 (1 − f (x) y ) σ m L − ε 2 4 σ M ,<label>(2)</label></formula><p>One can find that the right-hand side is positive as long as the upper bound of the Hessian norm is not too large, which is reasonable as previous works have shown that small hessian norm is critical to both standard <ref type="bibr" target="#b22">(Keskar et al., 2017)</ref> and robust generalization <ref type="bibr" target="#b28">(Moosavi-Dezfooli et al., 2019)</ref>.</p><p>Assigned label distribution is unchanged.</p><p>Despite the fact that the true label distribution is distorted by adversarial perturbation, we note that the assigned label distribution of adversarial examples is still the same as their clean counterparts.</p><p>Remark 4.1. In adversarial training, it is the common practice that directly copies the label of a clean input to its adversarial counterpart, namely ỹ = ỹ and P ( Ỹ |x ) = P ( Ỹ |x). </p><formula xml:id="formula_7">= {(x i , ỹi )} i∈[N ]</formula><p>, the label noise is lower-bounded by the mismatch between the true label distribution and the assigned label distribution. Specifically, with probability 1 − δ, we have </p><formula xml:id="formula_8">p e (D) ≥ E x P ( Ỹ |x) − P (Y |x) TV − 1 2N log 2 δ<label>(3)</label></formula><formula xml:id="formula_9">p e (D ) ≥ ε 2 (1 − q(D)) σ m L − ε 2 4 σ M − 1 2N log 2 δ (4)</formula><p>The above results suggest that as long as a training set is augmented by adversarial perturbation, but with assigned labels unchanged, label noise emerges. We demonstrate this by showing that standard training on a fixed adversarially augmented training set can also produce overfitting. Specifically, for each example in a clean training set we apply adversarial perturbation generated by a adversarially trained classifier. We then fix such an augmented training set and conduct standard training on it. We experiment on CIFAR-10 with WRN-28-5. A training subset of size 5k is randomly sampled to speed up the training. More details about the experiment settings can be found in the appendix. Figure <ref type="figure">3</ref> shows that prominent overfitting (as well as epoch-wise double descent) can be observed when the perturbation radius is relatively large.</p><p>On the other hand, if a training set is augmented by perturbation that will not distort the true label distribution, there will not be label noise. We demonstrate this by showing that standard training on a training set augmented with Gaussian noise will not induce overfitting. As shown in Figure <ref type="figure">4</ref>, even with a extremely large radius of Gaussian perturbation, no overfitting is observed. This also demonstrates that input perturbation not necessarily leads to overfitting.</p><p>Intuitive interpretation of label noise in adversarial training. We introduce a simple example to help understand the emergence of label noise in adversarial training. We now construct an adversarially augmented training subset</p><formula xml:id="formula_10">D = {(x i , ỹ i )} i∈[N ]</formula><p>, where ỹ = y and x is generated based on adversarial perturbation that distorts the true label distribution symmetrically. Specifically,</p><formula xml:id="formula_11">P (Y = j |x ) = 1 − η, if j = y, η/(K − 1), otherwise.</formula><p>Then by Lemma 4.2 we have p e (D ) η.</p><p>One can find that there is indeed η faction of noisy labels in D . This is because if we sample the labels of x based on its true label distribution, we expect 1 − η faction of x are labeled as y, while η fraction of x are labeled to be other classes. However, in D , all x are assigned with label y , which means η fraction of x are labeled incorrectly. In realistic datasets we can consider inputs with similar features for such reasoning.</p><p>The above example also shows that label noise in adversarial training may be stronger than one's impression. Even a slight distortion of the true label distribution, e.g. η = 0.1, will be equivalent to at least 10% noisy label in the training set. This is because the true label distribution of every training input is distorted, resulting in significant noise in the population.</p><p>Dependence of label noise in adversarial training. Theorem 4.3 shows that the label noise in adversarial training is proportional to (1) the perturbation radius (2) the data quality. Considering label noise can be an important source of variance in the generalization of deep neural networks <ref type="bibr" target="#b29">(Nakkiran et al., 2020;</ref><ref type="bibr" target="#b47">Yang et al., 2020)</ref>, such dependence of label noise explains the intriguing observations in the literature that robust overfitting (or epoch-wise double descent) in adversarial training will vanish with small perturbation radii <ref type="bibr" target="#b10">(Dong et al., 2021b)</ref> or high-quality data <ref type="bibr" target="#b9">(Dong et al., 2021a)</ref>. We conduct more controlled experiments to verify this correlation empirically, as shown in Figure <ref type="figure" target="#fig_5">5</ref>, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Adversarial perturbation generated by a realistic classifier</head><p>We now consider a realistic case where the adversarial perturbation is generated by a probabilistic classifier f θ .</p><p>Approximation of the true label distribution. We show that after sufficient adversarial training, the predictive label distribution of f θ can approximate the true label distribution with high probability. Lemma 4.4. Denote S = {x : (x, y) ∈ D} as the collection of all training inputs. Let ρ ≥ 1 and C be an ρε-external covering of S with covering number N ρε . Let f θ be a probabilistic classifier that minimizes the adversarial empirical risk (1). Assume f θ is L θ -locally Lipschitz continuous in a norm ball of radius ρε around x ∈ C. Let κ ≥ 1 and Ŝ be a subset of S with cardinality at least</p><formula xml:id="formula_12">(1−1/κ+1/(κN ρε ))N . Let N ε ( Ŝ) denote the neighborhood of the set Ŝ, i.e. N ε ( Ŝ) = x∈ Ŝ B ε (x).</formula><p>Then for any x ∈ N ε ( Ŝ), with probability at least 1 − δ,</p><formula xml:id="formula_13">f θ (x) − P (Y |x) TV ≤ κN ρε K 2N log 2 δ + 3 2 − 1 K L θ + L ρε,<label>(5)</label></formula><p>Label noise in adversarial training with a realistic classifier. Adversarial perturbation generated by a realistic classifier f θ will distort its predictive label distribution by gradient ascent. Subsequently, the true label distribution will also be distorted with high probability since the predictive label distribution of a realistic classifier f θ can approximate the true label distribution. Specifically, by the triangle inequality we have</p><formula xml:id="formula_14">P (Y |x)−P (Y |x ) TV ≥ f θ (x)−f θ (x ) TV −( f θ (x)−P (Y |x) TV + f θ (x )−P (Y |x ) TV ),<label>(6)</label></formula><p>where the last two terms are the approximation error of true label distribution on both clean and adversarial examples, which are guaranteed to be small. To conclude, we have the following result.</p><p>Theorem 4.5. Instantiate the notations of Lemma 4.4. For any x ∈ N ε ( Ŝ), with probability at least 1 − 3δ, we have</p><formula xml:id="formula_15">p e (D ) ≥ ε (1 − E x f θ (x) y ) σ m 2L θ − 2ρ 3 2 − 1 K L θ + L − ε 2 σ M 4 − ξ 1 2N log 2 δ ,<label>(7)</label></formula><p>where ξ = 1 + 4κN ρε K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Mitigate Label Noise in Adversarial Training</head><p>Since the label noise is incurred by the mismatch between the true label distribution and assigned label distribution of adversarial examples in the training set, we wish to find an alternative label (distribution) for the adversarial example to reduce such distribution mismatch. We've already shown that the predictive label distribution of a classifier trained by conventional adversarial training, which we denote as model probability in the following discussion, can in fact approximate the true label distribution. Here we show that it is possible to further improve the predictive label distribution and reduce the label noise by calibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Rectify model probability to reduce distribution mismatch</head><p>We show that it is possible to reduce the distribution mismatch by temperature scaling <ref type="bibr" target="#b18">(Hinton et al., 2015;</ref><ref type="bibr" target="#b15">Guo et al., 2017)</ref> enabled in the softmax function.</p><p>Theorem 5.1 (Temperature scaling can reduce the distribution mismatch). Let f θ (x; T ) denote the predictive probability of a probabilistic classifier scaled by temperature T , namely f θ (x; T ) j = exp(z j /T )/( j exp(z j /T )), where z is the logits of the classifier from x. Let x be an adversarial example correctly classified by a classifier f θ , i.e. arg max j f θ (x ) j = y , then there exists T , such that</p><formula xml:id="formula_16">f θ (x ; T ) − P (Y |x ) T V ≤ f θ (x ) − P (Y |x ) T V .</formula><p>Another way to further reduce the distribution mismatch is to interpolate between the model probability and the one-hot assigned label. We show that the interpolation works specifically for incorrectly classified examples and thus can be viewed as a complement to temperature scaling. Theorem 5.2 (Interpolation can further reduce the distribution mismatch). Let x be an adversarial example incorrectly classified by a classifier f θ , i.e. arg max j f θ (x ; T ) j = y . Assume max j P (Y = j|x ) ≥ 1/2, then there exists an interpolation ratio λ, such that</p><formula xml:id="formula_17">f θ (x ; T, λ) − P (Y |x ) T V ≤ f θ (x ; T ) − P (Y |x ) TV , where f θ (x ; T, λ) = λ • f θ (x ; T ) + (1 − λ) • P ( Ỹ |x ).</formula><p>As a summarization, to reduce the distribution mismatch, we propose to use f θ (x ; T, λ) as the assigned label of the adversarial example in adversarial training, which we refer as the rectified model probability.</p><p>In Appendix E, we show that the optimal hyper-parameters (i.e. T and λ) of almost all training examples concentrate on the same set of values by studying on a synthetic dataset with known true label distribution. Therefore it is possible to find an universal set of hyper-parameters that reduce the distribution mismatch for all adversarial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Determine the optimal temperature and interpolation ratio</head><p>The set of temperature and interpolation ratio in the rectified model probability that maximally reduces the distribution mismatch is not straightforward to find as the true label distribution of the adversarial example is unknown in reality. Fortunately, given a sufficiently large validation dataset as a whole, it is possible to measure the overall distribution mismatch in a frequentist's view without knowing the true label distribution of every single example. A popular metric adopted here is the negative log-likelihood (NLL) loss, which is known as a proper scoring rule <ref type="bibr" target="#b12">(Gneiting &amp; Raftery, 2007)</ref> and is also employed in the confidence calibration of deep networks <ref type="bibr" target="#b15">(Guo et al., 2017)</ref>. By Gibbs's inequality it is easy to show that the NLL loss will only be minimized when the assigned label distribution matches the true label distribution <ref type="bibr" target="#b16">(Hastie et al., 2001)</ref>, namely</p><formula xml:id="formula_18">−E (x ,y )∈D val log f θ (x ; T, λ) y ≥ −E P (Y ) P (Y |x ) log P (Y |x ).<label>(8)</label></formula><p>Therefore, we propose to find the optimal T and λ as As shown in Figure <ref type="figure" target="#fig_6">6</ref>, adversarial training on rectified model probability can mitigate the robust overfitting when the temperature T and interpolation ratio λ are optimal. Such optimal hyperparameters perfectly aligns with the ones automatically determined by Equation (9).</p><formula xml:id="formula_19">T, λ = arg min T,λ −E (x ,y )∈D val log f θ (x ; T, λ) y .<label>(9</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>Experiment setup.</p><p>We conduct experiments on three datasets including CIFAR-10, CIFAR-100 <ref type="bibr" target="#b23">(Krizhevsky, 2009)</ref> and Tiny-ImageNet <ref type="bibr" target="#b25">(Le &amp; Yang, 2015)</ref>. We conduct PGD training on preactivation ResNet-18 <ref type="bibr" target="#b17">(He et al., 2016)</ref> with 10 iterations and perturbation radius 8/255 by default. We evaluate robustness against ∞ norm-bounded adversarial attack with perturbation radius 8/255, and employ AutoAttack <ref type="bibr">(Croce &amp; Hein, 2020)</ref> for reliable evaluation. Appendix D.2 includes results on   <ref type="bibr" target="#b36">(Simonyan &amp; Zisserman, 2015)</ref>, WRN), adversarial training methods (e.g., TRADES <ref type="bibr" target="#b51">(Zhang et al., 2019)</ref>, FGSM <ref type="bibr" target="#b13">(Goodfellow et al., 2015)</ref>), and evaluation metrics (e.g., PGD-1000 (PGD attack with 1000 iterations), Square Attack <ref type="bibr" target="#b0">(Andriushchenko et al., 2020)</ref>, RayS <ref type="bibr" target="#b4">(Chen &amp; Gu, 2020)</ref>). More setup details can be found in Appendix G.</p><p>Results &amp; Discussions. Our method is essentially the baseline adversarial training with a robusttrained self-teacher, equipped with an algorithm automatically deciding the optimal hyper-parameters, which we now denote as KD-AT-Auto. We compare KD-AT-Auto with two baselines: regular adversarial training (AT), and adversarial training combined with self-distillation (KD-AT) with fixed temperature T = 2 and interpolation ratio λ = 0.5 as suggested by <ref type="bibr" target="#b5">Chen et al. (2021)</ref>.</p><p>As shown in Figure <ref type="figure" target="#fig_7">7</ref>, our method can effectively mitigate robust overfitting for all datasets, with both standard accuracy (SA) and robust accuracy (RA) constantly increasing throughout training.</p><p>In Table <ref type="table" target="#tab_1">1</ref>, we measure the difference between the RA at the best checkpoint (Best) and at the last checkpoint (Last) to clearly show the overfitting gap. Our method can reduce the overfitting gap to less than 0.5% for all datasets. One may note that self-distillation with fixed hyper-parameters is in fact inferior in terms of reducing robust overfitting, while its effectiveness can be significantly improved with the optimal hyper-parameters automatically determined by our method, which further verifies our understanding of robust overfitting. Compared with self-distillation with fixed hyper-parameters, our method can also boost both RA and SA at the best checkpoint for all datasets.</p><p>Our method can further be combined with orthogonal techniques such as Stochastic Weight Averaging (SWA) <ref type="bibr" target="#b21">(Izmailov et al., 2018)</ref> and additional standard teachers as mentioned in previous work <ref type="bibr" target="#b5">(Chen et al., 2021)</ref> to achieve better performance. More results and discussion can be found in Appendix D.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Discussions</head><p>In this paper, we show that label noise exists implicitly in adversarial training due to the mismatch between the true label distribution and the assigned label distribution of adversarial examples. Such label noise can explain the dominant overfitting phenomenon. Based on a label noise perspective, we also extend the understanding of robust overfitting and show that it is the early part of an epoch-wise double descent in adversarial training. Finally, we propose an alternative labeling of adversarial examples by rectifying model probability, which can effectively mitigate robust overfitting without any manual hyper-parameter tuning.</p><p>The label noise implicitly exists in adversarial training may have other important effects on adversarially robust learning. This can potentially consolidate the theoretical grounding of robust learning. For instance, since label noise induces model variance, from a model-wise view, one may need to increase model capacity to reduce the variance. This may partially explain why robust generalization requires significantly larger model than standard generalization. Zhang, J., Xu, X., Han, B., Niu, G., zhen Cui, L., Sugiyama, M., and Kankanhalli, M. Attacks which do not kill training make adversarial learning stronger. ArXiv, abs/2002.11242, 2020.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Checklist</head><p>The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [TODO] to [Yes] , <ref type="bibr">[No]</ref> , or [N/A] . You are strongly encouraged to include a justification to your answer, either by referencing the appropriate section of your paper or providing a brief inline description. For example:</p><p>• Did you include the license to the code and datasets? [Yes] See Section ??.</p><p>• Did you include the license to the code and datasets? <ref type="bibr">[No]</ref> The code and the data are proprietary.</p><p>• Did you include the license to the code and datasets? [N/A] Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.</p><p>1. For all authors... Here we prove that if there no label error in the clean dataset, then P ( Ỹ |x) = P (Y |x).</p><p>Proof. First, we note that</p><formula xml:id="formula_20">P ( Ỹ = j |x) = j P ( Ỹ = j |Y = j, x)P (Y = j|x).</formula><p>Since P (E = 1|Y = j, x) = 0 we have,</p><formula xml:id="formula_21">P ( Ỹ = j |Y = j, x) = e P ( Ỹ = j |E = e, Y = j, x)P (E = e|Y = j, x) = P ( Ỹ = j |E = 0, Y = j, x) = 1, if j = j, 0, otherwise.</formula><p>Therefore we have for all j ,</p><formula xml:id="formula_22">P ( Ỹ = j |x) = P (Y = j |x).</formula><p>Total Variation distance for discrete probability distributions.</p><p>For two discrete probability distributions P (Y ) and P (Y ) where Y, Y ∈ Y, the total variation distance between them can be equally defined as</p><formula xml:id="formula_23">P (Y ) − P (Y ) TV = sup J∈A |P (Y ∈ J) − P (Y ∈ J)| = sup J∈A j∈J P (Y = j) − j∈J P (Y = j) = 1 2 j |P (Y = j) − P (Y = j|)| A.2 Proofs in Section 4.1</formula><p>Proof of Lemma 4.1.</p><p>Proof. For simplicity, we consider the adversarial perturbation generated by FGSM. Other adversarial perturbation can be viewed as a Taylor series of such perturbation.</p><formula xml:id="formula_24">δ = −ε ∇ f (x) y ∇ f (x) y ,<label>(11)</label></formula><p>First, we bound the distribution mismatch by gradient norm.</p><formula xml:id="formula_25">P (Y |x) − P (Y |x ) TV = 1 2 j |P (Y = j|x) − P (Y = j|x )| TV distance ≥ 1 2 |P (Y = y|x) − P (Y = y|x )| = 1 2 |f (x) y − f (x ) y | = 1 2 −∇f (x) y • δ − 1 2 δ T ∇ 2 f (z) y δ ≥ 1 2 −∇f (x) y • δ − σ M 2 δ 2 2 Bounded Hessian ≥ 1 2 ε ∇f (x) y 2 2 ∇f (x) y − σ M 2 ε 2 ∇f (x) y 2 2 ∇f (x) y 2 . Now if • = • 2 , we have P (Y |x) − P (Y |x ) TV ≥ 1 2 ε ∇f (x) y 2 − σ M 2 ε 2 . (<label>12</label></formula><formula xml:id="formula_26">) If • = • ∞ , we can utilize the fact that • ∞ ≤ • 2 ≤ √ d • ∞ , thus P (Y |x) − P (Y |x ) TV ≥ 1 2 ε ∇f (x) y ∞ − σ M 2 ε 2 √ d . (<label>13</label></formula><formula xml:id="formula_27">)</formula><p>Second, we bound the gradient norm by the L-local Lipschitzness assumption. Let x * be a closest input that achieves the local maximum on the predicted probability at y, namely</p><formula xml:id="formula_28">x * = arg min z∈X,f (z)y=1 x − z . Because x * is the local maximum and f is continuously differentiable, ∇f (x * ) y = 0, thus ∇f (x) y = ∇f (x * ) y + ∇ 2 f (z) y (x − x * ) = ∇ 2 f (z) y (x − x * ).</formula><p>Therefore we have ∇f (x</p><formula xml:id="formula_29">) y = ∇ 2 f (z) y (x − x * ) ≥ σ m x − x * ≥ σ m |f (x * ) y − f (x) y | L = σ m L (1 − f (x) y ).</formula><p>Plug this into Equation ( <ref type="formula" target="#formula_25">12</ref>) or Equation ( <ref type="formula" target="#formula_26">13</ref>) we then obtain the desired result.</p><p>Proof of Lemma 4.2.</p><p>Proof. First, we show that the expectation of the label error is lower bounded by the mismatch between the true label distribution and the assigned label distribution. Using standard concentration inequality such as Hoeffding's inequality we have, with probability</p><formula xml:id="formula_30">P ( Ỹ |x) − P (Y |x) T V = 1 2 j |P ( Ỹ = j|x) − P (Y = j|x)| = 1 2 j |P ( Ỹ = j, Y = j|x) + P ( Ỹ = j, Y = j|x) − P (Y = j, Ỹ = j|x) − P (Y = j, Ỹ = j|x)| = 1 2 j |P ( Ỹ = j, Y = j|x) − P (Y = j, Ỹ = j|x)| ≤ 1 2 j P ( Ỹ = j, Y = j|x) + P (Y = j, Ỹ = j|x) = P (Y = Y |x) = P (E = 1|x)<label>(</label></formula><formula xml:id="formula_31">1 − δ, |p e (D) − P (E = 1)| ≤ 1 2N log 2 δ .</formula><p>This implies</p><formula xml:id="formula_32">p e (D) ≥ P (E = 1) − 1 2N log 2 δ .</formula><p>Since P (E = 1) = E x P (E = 1|x), we have, with probability 1 − δ,</p><formula xml:id="formula_33">p e (D) ≥ E x P ( Ỹ |x) − P (Y |x) TV − 1 2N log 2 δ .</formula><p>which means p e (D) &gt; 0 as long as N is large.</p><p>Proof of Theorem 4.3. Therefore, apply Lemma 4.2 to an adversarially augmented training set we have with probability</p><formula xml:id="formula_34">1 − δ, p e (D ) ≥ E x P ( Ỹ |x ) − P (Y |x ) TV − 1 2N log 2 δ ≥ E x P (Y |x) − P (Y |x ) TV − 1 2N log 2 δ .</formula><p>Further, apply Lemma 4.1 and the definition of data quality, we have with probability 1 − δ,</p><formula xml:id="formula_35">p e (D ) ≥ ε 2 (1 − E x f (x) y ) σ m L − ε 2 4 σ M − 1 2N log 2 δ ≥ ε 2 (1 − q(D)) σ m L − ε 2 4 σ M − 1 2N log 2 δ .</formula><p>A Let {S j } Nρε j=1 be any disjoint partition of S such that S j ⊆ {x | x − xj ≤ ρε}. We show that S j attains a property that the true label distribution of any input x in this subset will not be too far from the sample mean of one-hot labels ȳj = |S j | −1 x∈Sj 1 y in this subset. Specifically, let p(x) = P (Y |x). We have with probability</p><formula xml:id="formula_36">1 − δ, p(x) − ȳj 1 ≤ 2K |S j | log 2 δ + 2Lρε. (<label>15</label></formula><formula xml:id="formula_37">)</formula><p>To prove this property we first present two lemmas.</p><p>Lemma A.1 (Lipschitz constraint of the true label distribution). Let S j be a subset constructed above and ȳj = |S j | −1 x∈Sj 1 y . Then for any x ∈ S j we have,</p><formula xml:id="formula_38">p(x) − E[ȳ j ] 1 ≤ 2Lρε. (<label>16</label></formula><formula xml:id="formula_39">)</formula><p>Proof. First, since x ∈ S j , we have x − xj 1 ≤ ρε, which implies p(x) − p(x j ) 1 ≤ Lρε by the locally Lipschitz continuity of p. Then for any x, x ∈ S j , we will have p(x) − p(x ) ≤ 2Lρε by the triangle inequality. Let N S = |S j |. Therefore,</p><formula xml:id="formula_40">p(x) − 1 N S x∈Sj p(x) 1 ≤ 2 N S − 1 N S Lρε ≤ 2Lρε. (<label>17</label></formula><formula xml:id="formula_41">)</formula><p>Further, the linearity of the expectation implies</p><formula xml:id="formula_42">E[ȳ] = N S −1 x∈Sj E[1 y(x) ] = N S −1 x∈Sj p(x). (<label>18</label></formula><formula xml:id="formula_43">) Therefore p(x) − E[ȳ j ] ≤ 2Lρε.</formula><p>Lemma A.2 (Concentration inequality of the sample mean). Let S be a set of x with cardinality N . Let ȳ = N −1 x∈S 1 y be the sample mean. Then for any p-norm • and any ε &gt; 0, we have with probability 1 − δ,</p><formula xml:id="formula_44">ȳ − E [ȳ] 1 ≤ 2K N log 2 δ (<label>19</label></formula><formula xml:id="formula_45">)</formula><p>Proof. Note that ȳ obeys a multinomial distribution, i.e. ȳ ∼ N −1 multinomial(N, E[ȳ]). This lemma is thus the classic result on the concentration properties of multinomial distribution based on 1 norm <ref type="bibr" target="#b45">Weissman et al. (2003)</ref>; <ref type="bibr" target="#b33">Qian et al. (2020)</ref>.</p><p>One can see that Lemma A.1 bounds the difference between true label distribution of individual inputs and the mean true label distribution, while Lemma A.2 bounds the difference between the sample mean and the mean true label distribution. Therefore the difference between the true label distribution and the sample mean is also bounded, since by the triangle inequality we have with probability</p><formula xml:id="formula_46">1 − δ, p(x) − ȳ ≤ p(x) − E[ȳ] + ȳ − E[ȳ] ≤ 2K N log 2 δ + 2Lρε. (<label>20</label></formula><formula xml:id="formula_47">)</formula><p>We now show that given the locally Lipschitz constraint established in each disjoint partition we constructed above, the prediction given by the empirical risk minimizer will be close to the sample mean. As an example, we focus on the negative log-likelihood loss, namely (f θ (x), y) = −1 y • log f θ (x). Other loss functions that are subject to the proper scoring rule can be investigated in a similar manner. First, we regroup the sum in the empirical risk based on the partition constructed above, namely</p><formula xml:id="formula_48">R(f θ , S) = 1 N ρε Nρε j=1 R(f θ , S j ),<label>(21)</label></formula><p>where</p><formula xml:id="formula_49">R(f θ , S j ) = −|S j | −1 |Sj | i=1 1 yi • log f θ (x i )</formula><p>is the empirical risk in each partition. Since we are only concerned with the existence of a desired minimizer of the empirical risk, we can view f θ as able to achieve any labeling of the training inputs that suffices the local Lipschitz constraint. Thus the empirical risk minimization is equivalent to the minimization of the empirical risk in each partition. The problem can thus be defined as, for each j</p><formula xml:id="formula_50">= 1, • • • , N ρε , min f θ R(f θ , S j ) s.t. f θ (x) − f θ (x j ) 1 ≤ Lρε, ∀ x ∈ S j ,<label>(22)</label></formula><p>where the constraint is imposed by the locally-Lipschitz continuity of f θ . By the following lemma, we show that the minimizer of such problem is achieved only if f θ (x j ) is close to the sample mean.</p><formula xml:id="formula_51">Lemma A.3. Let ȳ = |S j | −1 x∈Sj 1 y . The minimum of the problem (22) is achieved only if f θ (x j ) = ȳj (1 + KL θ ρε) − L θ ρε.</formula><p>Proof. We note that since the loss function we choose is strongly convex, to minimize the empirical risk, the prediction of any input x must be as close to the one-hot labeling as possible. Therefore the problem ( <ref type="formula" target="#formula_50">22</ref>) can be formulated into a vector minimization where we can employ Karush-Kuhn-Tucker (KKT) theorem to find the necessary conditions of the minimizer.</p><p>Let p i := f θ (x i ) and ε = Lρε for simplicity. We rephrase the problem (22) as</p><formula xml:id="formula_52">min {pi} N i=1 − 1 N i 1 yi • log p i s.t. p i − p 1 ≤ ε, k p k i = 1, k p k = 1, p k i ≥ 0, p k ≥ 0.<label>(23)</label></formula><p>Case I. We first discuss the case when p k + ε &lt; 1 for all k. First, we observe that for any p, the minimum of the above problem is achieved only if p yi i = p yi + ε. Because by contradiction, if p yi i &lt; p yi + ε, we will have − log p yi i &gt; − log(p yi + ε), and p yi + ε belongs to the feasible set, which means p yi i does not attain the minimum. The above problem can then be rephrased as</p><formula xml:id="formula_53">min p − 1 N i log(p yi + ε), s.t. k p k = 1, p k ≥ 0,<label>(24)</label></formula><p>where we have neglected the condition associated with p k =yi i , since they do not contribute to the objective, they can be chosen arbitrarily as long as the constraints are sufficed, and clearly the constraints are underdetermined.</p><formula xml:id="formula_54">Let N k = i 1(y i = k), we have i log(p yi + ε) = k N k log(p k + ε). Therefore the above problem is equivalent to min p − k ȳk log(p k + ε), s.t. k p k = 1, p k ≥ 0,<label>(25)</label></formula><p>where</p><formula xml:id="formula_55">ȳ ≡ [N 1 /N, • • • , N k /N ] T is equal to the sample mean N −1 i 1 yi .</formula><p>To solve the strongly convex minimization problem (25) it is easy to employ KKT conditions to show that p = ȳ(1</p><formula xml:id="formula_56">+ K ε) − ε.</formula><p>Case II. We now discuss the case when p is the minimizer of ( <ref type="formula" target="#formula_52">23</ref>) and there exists k such that pk + ε ≥ 1. And p = p, where p = ȳ(1 + K ε) − ε is the form of the minimizer in the previous case.</p><p>Considering a non-trivial case p * k &lt; 1 − ε. Otherwise the true label distribution is already close to the one-hot labeling, which is the minimizer of the empirical risk. Therefore by k =k p k &gt; ε we have the condition</p><formula xml:id="formula_57">k =k ȳk &gt; K ε 1 + K ε<label>(26)</label></formula><p>Now considering the minimization objective R(p) = −N −1 i 1 yi •log p i . For all i with y i = k , we must have p yi i = 1, otherwise the optimal cannot be attained by contradiction. Then the minimization problem can be rephrased as</p><formula xml:id="formula_58">min k =k ȳk log(p + ε), s.t. k =k pk ≥ ε, pk ≥ 0,<label>(27)</label></formula><p>where the first constraint is imposed by pk ≥ 1 − ε.</p><p>Employ KKT conditions similarly we can have pk = ȳk /λ − ε where λ is a constant. By checking the constraint we can derive λ ≥ k ȳk /(K ε).</p><p>However, the minimization objective</p><formula xml:id="formula_59">min λ − k =k ȳk log ȳk λ ,</formula><p>requires λ to be minimized. Therefore λ = k =k ȳk /(K ε), which implies</p><formula xml:id="formula_60">pk = K ε ȳk k =k ȳk − ε.<label>(28)</label></formula><p>Now since p = arg min p R(p) and p = p, we must have R(p) &lt; R(p). This means</p><formula xml:id="formula_61">− k =k ȳk log K εȳ k k =k ȳk &lt; − k =k ȳk log[ȳ k (1 + K ε)],<label>(29)</label></formula><p>which is reduced to</p><formula xml:id="formula_62">k =k ȳk &lt; K ε 1 + K ε<label>(30)</label></formula><p>But this is contradict to our assumption.</p><p>We are now be able to bound the difference between the predictions of the training inputs produced by the empirical risk minimizer and the sample mean in each S j . To see that we have for each x ∈ S j .</p><formula xml:id="formula_63">f θ (x) − ȳj 1 ≤ f θ (x) − f θ (x j ) 1 + f θ (x j ) − ȳj 1 ≤ Lρε(1 + K ȳj − K −1 1 1 ) ≤ Lρε(1 + K 1 (•) − K −1 1 1 ). = Lρε 3 − 2 K<label>(31)</label></formula><p>By Equation ( <ref type="formula" target="#formula_36">15</ref>) we then have for any x ∈ S j , with probability 1 − δ,</p><formula xml:id="formula_64">f θ (x) − p(x) 1 ≤ 2K |S j | log 2 δ + L θ ρε 3 − 2 K + 2Lρε,<label>(32)</label></formula><p>which means the difference between the predictions and the true label distribution is also bounded.</p><p>Step III: Show the disjoint partition is non-trivial.</p><p>In (32), we have managed to bound the difference between the predictions yielded by an empirical risk minimizer and the true label distribution based on the cardinality of the subset |S j |, namely the number of inputs in j-partition. However |S j | is critical to the bound here as if |S j | = 1, then (32) becomes a trivial bound. Here we show |S j | is non-negligible based on simple combinatorics. </p><formula xml:id="formula_65">x | N (x) ≥ N κN ρε ≥ 1 − 1 κ + 1 κN ρε N.<label>(33)</label></formula><p>Proof. We note that the problem is to show the minimum number of x such that N (x) ≥ N/(κN ρε ). This is equivalent to find the maximum number of x such that N (x) ≤ N/(κN ρε ). Since we only have N ρε subsets, the maximum can be attained only if for N ρε −1 subsets S, |S| = N/(κN ρε ). Otherwise, if for any one of these subsets |S| &lt; N/(κN ρε ), then it is always feasible to let |S| = N/(κN ρε ) and the maximum increases. Similarly, if the number of such subsets is less than N ρε − 1, then it is always feasible to let another subset subject to |S| = N/(κN ρε ) and the maximum increases. We can then conclude that at most N (N ρε − 1)/(κN ρε ) inputs can have the property N (x) ≤ N/(κN ρε ).</p><p>The above lemma basically implies when partitioning N inputs into N ρε subsets, a large fraction of the inputs will be assigned to a subset with cardinality at least N/(κN ρε ). Here N ρε is the covering number and is bounded above based on the property of the covering in the Euclidean space. Apply Lemma A.4 to (32), and use the fact that • TV = • 1 /2 for category distributions, we then arrive at Lemma 4.4.</p><p>Proof of Theorem 4.5.</p><p>Proof. First, we show that adversarial perturbation generated by a realistic classifier can change its predictive distribution. Considering adversarial perturbation based on FGSM and cross-entropy loss,</p><formula xml:id="formula_66">namely x = x − ε ∇ f θ (x) y −1 ∇ f θ (x) y ,</formula><p>we can obtain a result similar to Lemma 4.1.</p><formula xml:id="formula_67">Lemma A.5. Assume f θ (x) y is L θ -locally Lipschitz around x with bounded Hessian. Let σ m = inf z∈Bε(x) σ min (∇ 2 f θ (z) y ) &gt; 0 and σ M = sup z∈Bε(x) σ max (∇ 2 f θ (z) y ) &gt; 0.</formula><p>Here σ min and σ max denote the minimum and maximum eigenvalues of the Hessian, respectively. We then have</p><formula xml:id="formula_68">f θ (x) − f θ (x ) TV ≥ ε 2 (1 − f θ (x) y ) σ m L θ − ε 2 4 σ M ,<label>(34)</label></formula><p>Second, We prove that the true label distribution will be distorted by the adversarial perturbation generated by a realistic classifier. This is guaranteed if the predictive distribution of a realistic classifier can approximate the true label distribution. Specifically, by utilizing Lemma A.5 and Lemma 4.4, we have with probability 1 − 2δ, </p><formula xml:id="formula_69">P (Y |x) − P (Y |x ) TV ≥ f θ (x) − f θ (x ) TV − ( f θ (x) − P (Y |x) TV + f θ (x ) − P (Y |x ) TV ) ≥ ε 2 (1 − f θ (x) y ) σ m L θ − ε 2 4 σ M − 2κN ρε K N log 2 δ − 3 2 − 1 K L θ + L 2ρε =ε (1 − f θ (x) y ) σ m 2L θ − 2ρ 3 2 − 1 K L θ + L − ε 2 σ M 4 − 2κN ρε K N log 2 δ .<label>(35)</label></formula><formula xml:id="formula_70">p e (D ) ≥ ε (1 − E x f θ (x) y ) σ m 2L θ − 2ρ 3 2 − 1 K L θ + L − ε 2 σ M 4 − ξ 1 2N log 2 δ ,<label>(36)</label></formula><p>where ξ = 1 + 4κN ρε K. Let T = T * , we have</p><formula xml:id="formula_71">f (x ; θ, T ) − P (Y |x ) T V = 1 2 j |f (x ; θ, T ) j − P (Y = j|x )| = 1 2 j,j =j * |f (x ; θ, T ) j − P (Y = j|x )| ≤ 1 2   j,j =j * f (x ; θ, T ) j + j,j =j * P (Y = j|x )   = 1 − P (Y = j * |x ),</formula><p>where the inequality holds by the triangle inequality.</p><p>Meanwhile, we have</p><formula xml:id="formula_72">P ( Ỹ |x ) − P (Y |x ) T V = P (Y |x) − P (Y |x ) T V = 1(y) − P (Y |x ) T V = 1 2   1 − P (Y = y|x ) + j,j =ŷ P (Y = y|x )   = 1 − P (Y = y|x ) ≥ 1 − P (Y = j * |x ).</formula><p>Therefore, it can seen that for T = T * ,</p><formula xml:id="formula_73">f (x ; θ, T ) − P (Y |x ) T V ≤ P ( Ỹ |x ) − P (Y |x ) T V .</formula><p>Proof of Theorem 5.2. Lemma A.6. Let x be an example incorrectly classified by a classifier f in terms of the true label distribution P (Y = j|x ), namely</p><formula xml:id="formula_74">arg max j f (x ; θ, T ) j = j * , where j * = arg max j P (Y = j|x ). Assume P (Y = j * |x ) ≥ 1/2, then f (x ; θ, T ) j * ≤ P (Y = j * |x ).</formula><p>Proof. We prove it by contradiction. Assume f (x ; θ, T ) j * &gt; P (Y = j * |x ), we have f (x ; θ, T ) j * &gt; P (Y = j * |x ) ≥ 1/2. Therefore,</p><formula xml:id="formula_75">f (x ; θ, T ) j ≤ j,j =j * f (x ; θ, T ) j = 1 − f (x ; θ, T ) j * &lt; 1/2, ∀j = j * ,</formula><p>which means f (x ; θ, T ) j &lt; f (x ; θ, T ) j * , ∀j = j * . This leads to j * = arg max j f (x ; θ, T ) j , which contradicts our condition. Let λ = λ * , we have</p><formula xml:id="formula_76">2 λ • f (x ; θ, T ) + (1 − λ) • P ( Ỹ |x ) − P (Y |x ) T V − f (x ; θ, T ) − P (Y |x ) T V =2 [ λ • f (x ; θ, T ) + (1 − λ) • 1(y) − P (Y |x ) T V − f (x ; θ, T ) − P (Y |x ) T V ] = j |λ • f (x ; θ, T ) j + (1 − λ) • 1(j = y) − P (Y = j|x )| − j |f (x ; θ, T ) j − P (Y = j|x )| = j |λ • f (x ; θ, T ) j + (1 − λ) • 1(j = Y ) − P (Y = j|x )| − j |f (x ; θ, T ) j − P (Y = j|x )| = j,j =j * |λ • f (x ; θ, T ) j − P (Y = j|x )| − j,j =j * |f (x ; θ, T ) j − P (Y = j|x )| − |f (x ; θ, T ) j * − P (Y = j * |x )| ≤ j,j =j * |λ • f (x ; θ, T ) j − f (x ; θ, T ) j | − |f (x ; θ, T ) j * − P (Y = j * |x )| = j,j =j * [f (x ; θ, T ) j − λ • f (x ; θ, T ) j ] − [P (Y = j * |x ) − f (x ; θ, T ) j * ] = j,j =j * [f (x ; θ, T ) j − λ • f (x ; θ, T ) j ] − [λ • f (x ; θ, T ) j * + (1 − λ) − f (x ; θ, T ) j * ] = j f (x ; θ, T ) j − λ j f (x ; θ, T ) j − (1 − λ) = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Limitations</head><p>We Model capacity. We modulate the capacity of the deep model by varying the widening factor of the Wide ResNet. To extend the lower limit of the capacity, we allow the widening factor to be less than 1. In such case, the number of channels in each residual block is scaled similarly but rounded, and the number of channels in the first convolutional layer will be reduced accordingly to ensure the width monotonically increasing through the forward propagation.</p><p>Model architecture. We also experiment on model architectures other than Wide ResNet, including pre-activation ResNet-18 <ref type="bibr" target="#b17">(He et al., 2016)</ref> and VGG-11 <ref type="bibr" target="#b36">(Simonyan &amp; Zisserman, 2015)</ref>. We select   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning rate scheduler.</head><p>A specific learning rate scheduler may shape the robust overfitting differently as suggested by <ref type="bibr" target="#b35">Rice et al. (2020)</ref>. We consider the following learning rate schedulers in our experiments.</p><p>• Piecewise decay: The initial learning rate rate is set as 0.1 and is decayed by a factor of 10 at the 100th and 500th epochs within a total of 1000 epochs.</p><p>• Cyclic: This scheduler was initially proposed by <ref type="bibr" target="#b38">Smith (2017)</ref> and has been popular in adversarial training. We set the maximum learning rate to be 0.2, and the learning rate will linearly increase from 0 to 0.2 for the initial 400 epochs and decrease to 0 for the later 600 epochs. • Cosine: This scheduler was initially proposed by <ref type="bibr" target="#b26">Loshchilov &amp; Hutter (2017)</ref>. The learning rate starts at 0.1 and gradually decrease to 0 following a cosine function for a total of 1000 epochs.</p><p>Experiments on various learning rate schedulers show the second descent can be widely observed except the piecewise decay, where the appearance of second descent might be delayed due to extremely small learning rate in the late stage of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D More experiment results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Training longer</head><p>As shown in Figure <ref type="figure" target="#fig_19">9</ref>, We show that our method can maintain the robust test accuracy with more training epochs. Here, we follow the settings in Figure <ref type="figure" target="#fig_7">7</ref> except we train for additional epochs up to 400 epochs for each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Adversarial training methods, neural architectures and evaluation metrics</head><p>In this section we conduct extensive experiments with different neural architectures, adversarial training methods and robustness evaluation metrics to verify the effectiveness of our method.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Combined with additional orthogonal techniques</head><p>We note that motivated from our theoretical analyses, our proposed method (KD-AT-Auto) is essentially the baseline knowledge distillation for adversarial training (KD-AT) with a robustly trained self-teacher, equipped with an algorithm that automatically finds its optimal hyperparameters (i.e. the temperature T and the interpolation ratio λ). Stochastic Weight Averaging (SWA) and additional standard teachers (KD-Std) employed in <ref type="bibr" target="#b5">(Chen et al., 2021)</ref> are orthogonal contributions. KD-AT-Auto can certainly be combined with SWA and KD-Std to achieve better performance.</p><p>As shown in Table <ref type="table" target="#tab_6">5</ref>, on CIFAR-10, KD-AT + KD-Std + SWA <ref type="bibr" target="#b5">(Chen et al., 2021)</ref> can already reduce the overfitting gap (difference between the best and last robust accuracy) to almost 0. It is thus hard to see any further reduction by combining our method. To this end, we introduce an extra dataset SVHN <ref type="bibr" target="#b30">(Netzer et al., 2011)</ref>. As shown in Table <ref type="table" target="#tab_6">5</ref>, on SVHN, KD-AT + KD-Std + SWA still produces a high overfitting gap (also see Appendix A1.3 in <ref type="bibr" target="#b5">(Chen et al., 2021)</ref>), whereas by combining with our algorithm to automatically find the optimal hyper-parameters (KD-AT-Auto + KD-Std + SWA), the overfitting gap can be further reduced to almost 0. This demonstrates the effectiveness and wide applicability of our principle-guided method on mitigating robust overfitting. Here, the interpolation ratio of the standard teacher is fixed as 0.2 and the SWA starts at the first learning rate decay for all experiments. We employ PGD-AT <ref type="bibr" target="#b27">(Madry et al., 2018)</ref> as the base adversarial training method and conduct experiments with a pre-activation ResNet-18. The robust accuracy is evaluated with AutoAttack. Other experiment details are in line with Appendix G.1.</p><p>Furthermore, we note that <ref type="bibr" target="#b5">(Chen et al., 2021)</ref> shows SWA and KD-Std are essential components to mitigate robust overfitting on top of KD-AT, while we show that KD-AT itself can mitigate robust overfitting by proper parameter tuning. We are thus able to separate these components and allow a more flexible selection of hyperparameters in diverse training scenarios without fear of overfitting. In particular, although <ref type="bibr" target="#b5">(Chen et al., 2021)</ref> suggests SWA starting at the first learning rate decay (exactly when the overfitting starts) mitigates robust overfitting, the effectiveness of SWA on mitigating overfitting may strongly depend on its hyper-parameter selection including s 0 , i.e., the starting epoch and τ , i.e., the decay rate 3 , which is also mentioned in recent work <ref type="bibr" target="#b34">(Rebuffi et al., 2021)</ref>. We also did some additional experiments on CIFAR-10 following the SWA setting in <ref type="bibr" target="#b34">(Rebuffi et al., 2021)</ref> to demonstrate the wide applicability of our method. As shown by Table <ref type="table" target="#tab_7">6</ref>, when changing the hyperparameters of SWA, KD-AT + KD-Std + SWA cannot consistently mitigate robust overfitting, while KD-AT-Auto + KD-Std + SWA can maintain an overfitting gap close to 0 and achieve better robustness as well. E Study on a synthetic dataset with known true label distribution 3 SWA can be implemented using an exponential moving average θ of the model parameters θ with a decay rate τ , namely θ ← τ • θ + (1 − τ ) • θ at each training step <ref type="bibr" target="#b34">(Rebuffi et al., 2021)</ref>. In Figure <ref type="figure" target="#fig_20">11</ref> left, we solve the optimal temperature for each correctly classified training example based on Theorem 5.1 with the interpolation ratio fixed as 1.0. One can find that the individual optimal temperatures mostly concentrate between 0.5 and 1.5. In Figure <ref type="figure" target="#fig_20">11</ref> right, we solve the optimal interpolation ratio for each incorrectly classified training example based on Theorem 5.2 with the temperature fixed as 1.0 . One can find that the individual optimal interpolation ratio mostly concentrate between 0.5 and 0.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Method details</head><p>F.1 Determine the optimal hyper-parameters One may note that Equation (9) cannot be directly optimized since the traditional adversarial label is only defined on the example in the training set and cannot be simply generalized to the validation set. A reasonable solution is using the nearest neighbour classifier to find the closest traditional adversarial label for every example in the validation set. However, to speed up the optimization, we propose to employ the classifier overfitted by the traditional adversarial labels on the training set as an surrogate, which works well in practice. Specially, we employ a model overfitted on the training set to generate approximate traditional adversarial label of the adversarial example in the validation set. Such overfitted model is typically the model at the final checkpoint when conducting regular</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>x 2 c e C k T c a K p I J N F Q c K R j t D o d d R h k h L N B 4 Z g I p m 5 F Z E e l p h o E 1D O h O D O v v y X 1 A 6 L 7 l H x t G L S c G C C L O z C H h T A h W M o w T m U o Q o E r u E B n u D Z i q x H 6 8 V 6 n b R m r O n M N v y C 9 f Y N P U u R 3 w = = &lt; / l a t e x i t &gt; P ( Ỹ |x) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g b Q S p 3 C E o 5 8 7 4 U W y 8 Z m 6 d 6 d K N o c = " &gt; A A A B 9 X i c b V B L S g N B E O 2 J v x h / o + 5 0 0 x i E u A k z I q i 7 g B s X L h I w H 0 n G 0 N N T k z T p + d D d o 4 Y x J / A C b l w o 4 t Z D e A N 3 3 s I j 2 P k s N P F B w e O 9 K q r q u T F n U l n W l 5 G Z m 1 9 Y X M o u 5 1 Z W 1 9 Y 3 z M 2 t m o w S Q a F K I x 6 J h k s k c B Z C V T H F o R E L I I H L o e 7 2 z o Z + / Q a E Z F F 4 q f o x O A H p h M x n l C g t X Z c L L c W 4 B + n V 4 P 7 u o G3 m r a I 1 A p 4 l 9 o T k S 2 b l + + N i 5 6 H c N j 9 b X k S T A E J F O Z G y a V u x c l I i F K M c B r l W I i E m t E c 6 0 N Q 0 J A F I J x 1 d P c D 7 W v G w H w l d o c I j 9 f d E S g I p + 4 G r O w O i u n L a G 4 r / e c 1 E + S d O y s I 4 U R D S 8 S I / 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the origin of label noise in adversarial training. The adversarial perturbation causes a mismatch between the true label distributions of clean inputs x and their adversarial examples x . Such a distribution mismatch is however neglected by the labels assigned to adversarial examples in the common practice of adversarial training, resulting in label noise implicitly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Standard training on a fixed adversarially augmented training set (e.g. ε = 16/255) can also produce prominent overfitting. In contrast, on the original training set without adversarial perturbation applied (ε = 0), no overfitting is observed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Label noise implicitly exists in adversarial training. In the adversarially augmented training set D , such distribution mismatch exists exactly. By Remark 4.1 we have P ( Ỹ |x ) = P ( Ỹ |x) and by property of the clean dataset (Assumption 3.1) we have P ( Ỹ |x) = P (Y |x), which together means P ( Ỹ |x ) = P (Y |x). However, Lemma 4.1 shows that P (Y |x ) = P (Y |x), which implies that P ( Ỹ |x ) = P (Y |x ). This indicates that label noise exists in the adversarially augmented training set. We now have the following theorem, which is our main result. Theorem 4.3. Assume f (x) y is L-locally Lipschitz around x with Hessian bounded below. Instantiate the same notations as in Lemma 4.1. With probability 1 − δ, we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Example 4. 1 (</head><label>1</label><figDesc>Label noise due to a symmetric distribution shift). Let D = {(x i , y i )} i∈[N ] be a clean labeled training subset where all inputs x i = x are identical and have a one-hot true label distribution, i.e., P (Y |x) = 1 y .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: (Left) Dependence of robust overfitting on the perturbation radius. A training subset of size 5k is randomly sampled to speed up the training. ε = 0/255 indicates the standard training where no double descent occurs. (Right) Dependence of robust overfitting on the data quality with a fixed perturbation radius (ε = 8/255). To construct a training subset with high data quality, we first calculate the predictive probability based on an ensemble of multiple models. We then rank all training examples based on the predictive probability and select the top-k ones. The curves are smoothed by a window of 5 epochs to reduce overlapping. Here we conduct PGD training on CIFAR-10 with WRN-28-5. More experiment details can be found in the Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: (Upper) NLL loss obtained on the validation set for different T and λ. (Bottom) Robust test accuracy at the best and last checkpoint by adversarial training with the rectified model probability with different T and λ. λ = 0.8 for grid search on T (Left) and T = 2 for grid search on λ (Right). Orange dashed lines indicate T and λ determined by Equation (9).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Our method can effectively mitigate robust overfitting for different datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See Section B (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section B (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [Yes] (b) Did you include complete proofs of all theoretical results? [Yes] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [N/A] (c) Did you include any new assets either in the supplemental material or as a URL? [N/A] (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>14) Second, given a sampled training set D = {(x i , ỹi )} i∈[N ] , the empirical measure of label error E should converge to its expectation almost surely, namely lim e i = E[E] = P (E = 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Proof.</head><label></label><figDesc>First, by the fact that P ( Ỹ |x ) = P ( Ỹ |x) and P ( Ỹ |x) = P (Y |x) we have P ( Ỹ |x ) = P (Y |x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Let D = (x, y) be the adversarially augmented training set. Let S = {x : (x, y) ∈ D} be the collection of all training inputs. First we note that the set of all training inputs can be grouped into several subsets such that the inputs in each subset possess similar true label distribution. More formally, Let C = {x j } Nρε j=1 be an ρε-external covering of S with minimum cardinality, namely S ⊆ x∈C {x | x − x ≤ ρε}, where we refer xj as the covering center, and N ρε is the covering number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Lemma A.4. Let {S j } Nρε j=1 be a disjoint partition of the entire training set S. Denote S(x) as the partition that includes x. Let N (x) = |S(x)| and N = |S|. Then for any κ ≥ 1,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Finally, we show</head><label></label><figDesc>that such distribution mismatch induces label noise in the adversarially augmented training set. Similar to the proof for the true classifier, by the common labeling practice of adversarial examples we have P ( Ỹ |x ) = P ( Ỹ |x) = P (Y |x). By utilizing Lemma 4.2 1 we then have with probability 1 − 3δ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Let j * = argmax P (Y = j|x ) and thus P(Y = j * |x ) ∈ [1/c, 1]. Let g(T ) := f (x ; θ, T ) j * , which is a continuous function defined on [0, ∞]. The condition j * = arg max j f (x ; θ, T ) j ensures that g(T ) ∈ [1/c, 1],where c is the number of classes. By the intermediate value theorem, there exists T * , such that g(T * ) = P (Y = j * |x ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Now we prove Theorem 5.2Proof. First let P (Y |x ) = P (y|x) ≈ 1(y). Let j * = arg max j P (Y = j|x ). By Lemma A.6 we have f (x ; θ, T ) j * ≤ P (Y * = j * |x ) ≤ 1. Then there exists λ * &gt; 0, such that λ * • f (x ; θ, T ) j * + (1 − λ * ) = P (Y = j * |x ) by the intermediate value theorem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Epoch-wise double descent curves in adversarial training with various model architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>Epoch-wise double descent curves in adversarial training with various learning rate schedulers. The curves are smoothed by a moving average with a window of 5 to avoid overlapping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Effect of model on the epoch-wise double descent curve</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Our method can maintain robust test accuracy for more training epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 11 :</head><label>11</label><figDesc>Figure 10: Sample image by mixup augmentation.Synthetic Dataset. Since the true label distribution is typically unknown for adversarial examples in real-world datasets, we simulate the mechanism of implicit label noise in adversarial training from a feature learning perspective. Specifically, we adapt mixup<ref type="bibr" target="#b50">(Zhang et al., 2018)</ref> for data augmentation on CIFAR-10. For every example x in the training set, we randomly select another example x in a different class and linearly interpolate them by a ratio ρ, namely x := ρx + (1 − ρ)x , which essentially perturbs x with features from other classes. Therefore, the true label distribution is arguablyy ∼ ρ • 1(y)+(1−ρ)•1(y ).Unlike mixup, we intentionally set the assigned label as ŷ ∼ 1(y), thus deliberately create a mismatch between the true label distribution and the assigned label distribution. We refer this strategy as mixup augmentation and only perform it once before the training. In this way, the true label distribution of every example in the synthetic dataset is fixed.Concentration of optimal temperature and interpolation ratio of individual examples.In Section 5.1 we have shown that in terms of individual examples, the rectified model probability can provably reduce the distribution mismatch between the assigned label distribution and true label distribution of the adversarial example. However, since the true label distribution is unknown in realistic scenarios, it is not possible to directly follow Theorems 5.1 and 5.2 and calculate the optimal set of hyper-parameters for each example in the training set. The best we can do is to employ a validation set and determine a universal set of hyper-parameters based on the NLL loss, which expects all training examples to share similar optimal temperatures and interpolation ratios. Here, based on the synthetic dataset where a true label distribution is known, we empirically verify this assumption is reasonable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance of our method on different datasets. * denotes the hyper-parameters automatically determined by our method. Auto 1.23 * 0.85 * 18.29 18.39 -0.10 47.46 47.56 -0.10 additional model architectures (e.g., VGG</figDesc><table><row><cell>Dataset</cell><cell>Setting</cell><cell>T</cell><cell>λ</cell><cell cols="2">Robust Acc. (%) Best Last Diff.</cell><cell>Standard Acc. (%) Best Last Diff.</cell></row><row><cell></cell><cell>AT</cell><cell>-</cell><cell>-</cell><cell cols="3">47.35 41.42 5.93 82.67 84.91 -2.24</cell></row><row><cell>CIFAR-10</cell><cell>KD-AT</cell><cell>2</cell><cell>0.5</cell><cell cols="3">48.76 46.33 2.43 82.89 85.49 -2.60</cell></row><row><cell></cell><cell cols="2">KD-AT-Auto 1.47  *</cell><cell>0.8  *</cell><cell>49.05 48.80</cell><cell>0.25</cell><cell>84.26 84.47 -0.21</cell></row><row><cell></cell><cell>AT</cell><cell>-</cell><cell>-</cell><cell cols="3">24.79 19.75 5.04 57.33 57.42 -0.09</cell></row><row><cell>CIFAR-100</cell><cell>KD-AT</cell><cell>2</cell><cell>0.5</cell><cell cols="3">25.77 23.58 2.19 57.24 60.04 -2.80</cell></row><row><cell></cell><cell cols="4">KD-AT-Auto 1.53  *  0.83  *  26.36 26.24</cell><cell>0.12</cell><cell>58.80 59.05 -0.25</cell></row><row><cell></cell><cell>AT</cell><cell>-</cell><cell>-</cell><cell cols="3">17.20 15.40 1.80 47.72 47.62 0.10</cell></row><row><cell>Tiny-ImageNet</cell><cell>KD-AT</cell><cell>2</cell><cell>0.5</cell><cell cols="2">17.86 17.18 0.68</cell><cell>47.73 48.28 -0.55</cell></row><row><cell></cell><cell>KD-AT-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>note that alternative labeling of adversarial examples proposed in this paper is based on the fact that the predictive distribution of a classifier trained with empirical risk minimization can approximate the true label distribution of training examples. However, such approximation may not be accurate especially if the classifier is not carefully regularized during training. Post-training confidence calibration techniques such as temperature scaling and interpolation can only improve the approximation in terms of the entire training set, but cannot improve it in a sample-wise manner. How to learn the true label distribution of adversarial training examples during adversarial training more accurately remains an open problem.</figDesc><table><row><cell>Also, such alternative labeling also requires to train another independent classifier beforehand, which</cell></row><row><cell>induces additional training cost.</cell></row><row><cell>C More empirical analyses</cell></row><row><cell>C.1 Epoch-wise double descent is ubiquitous in adversarial training</cell></row><row><cell>In this section, we conduct extensive experiments with different model architectures, and learning</cell></row><row><cell>rate schedulers to verify the connection between robust overfitting and epoch-wise double descent.</cell></row><row><cell>The default experiment settings are listed in Appendix G.2 in detail.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance of our method with different neural architectures.</figDesc><table><row><cell cols="2">Architecture Setting</cell><cell>T</cell><cell>λ</cell><cell>Robust Acc. (%) Best Last Diff.</cell><cell>Standard Acc. (%) Best Last Diff.</cell></row><row><cell></cell><cell>AT</cell><cell>-</cell><cell>-</cell><cell cols="2">42.21 39.12 3.09 73.95 80.45 -6.50</cell></row><row><cell>VGG-19</cell><cell>KD-AT</cell><cell>2</cell><cell>0.5</cell><cell cols="2">43.59 42.69 0.90 74.30 77.80 -3.50</cell></row><row><cell></cell><cell cols="5">KD-AT-Auto 1.28  *  0.79  *  44.27 44.24 0.03 76.41 76.79 -0.38</cell></row><row><cell></cell><cell>AT</cell><cell>-</cell><cell>-</cell><cell cols="2">49.85 42.89 6.96 84.82 85.87 -1.05</cell></row><row><cell>WRN-28-5</cell><cell>KD-AT</cell><cell>2</cell><cell>0.5</cell><cell cols="2">51.08 48.40 2.68 85.36 86.88 -1.52</cell></row><row><cell></cell><cell>KD-AT-Auto</cell><cell>1.6  *</cell><cell cols="3">0.82  *  51.47 51.10 0.37 86.05 86.24 -0.19</cell></row><row><cell></cell><cell>AT</cell><cell>-</cell><cell>-</cell><cell cols="2">52.29 46.04 6.25 86.57 86.75 -0.18</cell></row><row><cell>WRN-34-10</cell><cell>KD-AT</cell><cell>2</cell><cell>0.5</cell><cell cols="2">53.11 50.97 2.14 86.41 88.06 -1.65</cell></row><row><cell></cell><cell>KD-AT-Auto</cell><cell>1.6  *</cell><cell>0.83</cell><cell></cell></row></table><note>* 54.17 53.71 0.46 87.69 88.01 -0.32</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance of our method with different adversarial training methods.</figDesc><table><row><cell>Method</cell><cell>Setting</cell><cell>T</cell><cell>λ</cell><cell>Robust Acc. (%) Best Last Diff.</cell><cell>Standard Acc. (%) Best Last Diff.</cell></row><row><cell></cell><cell>AT</cell><cell>-</cell><cell>-</cell><cell cols="2">48.50 45.53 2.97 82.79 82.68 0.11</cell></row><row><cell>TRADES</cell><cell>KD-AT</cell><cell>2</cell><cell>0.5</cell><cell cols="2">48.74 47.52 1.22 82.30 83.03 -0.73</cell></row><row><cell></cell><cell cols="5">KD-AT-Auto 1.12  *  0.82  *  48.75 48.39 0.36 82.44 82.80 -0.36</cell></row><row><cell></cell><cell>AT</cell><cell>-</cell><cell>-</cell><cell cols="2">41.96 35.39 6.57 85.91 87.20 -1.29</cell></row><row><cell>FGSM</cell><cell>KD-AT</cell><cell>2</cell><cell>0.5</cell><cell cols="2">42.82 41.61 1.21 86.69 87.93 -1.24</cell></row><row><cell></cell><cell cols="5">KD-AT-Auto 2.18  *  0.78  *  44.11 43.75 0.36 87.38 87.66 -0.28</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance of our method under different adversarial attacks. PGD-1000 refers to PGD attack with 1000 attack iterations, with step size fixed as 2/255 as recommended byCroce &amp; Hein  (2020).</figDesc><table><row><cell>Attacks</cell><cell>Setting</cell><cell>T</cell><cell>λ</cell><cell>Robust Acc. (%) Best Last Diff.</cell></row><row><cell></cell><cell>AT</cell><cell>-</cell><cell>-</cell><cell>50.64 43.00 7.64</cell></row><row><cell>PGD-1000</cell><cell>KD-AT</cell><cell>2</cell><cell>0.5</cell><cell>51.79 48.43 3.36</cell></row><row><cell></cell><cell cols="2">KD-AT-Auto 1.47  *</cell><cell>0.8  *</cell><cell>52.05 51.71 0.34</cell></row><row><cell></cell><cell>AT</cell><cell>-</cell><cell>-</cell><cell>53.47 48.90 4.57</cell></row><row><cell>Square Attack</cell><cell>KD-AT</cell><cell>2</cell><cell>0.5</cell><cell>54.39 52.92 1.47</cell></row><row><cell></cell><cell cols="4">KD-AT-Auto 1.28  *  0.79  *  55.23 55.17 0.06</cell></row><row><cell></cell><cell>AT</cell><cell>-</cell><cell>-</cell><cell>55.76 51.63 4.13</cell></row><row><cell>RayS</cell><cell>KD-AT</cell><cell>2</cell><cell>0.5</cell><cell>56.59 55.50 1.09</cell></row><row><cell></cell><cell>KD-AT-Auto</cell><cell>1.6  *</cell><cell cols="2">0.82  *  57.74 57.54 0.20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Performance of our method combined with SWA and an additional standard teacher. Auto + KD-Std + SWA 1.53 * 0.83 * 50.58 50.09 0.49 90.54 90.76 -0.22</figDesc><table><row><cell>Dataset</cell><cell>Setting</cell><cell>T</cell><cell>λ</cell><cell>Robust Acc. (%) Best Last Diff.</cell><cell>Standard Acc. (%) Best Last Diff.</cell></row><row><cell></cell><cell>AT</cell><cell>-</cell><cell>-</cell><cell cols="2">47.35 41.42 5.93 82.67 84.91 -2.24</cell></row><row><cell>CIFAR-10</cell><cell>KD-AT + KD-Std + SWA</cell><cell>2</cell><cell>0.5</cell><cell>49.98 49.89 0.09</cell><cell>85.06 85.52 -0.46</cell></row><row><cell></cell><cell cols="2">KD-AT-Auto + KD-Std + SWA 1.47  *</cell><cell>0.8  *</cell><cell cols="2">50.03 50.05 -0.02 84.69 84.91 -0.22</cell></row><row><cell></cell><cell>AT</cell><cell>-</cell><cell>-</cell><cell cols="2">47.83 39.77 8.06 90.18 91.11 -0.93</cell></row><row><cell>SVHN</cell><cell>KD-AT + KD-Std + SWA</cell><cell>2</cell><cell>0.5</cell><cell>47.88 46.46 1.42</cell><cell>91.59 91.76 -0.17</cell></row><row><cell></cell><cell>KD-AT-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Performance of our method combined with SWA with different hyper-parameters</figDesc><table><row><cell>Setting</cell><cell>s0</cell><cell>τ</cell><cell cols="2">Robust Acc. (%) Best Last Diff.</cell><cell>Standard Acc. (%) Best Last Diff.</cell></row><row><cell>KD-AT + KD-Std + SWA</cell><cell cols="5">80 0.999 49.00 48.04 0.96 84.04 86.11 -2.07</cell></row><row><cell cols="4">KD-AT-Auto + KD-Std + SWA 80 0.999 49.35 49.25</cell><cell>0.1</cell><cell>85.38 85.91 -0.37</cell></row><row><cell>KD-AT + KD-Std + SWA</cell><cell>0</cell><cell cols="2">0.999 49.01 48.01</cell><cell>1.0</cell><cell>83.78 86.20 -2.42</cell></row><row><cell>KD-AT-Auto + KD-Std + SWA</cell><cell>0</cell><cell cols="4">0.999 49.32 49.25 0.07 84.78 85.48</cell><cell>-0.7</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note this is a result only associated with the training set, thus is not dependent on the specific classifier.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2"> and VGG-11 have 9.13×10 6 , 11.17×10 6 and 9.23×10 6 parameters, respectively.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>adversarial training for sufficient epochs. Mathematically, our final method to determine the optimal temperature and interpolation ratio in rectified model probability can be described as T, λ = arg min T,λ E (x ,y )∼D val (λ • f θ (x ; T ) + (1 − λ) • f θs (x ; T ), y ) , <ref type="bibr">(37)</ref> where f θs (x ; T ) denotes the temperature-scaled predictive probability of a surrogate model on x .</p><p>Here the validation set is constructed by applying adversarial perturbation generated by f θ to the clean validation set. For adversarial perturbation we utilize PGD attack with 10 iterations, the perturbation radius as 8/255 and the step size as 2/255. Note that Such process incurs almost no additional computation as we simply obtain the logits of a surrogate classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Experimental details G.1 Settings for main experiment results</head><p>Dataset. We include experiment results on CIFAR-10, CIFAR-100, Tiny-ImageNet and SVHN.</p><p>Training setting. We employ SGD as the optimizer. The batch size is fixed to 128. The momentum and weight decay are set to 0.9 and 0.0005 respectively. Other settings are listed as follows.</p><p>• CIFAR-10/CIFAR-100: we conduct the adversarial training for 160 epochs, with the learning rate starting at 0.1 and reduced by a factor of 10 at the 80 and 120 epochs. • Tiny-ImageNet: we conduct the adversarial training for 80 epochs, with the learning rate starting at 0.1 and reduced by a factor of 10 at the 40 and 60 epochs. • SVHN: we conduct the adversarial training for 80 epochs, with the learning rate starting at 0.01 (as suggested by <ref type="bibr" target="#b5">(Chen et al., 2021)</ref>) and reduced by a factor of 10 at the 40 and 60 epochs.</p><p>Adversary setting. We conduct adversarial training with ∞ norm-bounded perturbations. We employ adversarial training methods including PGD-AT, TRADES and FGSM. We set the perturbation radius to be 8/255. For PGD-AT and TRADES, the step size is 2/255 and the number of attack iterations is 10.</p><p>Robustness evaluation. We consider the robustness against ∞ norm-bounded adversarial attack with perturbation radius 8/255. We employ AutoAttack for reliable evaluation. We also include the evaluation results again PGD-1000, Square Attack and RayS.</p><p>Neural architectures. We include experiments results on pre-activation ResNet-18, WRN-28-5, WRN-34-10 and VGG-19.</p><p>Hardware. We conduct experiments on NVIDIA Quadro RTX A6000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Settings for analyzing double descent in adversarial training</head><p>Dataset. We conduct experiments on the CIFAR-10 dataset, without additional data.</p><p>Training setting.</p><p>We conduct the adversarial training for 1000 epochs unless otherwise noted. By default we use SGD as the optimizer with a fixed learning rate 0.1. When we experiment on a subset (see below) we use the Adam optimizer to improve training stability, where the learning rate is fixed as 0.0001. The batch size will be fixed to 128, and the momentum will be set as 0.9 wherever necessary. No regularization such as weight decay is used. These settings are mostly aligned with the empirical analyse of double descent under standard training <ref type="bibr" target="#b29">(Nakkiran et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample size.</head><p>To reduce the computation load demanded by an exponential number of training epochs, we reduce the size of the training set by randomly sampled a subset of size 5000 from the original training set without replacement. We adopt this setting for extensive experiments for analyzing the dependence of epoch-wise double descent on the perturbation radius and data quality (i.e. Figure <ref type="figure">5</ref>).. Adversary setting. We conduct adversarial training with ∞ norm-bounded perturbations. We employ standard PGD training with the perturbation radius set to 8/255 unless otherwise noted. The number of attack iterations is fixed as 10, and the perturbation step size is fixed as 2/255. Robustness evaluation. We consider the robustness against ∞ norm-bounded adversarial attack with perturbation radius 8/255. We use PGD attack with 10 attack iterations and step size set to 2/255.</p><p>Neural architecture. By default we experiment on Wide ResNet <ref type="bibr" target="#b49">(Zagoruyko &amp; Komodakis, 2016)</ref> with depth 28 and widening factor 5 (WRN-28-5) to speed up training.</p><p>Hardware. We conduct experiments on NVIDIA Quadro RTX A6000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 Estimation of the data quality</head><p>In this section we elaborate on the calculation of data quality for analyzing the dependence on label noise in adversarial training.</p><p>We use the predicative probabilities of classifiers trained on CIFAR-10 to score its training data. Similar strategy is employed in previous works to select high-quality unlabeled data to improve adversarial robustness <ref type="bibr" target="#b44">(Uesato et al., 2019;</ref><ref type="bibr" target="#b3">Carmon et al., 2019;</ref><ref type="bibr" target="#b14">Gowal et al., 2020)</ref>. Slightly deviating from these works focusing on out-of-distribution data, we use adversarially trained instead of regularly trained models to measure the quality of in-distribution data, since under standard training almost all training examples will be overfitted and gain overwhelmingly high confidence. Specifically, we adversarially train a pre-activation ResNet-18 with PGD and select the model at the best checkpoint in terms of the robustness. The quality of an example is estimated by the model probability corresponding to the true label without adversarial perturbation and random data augmentation (flipping and clipping). We repeat this process 10 times with random initialization to obtain a relatively accurate estimation. Training setting. We conduct the standard training for 1000 epochs. We use Adam as the optimizer with a fixed learning rate 0.0001 to improve training stability with a small training set (see below). The batch size will be fixed to 128, and the momentum will be set as 0.9 wherever necessary. No regularization such as weight decay is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample size.</head><p>To reduce the computation load demanded by an exponential number of training epochs, we reduce the size of the training set by randomly sampled a subset of size 5000 from the original training set without replacement.</p><p>Neural architecture. By default we experiment on Wide ResNet <ref type="bibr" target="#b49">(Zagoruyko &amp; Komodakis, 2016)</ref> with depth 28 and widening factor 5 (WRN-28-5).</p><p>Hardware. We conduct experiments on NVIDIA Quadro RTX A6000. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Square attack: a query-efficient black-box adversarial attack via random search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Flammarion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<idno>abs/1912.00049</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<idno>abs/1802.00420</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reconciling modern machine-learning practice and the classical bias-variance trade-off</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mandal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="15849" to="15854" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unlabeled data improves adversarial robustness. ArXiv, abs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905">1905.13736, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rays: A ray searching method for hard-label adversarial attack</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust overfitting may be mitigated by properly learned smoothening</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>abs/1708.04552</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Max-margin adversarial (mma) training: Direct input space margin maximization through adversarial training</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y C</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/1812.02637</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Data profiling for adversarial training: On the ruin of problematic data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<idno>abs/2102.07437</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploring memorization in adversarial training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1606">abs/2106.01606, 2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Classification in the presence of label noise: A survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Frénay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="845" to="869" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Strictly proper scoring rules, prediction, and estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gneiting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="359" to="378" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1412.6572</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Uncovering the limits of adversarial training against norm-bounded adversarial examples. ArXiv, abs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010.03593, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>abs/1706.04599</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The elements of statistical learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1603.05027</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning with a strong adversary</title>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
		<idno>abs/1511.03034</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adversarial examples are not bugs, they are features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Averaging weights leads to wider optima and better generalization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<idno>abs/1803.05407</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T P</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/1609.04836</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adversarial machine learning at scale. ArXiv, abs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1236">1611.01236, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Tiny imagenet visual recognition challenge</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><surname>Sgdr</surname></persName>
		</author>
		<title level="m">Stochastic gradient descent with warm restarts. arXiv: Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<idno>abs/1706.06083</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robustness via curvature regularization, and vice versa</title>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="9070" to="9078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep double descent: Where bigger models and more data hurt</title>
		<author>
			<persName><forename type="first">P</forename><surname>Nakkiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kaplun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1912.02292</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploring generalization in deep learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards the science of security and privacy in machine learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Wellman</surname></persName>
		</author>
		<idno>abs/1611.03814</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Concentration inequalities for multinoulli random variables. ArXiv, abs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fruit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pirotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaric</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001.11595, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fixing data augmentation to improve adversarial robustness</title>
		<author>
			<persName><forename type="first">S.-A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Calian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<idno>abs/2103.01946</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Overfitting in adversarially robust deep learning. ArXiv, abs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002.11569, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Low curvature activations reduce overfitting in adversarial training</title>
		<author>
			<persName><forename type="first">V</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feizi</surname></persName>
		</author>
		<idno>abs/2102.07861</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cyclical learning rates for training neural networks</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Relating adversarially robust generalization to flat minima</title>
		<author>
			<persName><forename type="first">D</forename><surname>Stutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno>abs/2104.04448</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Is robustness the cost of accuracy? -a comprehensive study on the robustness of 18 deep image classification models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<idno>abs/1312.6199</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<title level="m">Robustness may be at odds with accuracy. arXiv: Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adversarial risk and the dangers of evaluating against weak attacks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>O'donoghue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<idno>abs/1802.05666</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Are labels required for improving adversarial robustness? ArXiv, abs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905">1905.13725, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Inequalities for the l1 deviation of the empirical distribution</title>
		<author>
			<persName><forename type="first">T</forename><surname>Weissman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ordentlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Seroussi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Verdú</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
		<title level="m">Adversarial weight perturbation helps robust generalization. arXiv: Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking bias-variance trade-off for generalization of neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Understanding generalization in adversarial training via the bias-variance decomposition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dobriban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<idno>abs/2103.09947</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<title level="m">Wide residual networks. ArXiv, abs/1605.07146</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><surname>Mixup</surname></persName>
		</author>
		<idno>abs/1710.09412</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Theoretically principled trade-off between robustness and accuracy. ArXiv, abs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1901">1901.08573, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
