<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conjecturing-Based Discovery of Patterns in Data*</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-24">24 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">J</forename><forename type="middle">Paul</forename><surname>Brooks</surname></persName>
							<email>jpbrooks@vcu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Systems</orgName>
								<orgName type="institution">Virginia Commonwealth University</orgName>
								<address>
									<postCode>23284</postCode>
									<settlement>Richmond</settlement>
									<region>VA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
							<email>dedwards7@vcu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Statistical Sciences and Operations Research</orgName>
								<orgName type="institution">Virginia Commonwealth University</orgName>
								<address>
									<postCode>23284</postCode>
									<settlement>Richmond</settlement>
									<region>VA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Craig</forename><forename type="middle">E</forename><surname>Larson</surname></persName>
							<email>clarson@vcu.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Mathematics and Applied Mathematics</orgName>
								<orgName type="institution">Virginia Commonwealth University</orgName>
								<address>
									<postCode>23284</postCode>
									<settlement>Richmond</settlement>
									<region>VA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nico</forename><surname>Van Cleemput</surname></persName>
							<email>nico.vancleemput@gmail.com</email>
							<affiliation key="aff3">
								<orgName type="department">Department of Applied Mathematics, Computer Science and Statistics</orgName>
								<orgName type="institution">Ghent University</orgName>
								<address>
									<settlement>Ghent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Conjecturing-Based Discovery of Patterns in Data*</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-24">24 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">58BE52EFDFDDC441986331C542088CA0</idno>
					<idno type="arXiv">arXiv:2011.11576v3[cs.LG]</idno>
					<note type="submission">Article submitted to ; manuscript no.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-10-31T05:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>automated conjecturing</term>
					<term>computational scientific discovery</term>
					<term>interpretable artificial intelligence</term>
					<term>nonlinear pattern discovery</term>
					<term>boolean pattern discovery Set u = 0</term>
					<term>b = 0</term>
					<term>C = ∅ generateTree(u</term>
					<term>b) Set u = 0 u+ = 2 end if end while Set tree = new tree with single node generateTreeRec(tree</term>
					<term>u</term>
					<term>b) procedure generateTreeRec(tree</term>
					<term>u</term>
					<term>b) 20: generateLabeledTree(tree) Add child to v</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose the use of a conjecturing machine that generates feature relationships in the form of bounds involving nonlinear terms for numerical features and boolean expressions for categorical features. The proposed Conjecturing framework recovers known nonlinear and boolean relationships among features from data.</p><p>In both settings, true underlying relationships are revealed. We then compare the method to a previouslyproposed framework for symbolic regression and demonstrate that it can also be used to recover equations that are satisfied among features in a dataset. The framework is then applied to patient-level data regarding COVID-19 outcomes to suggest possible risk factors that are confirmed in medical literature.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Modern machine learning methods allow one to leverage complex relationships present in data to generate accurate predictions but do not reveal them to the investigator (Section 3). We propose an automated conjecturing framework for discovering nonlinear and boolean relationships among the features in a given dataset. Our primary goal is discovery -to provide the investigator with a manageable number of suggested relationships to inspire future investigation for validation.</p><p>The nonlinear relationships are produced in the form of bounds. Bounds are useful for scientific discovery from numeric data because they 1) suggest direct and indirect relationships among features, 2) suggest a functional form for the relationships, and 3) can subsequently be used as boolean features (e.g., is this bound satisfied by an observation?) for discovering more complex boolean relationships. Whereas previous related approaches seek to find equations for numeric data, our Conjecturing method produces bounds for numeric data, boolean expressions for discrete data, and bounds and boolean expressions for mixed data. <ref type="bibr" target="#b46">Udrescu and Tegmark (2020)</ref> proposed a system called AI Feynman that combines deep learning with methods for symbolic regression to recover nonlinear relationships in data. Impressively, they recover over 100 equations of varying complexity from data. In contrast to AI Feynman, our Conjecturing framework uses Fajtlowicz's Dalmatian heuristic <ref type="bibr" target="#b14">(Fajtlowicz 1995)</ref> to discover bounds rather than equations. Further, our framework can also be applied to categorical data to discover boolean relationships among features and already-discovered bounds. This work represents the first application of the Dalmatian heuristic to learning both nonlinear and boolean relationships from data. The bounds and conditions produce interpretable yet complex relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Previous Related Work</head><p>In this section, we provide background on our Conjecturing framework including examples of uses of bounds and sufficient conditions, a description of the core algorithm, and a survey of previous related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Motivating Examples for Conjectured Bounds and Sufficient Conditions</head><p>The algorithm we use to conjecture feature relationships is an adaptation of an algorithm that was originally designed to conjecture relationships for mathematical objects. To illustrate the potential value of bounds and sufficient conditions, we describe two problems and relevant results from graph theory. This paper extends these ideas regarding bounds and sufficient conditions to learning from data.</p><p>A graph is a collection of nodes V and edges E that are ordered pairs of nodes. Consider the problem of finding bounds for the independence number of a graph 1 . It is well known that the linear programming (LP) relaxation of an appropriate integer program provides a upper bound on the independence number <ref type="bibr" target="#b39">(Schrijver 2003)</ref>. The Lovász ϑ number of a graph also provides an upper bound that is known to be no larger than the LP relaxation bound for any graph <ref type="bibr" target="#b29">(Lovász 1979)</ref>.</p><p>Therefore, the Lovász ϑ bound dominates the LP relaxation bound, and such relationships are 1 The independence number is the largest number of nodes in a graph no two of which are contained in an edge. The definition of independence number is not important for this example, but only the fact that with every graph is associated a number called the "independence number". commonly pursued. However, relationships among bounds can be more nuanced. Consider a third bound on the independence number due to <ref type="bibr" target="#b17">Haemers (1979)</ref>. For some graphs it is a stronger bound than Lovász ϑ while on other graphs it is a weaker bound; for some graphs, Lovász ϑ is a sharp bound and Haemers's bound is not while for other graphs, Haemers's bound is a sharp bound and Lovász ϑ is not. It remains an open question whether there are a "small" number of bounds where the largest for value for any graph would provide a sharp bound on the independence number. In this paper, we describe a computational approach to discover bounds among numeric features in a dataset. As with the independence number, collections of bounds can provide valuable insight into relationships for the system from which the data was collected. Now consider the problem of finding sufficient conditions for a graph to be Hamiltonian 2 . <ref type="bibr" target="#b10">Chvátal (1972)</ref> proved that for a graph G with certain conditions on the vertex degrees, G is Hamiltonian. Also, <ref type="bibr" target="#b11">Chvátal and Erdös (1972)</ref> proved that if a graph satisfies a connectivity condition, then it is Hamiltonian. These are two conditions that are sufficient for a graph to be Hamiltonian, but neither implies the other. Some graphs satisfy both conditions, some graphs satisfy one condition, and some graphs satisfy neither condition. The existence and discovery of a (small) set of sufficient conditions that characterize all Hamiltonian graphs remains an open area of research. The pursuit of sufficient conditions of graph properties such as Hamiltonicity mirrors that of bounds <ref type="bibr" target="#b28">(Larson and Van Cleemput 2017)</ref>. In the context of learning from data, we show how categorical data, together with bounds discovered among numeric features, can be used as input to a computational approach for generating sufficient conditions for a property of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The Dalmatian Heuristic</head><p>Our Conjecturing framework is based on an implementation of Fajtlowicz's Dalmatian heuristic <ref type="bibr">(Fajtlowicz 1995, Larson and</ref><ref type="bibr" target="#b28">Van Cleemput 2017)</ref>. The heuristic was originally implemented in Graffiti <ref type="bibr" target="#b14">(Fajtlowicz 1995)</ref> which was the first program to produce research conjectures that led to new mathematical theory. The program produces statements that are relations between mathematical invariants which are numerical attributes of examples. Recent implementations of the Dalmatian heuristic have been applied to the discovery of relationships for graphs <ref type="bibr">(Larson and</ref> Van Cleemput 2016) and game strategies <ref type="bibr" target="#b6">(Bradford et al. 2020)</ref>. The heuristic was adapted to work with properties which are boolean attributes of examples by <ref type="bibr" target="#b28">Larson and Van Cleemput (2017)</ref>. We built our framework using a more recent implementation of the Dalmatian heuristic available here: http://nvcleemp.github.io/conjecturing/.</p><p>We now describe invariant conjecturing using Fajtolwicz's Dalmatian heuristic. The inputs include the following. Let E be a set of examples of a given type (e.g., graphs or data observations).</p><p>Article submitted to ; manuscript no.</p><p>Let A = {α 1 , α 2 , . . . , α m } be real number invariants. In this work the examples are n data observations and the invariants are m numeric features. The real-numbered value of example i for invariant α j is α j (i) = x ij for i = 1, . . . , n and j = 1, . . . , m. Let O be a collection of unary operators and binary operators. Examples of unary operators include adding 1, squaring, square-rooting, and division by 2. Binary operators include addition, multiplication, and subtraction. Let α * ∈ A be the invariant for which upper and lower bounds are of interest, and let α * (i) be the value of the invariant of interest for example i.</p><p>The aim is to generate conjectured bounds that are true for any realization of input examples E.</p><p>The Dalmatian heuristic provides criteria for generating conjectured bounds that are the best for E. Algorithm 1 provides a way to generate expressions of increasing complexity, apply the heuristic, and store conjectures. The complexity of an expression is the number of nodes in the corresponding expression tree (Figure <ref type="figure" target="#fig_0">1</ref>) and is the sum of the number of invariants, number of unary operators, and number of binary operators. The algorithm proceeds by generating unlabeled trees and then labeling the nodes with operators and invariants. Expressions satisfying the Dalmatian heuristic conditions are retained as conjectures C. For a conjecture c, let c(i) be the conjectured bound for example i.</p><p>With examples E, invariants A, operators O, invariant of interest α * , an upper limit on the proportion of missing values allowed for an invariant skips, and a direction indicating if the algorithm will produce upper or lower bounds (U P P ER or LOW ER), procedure Conjecturing-INV is called (Algorithm 1, line 1). The number of unary nodes u and binary nodes b of an expression tree are initialized to zero and the conjectures C is initialized to the empty set (Algorithm 1, line 2). Line 3 of Algorithm 1 refers to the stopping criteria of the expression generator. For invariant conjecturing for upper bounds, if the minimum conjectured bound is tight for each example (i.e., min c∈C c(i) = α * (i) for i ∈ E), then the expression generator is stopped. Otherwise, expression generation continues until a time limit is reached. Line 4 calls a procedure to generate a tree, the branching nodes of which will be operators and the leaf nodes are invariants. Lines 5-11 help to enumerate every tree where each vertex connected to a leaf node has degree one or two. These branching nodes will correspond to unary or binary operators, respectively, when the tree is labeled. The leaf nodes will correspond to invariants. Unlabeled trees are grown recursively and then the nodes are labeled with operators and invariants.</p><p>The procedure generateTree (Algorithm 1, line 4) creates a new tree with a single node, then calls procedure generateTreeRec to add new nodes until there are u unary nodes and b binary nodes. The procedure generateTreeRec (Algorithm 1, line 19) either calls generate-LabeledTree to apply labels by assigning invariants to leaf nodes and operators to branching nodes to generate an expression (Algorithm 1, line 21), or adds nodes to grow the tree (lines 23-32).  we implement the following design choice. In our implementation, when a tree is labeled, operators can be reused, but invariants cannot. We make this design choice so that more expressions can be generated in a smaller amount of time. In Section 5.4, we will demonstrate how this limitation can be overcome in situations where repeating invariants is warranted.  </p><formula xml:id="formula_0">+ ÷ x 1 x 2 x 3 (a) ÷ × pow × k m 1 m 2 r 2 (b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>38:</head><p>for each fully labeled tree do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>39:</head><p>if the corresponding bound c is valid for all examples in E and is not dominated by existing bounds in C. then</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>40:</head><p>Set C = C ∪ c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>41:</head><p>Remove dominated conjectures from C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>42:</head><p>end if</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>43:</head><p>end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>44: end procedure</head><p>The stopping criterion for property conjecturing for the case that direction is SU F F ICIEN T is obtaining a set of conjectures where for every example with π * (i) = true, each example evaluates to true for at least one conjecture. Otherwise, expression generation continues until a time limit is reached.</p><p>The Dalmatian heuristic for property conjectures is applied as follows. A conjectured sufficient condition c is only retained in the database of conjectures C if the expression passes the following two tests:   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Other Related Work</head><p>In this section, we explain how our work is related to previous work in automated scientific discovery, machine learning interpretability, automated feature engineering, and empirical model building.</p><p>Symbolic regression has been used as a tool for automated scientific discovery. Symbolic regression is the use of genetic programming to approximate a target function on training data and generalize to produce predictions on new data <ref type="bibr" target="#b32">(Nicolau and Agapitos 2021)</ref> and until the work of <ref type="bibr" target="#b38">Schmidt and Lipson (2009)</ref>, the focus was on improving prediction accuracy by approximating an underlying function rather than a focus on discovering true functional relationships among fea-tures. <ref type="bibr" target="#b38">Schmidt and Lipson (2009)</ref>  aid in revealing relationships among observations. Jantzen (2020) proposes an algorithm with the similar purpose of detecting types of dynamical systems called dynamical kinds. Subsequently, these kinds "are then targets for law-like generalization" (Jantzen 2020). While Jantzen's work provides a method for discovering the kinds, it does not suggest how to recover the "laws". It is these relationships that we aim to discover with the Conjecturing framework.</p><p>Our work is distinguished from these previous works in that 1) we focus on generating bounds for invariants that serve as hypotheses for the investigator rather than recover true functional forms or generate accurate predictions, 2) our invariant conjecturing algorithm is paired with a property conjecturing algorithm for discovering both nonlinear bounds and boolean relationships, 3) our framework is designed for a given static observational dataset rather than on discovering laws for dynamical systems, and 4) rather than a stochastic search over the space of functional forms, our Conjecturing system leverages sophisticated techniques for enumerating expressions of increasing complexity (described in <ref type="bibr" target="#b27">Larson and Van Cleemput (2016)</ref> for "noiseless" data involving mathematical objects such as graphs). In our system, the human remains "in the loop" to evaluate the plausibility of suggested bounds and conditions. <ref type="bibr" target="#b8">Brunton et al. (2016)</ref> introduce SINDy for combining sparse regression and expert knowledge to developing models of dynamical systems. We adopt a similar approach to incorporating prior knowledge in Section 5.4 for the Nguyen benchmark suite <ref type="bibr" target="#b31">(Nguyen et al. 2011)</ref> where we provide the Conjecturing framework with candidate non-linear functions as building blocks. Unlike our Conjecturing framework, theirs is designed for recovering equations governing dynamical systems rather than bounds and theirs is not capable of recovering boolean relationships.</p><p>Langley (2019) provides a review of past efforts in computational scientific discovery. Several frameworks have their origins in analyzing mass spectroscopy and other electrochemical data.</p><p>Bacon <ref type="bibr" target="#b25">(Langely et al. 1987</ref>) is a general framework for scientific discovery based on suggesting and executing a series of designed experiments. <ref type="bibr" target="#b40">Tallorin et al. (2018)</ref> proposed a method called POOL that uses Bayesian optimization and machine learning in an iterative fashion for experiments to discover peptide substrates for enzymes. Bacon and POOL both make recommendations regarding additional data to collect while our system assumes that a fixed dataset is provided that may or may not be the result of a designed experiment.</p><p>Precise definitions of "explainability" and "interpretability" are still being developed <ref type="bibr" target="#b48">(Vilone and Longo 2020</ref><ref type="bibr" target="#b30">, Lu et al. 2019</ref><ref type="bibr" target="#b15">, Fürnkranz et al. 2020)</ref> as research in the area has rapidly accelerated.</p><p>According to the convention of <ref type="bibr" target="#b35">Rudin (2019)</ref>, explainability is concerned with post-hoc analyses of black box models to create simple explanations of model behavior. Motivated by observed accuracies of deep learning models, work in this area includes identifying important features for prediction, building simple local models, conducting sensitivity analyses, and deriving prototype examples <ref type="bibr" target="#b37">(Samek and</ref><ref type="bibr">Müller 2019, Elton 2020)</ref>. <ref type="bibr" target="#b43">Tsang et al. (2018a</ref><ref type="bibr">Tsang et al. ( ,b, 2020) )</ref> develop neural network frameworks for identifying sets of features for which there is an interaction -a non-additive relationship among predictive features that influence a response value. These methods provide explainability in that they identify sets of features that interact, but the framework is not designed to reveal the functional form of the nonlinear interaction.</p><p>Rudin (2019) advocates the development of interpretable models where the mechanisms for predictions are simple relationships that are readily apparent to the investigator. Much of the recent work in this area is in the development of decision rules (e.g., <ref type="bibr" target="#b18">(Hammer and Bonates 2006</ref><ref type="bibr" target="#b12">, Dash et al. 2018</ref><ref type="bibr" target="#b3">, Bellomarini et al. 2020</ref>)) or decision lists and trees (e.g., <ref type="bibr" target="#b49">(Wang and Rudin 2015</ref><ref type="bibr" target="#b51">, Wang et al. 2017</ref><ref type="bibr" target="#b36">, Rudin and Ertekin 2018</ref><ref type="bibr" target="#b4">, Bertsimas and Dunn 2017</ref><ref type="bibr" target="#b47">, Verwer and Zhang 2019</ref><ref type="bibr" target="#b5">, Blanquero et al. 2021</ref><ref type="bibr" target="#b1">, Aghaei et al. 2021</ref><ref type="bibr" target="#b2">, Akyüz and Birbil 2021)</ref>). Different from these works, our Conjecturing framework automates the discovery of nonlinear features. In addition, as with work on decision rules in general, our framework can combine the discrete features in data with the discovered nonlinear features to discover a potentially richer set of boolean relationships when compared to optimization-based trees and decision lists. <ref type="bibr" target="#b24">Khurana et al. (2018)</ref> propose a system that leverages reinforcement learning to search expression trees for predictive features. ExploreKit <ref type="bibr" target="#b23">(Katz et al. 2016</ref>) is a framework for automatic feature engineering that combines features using basic arithmetic operations and then uses machine learning to rank and select those with high predictive ability. The Data Science Machine (Kanter and Veeramachaneni 2015) automatically generates features for entities in relational databases with possible dependencies between tables followed by singular value decomposition. In none of these works is model transparency evaluated but rather only model performance. An important distinction of our work from these is that they focused on improving prediction accuracy, sometimes at the expense of understandable features, and not on scientific discovery.</p><p>Traditional statistical methods for empirical model building (e.g. regression analysis) tend to focus on first-and second-order polynomial models; interaction terms up to a certain degree are often included. Empirical models are intended to provide adequate prediction performance while also providing a simple assessment of feature importance via model coefficients. Techniques such as all-subsets, stepwise selection, and regularization methods (e.g., LASSO <ref type="bibr" target="#b42">(Tibshirani 1996</ref>)) are commonly used to perform feature selection over model spaces of increasing complexity. However, domain knowledge is typically required for reciprocal or non-polynomial relationships. Our</p><p>Conjecturing framework provides a search over a much broader class of nonlinear functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Two Motivating Examples</head><p>In this section, we describe two datasets where a "typical" knowledge discovery workflow fails to reveal important relationships among features.</p><p>Research on machine learning does, of course, lead to conjectured relationships between variables which are in turn used to make predictions of one or more variables in terms of others. A trained neural net, for instance, can be viewed as a black box representing a function which produces an output for every input in its domain. These functions are complex and of a different character than classical scientific laws: in particular, there is little hope of deriving these functions or relationships from simpler existing laws. Our Conjecturing framework aims to help fill this gap in current capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Discovering Gravity</head><p>In this example, a numeric invariant of interest is determined by a more complex nonlinear relationship with three numeric predictors. Consider measurements including the masses of two objects m 1 and m 2 , their distance r, and the gravitational force between them F . The goal is to recover the dependence of F on m 1 , m 2 , and r, or</p><formula xml:id="formula_1">F = k m 1 m 2 r 2 ,</formula><p>where k is the gravitational constant. Following the demonstration by <ref type="bibr" target="#b25">Langely et al. (1987)</ref>, we create a fictional dataset using a predefined value for k that is a random number between 0 and 1. For our illustrative example, we generated 1,000 training data points and 1,000 test data points with k = 0.057098. Values for m 1 , m 2 , and r are samples from Uniform(1,100000) distributions, and F is calculated for each sample with no noise.</p><p>A linear regression model will fail to capture the nonlinear interaction of the variables. Offthe-shelf machine learning methods such as random forests and neural networks can leverage the nonlinear relationship in the data but cannot present the relationship to the investigator. In the next section, we propose a framework for producing bounds on F that are functions of the other features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Discovering an Interaction in Real Estate Valuation Data</head><p>The second example is a case where a boolean variable of interest is completely determined by the product of two numeric features in the dataset; i.e., the second-order interaction term completely defines the relationship.</p><p>Consider a dataset on residential real estate properties for sale obtained from https://www. redfin.com. The goal is to predict whether a home with given feature values has a list price above or below $300,000.</p><p>This dataset includes both the price per square foot and total square footage along with eight additional features such as the number of bathrooms and bedrooms. The property of interest (above vs. below) can be determined by multiplying the price per square foot by square footage and setting a threshold. Thus, the interaction of price per square foot and square footage, hereafter called the active interaction, completely describes the relationship between the predictors and response.</p><p>Data are partitioned into a training dataset with 1,000 houses and a test dataset with 30,156 houses. In the next section, we leverage our framework for invariant bounds and then extend it to produce boolean relationships to recover the active interaction term and how it determines class membership.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">A Conjecturing Framework for Discovering Patterns in Data</head><p>We now describe a framework that leverages a conjecturing algorithm to discover nonlinear and boolean feature relationships in data. All experiments were run on a computer with an Intel i7-2600 CPU @ 3.4GHz and 16 GB RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Conjecturing for Nonlinear Relationships</head><p>The invariant version of the conjecturing method (procedure Conjecturing-INV) can be used for discovering nonlinear relationships in data. Invariant conjectures are generated that provide upper and lower bounds on the invariant of interest. These conjectures are the nonlinear functions that can be used as new features and/or as a complete model for the system.</p><p>For the gravity case from Section 3.1, the invariants are A = {F, m 1 , m 2 , r} and the invariant of interest is the force F . The examples E are the observations in the data.</p><p>The Conjecturing framework is not designed to recover constants such as the gravitational constant k. In general, for a functional relationship with a constant k such that 0 &lt; k &lt; 1, the expression without the constant provides a lower bound for the response and the reciprocal expression provides an upper bound. In cases where the constant is not between 0 and 1, the converse is true. For this example, F = km 1 m 2 /r 2 with 0 &lt; k &lt; 1 and so</p><formula xml:id="formula_2">r 2 /m 1 m 2 ≤ F ≤ m 1 m 2 /r 2 . The</formula><p>Conjecturing framework can potentially recover the bounds F ≤ m 1 m 2 /r 2 or F ≥ r 2 /m 1 m 2 .</p><p>For our example, Conjecturing-INV returns 19 upper bounds and 24 lower bounds for F .</p><p>Among the upper bounds is</p><formula xml:id="formula_3">F ≤ m 1 m 2 /r 2 ,</formula><p>which approximates the true gravity relationship used to generate the data. The bound does not include the constant k. The lower bound of r 2 /m 1 m 2 is not recovered. Other bounds generated by</p><formula xml:id="formula_4">Conjecturing-INV include F ≤ 2m 2 / √ r,<label>(1)</label></formula><formula xml:id="formula_5">F ≤ 2|m 1 − m 2 |,<label>(2)</label></formula><formula xml:id="formula_6">F ≥ 8m 2 /r 2 ,<label>(3)</label></formula><formula xml:id="formula_7">F ≥ −1/(r − 2m 2 ).<label>(4)</label></formula><p>Eight of the upper bounds and 15 of lower bounds for F are depicted in Figure <ref type="figure" target="#fig_1">2</ref>. The upper bound m 1 m 2 /r 2 in Figure <ref type="figure" target="#fig_1">2</ref>(a) is blue, while the true value km 1 m 2 /r 2 is gold.</p><p>As the primary goal of our approach is discovery, the bounds produced are suggestions that require further validation. We consider it a success that the relationship F ∝ m 1 m 2 /r 2 is included in one of the bounds. Among the other bounds produced, we see that true relationships F ∝ m 1 , F ∝ m 2 , and F ∝ 1/r 2 are all suggested, along with false relationships F ∝ 1/ √ r and F ∝ 1/r.</p><p>Follow up investigations can be used to inspect these relationships and potentially recover the gravitational constant. The lower bound that is not recovered, while valid, does not reflect the true proportionality relationships which may be why it is dominated by other bounds. An approximation of the gravitational constant of 0.057 could be represented as (+1 + 1 + 1 + 1 + 1) × 10 −1−1 + (+1 + 1 + 1 + 1 + 1) × 10 −1−1−1 which by itself has complexity 19. The expression m 1 m 2 /r 2 has complexity 6.</p><p>In this example, the Conjecturing framework recovers the true nonlinear relationship up to a constant of proportionality along with 44 additional suggested bounds. Therefore, isolating a single true bound, in the case where the bound is unknown, can require additional analysis and/or experiments. The additional bounds can provide potential insight into feature interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Conjecturing for Nonlinear and Boolean Relationships with Mixed Data</head><p>Our Conjecturing framework for mixed data leverages the invariant version (procedure Conjecturing-INV) and the property version (procedure Conjecturing-PROP) of the conjecturing algorithm. For mixed data, we propose a framework to produce conjectures of nonlinear and boolean patterns. These conjectures can capture complex patterns while maintaining interpretability.</p><p>We assume that we are given a dataset with numeric features N , boolean features B, and a categorical feature of interest with levels Y. Note that a categorical feature with more than two levels can be converted to a series of boolean features. Let π y be the property that an observation has value y, for y ∈ Y.</p><p>For each level y ∈ Y, the algorithm discovers bounds for the numeric features that are satisfied by each observation in the class (Algorithm 2, Lines 4-14). These inequalities are converted to properties of the form "if the inequality is satisfied, then true; false, otherwise" (Algorithm 2, Line 12). These new properties are combined with the original boolean features in the data (Algorithm 2, Line 13). The properties from across all classes are pooled together and the observations belonging to all classes are pooled together as examples and then, for each level y ∈ Y, the property version of conjecturing is applied to discover sufficient conditions for π y (Algorithm 2, Lines 15-20).</p><p>We now provide further details on Algorithm 2 using the real estate valuation case from Section 3.2 as an illustrative example. First, we convert the categorical feature propertyType into boolean features condo, mobileHome, singleFamily, townhouse, multiFamily2-4Unit, multifFamily5PlusUnit, and Other. We also add a feature that is a constant value of 300,000 for each observation because it is the price cutoff and call it 300K. </p><p>(squareFootage</p><formula xml:id="formula_9">? ≤ 300K/pricePerSquareFoot + bathrooms)<label>(23)</label></formula><p>(squareFootage</p><formula xml:id="formula_10">? ≥ 300K/(pricePerSquareFoot + 1)). (<label>24</label></formula><formula xml:id="formula_11">)</formula><p>These properties can be used as boolean features that indicate whether a nonlinear relationship among numeric features is satisfied for an observation.</p><p>The properties generated for each level {below, above} are collected in a set Π along with π y and the seven original boolean features (Algorithm 2, Line 13).</p><p>For An inspection of the data reveals that for some of the houses, there is some rounding error when comparing the price to the square footage multiplied by the price per square foot. The conjecturing algorithm compensates by using invariants as error terms. In the first property, the error term is bathrooms × pricePerSquareFoot. In the second property, the error term is squareFootage + 1.</p><p>When these properties are applied as classification rules for predicting whether a house will be above or below $300,000, they produce no error on the training data. The first property misclassifies 37 of 30,156 houses in the test data for an accuracy of 0.999. The second property misclassifies 26 houses. The misclassified houses are due to rounding error and miscoding of data. For example, one house in the test data is listed as having 31,248 bathrooms and another is listed as having a price of $459.</p><p>Despite the noise and rounding error in the data, the Conjecturing framework was able to recover the active interaction term and these properties can be used as features for classifiers with near-perfect accuracy.</p><p>Hereafter, we use "Conjecturing framework" to imply: Output: A set of conjectured properties P. </p><formula xml:id="formula_12">Set R = R ∪ R U ∪ R L .</formula><p>11:</p><p>end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Convert the new invariant relations R into properties Π R .</p><p>13: 1. In the case that all features are numeric, apply procedure Conjecturing-INV.</p><formula xml:id="formula_13">Set Π = Π ∪ π y ∪ Π R . /*</formula><p>2. In the case that all features are categorical, convert the features to a series of properties (boolean features) and apply procedure Conjecturing-PROP.</p><p>3. In the case of mixed data, apply Algorithm 2.</p><p>If there is no invariant of interest or no property of interest, each invariant and/or property can serve as the invariant/property of interest in turn, and conjectures can be generated for each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Additional Computational Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Sensitivity to the Number of Features</head><p>To investigate the impact of the number of features on the performance on the Conjecturing framework, we conduct experiments adding noisy features to the gravity example. We use the same experimental setup as described in Section 4.1 including a time limit of five seconds. For k = 0, . . . , 10, we add k noise invariants generated from a standard normal distribution and check Figure <ref type="figure">4</ref> contains plots of the number of conjectures produced, number of expressions evaluated, and the number of valid expressions produced within the five second time limit. The number of conjectures produced and expressions evaluated increases as the number of columns increases and tends to be larger for lower bounds than upper bounds. The number of conjectures prodcued ranges from 22 to 174. The number of valid expressions fluctuates between 75,000 and 135,000 and there is no discernible pattern effect of the number of noise invariants k. As k increases, there are more invariants available and the number of low-complexity expressions increases exponentially as does the number of low-complexity expressions comprised of the noise invariants. Low-complexity expressions can be generated and checked more quickly which is why the number of expressions evaluated increases with k. The number of expressions evaluated in five seconds ranges between about 300,000 and 1.4 million.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Sensitivity to Training Examples</head><p>To investigate the effect of different subsets of training examples on the ability of the Conjecturing framework to recover true relationships, we apply the framework to the real estate experiment </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4</head><p>The number of conjectures produced, expressions tested, and valid experessions produced as a function of the number of noise invariants added to the gravity experiment.</p><p>with 10 random samples of 1,000 training examples. We use the same experimental setup as described in Section 2 including a time limit of five seconds.  In this section, we compare the ability of Conjecturing to recover equations from datasets used by <ref type="bibr" target="#b46">Udrescu and Tegmark (2020)</ref> with their algorithm AI Feynman. We then apply the implementation of AI Feynman to the gravity and real estate datasets described in Section 3. We note that the primary goal of Conjecturing is for discovery of nonlinear and boolean relationships while the primary goal of AI Feynman is recovery of equations.</p><p>Performance on Feynman Equations. We apply Conjecturing to the first 10 equations listed in Table <ref type="table" target="#tab_16">4</ref> of <ref type="bibr" target="#b46">(Udrescu and Tegmark 2020)</ref> to draw comparisons based on solution time and noise tolerance. We used the data published by the authors here: https://space.mit.edu/home/tegmark/aifeynman.html. As in <ref type="bibr" target="#b46">(Udrescu and Tegmark 2020)</ref>, for each instance we apply Conjecturing with three subsets of operators in turn:</p><formula xml:id="formula_14">{+, −, ×, ÷, +1, −1, 2 , √ }, {+, −, ×, ÷, +1, −1, 2 , √ , sin, cos, ln, −1 , e}, {+, −, ×, ÷, +1, −1, 2 , √ , sin, cos, ln, −1 , e, | • |, sin −1 , tan −1 }.</formula><p>For instances where an equation includes the constant π, we include π as a constant invariant. For each instance, we use the first 10 samples in each dataset and run Conjecturing for 10, 100, and 1,000 seconds. We also run Conjecturing for the noise tolerance of and time required by AI Feynman to recover the equations as reported in Table <ref type="table" target="#tab_16">4</ref> of <ref type="bibr" target="#b46">(Udrescu and Tegmark 2020)</ref>.</p><p>Article submitted to ; manuscript no.</p><p>Tables <ref type="table" target="#tab_12">2 and 6</ref> contain the results of applying Conjecturing to the datasets. Conjecturing produces bounds that match the equation for four of the ten instances. Conjecturing finds a match for all equations with complexity 10 or less and is unable to find a match for equations with higher complexity. <ref type="bibr" target="#b46">Udrescu and Tegmark (2020)</ref>  Despite the fact that Conjecturing is not designed for recovery of equations, we see that it can be successful in doing so for lower-complexity, albeit nonlinear, equations.</p><p>Table <ref type="table">6</ref> in the Appendix contains the results of applying Conjecturing to the datasets when noise is added to the invariant of interest. The noise in the table is the standard deviation of the normal distribution with mean zero added to the invariant of interest; the noise is the noise level tolerated by AI Feynman as reported by <ref type="bibr" target="#b46">Udrescu and Tegmark (2020)</ref>. The time is the time reported by <ref type="bibr" target="#b46">Udrescu and Tegmark (2020)</ref> for AI Feynman to recover the equation.</p><p>Conjecturing is unable to achieve exact recovery of the equations with the introduction of noise. The NRMSE is less than 1.0 for six of the 10 equations and does not exceed 1.625. The strict requirements of the Dalmatian heuristic seem to prevent exact recovery in the presence of noise, but Conjecturing is still able to produce good approximations of the equations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance of AI Feynman (Udrescu and Tegmark 2020) on Gravity and Real</head><p>Estate Examples. We apply the implementation of AI Feynman available here: https: //github.com/SJ001/AI-Feynman to the gravity example described in Section 3.1 and the real estate example described in Section 3.2. A difference between our gravity example and the datasets used by <ref type="bibr" target="#b46">Udrescu and Tegmark (2020)</ref> is that for the gravity example, the gravitational constant G is the same for every data point, but for the datasets used by <ref type="bibr" target="#b46">Udrescu and Tegmark (2020)</ref>, G is treated as a variable and is different for each point.</p><p>For the real estate data, we apply AI Feynman to the data to attempt to discover the relationship between the property of interest and the input features. The original numeric features are supplied along with boolean features corresponding to the levels of the propertyType feature. Note that AI Feynman is designed for recovering numeric functions and is therefore not suitable for boolean relationships such as those in the real estate example.</p><p>For both instances, the AI Feynman implementation aborts with an error regarding an eigenvalue calculation. We suspect that the source of the failure in both cases may be due to the difference in treatment of constants. In our gravity example, the gravitational constant is the same for all points while in analogous examples, <ref type="bibr" target="#b46">Udrescu and Tegmark (2020)</ref> treat constants as variables and generate a unique value for each observation. Our practice of treating the gravitational constant as the same for all observations may be contributing to an error in matrix calculations for AI Feynman. In the real estate example, each observation has a feature with the same value (the $300,000 cutoff). This constant column in the data matrix could also be contributing to an error in matrix calculations for AI Feynman. In the electronic companion, we include the code and output for AI Feynman applied to 1) their Example 1, demonstrating that our installation is functional, 2) our gravity example, including the error message, and 3) our real estate example, including the error message. These examples show that the Conjecturing framework can provide useful insights on examples where AI Feynman cannot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Experiments with the Nguyen Benchmark Suite Nguyen et al. (2011)</head><p>We apply our invariant conjecturing implementation to the Nguyen benchmark suite <ref type="bibr" target="#b31">(Nguyen et al. 2011</ref>) so as to draw comparisons with symbolic regression methods described by <ref type="bibr" target="#b34">Petersen et al. (2021)</ref>. The Nguyen benchmark suite is a set of 12 equations. As mentioned before, our</p><p>Conjecturing framework is designed for discovering nonlinear relationships in the form of bounds Article submitted to ; manuscript no.</p><p>and boolean relationships while symbolic regression methods are designed to recover equations.</p><p>In these experiments, we investigate the ability of invariant conjecturing to recover equations (or approximations) among the discovered bounds.</p><p>The benchmark equations are in Table <ref type="table" target="#tab_13">3</ref>. We generate an instance for each using the protocols described by <ref type="bibr" target="#b34">Petersen et al. (2021)</ref>. For each equation, 20 training examples and 20 test examples are generated. For equations 1 though 6, x is sampled from a Uniform(-1,1) distribution; for equation 7, x is sampled from a Uniform(0,2) distribution; for equation 8, x is sampled from a Uniform(0,4) distribution; for equations 9 through 12, x and y are sampled from a Uniform(0,1) distribution. For each equation, we allow a time limit 10,000 seconds for generating upper and lower bounds. The operators include unary operators sine, cosine, natural log, and natural exponential, and binary operators addition, subtraction, multiplication, and division.</p><p>As noted in Section 2.2, our Conjecturing framework does not allow repeated invariants in conjectures. Therefore, most of the equations in Table <ref type="table" target="#tab_13">3</ref> cannot be recovered by our framework.</p><p>For each equation, we first evaluate the ability of the conjectured bounds to approximate the equation by reporting the normalized root mean squared error (NRMSE) as defined and reported by <ref type="bibr" target="#b34">Petersen et al. (2021)</ref> and described in Section 5.3. We report NRMSE for the conjecture with the lowest mean average error for the training examples.</p><p>The results in Table <ref type="table" target="#tab_13">3</ref> indicate that our Conjecturing framework is able to recover only equation 11 f = x y . It is able to do so despite the fact that the exponent operator is not included.</p><p>Conjecturing produces the expression e y log(x) and simplified it to x y . The NRMSE values for the best bounds for the other instances range from 0.22 to 1.08. These values are larger than those reported for symbolic regression methods as reported in Table <ref type="table" target="#tab_20">10 of</ref>  For expressions 1 through 8, the only invariants are the invariant of interest f and the input invariant x. Because of the fact that no invariants can be repeated, the Conjecturing framework is limited to the application of only unary operators to x and no expressions with binary operators are produced. As an example, for the first equation, the best conjectured lower bound is sin(sin(sin(sin(log(sin(sin(cos(cos(sin(e </p><formula xml:id="formula_15">f (x) ≥ sin(e e cos(</formula><formula xml:id="formula_16">x )))))))))))</formula><p>).</p><p>We now describe how our Conjecturing framework can be adapted to allow for repeated invariants, and report results for the adapted method. To address repeated invariants, we can add additional invariants using commonly-occurring functional forms. For equations 1 through 8, we add invariants x 2 , x 3 , x 4 , x 5 , x 6 , sin(x), cos(x), √ x along with two copies of the constant 1, and the constant 2. Two copies of the constant 1 are included because it appears twice in equation 7. For equations 9 through 12 we also add y 2 as an invariant. Recall that in our implementation, while invariants cannot repeat in a conjectures, operators can. Therefore, an alternative approach to addressing the constants is to include the unary operator of addition by 1.</p><p>As shown in Table <ref type="table" target="#tab_13">3</ref>, the Conjecturing framework is able to exactly recover each equation as a bound when the additional invariants are included so that the NRMSE values are 0.000 for all equations.</p><p>The practice of adding the invariants that are nonlinear functions of the original input might appear to be impractical. However, as suggested by <ref type="bibr" target="#b8">Brunton et al. (2016)</ref>, specifying these invariants can reflect expert knowledge on the system being investigated. They note that identifying candidate functions for SINDy "must be a coordinated effort to incorporate expert knowledge, feature extraction, and other advanced methods." Conjecturing offers distinct capabilities for discovery, as nonlinear functions can be specified as invariants or may still be discovered so long as they do not involve repeated input invariants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Application to COVID-19 Data</head><p>In this section, we demonstrate the Conjecturing framework on synthetic patient-level COVID-19 data that was provided as part of the Veterans Health Administration (VHA) Innovation</p><p>Ecosystem and precisionFDA COVID-19 Risk Factor Modeling Challenge (https://precision.</p><p>fda.gov/challenges/11/view). The data include synthetic veteran patient health records including medical encounters, conditions, medications, and procedures. All subjects are located in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Massachusetts.</head><p>Article submitted to ; manuscript no.</p><p>The goal of the challenge was to better understand risk and protective factors for COVID-19 outcomes. Participants were asked to predict alive/deceased status. Since our goal is to discover potential risk and protective factors, we evaluate the performance of Conjecturing by checking the performance of the feature relationships on holdout test data rather than on prediction accuracy. Establishing the risk and protective factors as causal would require additional controlled experiments.</p><p>Predictions were based on information obtained through December 31, 2019. In the training data, we drop all information pertaining to events on or after January 1, 2020 and drop subjects who died before January 1, 2020.</p><p>We add a number of features for each patient based on the data, described in Table <ref type="table">7</ref> in the Appendix. In addition, for each numeric observation, we created invariants for the mean and most recent value and for each reported allergy, device, immunization, procedure, and discretelymeasured observation we create a property corresponding to each level. In total, we use 309 invariants and 362 properties. We use a training set consisting of 100 subjects from each outcome class (deceased/alive). We compare the results of applying Conjecturing with classification and regression trees (CART) <ref type="bibr" target="#b7">Breiman et al. (1984)</ref> which is another interpretable method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Results for Conjecturing</head><p>Upper and lower bounds are generated for each invariant, and for each outcome. These bounds, along with the 362 properties in the data, are used as properties for Conjecturing-PROP.</p><p>Conjectures are generated for both outcomes. The parameter skips is set to 90%. We use the remaining 73,497 subjects as a test data set thereby allowing us to ascertain the effects of potential overfitting to the 200 subjects used for training.</p><p>Among those with COVID-19 in the test data, 5,468 (8.0%) have a status of deceased, and 68,029 (92.0%) are alive. There are 38 conjectures for sufficient conditions for alive status and 40 conjectures for sufficient conditions for deceased status produced by the framework. Tables 8-11 in the Appendix contain the conjectures and evaluations.</p><p>Tables <ref type="table" target="#tab_19">9 and 11</ref> in the Appendix contain quantitative evaluations of the performance of the conjectures. Each table contains the precision, support, and lift of each conjecture. Note that each conjecture is a sufficient condition expressed as a conditional statement. The precision is the percentage of test examples for which the conditional statement evaluates to true among those for which the antecedent is true. Precision may be thought of as the "hit rate" of the conjecture. The support is the number of test examples for which the antecedent evaluates to true. The lift is the ratio of the precision to the proportion of examples for which the consequent is true. If the lift is greater than 1, then the conjecture is better at identifying people for which the consequent is true than a random selection from the population.</p><p>Of the 38 conjectures for alive status, 22 (57.9%) have lift at least 1.00. The lift ranges from 0.81 to 1.07; note that the maximum possible lift for a conjecture for alive status is 1.08 (1/(68029/73497)).</p><p>Of the 40 conjectures for deceased status, 34 (85%) have lift at least 1.00. The lift ranges from 0.17 to 4.15.</p><p>Consider the sufficient conditions for deceased status in The conjecture indicates that subjects in the east who are older and have a larger percentage of medications covered by the payer are at higher risk of death. The presence of longitude in the conjecture could be an indication of higher risk in population centers in the east such as Boston.</p><p>The percentage of medications covered by the VHA is higher for subjects with more preexisting conditions and for those with more expensive medications because there is a low copay annual cap (currently $700 3 ). Further, the conjecture produces a suggestion of a functional form for the relationship between these factors. The conjecture confirms the CDC guidance that older subjects and those with more preexisting conditions are a higher risk of death from Covid 4 .</p><p>The conjecture with the second-highest precision and lift is medicationsActive &gt; hemoglobinA1cHemoglobinTotalInBlood → Deceased, with a lift of 3.29. The condition includes the number of active medications and the ratio of hemoglobin A1c to total hemoglobin (an HbA1c test). The conjecture indicates that those with more active medications than the HbA1c percentage are at higher risk of death, which again agrees with the CDC guidance concerning preexisting conditions. Typical values for HbA1c for nondiabetic patients are below 5.7%, while diabetic subjects can have values between 6.5% and 10.0%</p><p>5 . The number of active medications can indicate a larger number of preexisting conditions, and the conjecture suggests that for diabetic patients, those with additional conditions are at higher risk.</p><p>The conjecture with the third-highest precision and lift is age &gt; carbonDioxide × potassium → Deceased, with a lift of 3.12. The conjecture suggests that older subjects with lower CO 2 levels and lower potassium are at higher risk of death. Lower CO 2 levels and abnormal potassium levels, particularly lower levels, has been independently studied and associated with COVID-19 morbidity and mortality <ref type="bibr" target="#b19">(Hu et al. 2021</ref><ref type="bibr" target="#b33">, Noori et al. 2022</ref>). In addition to validating a role for these invariants, the conjecture suggests a potential nonlinear relationship among them.</p><p>For both outcomes, the Conjecturing framework is able to generate new sufficient conditions that are true for the respective outcome at higher rates than would be expected for a patient selected at random. These results indicate that the conjecturing process is capturing relationships that hold across the population and are not merely reflective of the 200 training samples. In other words, overtraining appears to be mitigated. The discovered relationships, and the direct and indirect relationships that they indicate among features, are validated by the medical literature and provide suggestions for deeper investigations into the functional form of the relationships and the extent of causality. The number of conjectures generated is not overwhelming for a human investigator to consider and further investigate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Comparison with an Interpretable Model</head><p>We now consider the results of applying classification and regression trees (CART) <ref type="bibr" target="#b7">(Breiman et al. 1984)</ref>      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure 1 Expression trees for (a) an upper bound on square footage x1/x2 + x3 where x1 is 300K, x2 is pricePerSquareFoot, and x3 is bathrooms and (b) gravitational force km1m2/r 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2 displays (a) upper bounds and (b) lower bounds derived for test instances for data generated based on a formula for gravity. The gray curves correspond to bounds, and each must be the best on at least one training example instance in order to be retained. More details on this experiment are provided in Section 3.1 and Section 4.1. Algorithm 1 can be adapted for property conjecturing with few modifications. We now detail the differences. Let E be a set of examples and let Π = {π 1 , π 2 , . . . , π m } be properties. The examples are n data observations and the properties are m boolean features. The truth value of example ifor property π j is π j (i). Let O be the following collection of operators: NOT (¬), AND (&amp;), OR (|), XOR (exclusive or) (⊕), and IMPLIES (→). NOT is a unary operator and the remaining operators are binary operators. Let π * ∈ Π be the property for which sufficient and/or necessary conditions are of interest, and let π * (i) be the truth value of the property of interest for example i.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 2 (a) Upper bounds and (b) lower bounds generated for gravitational force using Conjecturing-INV, the invariant version of the conjecturing algorithm. Instances from the training data are on the x-axis. The gold curve is the true value for the instances. The blue curve in (a) is the true value without the constant of proportionality and is one of the upper bounds. The red curve is the (a) maximum and (b) minimum of the discovered bounds.</figDesc><graphic coords="8,75.99,244.10,360.00,146.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 3 Schematic of property conjecturing. In (a), conditions 1 and 2 evaluate to true for a subset of examples with the property of interest and for no samples that do not have the property of interest. Condition 3 evaluates to true for examples with and without the property of interest, and so it is discarded. In (b), the union of sufficient conditions covers all examples with the property of interest.</figDesc><graphic coords="9,75.99,294.38,360.01,200.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1:</head><label></label><figDesc>Set P = ∅. /* Initialize properties set. */ 2: Set A = {α j : j ∈ N }. /* Define the set of invariants to be the original numeric features in the data. */ 3: Set Π = {π j : j ∈ B}. /* Define the set of properties to be the original boolean features in the data. */ 4: for y ∈ Y do /* Loop on the levels of the categorical feature of interest. */ 5: Set R = ∅. /* Initialize invariant relations set. */ 6: Set E = {i : π y }. /* Define the set of observations with level y as the examples. */ 7: for j ∈ N do /* Loop on original numeric features. */ 8: Set R U = Conjecutring-INV(E, A, O, α j , U P P ER) /* Submit examples, invariants, and the invariant of interest to the invariant version of Conjecturing for upper bounds. */ 9: Set R L = Conjecturing-INV(E, A, O, α j , LOW ER) /* Submit examples, invariants, and the invariant of interest to the invariant version of Conjecturing for lower bounds. */ 10:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>1) whether Conjecturing-INV recovers m 1 m 2 /r 2 , 2) the number of conjectures produced, 3) the number of expressions evaluated, and 4) the number of valid expressions produced. Valid expressions are bounds that are valid for all training examples.For k = 0, . . . , 6, Conjecturing-INV recovers m 1 m 2 /r 2 as a conjectured upper bound and for k = 7, . . . , 10 the bound is not recovered because of the additional noise invariants. The number of original invariants is five including the force F , so in this experiment we can add more than 100% additional invariants and still recover the true proportionality relationship F ∝ m 1 m 2 /r 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc><ref type="bibr" target="#b34">Petersen et al. (2021)</ref>; the methods include deep symbolic regression (DSR)<ref type="bibr" target="#b34">Petersen et al. (2021)</ref>, priority queue training (PQT)<ref type="bibr" target="#b0">Abolafia et al. (2018)</ref>, vanilla policy gradient (VPG)<ref type="bibr" target="#b34">Petersen et al. (2021)</ref>, genetic programming (GP), and a method implemented in Mathematica based on Markov chain Monte Carlo and nonlinear regression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The procedure is the same for generating lower bounds with the only difference being how the Dalmatian heuristic criteria are evaluated in lines 39 and 41.The computational requirements of Algorithm 1 increase exponentially with the number of invariants and with the number of operators. The computation time increases with the number of examples because of the check in Step 39. To facilitate generation of more candidate expressions in less time, one can use fewer examples as input to the algorithm. To achieve additional efficiency,</figDesc><table><row><cell>bound.</cell></row><row><cell>Line 41 ensures that the number of conjectures is no larger than the number of examples; i.e.,</cell></row><row><cell>|C| ≤ |E|.</cell></row></table><note>1.(Truth test). The candidate conjecture α * (i) ≤ c(i) is true for all examples i ∈ E, and 2. (Non-dominance test.) There is an example i where c(i) &lt; min{c (i) : c ∈ C \ {c}}. That is, the candidate conjecture would give a better bound for α * (i) than any previously conjectured (upper)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>: Examples E, Invariants A, operators O, invariant of interest α * , invariant missing value limit skips, direction (U P P ER or LOW ER). Output: Conjectured C in the form of conjectured bounds on the invariant of interest α * .</figDesc><table><row><cell>Algorithm 1 Invariant Conjecturing.</cell></row><row><cell>The aim is to generate conjectured sufficient or necessary conditions for the property of interest</cell></row><row><cell>that are valid for any realization of input examples E. The algorithm for property conjecturing</cell></row><row><cell>procedure Conjecturing-PROP generates unlabeled trees as in Algorithm 1 but then labels</cell></row><row><cell>the nodes with operators and properties. Logical expressions satisfying the Dalmatian heuristic</cell></row><row><cell>conditions are retained as conjectures C. For a conjecture c, let c(i) be the conjectured truth value</cell></row><row><cell>for example i.</cell></row><row><cell>The inputs to property conjecturing are examples E, properties Π, operators O, a property of</cell></row><row><cell>interest π  *  , and a direction (SU F F ICIEN T , N ECESSARY ) indicating if the algorithm will</cell></row><row><cell>produce sufficient or necessary conditions for the property of interest.</cell></row></table><note>Input</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>The original binary features from the data are also encoded as properties for a total of 1, 280 + 1, 457 + 7 = 2, 744 properties. Examples of encoded properties from the invariant relations are:</figDesc><table><row><cell></cell><cell cols="2">Article submitted to ; manuscript no.</cell></row><row><cell cols="2">bathrooms ≤ 2 × bedrooms ?</cell><cell>(5)</cell></row><row><cell cols="2">bedrooms ≥ bathrooms − 1 bathrooms ≤ 2 × bedrooms</cell><cell>(6) (21)</cell></row><row><cell cols="2">lotSize ≥ (squareFootage − yearBuilt) × bedrooms (yearBuilt</cell><cell>(7)</cell></row><row><cell cols="2">Also included are less-interpretable bounds such as:</cell><cell></cell></row><row><cell cols="2">yearBuilt ≥ hoaPerMonth × log(10)/ log(2 × daysOnMarket)</cell><cell>(8)</cell></row><row><cell>daysOnMarket ≤ e e</cell><cell>√ 2×lotSize</cell><cell>(9)</cell></row><row><cell cols="2">hoaPerMonth ≤ 10 2×bathrooms + squareFootage</cell><cell>(10)</cell></row><row><cell cols="3">There were also several bounds discovered that are close approximations of the relationship</cell></row><row><cell cols="2">present in the active interaction term, including</cell><cell></cell></row><row><cell cols="2">squareFootage ≤ 300K/pricePerSquareFoot + bathrooms</cell><cell>(11)</cell></row><row><cell cols="2">squareFootage ≤ 300K/pricePerSquareFoot + bedrooms</cell><cell>(12)</cell></row><row><cell cols="2">squareFootage ≤ 300K/pricePerSquareFoot + daysOnMarket</cell><cell>(13)</cell></row><row><cell cols="2">squareFootage ≤ 300K/(pricePerSquareFoot − 1) − 1</cell><cell>(14)</cell></row><row><cell cols="2">pricePerSquareFoot ≤ −300K/(bedrooms − squareFootage)</cell><cell>(15)</cell></row><row><cell cols="2">pricePerSquareFoot ≤ 300K/squareFootage</cell><cell>(16)</cell></row><row><cell cols="2">300K ≥ −(bathrooms − squareFootage) × pricePerSquareFoot</cell><cell>(17)</cell></row><row><cell cols="3">For houses with property above, there are 1,457 bounds derived including a mix of simple relations</cell></row><row><cell cols="3">mobileHome, singleFamily, townhouse, multiFamily2-4Unit, multifFamily5PlusUnit, Other} (Algo-and less intuitive relations. Also included are the following three relations that are nearly identical</cell></row><row><cell>rithm 2, Line 3). to the active interaction relation:</cell><cell></cell><cell></cell></row><row><cell cols="3">In our training set, there are 1,000 observations that are used as examples. For each value</cell></row><row><cell cols="3">of the property of interest, {below, above}, the corresponding observations serve as the examples squareFootage ≥ 300K/(pricePerSquareFoot + 1) (18)</cell></row><row><cell cols="3">(Algorithm 2, Line 6). For each numeric feature, upper and lower bounds on that feature are pricePerSquareFoot ≥ 300K/squareFootage + 1 (19)</cell></row><row><cell cols="3">found that are functions of the other numeric features (Algorithm 2, Lines 8-9). These are found</cell></row></table><note>The resulting 18 features are partitioned into numeric features N = {bedrooms, bathrooms, squareFootage, lotSize, yearBuilt, daysOnMarket, pricePerSquareFoot, hoaPerMonth, latitude, longitude, 300K} (Algorithm 2, Line 2) and boolean features B = {condo, by applying the invariant relations version of the conjecturing method (Conjecturing-INV).For houses with property below, there are 1,280 bounds derived. Included are plausible relations concerning house features that are seemingly irrelevant to the classification task such as300K ≤ (pricePerSquareFoot + 1) × squareFootage(20)The resulting invariant relations are pooled together (Algorithm 2, Line 10). The invariant relations are encoded as properties (Algorithm 2, Line 12).?≥ hoaPerMonth × log(10)/ log(2 × daysOnMarket))</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>each level {below, above}, apply the property version of conjecturing to the properties Π with the training data observations serving as the examples E and level as the property of interest (Algorithm 2, Lines 15-20). The result is a set of properties that are sufficient conditions for the levels.</figDesc><table><row><cell>bathrooms ≥ −300K/pricePerSquareFoot + squareFootage</cell><cell>→ isBelow</cell><cell>(25)</cell></row><row><cell>squareFootage ≥ (300K + 1)/(pricePerSquareFoot − 1)</cell><cell>→ isAbove</cell><cell>(26)</cell></row></table><note>Conjecturing-PROP returns only two properties. They both approximate the underlying active interaction.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Article submitted to ; manuscript no.Algorithm 2 Conjecturing framework for nonlinear and boolean relationships with mixed data Input: Data observations {1, . . . , n} with numeric features N , boolean features B, and a categorical feature of interest with levels Y; a set of invariant operators O and a set of property operators P .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Article submitted to ; manuscript no.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Conjectures</cell><cell></cell><cell></cell><cell></cell><cell cols="5">Expressions Evaluated</cell><cell></cell><cell cols="4">Valid Expressions</cell><cell></cell></row><row><cell>150</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1e+05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1e+06</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5e+04</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5e+05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0e+00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0e+00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Number of Noise Invariants</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Bound</cell><cell></cell><cell>Upper</cell><cell></cell><cell>Lower</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 1</head><label>1</label><figDesc>contains the conjectures produced by the Conjecturing framework for each of the 10 replications. As in the experiment in Section 4.2, the Conjecturing framework makes use of invariants and operators to compensate for rounding error. The invariants employed as tolerances are bathrooms and longitude. The framework also employs operators +1, −1, and • to account for deviations from the underlying active interaction. Each of the bounds can be rewritten in terms of squareFootage × pricePerSquareFoot −300K plus or minus a small error term containing at most one additional invariant. Therefore, we see that in this instance, the method is not sensitive</figDesc><table /><note>to the choice of training examples. Further experiments are needed to understand how well the Conjecturing framework can recover underlying relationships in the presence of different kinds of noise. This is the subject of future work.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 1</head><label>1</label><figDesc>Sufficient Conditions Produced by Conjecturing for Different Training Set Samples for the Real Estate</figDesc><table><row><cell>Valuation Data</cell></row></table><note>squareF ootage ≥ (300K + 1)/(priceP erSquareF oot − 1) → isAbove 7 squareF ootage ≤ (300K + longitude)/priceP erSquareF oot → isBelow squareF ootage ≥ (300K + 1)/(priceP erSquareF oot − 1)) → isAbove 8 bathrooms ≥ −300K/priceP erSquareF oot + squareF ootage → isBelow squareF ootage ≥ (300K + 1)/(priceP erSquareF oot − 1)) → isAbove 9 squareF ootage ≤ (300K − daysOnM arket)/priceP erSquareF oot → isBelow squareF ootage ≥ (300K + 1)/(priceP erSquareF oot − 1) → isAbove 10 bathrooms ≥ −300K/priceP erSquareF + squareF ootage → isBelow squareF ootage ≥ (300K + 1)/(priceP erSquareF oot − 1)) → isAbove 5.3. Comparison to AI Feynman (Udrescu and Tegmark 2020)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>report that AI Feynman resolves all of the equations while Eureqa<ref type="bibr" target="#b38">(Schmidt and Lipson 2009)</ref> resolves four of the ten equations, three of which are different from those found by Conjecturing. These results indicate that Conjecturing is well suited for recovering equations of complexity 10 or less within 1,000 seconds and sometimes within much shorter times. Higher-complexity formulas with more invariants require additional time.Equations I.6.2 and I.6.2.b in Table2each have a repeated invariant σ. As noted in Section 2.2, Conjecturing does not allow repeated invariants and so these equations will not be recoverable as bounds. In Section 5.4, we describe ways to address this deficiency and recover equations such as I.6.2 and I.6.2.b.</figDesc><table /><note>The normalized root-mean-square error (NRMSE) calculated for 100 test examples for the bestperforming conjecture on the training data based on mean absolute error. NRMSE is calculated as 1/σ f multiplied by the root-mean-square error on the test examples, where σ f is the standard deviation of the invariant of interest for the test examples. Conjecturing produced conjectures with NRMSE less than 1.0 for 7 of the 10 equations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 2</head><label>2</label><figDesc>Results for Conjecturing on Datasets from (Udrescu and Tegmark 2020)Gm 1 m 2 (x 2 −x 1 ) 2 +(y 2 −y 1 ) 2 +(z 2 −z 1 ) 2</figDesc><table><row><cell></cell><cell></cell><cell>Number of</cell><cell></cell><cell></cell><cell></cell><cell>Recovered</cell></row><row><cell cols="2">Instance Equation I.6.2.a f = e −θ 2 /2 / √ I.6.2 f = e −θ 2 /2σ 2 / 2π √ 2πσ 2 I.6.2.b f = e −(θ−θ 1 ) 2 /2σ 2 / √ 2πσ 2</cell><cell cols="5">Invariants Complexity Time (s) NRMSE by Eureqa? 2 9 10 3 0.000 No 3 13 NA 0.553 No 5 16 NA 1.092 No</cell></row><row><cell>I.8.14</cell><cell cols="2">d = (x2 − x1) 2 + (y2 − y1) 2 4</cell><cell>10</cell><cell>10 3</cell><cell>0.000</cell><cell>No</cell></row><row><cell>I.9.18</cell><cell></cell><cell>9</cell><cell>17</cell><cell>NA</cell><cell>1.100</cell><cell>No</cell></row><row><cell>I.10.7</cell><cell>m = m 0 c 2 1− v 2</cell><cell>3</cell><cell>9</cell><cell>5</cell><cell>0.000</cell><cell>No</cell></row><row><cell>I.11.19</cell><cell>A = x1y1 + x2y2 + x3y3</cell><cell>6</cell><cell>11</cell><cell>NA</cell><cell>1.014</cell><cell>Yes</cell></row><row><cell>I.12.1</cell><cell>F = µNn</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>0.000</cell><cell>Yes</cell></row><row><cell>I.12.2 I.12.4</cell><cell>F = q 1 q 2 4π r 2 E f = q 1 4π r 2</cell><cell>5 4</cell><cell>12 11</cell><cell>NA NA</cell><cell>0.671 0.135</cell><cell>Yes Yes</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 3</head><label>3</label><figDesc>Results for Conjecturing on the Nguyen Benchmark Suite<ref type="bibr" target="#b31">Nguyen et al. (2011)</ref> </figDesc><table><row><cell>Without Additional Invariants With Additional Invariants</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Table 10 in the Appendix. The conjecture with the highest precision and lift is longitude &gt; −age × medicationsLifetimePercCovered → Deceased, and has a lift of 4.15 meaning that a subject for which longitude &gt; −age × medicationsLifetimePercCovered is 4.15 times as likely to die as a randomly selected subject.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>to the COVID-19 data. A model is fit using the implementation in the R library rpart Nodes 4, 6, and 10 correspond to sufficient conditions for alive status, and nodes 7 and 11 correspond to sufficient conditions for deceased status. Tables4 and 5contain the conditions and quantitative evaluations of the conditions produced by CART. When comparing the results with those of Conjecturing in Tables 8-11 in</figDesc><table><row><cell>(Therneau and Atkinson 2019). The tree produced by rpart is n= 200 node), split, n, loss, yval, (yprob) * denotes terminal node 1) root 200 100 False (0.50000000 0.50000000) 2) age&lt; 66.48323 92 17 False (0.81521739 0.18478261) 4) active_care_plan_length&lt; 33.3039 68 5 False (0.92647059 0.07352941) * 5) active_care_plan_length&gt;=33.3039 24 12 False (0.50000000 0.50000000) 10) Body_Height&gt;=167.15 8 1 False (0.87500000 0.12500000) * 11) Body_Height&lt; 167.15 16 5 True (0.31250000 0.68750000) * 3) age&gt;=66.48323 108 25 True (0.23148148 0.76851852) 6) mean_Potassium&gt;=4.85 10 4 False (0.60000000 0.40000000) * 1. produces many fewer conditions (3 versus 38 for alive status, 2 versus 40 for deceased status), 2. produces conditions that are in conjunctive normal form where each clause consists of a single numeric bound while Conjecturing tends to leverage nonlinear relationships among invariants as the basis for conditions, 3. produces two conditions with much larger support in the test data than those produced by Conjecturing (node 4 has 36,531 and node 7 has 12,443), 4. produces only two conditions that have lift greater than 1.0 (node 4 has lift 1.06 and node 7 has lift 2.85), and 5. does not produce conditions with better precision or lift than the best conditions produced 7) mean_Potassium&lt; 4.85 98 19 True (0.19387755 0.80612245) the Appendix, we note that CART by Conjecturing.</cell></row></table><note>*Nodes 4, 6, 7, 10, and 11, correspond to leaf nodes. Each leaf node corresponds to a sufficient condition for deceased status (True) or alive status (False).We note that both CART and Conjecturing are able to leverage categorical variables for conditions, though CART does not do so for this training set. An example of such a condition is conjecture 27 in Table10in the Appendix.Similar to many decision tree frameworks, CART leverages univariate bounds as component properties in its invariant clauses. Conjecturing is unlikely to derive numeric bounds for individual features but instead produces more nonlinear relationships between invariants. Decision tree frameworks such as CART and Conjecturing are complementary approaches for discovery of patterns among numeric and categorical features, but we see that Conjecturing is capable of producing more complex yet interpretable relationships.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 4</head><label>4</label><figDesc>Conditions from CART for Alive/Deceased Status Among Those with COVID</figDesc><table><row><cell>Node Number</cell></row></table><note>age &lt; 66.48 &amp; activeCarePlanLength ≥ 33.30 &amp; bodyHeight &lt; 167.15 → Deceased</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 5</head><label>5</label><figDesc>Evaluation of Conditions from CART Among Those with COVIDWe have demonstrated that automated search for conjectured feature-relations can enhance existing learning algorithms. The discovery of these kinds of feature relationships can also initiate new collaboration with domain scientists and lead to new scientific knowledge.Our Conjecturing framework was able to recover the functional form for gravity with only the measured force, masses, and distance. The framework also recovered a hidden interaction between price per square foot, square footage, and price in real estate data that leads to improved classification performance. Using synthetic patient-level COVID-19 data, the framework produced conjectures that provide insight into the risk of death.Another area for potential research involves the so-called p &gt;&gt; n problem. That is, if the number of features is larger than the number of observations, then there are insufficient degrees of freedom to estimate a linear model with all p features or any more complex model. In such situations, feature and/or model selection tools are needed to search over potentially large model spaces. As Gm 1 m 2 (x 2 −x 1 ) 2 +(y 2 −y 1 ) 2 +(z 2 −z 1 ) 2</figDesc><table><row><cell cols="4">Node Number Consequent Precision Support Lift</cell></row><row><cell>4</cell><cell>Alive</cell><cell>98.28%</cell><cell>36531 1.06</cell></row><row><cell>6</cell><cell>Alive</cell><cell>81.69%</cell><cell>2070 0.88</cell></row><row><cell>10</cell><cell>Alive</cell><cell>91.57%</cell><cell>5371 0.99</cell></row><row><cell>7</cell><cell>Deceased</cell><cell>21.17%</cell><cell>12443 2.85</cell></row><row><cell>11</cell><cell>Deceased</cell><cell>5.37%</cell><cell>3746 0.72</cell></row><row><cell>7. Conclusions</cell><cell></cell><cell></cell><cell></cell></row></table><note>The current version of Conjecturing requires that conjectures are true for every example. Future research will adjust the algorithm to better handle noisy data by generating conjectures that do not necessarily hold for all examples. If the handling of noise can be improved, thenConjecturing may be able to be adapted to support predictive modeling efforts.If the Conjecturing framework can provide functional relationships without constants of proportionality, the constant can be determined using regression with the original data. Suppose that the Conjecturing framework indicates a relationship between the response y and predictors x of the form y ≤ b 1 f (x) for an unknown constant b 1 . A regression model can be fit of the form ŷ = b 0 + b 1 f (x) using the data (x i , y i ), i = 1, . . . , n. The best strategy for determining constants of proportionality is another avenue for future research.45 two-way interactions. In this example, simply considering models with only 10 variables requires searching over a model space larger than 2.9 billion. Future research will investigate the ability of the Conjecturing framework to simplify model spaces and hence, provide a mechanism for a more expeditious search of plausible models.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 8</head><label>8</label><figDesc>Conjectures for Alive Status Among Those with COVID Alive 13 age &lt; lifetimeConditions × log(latitude) → Alive 14 anemiaDisorder &amp; activeConditionLength &lt; min{ureaNitrogen, |activeCarePlanLength|} → Alive 15 lifetimeCarePlans &gt; max{DALY, potassium } → Alive 16 healthcareCoverage &gt; DALY × lifetimeCarePlanLength → Alive 17 lifetimeConditionLength &gt; (encountersLifetimeTotalCost − 1)/proceduresLifetime → Alive 18 healthcareCoverage &gt; lifetimeConditionLength 2 /imagingStudiesLifetime → Alive 19 immunizationsLifetimeCost &gt; (bodyHeight − 1) immunizationsLifetime → Alive 20 numAllergies &lt; activeCarePlanLength − age + 1 → Alive 21 activeCarePlans &lt; microalbuminCreatineRatio − proceduresLifetimeCost → Alive 22 latitude &lt; √ encountersLifetimeTotalCost − medicationsLifetime → Alive 23 bodyMassIndex40 → Alive 24 activeCarePlanLength &gt; age + proceduresLifetime − 1 → Alive 25 medicationsLifetimeLength &lt; 2 × activeCarePlan × deviceLifetimeLength → Alive 26 carbonDioxide &gt; respiratoryRate × hemoglobinA1cHemoglobinTotalInBlood → Alive 27 osteoporosisDisorder &amp; lifetimeCarePlanLength &lt; min{painSeverity, 2 × activeCarePlanLength} → Alive 28 healthcareCoverage &lt; √ encountersLifetimePayerCoverage × medicationsLifetime → Alive 29 respiratoryRate &lt; painSeverity + leukocytesVolumeInBlood → Alive 30 meanPainSeverity &gt; max{proceduresLifetime, hemoglobinA1cHemoglobinTotalInBlood } → Alive 31 prediabetes &amp; meanDiastolicBloodPressure &gt; carbonDioxide + meanHeartRate → Alive 32 latitude &lt; encountersCount × QOLS → Alive 33 painSeverity &lt; meanPainSeverity − 1 → Alive 34 lifetimeCarePlans &gt; max{DALY, potassium } → Alive 35 latitude &lt; 2 × DALY × encountersLifetimePercCovered → Alive 36 activeCarePlanLength &gt; max{sodium, hematocritVolume } → Alive 37 medicationsLifetimePercCovered &gt; latitude 2 /medicationsLifetimeDispense → Alive 38 healthcareCoverage &gt;</figDesc><table><row><cell>Conjecture</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 9</head><label>9</label><figDesc>Evaluation of Conjectures for Alive Status Among Those with COVID</figDesc><table><row><cell cols="3">Conjecture Precision Support Lift</cell></row><row><cell>1</cell><cell>99.11%</cell><cell>3042 1.07</cell></row><row><cell>2</cell><cell>98.54%</cell><cell>3700 1.06</cell></row><row><cell>3</cell><cell>98.51%</cell><cell>9480 1.06</cell></row><row><cell>4</cell><cell>98.25%</cell><cell>8231 1.06</cell></row><row><cell>5</cell><cell>98.23%</cell><cell>6103 1.06</cell></row><row><cell>6</cell><cell>98.05%</cell><cell>7937 1.06</cell></row><row><cell>7</cell><cell>97.79%</cell><cell>4161 1.06</cell></row><row><cell>8</cell><cell>97.65%</cell><cell>5453 1.05</cell></row><row><cell>9</cell><cell>97.65%</cell><cell>2939 1.05</cell></row><row><cell>10</cell><cell>97.41%</cell><cell>1969 1.05</cell></row><row><cell>11</cell><cell>97.07%</cell><cell>4774 1.05</cell></row><row><cell>12</cell><cell>96.79%</cell><cell>2242 1.05</cell></row><row><cell>13</cell><cell>96.61%</cell><cell>1650 1.04</cell></row><row><cell>14</cell><cell>95.65%</cell><cell>2045 1.03</cell></row><row><cell>15</cell><cell>95.39%</cell><cell>6311 1.03</cell></row><row><cell>16</cell><cell>95.31%</cell><cell>1236 1.03</cell></row><row><cell>17</cell><cell>95.20%</cell><cell>1687 1.03</cell></row><row><cell>18</cell><cell>95.07%</cell><cell>4017 1.03</cell></row><row><cell>19</cell><cell>94.32%</cell><cell>440 1.02</cell></row><row><cell>20</cell><cell>94.00%</cell><cell>500 1.02</cell></row><row><cell>21</cell><cell>93.59%</cell><cell>1498 1.01</cell></row><row><cell>22</cell><cell>93.25%</cell><cell>1185 1.01</cell></row><row><cell>23</cell><cell>92.09%</cell><cell>834 0.99</cell></row><row><cell>24</cell><cell>91.01%</cell><cell>1213 0.98</cell></row><row><cell>25</cell><cell>90.79%</cell><cell>999 0.98</cell></row><row><cell>26</cell><cell>89.77%</cell><cell>831 0.97</cell></row><row><cell>27</cell><cell>89.41%</cell><cell>727 0.97</cell></row><row><cell>28</cell><cell>88.61%</cell><cell>2389 0.96</cell></row><row><cell>29</cell><cell>88.27%</cell><cell>358 0.95</cell></row><row><cell>30</cell><cell>88.21%</cell><cell>704 0.95</cell></row><row><cell>31</cell><cell>87.67%</cell><cell>1890 0.95</cell></row><row><cell>32</cell><cell>87.20%</cell><cell>2250 0.94</cell></row><row><cell>33</cell><cell>87.14%</cell><cell>583 0.94</cell></row><row><cell>34</cell><cell>85.79%</cell><cell>1612 0.93</cell></row><row><cell>35</cell><cell>84.50%</cell><cell>755 0.91</cell></row><row><cell>36</cell><cell>82.76%</cell><cell>586 0.89</cell></row><row><cell>37</cell><cell>76.22%</cell><cell>677 0.82</cell></row><row><cell>38</cell><cell>74.75%</cell><cell>99 0.81</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 10</head><label>10</label><figDesc>Conjectures for Deceased Status Among Those with COVID Deceased 3 age &gt; carbonDioxide × potassium → Deceased 4 deviceLifetimeLength ≤ 2 × creatinine healthcareExpenses → Deceased 5 implantableCardiacPacem → Deceased 6 latitude &lt; log(age)/ log(10) activeCarePlans → Deceased 7 medicationsActive &gt; log(alkalinePhosphataseEnzymatic Activity)/ log(10) → Deceased 8 immunizationsLifetimeCost &lt; age × immunizationsLifetime 2 → Deceased 9 colonoscopy &amp; coronaryHeartDisease → Deceased 10 activeCarePlans &lt; min{deviceLifetimeLength, medicationsActive} → Deceased 11 glucose &gt; creatinine × meanGlucose → Deceased 12 bodyWeight &gt; meanBodyWeight + 1 → Deceased 13 healthcareExpenses &lt; deviceLifetimeLength 2 × lifetimeConditionLength → Deceased 14 lifetimeConditions &gt; activeCarePlans + ureaNitrogen → Deceased 15 activeCarePlans &gt; log(triglycerides) → Deceased 16 healthcareExpenses &lt; lifetimeConditionLength 2 + encountersLifetimeTotalCost → Deceased 17 overlappingMalignantNeo → Deceased 18 latitude &gt; ureaNitrogen albuminMassVolumeInSerumOrPlasma → Deceased 19 activeConditionLength &gt; erythrocytesVolumeInBlood × hemoglobinMassVolumeInBlood → Deceased 20 chronicObstructiveBronc → Deceased 21 longitude &gt; √ healthcareCoverage − encountersCount → Deceased 22 age &gt; 10 medicationsActive − longitude → Deceased 23 chloride &lt; meanChloride − lifetimeCarePlans → Deceased 24 medicationsActive &gt; max{respiratoryRate, log(latitude)} → Deceased 25 localizedPrimaryOsteoa → Deceased 26 rheumatoidArthritis → Deceased 27 chronicPain &amp; smokesTobaccoDaily → Deceased 28 latitude &lt; erythrocyteDistributionWidth − meanPainSeverity → Deceased 29 activeCarePlans &gt; 10 medicationsActive /imagingStudiesLifetime → Deceased 30 tubalPregnancy → Deceased 31 activeConditions &lt; medicationsActive 2 − medicationsLifetime → Deceased 32 alcoholism &amp; majorDepressionDisorder → Deceased 33 creatinine &lt; meanCreatinine /lifetimeCarePlans → Deceased 34 healthcareCoverage &lt; encoutnersLifetimePayerCoverage × log(latitude)/ log(10) → Deceased 35 encountersCount &lt; min{DALY, 10 immunizationsLifetime } → Deceased 36 lifetimeCarePlanLength &gt; age + e medicationsLifetime → Deceased 37 healthcareCoverage &lt; activeCondionLength 2 − encountersLifetimeTotalCost → Deceased 38 age &gt; 1/2 × healthcareExpenses/immunizationsLifetimeCost → Deceased 39 activeCarePlanLength &gt; activeConditionLength × e DALY → Deceased 40 medicationsLifetime &lt; √ encountersLifetimePayerCoverage − age → Deceased Table 11 Evaluation of Conjectures for Deceased Status Among Those with COVID</figDesc><table><row><cell>Conjecture</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">A Hamiltonian graph is a graph with a spanning cycle<ref type="bibr" target="#b52">(West 2001)</ref>. The definition of Hamiltonian is not important for this example, but only the fact that any graph either is or is not Hamiltonian.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.va.gov/health-care/copay-rates/, accessed July 10th, 2022 4 https://www.cdc.gov/coronavirus/2019-ncov/need-extra-precautions/people-with-medical-conditions. html, accessed July 10th, 2022 5 https://www.cdc.gov/diabetes/managing/managing-blood-sugar/a1c.html, accessed July 10th, 2022</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>High Performance Computing resources provided by the High Performance Research Computing (HPRC) Core Facility at Virginia Commonwealth University (https://chipc.vcu.edu) were used for conducting the research reported in this work.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: Additional Results for Conjecturing for Recovery of Equations</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Abolafia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.03526v2</idno>
		<ptr target="http://arxiv.org" />
		<title level="m">Neural Program Synthesis with Priority Queue Training</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>pdf</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Aghaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vayanos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15965.pdf</idno>
		<ptr target="http://arxiv.org" />
		<title level="m">Strong Optimal Classification Trees</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Discovering Classification Rules for Interpretable Learning with Linear Programming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Akyüz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Birbil</surname></persName>
		</author>
		<author>
			<persName><surname>¸i</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10751.pdf</idno>
		<ptr target="http://arxiv.org" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Vadalog: A Modern Architecture for Automated Reasoning with Large Knowledge Graphs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bellomarini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Benedetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gottlob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sallinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 101528</date>
		</imprint>
	</monogr>
	<note>Information Systems in press</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Optimal Classification Trees</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="1039" to="1082" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimal Randomized Classification Trees</title>
		<author>
			<persName><forename type="first">R</forename><surname>Blanquero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Carrizosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Molero-Río</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romero</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Operations Research</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page">105281</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automated conjecturing II: Chomp and Intelligent Game Play</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bradford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Muncy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kaperick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Van Cleemput</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="447" to="461" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><surname>Cj S</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<pubPlace>Wadsworth &amp; Brooks</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discovering Governing Equations from Data by Sparse Identification of Nonlinear Dynamical Systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brunton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Proctor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kutz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of the Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="3932" to="3937" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Data Smashing: Uncovering Lurking Order in Data</title>
		<author>
			<persName><forename type="first">I</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Society Interface</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2014">2014. 20140826</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On Hamilton&apos;s Ideals</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chvátal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Combinatorial Theory, Series B</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="163" to="168" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A note on hamiltonian circuits</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chvátal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Erdös</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Mathematics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="113" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Boolean Decision Rules via Column Generation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Günlük</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Conference on Neural Information Processing Systems (NeurIPS-18)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Elton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05149v4.pdf</idno>
		<ptr target="http://arxiv.org" />
		<title level="m">Self-Explaining AI as an Alternative to Interpretable AI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On Conjectures of Graffiti</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fajtlowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graph Theory, Combinatorics, and Algorithms</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="367" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On Cognitive Preferences and the Plausibility of Rule-Based Models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fürnkranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kliegr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Paulheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="853" to="898" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Article submitted to</title>
		<imprint/>
	</monogr>
	<note>manuscript no</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On Some Problems of Lovász Concerning the Shannon Capacity of a Graph</title>
		<author>
			<persName><forename type="first">W</forename><surname>Haemers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="231" to="232" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Logical Analysis of Data -An Overview: From Combinatorial Optimization to Medical Applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bonates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Operations Research</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Decreased CO 2 Levels as Indicators of Possible Mechanical Ventilation-Induced Hyperventilation in COVID-19 Patients: A Retrospective Analysis</title>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Public Health</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">596168</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">B</forename><surname>Jantzen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04933.pdf</idno>
		<ptr target="http://arxiv.org" />
	</analytic>
	<monogr>
		<title level="j">Dynamical Kinds and Their Discovery</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep Feature Synthesis: Towards Automating Data Science Endeavors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kanter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Veeramachaneni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Science and Advanced Analytics</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ExploreKit: Automatic Feature Generation and Selection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ecr</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th IEEE International Conference on Data Mining</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature Engineering for Predictive Modeling Using Reinforcement Learning</title>
		<author>
			<persName><forename type="first">U</forename><surname>Khurana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Samulowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Turaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Langely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Zytkow</surname></persName>
		</author>
		<title level="m">Scientific Discovery: Computational Explorations of the Creative Process</title>
				<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scientific Discovery, Causal Explanation, and Process Model Induction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mind &amp; Society</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="43" to="56" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automated conjecturing I: Fajtlowicz&apos;s Dalmatian heuristic revisited</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Van Cleemput</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">231</biblScope>
			<biblScope unit="page" from="17" to="38" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automated conjecturing III: Property-relations conjectures</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Van Cleemput</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="315" to="327" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the Shannon capacity of a graph. Information Theory</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lovász</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Good Explanation for Algorithmic Transparency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Danks</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.3503603</idno>
		<ptr target="http://dx.doi.org/10.2139/ssrn.3503603" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantically-Based Crossover in Genetic Programming: Application to Real-Valued Symbolic Regression</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>O'neill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mckay</surname></persName>
		</author>
		<author>
			<persName><surname>Galván-López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genetic Programming and Evolvable Machines</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="91" to="119" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Choosing Function Sets with Better Generalisation Performance for Symbolic Regression Models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nicolau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agapitos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genetic Programming and Evolvable Machines</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="73" to="100" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Epidemiology, Prognosis and Management of Potassium Disorders in Covid-19</title>
		<author>
			<persName><forename type="first">M</forename><surname>Noori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nejadghaderi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Carson-Chahhoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Kolahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Safiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reviews in Medical Virology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">e2262</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep Symbolic Regression: Recovering Mathematical Expressions from Data via Risk-Seeking Policy Gradients</title>
		<author>
			<persName><forename type="first">B</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Larma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mundhenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Santiago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representation</title>
				<meeting>the International Conference on Learning Representation</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="206" to="215" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning Customized and Optimized Lists of Rules with Mathematical Programming</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ertekin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="659" to="702" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Towards Explainable Artificial Intelligence</title>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Explainable AI: Interpreting, Explaining and Visualizing Deep Learning</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distilling Free-Form Natural Laws from Experimental Data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">324</biblScope>
			<biblScope unit="issue">5923</biblScope>
			<biblScope unit="page" from="81" to="85" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Combinatorial optimization: polyhedra and efficiency</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schrijver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Discovering de novo Peptide Substrates for Enzymes using Machine Learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tallorin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Gilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">I</forename><surname>Frazier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Burkart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Gianneschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">rpart: Recursive Partitioning and Regression Trees</title>
		<author>
			<persName><forename type="first">T</forename><surname>Therneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Atkinson</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=rpart" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>R package version 4.1-15</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Regression Shrinkage and Selection via the LASSO</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Detecting Statistical Interactions from Neural Network Weights</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Sixth International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
	<note>ICLR-18</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavankumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Neural Interaction Transparency (NIT): Disentangling Learned Interactions for Improved Interpretability. The Thirty-Second Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note>NeurIPS-18</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rambhatla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">How Does This Interaction Affect Me? Interpretable Attribution for Feature Interactions. The Thirty-Fourth Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>NeurIPS-20</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">AI Feynman: A Physics-Inspired Method for Symbolic Regression</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Udrescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tegmark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Advances</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2020">2020. eaay2631</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning Optimal Classification Trees Using a Binary Linear Program Formulation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Verwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Vilone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Longo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00093.pdf</idno>
		<ptr target="http://arxiv.org" />
	</analytic>
	<monogr>
		<title level="j">Explainable Artificial Intelligence: A Systematic Review</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Falling Rule Lists</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 18th International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Article submitted to</title>
		<imprint/>
	</monogr>
	<note>manuscript no</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A Bayesian Framework for Learning Rule Sets for Interpretable Classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Klampfl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Macneille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Introduction to Graph Theory</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>West</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Prentice Hall</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>nd edition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
