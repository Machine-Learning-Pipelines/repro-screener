<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reinforcement Learning for Ridesharing: An Extended Survey *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-24">24 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tony</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hongtu</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jieping</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Lyft Rideshare Labs San Francisco</orgName>
								<address>
									<postCode>94107</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of North Carolina Chapel Hill</orgName>
								<address>
									<postCode>27514</postCode>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">DiDi Labs</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reinforcement Learning for Ridesharing: An Extended Survey *</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-24">24 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">2C6ED435D98BD1901B3B1BDEFA52EA6B</idno>
					<idno type="arXiv">arXiv:2105.01099v8[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-10-31T05:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a comprehensive, in-depth survey of the literature on reinforcement learning approaches to decision optimization problems in a typical ridesharing system. Papers on the topics of rideshare matching, vehicle repositioning, ride-pooling, routing, and dynamic pricing are covered. Most of the literature has appeared in the last few years, and several core challenges are to continue to be tackled: model complexity, agent coordination, and joint optimization of multiple levers. Hence, we also introduce popular data sets and open simulation environments to facilitate further research and development. Subsequently, we discuss a number of challenges and opportunities for reinforcement learning research on this important domain. * This survey is a significantly expanded and refined version of (Qin, Zhu &amp; Ye 2021). Please cite as Zhiwei (Tony) Qin, Hongtu Zhu, and Jieping Ye. Reinforcement learning for ridesharing: An extended survey.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The emergence of ridesharing 3 , led by companies such as DiDi, Uber, and Lyft, has revolutionized the form of personal mobility. It is projected that the global rideshare industry will grow to a total market value of $218 billion by 2025 (MarketsAndMarkets 2018). However, how to improve operational efficiency is a major challenge for rideshare platforms, e.g., long passenger waiting time <ref type="bibr" target="#b101">(Smith 2019)</ref> and as high as 41% vacant time for ridesharing vehicles in a large city <ref type="bibr" target="#b15">(Brown 2020)</ref>. The success of ridesharing, from the perspectives of the platforms, drivers, and passengers, requires sophisticated optimization of all the integrated components that collectively deliver the services.</p><p>Reinforcement learning (RL) is a machine learning paradigm that trains an agent to take optimal actions (measured by total cumulative reward) through interaction with the environment and getting feedback signals. It is a class of optimization methods for solving sequential decision-making problems with a long-term objective in a stochastic environment. Thanks to the rapid advancement in deep learning research and computing power, the integration of deep neural networks and RL has generated explosive progress in solving complex large-scale decision problems <ref type="bibr" target="#b98">(Silver &amp; Hassabis 2016</ref><ref type="bibr" target="#b12">, Berner et al. 2019)</ref>, attracting huge amount of renewed interests in the recent years. We are witnessing a similar trend in the ridesharing domain, where the demand and supply are highly stochastic and non-stationary, and the operational decisions are often sequential in nature and have strong spatiotemporal dependency. Simple greedy heuristics that only optimizes for immediate returns tend to produce short-sighted reactive policies, which do not align well with cumulative nature of the true performance measure of interest. The multi-step sequential nature of the decision-making (e.g., pricing, matching, repositioning) and the supply-demand stochasticity in the environment pose enormous challenges to traditional predictive and optimization methods, spanning such issues as forecast accuracy, decision-time computational complexity, and adaptability to real-time changes. RL, on the other hand, presents itself as an excellent promising approach to these ridesharing optimization problems. RL methods are often highly data-driven, making them more suitable to situations where it is hard to build accurate predictive models. They are forward-looking, and yet, they do not explicitly depend on forecasting. And, by design, RL-based policies are dynamic and often light in decision-time complexity.</p><p>There are excellent surveys on RL for intelligent transportation <ref type="bibr" target="#b39">(Haydari &amp; Yilmaz 2020</ref><ref type="bibr" target="#b139">, Yau et al. 2017)</ref>, with in-depth coverage of traffic signals control and autonomous driving. <ref type="bibr">(Wang &amp; Yang 2019</ref>) offers a broad review of ridesharing systems, whereas <ref type="bibr" target="#b111">Tong et al. (2020)</ref> surveys spatial crowdsourcing, which is a more general field than ridesharing. There has been no comprehensive review of the literature on RL for ridesharing, even though the field has attracted much attention and interest from the research communities for both RL and transportation just within the last few years (e.g., <ref type="bibr" target="#b90">(Shah et al. 2020</ref><ref type="bibr">, Tang et al. 2019</ref><ref type="bibr" target="#b0">, Al-Kanj et al. 2020</ref><ref type="bibr" target="#b96">, Shou &amp; Di 2020b)</ref>). This paper aims to fill that gap by surveying the literature of this domain published in top conferences and journals in transportation (e.g., Transportation Research series, IEEE Transactions on Intelligent Transportation Systems, Transportation Science), data mining (e.g., KDD, ICDM, WWW, CIKM), and machine learning/AI (e.g., NeurIPS, AAAI, IJCAI). We describe the research problems associated with the various aspects of a ridesharing system, review the existing RL approaches proposed to tackle each of them, and discuss the challenges and opportunities. Reinforcement learning also has close relationship with approximate dynamic programming. Although it is not our goal in this paper to have a comprehensive review of this class of methods for problems in ridesharing, we aim to point the readers to the representative works so that they can refer to the further literature therein. This survey is organized as follows: We lay out the ridesharing system architecture in Section 2 and define the scope of the problems to be reviewed. Within this section, as well as the subsequent sections of the survey, we clarify and draw connections among the different names that the problems are referred to, which are often due to the different communities that the authors are from and are easy to confuse by researchers new to ridesharing. In Section 3, we provide a concise review of the RL basics and the major algorithms adopted by the works in this survey. We review in details in Section 4 the literature for each problem described in Section 2 and the relevant data sets and environments. Finally in Section 5, we discuss some challenges and opportunities that we feel crucial in advancing RL for ridesharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Ridesharing</head><p>We first describe the architecture of a ridesharing system in this section, followed by explanation and clarification on the scopes of the problem associated with each module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Architecture</head><p>A ridesharing service, in contrast to taxi hailing, matches passengers with drivers of vehicles for hire using mobile apps. In a typical mobile ridesharing system, there are five major modules: pricing, matching, repositioning, pooling, and routing. Figure <ref type="figure">1</ref> illustrates the process and decision modules. When a potential passenger submits a trip request, the pricing module offers a quote, which the passenger either accepts or rejects. Upon acceptance, the matching module attempts to assign the request to an available driver. Depending on driver pool availability, the request may have to wait in the system until a successful match. Pre-match cancellation may happen during this time. The assigned driver then travels to pick up the passenger, during which time post-match cancellation may also occur. The pick-up location is usually where the passenger is making the request or he/she specifies. In some cases, it could be a public designated area, e.g., outside an airport or train station.</p><p>After the driver successfully transports the passenger to the destination, she receives the trip fare and becomes available again. The repositioning module guides idle vehicles to specific locations in anticipation of fulfilling more requests in the future. Following the reposition recommendations is usually on a voluntary basis unless it is an autonomous ridesharing setting. Hence, it is common that the platform offers incentives to drivers for completing the repositions. When each driver takes only one passenger request at a time, i.e. only one passenger shares the ride with the driver, this mode is more commonly called 'ride-hailing'. Ridesharing can refer to both ride-hailing and ride-pooling. 4 In the ride-pooling mode, multiple passengers with different trip requests can share one single vehicle, so the pricing, matching, repositioning, and routing problems are different from those for ride-hailing and require specific treatment, in particular, considering the passengers already on board. The routing module provides turn-by-turn guidance on the road network to drivers/vehicles either in service of a passenger request or performing a reposition. The goal is to guide the vehicle to its destination efficiently and safely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Problem Scopes</head><p>First, we start from the pricing module. Since the trip fare is both the price that the passenger has to pay for the trip and the major factor for the income of the driver, pricing decisions influence both demand and supply distributions through price sensitivities of users, e.g., the use of surge pricing during peak hours. This is illustrated by the solid arrows pointing from the pricing module to orders and idle vehicles respectively in Figure <ref type="figure">1</ref>. The pricing problem in the ridesharing literature is in most cases dynamic pricing, which adjusts trip prices in real-time in view of the changing demand and supply. The pricing modules sits at the upstream position with respect to the other modules and is a macro-level lever to achieve supply-demand (SD) balance. Although technically, driver pay can be determined by a separate module from pricing and has its own implication on supply elasticity and driver behavior, this paper follows the common setting where driver pay is closely associated (approximately proportional) to the trip fare so that pricing has the dual effect on demand and supply.</p><p>The ridesharing matching problem <ref type="bibr" target="#b134">(Yan et al. 2020</ref><ref type="bibr" target="#b77">, Özkan &amp; Ward 2020</ref><ref type="bibr">, Qin et al. 2020</ref>) may appear under different names in the literature, e.g., order dispatching <ref type="bibr" target="#b82">(Qin et al. 2020)</ref>, trip-vehicle assignment <ref type="bibr" target="#b9">(Bei &amp; Zhang 2018)</ref>, and on-demand taxi dispatching <ref type="bibr" target="#b111">(Tong et al. 2020)</ref>. It is an online bipartite matching problem where both supply and demand are dynamic, with the uncertainty coming from demand arrivals, travel times, and the entrance-exit behavior of the drivers. Matching can be done continuously in a streaming manner or at fixed review windows (i.e., batching). Sophisticated matching algorithms often leverage demand prediction in some form beyond the actual requests, e.g., the value function in RL. Online request matching is not entirely unique to ridesharing. Indeed, ridesharing matching falls into the family of more general dynamic matching problems for on-demand markets <ref type="bibr" target="#b43">(Hu &amp; Zhou 2022)</ref>. A distinctive feature of the ridesharing problem is its spatiotemporal nature. A driver's eligibility to match and serve a trip request depends in part on her spatial proximity to the request. Trip requests generally take different amount of time to finish, and they change the spatial states of the drivers, affecting the supply distribution for future matching. The drivers and passengers generally exhibit asymmetric exit behaviors in that drivers usually stay in the system for an extended period of time, whereas passenger requests are lost after a much shorter waiting period in general.</p><p>Single-vehicle repositioning may refer to as taxi routing or passenger seeking in the literature. Taxi routing slightly differs in the setting from repositioning a rideshare vehicle in that a taxi typically has to be at a visual distance from the potential passenger to take the request whereas the matching radius of a mobile rideshare request is considerably longer, sometimes more than a mile. System-level vehicle repositioning, also known as driver dispatching, vehicle rebalancing/reallocation, or fleet management, aims to rebalance the global SD distributions by proactively dispatching idle vehicles to different locations. Repositioning and matching are similar to each other in that both relocate a vehicle to a different place as a consequence. In theory, one can treat repositioning as matching a vehicle to a virtual trip request, the destination of which is that of the reposition action, so that both matching and repositioning can be solved in a single problem instance. Typically in practice, these two problems are solved separately because they are separate system modules on most ridesharing platforms with different review intervals and objective metrics among other details.</p><p>Figure <ref type="figure">1</ref>: The process flow of ridesharing operations. The solid orange rectangular boxes represent the modules described in Section 2, and the literature on the optimization problems associated with the modules are reviewed in the paper. The blue text and arrow apply exclusively to ride-pooling to account for the fact that order combinations and in-service vehicles are also eligible to participate in matching.</p><p>The routing module described in Section 2.1 performs route guidance, which could be dynamic routing or route planning depending on the decision points. Dynamic routing is also called dynamic route choice, and route planning is alternatively referred to as the traffic assignment problem <ref type="bibr" target="#b95">(Shou &amp; Di 2020a)</ref>. In some cases, the reposition policy directly provides link-level turn-by-turn guidance with the goal of maximizing the driver's income, thus covering the role of dynamic routing albeit with a different objective. Dynamic routing is generally different from the vehicle routing problem (VRP) <ref type="bibr" target="#b22">(Dantzig &amp; Ramser 1959)</ref>. In VRP, the set of destinations that the vehicle has to visit is known in advance, and hence, it is a static problem. It mainly concerns with the sequence in which the destinations should be visited, considering the estimated travel time from point to point. In contrast, dynamic routing is associated with a road network, and the decision to make is which outgoing road (link) to follow at each intersection (node). The decision is adaptive to the changing traffic condition on the road network in real time. In the context of ride-pooling, there is another emerging problem in which the dynamic routing decisions with passenger(s) on board have to align with the overall objective of ride-pooling. Some literature refers to the mode of multiple passengers sharing a ride as 'ridesharing'. In this paper, we use term 'ride-pooling' (or 'carpool') to disambiguate the concept, as 'ridesharing' can refer to both single-and multiple-passenger rides. The seminal paper of Alonso-Mora, Samaranayake, <ref type="bibr" target="#b2">Wallar, Frazzoli &amp; Rus (2017)</ref> shows that through ride-pooling optimization, the number of taxis required to meet the same trip demand can be significantly reduced with limited impact on passenger waiting times. In a pooling-enabled rideshare system, the matching, repositioning, and pricing modules all have to adapt to additional complexity. Compared to the regular ride-hailing problem, the one with ride-pooling has considerably more complexity due to the more dynamic nature of the problem and the additional constraints and multiple objectives that have to be considered. In this case, the set of available vehicles are augmented, including both idle vehicles and occupied ones not at capacity. It is non-trivial to determine the set of feasible actions (one or more passengers to be assigned to a vehicle) for matching. Every time a new passenger is to be added to a non-idle vehicle, the route has to be recalculated using a VRP solver to account for the additional pick-up and drop-off, the travel times for all the passengers already on board are updated, and the vehicle capacity, the waiting time and detour distance constraints are checked. In-service routing in the context of ride-pooling is discussed in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Reinforcement Learning</head><p>We briefly review the RL basics and the major algorithms, especially those used in the works reviewed by this survey. For a complete reference, see, e.g., <ref type="bibr" target="#b105">(Sutton &amp; Barto 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basics</head><p>RL is based on the Markov decision process (MDP) framework, where the agent (the decision-making entity) has a state s (e.g., the location of a vehicle) in the state space S and can perform an action a (e.g., to dispatch or idle) in the action space A. The action is determined by the agent's policy, π(s) : S → A. If the policy is stochastic, then π(a|s) gives the probability of selecting a given s. After executing the action, the agent receives an immediate reward R(s, a) from the environment, and its state changes according to the transition probabilities P (•|s, a). The process repeats until a terminal state or the end of the horizon is reached, giving a sequence of the triplets (s t , a t , r t ) t=T t=0 , where t is the epoch index, T is the final epoch at the terminal state or the end of the horizon, and r t is a sample of R. The objective of the MDP is to maximize the cumulative reward over the horizon.</p><p>A key quantity to compute is the value function associated with π,</p><formula xml:id="formula_0">V π (s) := E π t=T t=0 γ t r t s 0 = s , which satisfies the Bellman equation, V π (s t ) = at π(a t |s t ) st+1,rt P (s t+1 , r t |s t , a t ) r t (s t , a t ) + γV π (s t+1 ) .</formula><p>(1)</p><p>Similarly, we have the action-value function,</p><formula xml:id="formula_1">Q π (s, a) := E π t=T t=0 γ t r t s 0 = s, a 0 = a ,</formula><p>which conditions on both s and a. The optimal state-and action-values are denoted by V * and Q * , evaluated at the optimal policy π * . The objective of an MDP is to find an optimal policy π * that maximizes the long-term cumulative discounted reward, i.e., π * := arg max π E s [V π (s)].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Algorithms</head><p>Given P and R, which specifies the MDP, and π, we can compute V π by iteratively applying the Bellman equation (1),</p><formula xml:id="formula_2">V π (s) ← a π(a|s) s ,r P (s , r|s, a) r + γV π (s ) .<label>(2)</label></formula><p>This is called policy evaluation. Using policy evaluation as a sub-routine, we can again iteratively improve the policy through policy iterations, which generate a new policy at each outer iteration by acting greedily with respect to V π , a * ← arg max a s ,r P (s , r|s, a) r + γV π (s ) .</p><p>(3)</p><p>As a special instance, we can collapse the inner policy evaluation loop to a single iteration and compute V * (s), ∀s ∈ S, by the value iterations,</p><formula xml:id="formula_3">V (s) ← max a s ,r P (s , r|s, a) r + γV (s ) .<label>(4)</label></formula><p>If we estimate (P, R) from data, we have a basic model-based RL method. A model-free method learns the value function and optimizes the policy directly from data without learning and constructing a model of the environment. A common example is temporal-difference (TD) learning <ref type="bibr" target="#b104">(Sutton 1988)</ref>, which iteratively updates V π by TD-errors using π-generated trajectory samples and bootstrapping,</p><formula xml:id="formula_4">V π (s) ← V π (s) + α r + γV π (s ) − V π (s) ,<label>(5)</label></formula><p>where s is the next state in the trajectory after s, α is the step size (or learning rate), and the term that it scales is the TD error. If learning the optimal action-values for control is the goal, we similarly have Q-learning <ref type="bibr" target="#b125">(Watkins &amp; Dayan 1992)</ref>, which updates the action-value function Q(s, a) to approximate</p><formula xml:id="formula_5">Q * (s, a) by Q(s, a) ← Q(s, a) + α r + γ max a Q(s , a ) − Q(s, a) .<label>(6)</label></formula><p>Q-learning is an off-policy algorithm, where the behavior policy, which collects the experience data and typically involves exploration, is different from the target policy that we are trying to learn and in this case, is the optimal policy. The on-policy counterpart of Q-learning is SARSA, which basically generalizes TD-learning (5) to the action-value function associated with the behavior policy π (same as the target policy):</p><formula xml:id="formula_6">Q π (s, a) ← Q π (s, a) + α r + γQ π (s , a ) − Q π (s, a) , (<label>7</label></formula><formula xml:id="formula_7">)</formula><p>where a is the action executed by the agent at state s in the experience data. The deep Q-network (DQN) <ref type="bibr" target="#b73">(Mnih et al. 2015</ref>) approximates Q(s, a) by a neural network Q w parametrized by w along with a few heuristic techniques like experience replay <ref type="bibr" target="#b59">(Lin 1992</ref>) and a target network to improve training stability. These techniques are critical to the successful of DQN in playing Atari games and many other applications, due to the deadly triad issue <ref type="bibr" target="#b105">(Sutton &amp; Barto 2018)</ref> of reinforcement learning when one tries to combine bootstrapping (i.e., TD-learning and Q-learning), off-policy training (i.e., Q-learning), and function approximations (i.e., neural networks), which may lead to instability and divergence. The algorithms introduced so far are all value-based methods, which focus on learning the value function, and the policy is derived from the learned value function by, e.g., arg max a Q(s, a). Neural network-based value function approximation is important to ridesharing applications because the state is often high dimensional with the incorporation of SD contextual features. Tabular methods suffer from the curse of dimensionality and are not tractable in this case.</p><p>A policy-based method directly learns π (which is also called the actor and parametrized by θ) by performing stochastic gradient descent. The central step is computing the policy gradient (PG), the gradient of the cumulative reward J(θ) with respect to the policy parameters θ,</p><formula xml:id="formula_8">∇ θ J(θ) = s µ(s) a Q π (s, a)∇ θ π(a|s, θ), (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>where µ is the on-policy distribution under π. A more common (equivalent) form of the PG (8) is</p><formula xml:id="formula_10">s µ(s) a π(a|s, θ)Q π (s, a)∇ θ log π(a|s, θ),<label>(9)</label></formula><p>which most of the policy-based methods are based on. REINFORCE <ref type="bibr" target="#b128">(Williams 1992</ref>) is a classical PG method, which uses Monte Carlo (MC) rollout to obtains the sample-based update</p><formula xml:id="formula_11">θ t+1 ← θ t + αG t ∇ θ log π(a|s, θ),<label>(10)</label></formula><p>where G t is an MC approximation of Q π . As in a value-based method, we can also use function approximation for the action values. The function approximator (e.g., a neural network) Q w is called the critic, and the resulting algorithm is an actor-critic (AC) method. It is well-recognized that the 'baseline' version of (8),</p><formula xml:id="formula_12">s µ(s) a π(a|s, θ) Q π (s, a) − b(s) ∇ θ log π(a|s, θ),<label>(11)</label></formula><p>where b(s) is an action-independent baseline, reduces the variance in the sample gradient and helps speed up learning. Since a natural choice of such a baseline is the state value, the critic often learns the advantage function Q(s, a) − V (s), and the method is called Advantage Actor Critic (A2C). The evaluation of an action is based on how good it can be with respect to the average over all actions, the benefit of which is to reduce the high variance in the actor and to stabilize the model. <ref type="bibr" target="#b72">Mnih et al. (2016)</ref> extend A2C to an asynchronous version (A3C) where independent agents interact with their own copy of the environment and update their model parameters with the master copy asynchronously. This architecture enables much more efficient utilization of the CPU cores through parallel threads and hence accelerates the training. The proximal policy optimization (PPO) <ref type="bibr" target="#b89">(Schulman et al. 2017</ref>) optimizes a clipping surrogate objective with respect to the advantage to promote conservative updates to π and is a popular choice of training algorithm for RL problems where policy-based methods are suitable (e.g., with continuous actions).</p><p>An MDP can be extended to a Markov game involving multiple agents to form the basis for multiagent RL (MARL). Many MARL algorithms, e.g., <ref type="bibr" target="#b137">Yang et al. (2018)</ref>, <ref type="bibr" target="#b63">Lowe et al. (2017)</ref> focus on agent communication and coordination, in view of the intractable action space in MARL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Approximate Dynamic Programming</head><p>A family of methods closely related to RL is approximate dynamic programming (ADP) <ref type="bibr" target="#b80">(Powell 2007)</ref> for solving stochastic dynamic programs (DP), of which the Bellman equation for MDP is an instance. In ADP methods, unlike that typically seen in RL, a post-decision state s x t is often defined to represent the intermediate state to which the current state s t will transition deterministically given the action a t before the random factors ω t (e.g., demand appearance and cancellation) in the environment realize. With ω t fully realized, the state transitions into the next pre-decision state s t+1 . The value function in an ADP method is defined on the post-decision state and is approximated by a particular functional form. Given the approximated values, the original optimization problem is solved to obtain the decision solution for the current time step. Linear function approximation is popular (e.g., <ref type="bibr" target="#b99">(Simao et al. 2009</ref><ref type="bibr" target="#b142">, Yu &amp; Shen 2019</ref><ref type="bibr" target="#b0">, Al-Kanj et al. 2020</ref>)) because the dual variables associated with the solution to the current-stage optimization can be used to update the linear function parameters. Then, the state is advanced to the next pre-decision state, and the iteration continues until convergence. By nature, ADP methods are on-policy methods. Recently, neural network-based value function approximation <ref type="bibr" target="#b90">(Shah et al. 2020</ref>) has also been adopted and developed due to their higher level of flexibility. In this case, the value function updates largely follow the DQN scheme. The ADP methods for ridesharing reviewed in this survey solve system-level stochastic DP problems (e.g., matching and repositioning) and aim to approximate the system value by decomposing it into local or driver-centric values, and the update schemes employed fall into the family of approximate value iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Reinforcement Learning for Ridesharing</head><p>We review the RL literature for ridesharing in this section grouped by the core operational problems described in Section 2. We first cover pricing, matching, repositioning, and routing in the context of ride-hailing. Then, we will review works on those problems specific to ride-pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pricing</head><p>RL-based approaches have been developed for dynamic pricing in one-sided retail markets <ref type="bibr" target="#b84">(Raju et al. 2003</ref><ref type="bibr" target="#b13">, Bertsimas &amp; Perakis 2006)</ref>, where pricing changes only the demand pattern per customers' price elasticity. The ridesharing marketplace, however, is more complex due to its two-sided nature and spatiotemmporal dimensions. In this case, pricing is also a lever to change the supply (driver) distribution if price changes are broadcast to the drivers. <ref type="bibr" target="#b18">Chen et al. (2021)</ref> describe examples of such elasticity functions for both demand and supply for their simulation environment.</p><p>The challenges in dynamic pricing for ridesharing lie in both its exogeneity and endogeneity. Dynamic pricing on trip inquiries changes the subsequent distribution of the submitted requests through passenger price elasticity. The requests distribution, in turn, influences future supply distribution as drivers fulfill those requests. On the other hand, the trip fares influence the demand for ridesharing services at given locations, and these changes will affect the pool of waiting passengers, which further affects the passengers' expected waiting times. Again, it will influence the demand either through cancellation of the current requests or the conversion of future trip inquiries. Because of its close ties to SD distributions, dynamic pricing is often jointly optimized with order matching or vehicle repositioning. Within the (non-RL) operations research literature, dynamic pricing for ridesharing has already been studied and analyzed in conjunction with matching <ref type="bibr" target="#b134">(Yan et al. 2020</ref><ref type="bibr" target="#b77">, Özkan &amp; Ward 2020)</ref> and from the spatiotemporal perspective <ref type="bibr" target="#b65">(Ma et al. 2020</ref><ref type="bibr" target="#b14">, Bimpikis et al. 2019</ref><ref type="bibr">, Hu et al. 2022)</ref>, covering optimality and equilibrium analyses.</p><p>The complex interaction between pricing and the SD makes it hard to explicitly develop mathematical models that adapt well to dynamic and stochastic environments, and RL comes in as a promising direction to address these challenges by considering endogeneity and exogeneity as part of the environment dynamics.</p><p>Table <ref type="table" target="#tab_0">1</ref> summarizes the reviewed works on RL for dynamic pricing in ridesharing. As one of the early RL works, <ref type="bibr" target="#b131">Wu et al. (2016)</ref> consider a simplified ridesharing environment which captures only the two-sidedness of the market but not the spatiotemporal dimensions. The state of the MDP is the current price plus SD information. The action is to set a price, and the reward is the generated profit. A Q-learning agent is trained in a simple simulator, and empirical advantage in the total profit is demonstrated against other heuristic approaches. More recent works leverage the spatiotemporal nature of the pricing actions and take into account the spatiotemporal long-term values in the pricing decisions. <ref type="bibr" target="#b19">Chen, Jiao, Qin, Tang, Li, An, Zhu &amp; Ye (2019)</ref> integrate contextual bandits and the spatiotemporal value network developed in <ref type="bibr" target="#b106">(Tang et al. 2019)</ref> for matching to jointly optimize pricing and matching decisions. In particular, the pricing actions are the discretized price percentage changes and are selected by a contextual bandits algorithm, where the long-term values learned by the value network are incorporated into the bandit rewards. In <ref type="bibr" target="#b114">(Turan et al. 2020)</ref>, the RL agent determines both the price for each origin-destination (OD) pairs and the reposition/charging decisions for each electric vehicle in the fleet. The state contains global information such as the electricity price in each zone, the passenger queue length for OD pair, and the number of vehicles in each zone and their energy levels. The reward accounts for trip revenue, penalty for the queues, and operational cost for charging and repositioning. Due to the multi-dimensional continuous action space, PPO is used to train the agent in a simulator. <ref type="bibr" target="#b102">Song et al. (2020)</ref> perform a case study of ridesharing in Seoul. They use a tabular Q-learning agent to determine spatiotemporal pricing, and extensive simulations are performed to analyze the impact of surge pricing on alleviating the problem of marginalized zones (areas where it is consistently hard to get a taxi) and on improving spatial equity. <ref type="bibr" target="#b18">Chen et al. (2021)</ref> adopt PPO to optimize the spatiotemporal pricing decisions for each hexagonal cell in terms of the per-km rate for the excess mileage beyond a base trip distance and the per-km rate for driver wage, for the objective of maximizing profits (revenue minus wage). The agent is modeled as a global decision-maker with state information of the numbers of open requests, vacant vehicles, occupied vehicles in each grid cell at time t and historical demand at time t − 1. Unlike the works above that focus on the pricing decisions, <ref type="bibr" target="#b70">Mazumdar et al. (2017)</ref> study from a different perspective of the pricing problem. The proposed risk-sensitive inverse RL method <ref type="bibr" target="#b75">(Ng et al. 2000)</ref> recovers the policies of different types of passengers (risk-averse, risk-neutral, and risk-seeking) in view of surge pricing. The policy determines whether the passenger should wait or take the current ride.</p><p>As discussed in Section 2.2, under the setting where the driver pay is associated with the trip fare, the dynamic pricing policy also affects supply elasticity, i.e., drivers' decisions on participation in a given marketplace, working hours, and in some cases, the probability of accepting a given assignment, depending on the rules of the particular ridesharing platform <ref type="bibr" target="#b20">(Chen &amp; Sheldon 2016</ref><ref type="bibr">, Sun et al. 2019</ref><ref type="bibr" target="#b4">, Angrist et al. 2021).</ref> Although not yet being widely considered in RL approaches to pricing, supply elasticity is an important piece of system state information that has significant implication to the sequence of pricing decisions. For the closely related topic of driver incentives design, <ref type="bibr" target="#b92">Shang et al. (2019</ref><ref type="bibr" target="#b91">Shang et al. ( , 2021</ref>) adopts a learning-based approach to construct a generative model for driver behavior with respect to the incentives policy and subsequently trains an RL agent to optimize the incentives design for system-level metrics. Perhaps this example sheds some light on how RL is able to help improve pricing policies in view of supply-side effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Online Matching</head><p>The rideshare matching problem and its generalized forms have been investigated extensively in the field of operations research (see e.g., <ref type="bibr" target="#b77">(Özkan &amp; Ward 2020</ref><ref type="bibr" target="#b43">, Hu &amp; Zhou 2022</ref><ref type="bibr" target="#b62">, Lowalekar et al. 2018)</ref> and the references therein). Typically, both the open trip requests and available drivers are batched within time windows of fixed length as they arrive at the system, and they are matched at predefined discrete review times. See Figure <ref type="figure">2</ref> for an illustration. Hence, ridesharing matching is an online stochastic problem <ref type="bibr" target="#b82">(Qin et al. 2020)</ref>. Outside the RL literature, <ref type="bibr" target="#b62">Lowalekar et al. (2018)</ref> approach the problem through stochastic optimization and use Bender's decomposition to solve it efficiently. To account for the temporal dependency of the decisions, <ref type="bibr" target="#b43">Hu &amp; Zhou (2022)</ref> formulate the problem as a stochastic DP and propose heuristic policies to compute the optimal matching decisions. For a related problem, the truckload carriers assignment problem, <ref type="bibr" target="#b99">Simao et al. (2009)</ref> also formulate a dynamic DP but with post-decision states so that they are able to solve the problem using ADP. In each iteration, a demand path is sampled, and the value function is approximated in a linear form and updated using the dual variables from the LP solution to the resulting optimization problem.</p><p>The RL literature for rideshare matching (see Table <ref type="table" target="#tab_2">2</ref>) typically aims to optimize the total driver income and the service quality over an extended period of time. Service quality can be quantified by response rate and fulfillment rate. Response rate is the ratio of the matched requests to all trip requests. Since the probability of pre-match cancellation is primarily a function of response time (pre-match waiting time), the total response time is an alternative metric to response rate. Fulfillment rate is the ratio of completed requests to all requests and is no higher than the response rate. The gap is due to post-match cancellation, usually because of the waiting for pick-up. Hence, the average pick-up distance is also a relevant quantity to observe. Figure <ref type="figure" target="#fig_0">3</ref> shows the detailed flow of matching a single trip request together with the quantities discussed above.</p><p>In terms of the MDP formulation, driver agent is a convenient modeling choice for its straightforward definition of state, action, and reward, in contrast to system-level modeling where the action space is exponential. In this case, the rideshare platform is naturally a multi-agent system with a global objective. A common approach is to crowdsource all drivers' experience trajectories to train a single agent and apply it to all the drivers to generate their matching policies <ref type="bibr" target="#b133">(Xu et al. 2018</ref><ref type="bibr">, Wang et al. 2018</ref><ref type="bibr">, Tang et al. 2019</ref>). Since the system reward is the sum of the drivers' rewards, the system value function does decompose into the individual drivers' value functions computed by each driver's own trajectories. The approximation here is using a single value function learned from all drivers' data. See <ref type="bibr" target="#b82">(Qin et al. 2020</ref>) for detailed discussions. Specifically, <ref type="bibr" target="#b133">Xu et al. (2018)</ref> learn a tabular driver value function using TD(0), and <ref type="bibr" target="#b124">Wang et al. (2018)</ref>, <ref type="bibr" target="#b106">Tang et al. (2019</ref><ref type="bibr" target="#b40">), Holler et al. (2019)</ref> apply DQN-Figure <ref type="figure">2</ref>: The order matching process with batching from the system perspective <ref type="bibr" target="#b82">(Qin et al. 2020</ref>). The assignments are for illustration only. This type of single-agent approach avoids dealing explicitly with the multi-agent aspect of the problem and the interaction among the agents during training. Besides simplicity, this strategy has the additional advantage of being able to easily handle a dynamic set of agents (and hence, a changing action space) <ref type="bibr">(Ke, Yang, Ye et al. 2020</ref>). On the other hand, order matching requires strong system-level coordination in that a feasible solution has to satisfy the one-to-one constraints. To address this issue, <ref type="bibr" target="#b133">Xu et al. (2018)</ref>, <ref type="bibr" target="#b106">Tang et al. (2019)</ref> use the learned state values to populate the edge weights of a bipartite assignment problem to generate a collective-greedy policy <ref type="bibr" target="#b82">(Qin et al. 2020)</ref> with respect to the state values. <ref type="bibr" target="#b40">Holler et al. (2019)</ref> assume a setting where drivers are matched or repositioned sequentially so that the policy output always satisfies the matching constraints.</p><p>Leveraging MARL, <ref type="bibr" target="#b56">Li et al. (2019)</ref>, <ref type="bibr" target="#b46">Jin et al. (2019)</ref>, <ref type="bibr" target="#b150">Zhou et al. (2019)</ref> directly optimize the multiagent system. One significant challenge is scalability since any realistic ridesharing setting can easily involve thousands of agents, precluding the possibility of dealing with an exact joint action space. <ref type="bibr" target="#b56">Li et al. (2019)</ref> apply mean-field MARL to make the interaction among agents tractable, by taking the 'average' action of the neighboring agents to approximate the joint actions. <ref type="bibr" target="#b150">Zhou et al. (2019)</ref> argue that no explicit communication among agents is required for order matching due to the asynchronous nature of the transitions and propose independent Q-learning with centralized KL divergence (of the supply and demand distributions) regularization. Both <ref type="bibr" target="#b56">Li et al. (2019)</ref>, <ref type="bibr" target="#b150">Zhou et al. (2019)</ref> follow the centralized training decentralized execution paradigm. <ref type="bibr" target="#b46">Jin et al. (2019)</ref> take a different approach treating each spatial grid cell as a worker agent and a region of a set of grid cells as a manager agent, and they adopt hierarchical RL to jointly optimize order matching and vehicle repositioning.</p><p>In practical settings, the online matching policy often has to balance among multiple objectives <ref type="bibr" target="#b64">(Lyu et al. 2019</ref>), e.g., financial metrics and customer experience metrics. The rationale is that persistent negative customer experience will eventually impact long-term financial metrics as users churn the service or switch to competitors. There are two potential ways that one can leverage RL to approach this problem. The explicit approach is to directly learn a policy that dynamically adjusts the weights to combine the multiple objectives into a single reward function. With the abundance of historical experience data, inverse RL can be used to learn the relative importance of multiple objectives under a given unknown policy <ref type="bibr" target="#b149">(Zhou et al. 2021)</ref>. The implicit approach is to capture the necessary state signals that characterize the impact of the metrics not explicitly in the reward function, so that the learned value function correctly reflect the long-term effect of the multiple metrics. As discussed in Section 5.5, the long feedback loop is a potential challenge here.</p><p>Besides the driver-passenger pairing decisions, there are other important levers that can be optimized within the matching module, namely the matching window and the matching radius <ref type="bibr" target="#b135">(Yang, Qin, Ke &amp; Ye 2020)</ref>. The matching window determines when to match a request (or a batch of requests). A larger window increases pre-match waiting time but may decrease pick-up time for matched requests because of more available drivers. There have been several RL works on the matching window optimization, which can be done from the perspective of a request itself <ref type="bibr">(Ke, Yang, Ye et al. 2020)</ref> or the system <ref type="bibr">(Wang et al. 2019</ref><ref type="bibr" target="#b81">, Qin, Luo, Yin, Sun &amp; Ye 2021)</ref>. In <ref type="bibr">(Ke, Yang, Ye et al. 2020)</ref>, each trip request is an agent. An agent network is trained centrally using pooled experience from all agents to decide whether or not to delay the matching of a request to the next review window, and all the agents share the same policy. To encourage cooperation among the agents, a specially shaped reward function is used to account for both local and global reward feedback. They also modify the RL training framework to address the delayed reward issue by sampling complete trajectories at the end of training epochs to update the network parameters. <ref type="bibr">Wang et al. (2019)</ref> take a system's view and propose a Restricted Q-learning algorithm to determine the length of the current review window (or batch size). They show theoretical analysis results on the performance guarantee in terms of competitive ratio for dynamic bipartite graph matching with adaptive windows. Qin, Luo, Yin, Sun &amp; Ye (2021) take a similar modeling perspective but use the AC method with experience replay (ACER) <ref type="bibr" target="#b123">(Wang et al. 2016</ref>) that combines on-policy updates (through a queuing-based simulator) with off-policy updates. The matching radius defines how far an idle driver can be from the origin of a given request to be considered in the matching. It can be defined in travel distance or time. A larger matching radius may increase the average pick-up distance but requests are more likely to be matched within a batch window, whereas a smaller radius renders less effective driver availability but it may decrease the average pick-up distance. Both the matching window and radius are trade-offs between pre-match and post-match waiting times (and hence, cancellation). So far, few effort through RL has been devoted to matching radius optimization. The joint optimization of the matching window and radius is certainly another interesting line of research.</p><p>Because of its generalizability, matching for ridesharing is closed related to a number of online matching problems in other domains, the RL methods to which are also relevant and can inform the research in rideshare matching. Some examples are training a truck agent using DQN with pooled experience to dispatch trucks for mining tasks <ref type="bibr" target="#b145">(Zhang, Wang, Li &amp; Xu 2020)</ref>, learning a decentralized value function using PPO with a shaped reward function for cooperation (in similar spirit as <ref type="bibr">(Ke, Yang, Ye et al. 2020</ref>)) to dispatch couriers for pick-up services <ref type="bibr" target="#b21">(Chen, Qian, Yao, Wu, Li, Zhou, Hu &amp; Xu 2019)</ref>, and designing a self-attention, pointer network-based policy network for a system agent to assign participants to tasks in mobile crowdsourcing <ref type="bibr" target="#b93">(Shen et al. 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Vehicle Repositioning</head><p>Vehicle repositioning from a single-driver perspective (i.e., taxi routing) has a relatively long history of research since taxi service has been in existence long before the emergence of rideshare platforms. Likewise, research on RL-based approaches for this problem (see Table <ref type="table" target="#tab_5">3</ref>) also appeared earlier than that on system-level vehicle repositioning.  For the taxi routing problem, each driver is naturally an agent, and the objective thus focuses on optimizing individual reward. Common reward definitions include trip fare <ref type="bibr" target="#b86">(Rong et al. 2016)</ref>, net profit (income -operational cost) <ref type="bibr" target="#b118">(Verma et al. 2017)</ref>, idle cruising distance <ref type="bibr" target="#b31">(Garg &amp; Ranu 2018)</ref>, and ratio of trip mileage to idle cruising mileage <ref type="bibr" target="#b30">(Gao et al. 2018)</ref>. Earlier works <ref type="bibr" target="#b38">Han et al. (2016)</ref>, <ref type="bibr" target="#b127">Wen et al. (2017)</ref>, <ref type="bibr" target="#b118">Verma et al. (2017)</ref>, <ref type="bibr" target="#b31">Garg &amp; Ranu (2018)</ref> optimize the objective within a horizon up to the next successful match (i.e., A-to-B and A-to-C in Figure <ref type="figure" target="#fig_1">4</ref>), but it is now more common to consider a long-term horizon, where an episode usually consists of a trajectory over one day <ref type="bibr" target="#b58">(Lin et al. 2018</ref><ref type="bibr" target="#b97">, Shou et al. 2020</ref><ref type="bibr" target="#b45">, Jiao et al. 2021</ref>). We illustrate these concepts in Figure <ref type="figure" target="#fig_1">4</ref>.</p><p>The type of actions of an agent depends on the physical abstraction adopted. A simpler and more common way of representing the spatial world is a grid system, square or hexagonal 5 <ref type="bibr" target="#b38">(Han et al. 2016</ref><ref type="bibr" target="#b127">, Wen et al. 2017</ref><ref type="bibr" target="#b118">, Verma et al. 2017</ref><ref type="bibr" target="#b30">, Gao et al. 2018</ref><ref type="bibr" target="#b58">, Lin et al. 2018</ref><ref type="bibr" target="#b86">, Rong et al. 2016</ref><ref type="bibr" target="#b45">, Jiao et al. 2021</ref><ref type="bibr" target="#b97">, Shou et al. 2020)</ref>. In this setting, the action space is the set of neighboring cells (often including the current cell). <ref type="bibr" target="#b96">Shou &amp; Di (2020b)</ref> explain the justification for this configuration. Determination of the specific destination point is left to a separate process, e.g., pick-up points service <ref type="bibr" target="#b45">(Jiao et al. 2021)</ref>. The more realistic abstraction is a road network, in which the nodes can be intersections or road segments <ref type="bibr" target="#b31">(Garg &amp; Ranu 2018</ref><ref type="bibr">, Yu et al. 2019</ref><ref type="bibr">, Zhou et al. 2018</ref><ref type="bibr" target="#b88">, Schmoll &amp; Schubert 2020)</ref>. The action space is the adjacent nodes or edges of the current node. This approach supports a turn-by-turn guiding policy but requires more map information at run time.</p><p>Most of the papers adopt a tabular value function, so the state is necessarily low-dimensional, including spatiotemporal information and sometimes additional categorical statuses.  <ref type="bibr" target="#b127">(Wen et al. 2017)</ref>. <ref type="bibr" target="#b45">(Jiao et al. 2021</ref>) is a hybrid approach in that it performs an action tree search at the online planning stage using estimated matching probabilities and a separately learned state value network. <ref type="bibr" target="#b31">(Garg &amp; Ranu 2018)</ref> is in a similar spirit by augmenting the multi-arm bandits with Monte Carlo tree search.</p><p>The problem formulation most relevant to the ridesharing service provider is system-level vehicle repositioning. Similar to order matching, the ridesharing platform reviews the vehicles' states at fixed time intervals which are significantly longer than those for order matching. See Figure <ref type="figure">2</ref> for illustration. Idle vehicles that meet certain conditions, e.g., being idle for sufficiently long time and not in the process of an existing reposition task, are sent reposition recommendations, which specify the desired destinations and the associated time windows. The motivation here is to explicitly modify the current distribution of the available vehicles so that collectively they are better positioned to fulfill more requests more efficiently in the future. Figure <ref type="figure" target="#fig_2">5</ref> explains the idea with a concrete example. If the vehicles reposition independently (following the orange arrows), they both move to the orange-circled area and there will be a surplus of supply while the demand in the green-circled area will not be served. In contrast, if the vehicles coordinate and the one in the south repositions by the blue arrow, both vehicles will be matched, and all the requests are served.</p><p>The agent can be either the platform or a vehicle, latter of which calls for a MARL approach. All the works in this formulation have global SD information (each vehicle and request's status or SD distributions) in the state of the MDP, and a vehicle agent will additionally have its spatiotemporal status in the state. The rewards are mostly the same as in the taxi routing case, except that Mao et al.</p><p>(2020) consider the monetized passenger waiting time. The actions are all based on grid or taxi zone systems.</p><p>The system-agent RL formulation has only been studied very recently, in view of the intractability of the joint action space of all the vehicles (see Table <ref type="table" target="#tab_6">4</ref>). To tackle this challenge of scalability, 5 The hexagonal grid system is the industry standard.  Feng et al. (2020) decompose the system action into a sequence of atomic actions corresponding to passenger-vehicle matches and vehicle repositions. The MDP encloses a 'sequential decision process' in which all feasible atomic actions are executed to represent one system action, and the MDP advances its state upon complete of the system action. They develop a PPO algorithm for the augmented MDP to determine the sequence of the atomic actions. The system policy in <ref type="bibr" target="#b67">(Mao et al. 2020</ref>) produces a reposition plan that specifies the number of vehicles to relocate from zone i to j so that the action space is independent from the number of agents (at the expense of additional work at execution). The agent network, trained by a batch AC method, outputs a value for each OD pair, which after normalization gives the percentage of vehicles from each zone to a feasible destination.</p><p>The vehicle-agent approaches have to address the coordination issue among the agents.  <ref type="bibr" target="#b16">(Chaudhari et al. 2020a)</ref>, where a vehicle agent executes a mix of independent and coordinated actions. The central module determines the need for coordination based on SD gaps, and explicit coordination is achieved by solving an assignment problem to move vehicles from excess zones to deficit zones.</p><p>For joint matching and repositioning optimization, one major challenge is the heterogeneous review cadence. Matching and reposition decisions are typically made asynchronously in practice. To address this issue, <ref type="bibr" target="#b107">Tang et al. (2021)</ref> allow the two modules to operate independently but share the same spatiotemporal state value function which is updated online. If the two decisions are formulated into the same problem, the action space can be masked depending on the state <ref type="bibr" target="#b40">(Holler et al. 2019</ref>).</p><p>Existing RL literature on repositioning often assumes the drivers' full compliance to reposition, i.e., the autonomous vehicle setting. How non-compliance affects the overall performance of a reposition algorithm is a natural question to ask when considering a real-world ridesharing system, in which we expect to see a combination of drivers' independent cruising strategies <ref type="bibr" target="#b117">(Urata et al. 2021</ref><ref type="bibr" target="#b129">, Wong et al. 2014</ref>) and system-guided idle cruising behavior. It is also interesting and practically necessary to investigate incentives design and strategies that facilitate the repositioning process. In <ref type="bibr" target="#b154">(Zhu et al. 2021)</ref>, for example, a mean-field MDP is developed for modeling drivers' strategies, and empirical investigations are performed on how spatiotemporal driver incentives affect driver behavior and the system performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Route Guidance (Navigation)</head><p>Routing in this paper refers to low-level navigation decisions on a road network, typically with output of matching and repositioning algorithms as input. The road network, combined with traffic conditions on the links (exhibited as link costs), forms the traffic network which is a non-stationary stochastic network <ref type="bibr" target="#b68">(Mao &amp; Shen 2018)</ref>. It is known that standard static shortest-path algorithms do not find the path with minimum expected cost in this case, and the optimal route is not a simple route but a policy <ref type="bibr" target="#b37">(Hall 1986</ref><ref type="bibr" target="#b52">, Kim et al. 2005)</ref>. There are two types of set-up for the routing problem, depending on the decision review time. In the first type of set-up, each vehicle on the road network selects a route for a given OD pair from a set of feasible routes. The decision is only reviewed and revised after a trip is completed. Hence, it is called route planning or route choice. When the routes for all the vehicles are planned together, it is equivalent to assigning the vehicles to each link in the network, and hence, the problem is called traffic assignment problem (TAP), which is typically for transportation planning purposes. In the second type of set-up, the routing decision is made at each intersection to select the next outbound road (link) to enter. adaptive navigation decisions for vehicles to react to the changing traffic state of the road network.</p><p>The problem corresponding to this set-up is called dynamic routing, dynamic route choice, or route guidance.</p><p>Routing on a road network is a typical multi-agent problem, where the decisions made by one agent has influence on the other agents' performance, simply in that the congestion level of a link depends directly on the number of vehicles passing through that link at a given time and has direct impact on the travel time for all the vehicles on that link within the same time interval. The literature for route planning and TAP often consider the equilibrium property of the algorithms when a population of vehicles adopt them. TAP is typically from a traffic manager's (i.e., system's) perspective. Its goal is to reach system equilibrium (SE, or also often referred to as the system optimum). Some works focus on route planning or TAP from an individual driver's perspective and maximize individual reward. These algorithms try to reach user equilibrium (UE) or Nash equilibrium, under which no agent has the incentive to change its policy because doing so will not achieve higher utility. This is the best that selfish agents can achieve but may not be optimal for the system.</p><p>Value-based RL is by far the most common approach for route planning and TAP aiming to reach UE (see Table <ref type="table">5</ref>). In the MDP formulation, the agent is a vehicle (or equivalently, a task) with a given OD pair. The objective is to minimize the total travel time for an individual vehicle (task) <ref type="bibr" target="#b66">(Mainali et al. 2008</ref><ref type="bibr" target="#b85">, Ramos et al. 2018</ref><ref type="bibr" target="#b148">, Zhou, Song, Zhao &amp; Liu 2020)</ref>, i.e., the agent is selfish. The immediate reward is the total travel time of a trip for an individual and a particular run. This MDP is stateless, so strictly speaking, it is a multi-arm bandits or contextual bandits problem <ref type="bibr" target="#b55">(Li et al. 2010)</ref> if considering time as a contextual feature. The action to take at decision time is to select a route from the set of feasible routes for the associated OD pair <ref type="bibr" target="#b85">(Ramos et al. 2018</ref><ref type="bibr" target="#b148">, Zhou, Song, Zhao &amp; Liu 2020</ref><ref type="bibr" target="#b7">, Bazzan &amp; Chira 2015)</ref>. The value function that governs the route choice decisions represents the long-term expected travel time for the trip identified by the given OD pair. Due to the multi-agent nature, the environment w.r.t. each agent is non-stationary in that the reward function is changing with the policy updates from the other agents. Empirical convergence to UE is demonstrated by <ref type="bibr" target="#b85">Ramos et al. (2018)</ref>. <ref type="bibr" target="#b148">Zhou, Song, Zhao &amp; Liu (2020)</ref> further develop a Bush-Mosteller RL scheme for MARL and formally establishes its UE convergence property. We also highlight some unique features of the papers. <ref type="bibr" target="#b85">Ramos et al. (2018)</ref> consider a different objective from the common and minimizes the driver's regret. To do that, the Q-learning updates are modified using the estimated action regret, which can be computed by local observations and global travel time information communicated by an app. <ref type="bibr" target="#b7">Bazzan &amp; Chira (2015)</ref> propose a hybrid method, with Q-learning for individual agents and Genetic Algorithm for reaching system equilibrium, minimizing the average travel time over different trips in the network. This method is thus able to achieve SE. <ref type="bibr" target="#b66">Mainali et al. (2008)</ref> adopt Q-iterations with a model set-up similar to that of dynamic routing to be discussed next.</p><p>Most applications of RL to routing concern with the dynamic routing (DR) problem (see Table <ref type="table">5</ref>).</p><p>The MDP is modeled around a vehicle agent. The basic state information is the traffic state of the current node (i.e., intersection). Some works consider state features of the neighboring nodes <ref type="bibr" target="#b52">(Kim et al. 2005</ref><ref type="bibr" target="#b68">, Mao &amp; Shen 2018</ref>) so that the agent has a broader view of the environment. The action space comprises the set of outbound links (i.e., roads) or adjacent nodes from the current node, so the policy provides a turn-by-turn navigation guidance until the destination is reached. While it is most common to use travel time on a link as the reward function, <ref type="bibr" target="#b113">Tumer et al. (2008)</ref>, <ref type="bibr" target="#b32">Grunitzki et al. (2014)</ref> stand out by defining a new form called difference reward, which is the difference in average travel time on a link with and without the agent in the system. This applied to only a reward function dependent on the number of agents using the traversed link. In particular, travel distance cannot be used to define a difference reward. Whether solving a specific formulation achieves UE or SE depends on the reward function used. The average travel time on a link is a global reward because it is an aggregate of local rewards (i.e., individual travel times) of all the agents on that link. The difference reward, by definition, is a global reward that also reflects individual effect. If all the agents in the system learn by global reward <ref type="bibr" target="#b113">(Tumer et al. 2008</ref><ref type="bibr" target="#b32">, Grunitzki et al. 2014</ref><ref type="bibr" target="#b95">, Shou &amp; Di 2020a</ref>), then the system is expected to achieve SE. Otherwise, the agents learn by their local rewards, and we will have UE or Nash equilibrium <ref type="bibr" target="#b52">(Kim et al. 2005</ref><ref type="bibr" target="#b140">, Yu et al. 2012</ref><ref type="bibr" target="#b68">, Mao &amp; Shen 2018</ref><ref type="bibr" target="#b8">, Bazzan &amp; Grunitzki 2016</ref><ref type="bibr" target="#b126">, Wen et al. 2019)</ref>.</p><p>Most works in the literature adopt Q-learning or its variant as the training algorithm. We report several notable developments. To tackle the sample efficiency issue of online model-free methods, <ref type="bibr" target="#b68">Mao &amp; Shen (2018)</ref> propose an offline batch RL approach (fitted Q-iterations) with a tree-based function approximator (Extreme Randomized Trees) that empirically shows good convergence property. Hierarchical methods have also been adopted to address the complexity of a large-scale problem. In <ref type="bibr" target="#b126">(Wen et al. 2019)</ref>, the global road network is divided into sub-networks by differential evolution-based clustering. The top-level network contains only the boundary nodes of the original network. The top-level policy produces the destination node for a sub-network. The sub-level policy provides link-level guidance to reach its sub-destination. Shou &amp; Di (2020b) adopt a bilevel optimization scheme. At the lower level, a mean-field MARL algorithm solves for the dynamic routing problem for the travelers, while at the upper level, a Bayesian optimization module optimizes the control (i.e., reward parameter of the travelers) by the city planner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ride-pooling (Carpool)</head><p>Ride-pooling optimization typically concerns with matching, repositioning, routing (see e.g., <ref type="bibr" target="#b146">(Zheng et al. 2018</ref><ref type="bibr">, Alonso-Mora, Wallar &amp; Rus 2017</ref><ref type="bibr" target="#b2">, Alonso-Mora, Samaranayake, Wallar, Frazzoli &amp; Rus 2017</ref><ref type="bibr" target="#b110">, Tong et al. 2018)</ref>). The RL literature has primarily focused on the first two problems. The ride-pooling matching problem differs from that in Section 4.2 in that a combination of multiple passengers, and hence their combined trip, can be matched to a vehicle that may or may not be empty. See stages B and C in Figure <ref type="figure" target="#fig_3">6</ref> from (Alonso-Mora, Samaranayake, Wallar, Frazzoli &amp; Rus 2017) for an illustration. The repositioning problem is similar to the ride-hailing case, except that the objective is to optimize some pooling-specific metrics that we define next. The routing problem solves for the sequence of pick-ups and drop-offs given the assigned passengers for a vehicle. The routing problem could also concern with route guidance on the road network. See stage D in Figure <ref type="figure" target="#fig_3">6</ref>.</p><p>Many works have multiple objectives and define the reward as a weighted combination of several quantities, with hand-tuned weight parameters. Passenger wait time is the duration between the request time and the pick-up time. Detour delay is the extra time a passenger spends on the vehicle due to the participation in the ride-pooling. In some cases, these two quantities define the feasibility of a potential pooled trip instead of appearing in the reward <ref type="bibr" target="#b90">(Shah et al. 2020)</ref>. Effective trip distance is the travel distance between the origin and destination of a trip request, should it be fulfilled without ride-pooling. <ref type="bibr" target="#b142">Yu &amp; Shen (2019)</ref> consider minimizing passenger wait time, detour delay, and lost demand. <ref type="bibr" target="#b34">Guériau &amp; Dusparic (2018)</ref> maximize the number of passengers served. <ref type="bibr" target="#b47">Jindal et al. (2018)</ref> maximize the total effective trip distance within an episode, which is just the number of served requests weighted by individual trip distance. Considering a fixed number of requests within an episode (hence fixed maximum effective distance), this metric reflects the efficiency of ride-pooling. Table <ref type="table">5</ref>: Summary of literature for Route Guidance.</p><p>The state of an agent usually consists of global SD information, similar to that for matching and reposition, but the vehicle status contains key additional information of occupancy and OD's of the passengers on board.</p><p>The action space depends on whether the agent is modeled at vehicle level or system level. Existing RL-based works all require that a vehicle drops off all the passengers on board according to a planned route before a new round of pooling. An individual vehicle agent can then match to a feasible group of passengers (in terms of capacity and detour delay) <ref type="bibr" target="#b47">(Jindal et al. 2018)</ref>, reposition to another location <ref type="bibr" target="#b1">(Alabbasi et al. 2019</ref><ref type="bibr" target="#b35">, Haliem et al. 2020</ref><ref type="bibr">, 2021)</ref>, or both <ref type="bibr" target="#b34">(Guériau &amp; Dusparic 2018)</ref>. A system-level agent has to make action decisions for the entire fleet together <ref type="bibr" target="#b142">(Yu &amp; Shen 2019</ref><ref type="bibr" target="#b90">, Shah et al. 2020)</ref>.</p><p>The feasible combinations of passengers are typically determined by a separate process based on a pairwise shareability graph or a trip-vehicle graph(Alonso-Mora, Samaranayake, Frazzoli &amp; Rus 2017). (See illustration in Figure <ref type="figure" target="#fig_3">6</ref>.)</p><p>Papers with vehicle-level policy commonly train a single agent and apply to all the vehicles independently (see e.g., <ref type="bibr" target="#b36">(Haliem et al. 2021)</ref>). DQN is a convenient choice of training algorithm for this setting. For system-level decision-making, both <ref type="bibr" target="#b142">Yu &amp; Shen (2019)</ref> and <ref type="bibr" target="#b90">Shah et al. (2020)</ref> employ an ADP approach and consider matching decisions only. <ref type="bibr" target="#b142">Yu &amp; Shen (2019)</ref> follow a similar strategy as <ref type="bibr" target="#b99">(Simao et al. 2009</ref>) and use a linear approximation for the value function. In contrast, Shah et al.</p><p>(2020) decompose the system value function into vehicle-level and adopts a neural network for the individual value function, which is updated by mini-batch stochastic gradient descent similar to that in DQN.</p><p>It has become increasingly clear that dynamic routing and route planning in the context of ridepooling require specific attention. In particular, there are two aspects unique to ride-pooling. First, the trips are known only at their request times. Hence, the routes taken by the pooled vehicles (i.e., the sequences of pick-ups and drop-offs) have to be updated dynamically to account for the newly joined passengers. <ref type="bibr" target="#b110">Tong et al. (2018)</ref>, <ref type="bibr" target="#b132">Xu et al. (2020)</ref> formulate the route planning problem for ride-pooling and develop efficient DP-based route insertion algorithms for carpool. In Section 4.6, we will see that this is also part of the stochastic dynamic vehicle routing problem. Second, within a given route plan, the route taken by a pooled vehicle from an origin to a destination can affect the chance and quality of its future pooling. Hence, dynamic routing (or route choice) between an OD pair can be optimized in that direction, e.g., <ref type="bibr" target="#b143">Yuen et al. (2019)</ref> go beyond the shortest-path to make route recommendations for better chance of pooling. <ref type="bibr" target="#b33">Guériau et al. (2020)</ref> evaluate the SAMoD system proposed in <ref type="bibr" target="#b34">(Guériau &amp; Dusparic 2018)</ref> in a microscopic environment based on SUMO with a traffic congestion-aware (non-RL) routing component. We expect to see more RL-based algorithms for the ride-pooling dynamic routing problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Vehicle Routing Problem (VRP)</head><p>VRP has close connection to the ridesharing problem in that variants of VRP could serve as a subroutine of the ride-pooling problem and could even used to model the entire ridesharing problem itself. The main challenge, in the context of ridesharing, is that new demand (a pair of pick-up and drop-off locations) appears in an online nature and has to be inserted into the existing route dynamically. So reviewing the RL literature for VRP is not only for the completeness of this survey but also essential for one to appreciate the complexity and challenges in tackling ridesharing via RL.</p><p>VRP has many variants in its rich literature, so it is important to be clear on their differences and on the variant that each paper claims to solve. The basic setup of a VRP consists of a transportation network G := (V, E, w) and a fleet of K vehicles, where V is the set of nodes (customer and depot locations), and E is the set of edges such that e ij ∈ E indicates that it is possible to travel from node v i to node v j . w(x ij ) := w ij is the edge cost, typically distance or travel time. The depot v 0 ∈ V is a special node where all the vehicles depart and return at. The vanilla VRP is to find an optimal set of disjoint routes (one for each vehicle) that start and end at the depot, that collectively cover all the nodes, and whose total cost (summing over all the edges in those routes) is minimized. A traveling salesman problem (TSP) is a special (simplified) instance of VRP, in which there is no depot, and the fleet consists of a single vehicle. In a capacitated VRP (CVRP), each node has a demand quantity to be fulfilled by one of the vehicles. Each vehicle has a limited capacity, and it starts from the depot with a full load of goods to fulfill the demand of the nodes on its route. The total capacities of the fleet is sufficiently large, and all the demand has to be fulfilled. A CVRP with split delivery allows the demand of each node to be fulfilled by multiple vehicles. In a VRP with time windows, each node has a delivery window within which the node has to be visited or its demand has to be satisfied. If the time window constraints are soft, they can be violated with the price of a penalty that contributes to the total cost function. In some variants of the VRP, the goods for the demand of a node (destination) has to be picked up from another designated non-depot location (origin) before being delivered to it. This is known as a VRP with pick-up and delivery, also known as the dial-a-ride problem (DARP).</p><p>If the fleet consists of electric vehicles (EV), the set of nodes also include charging stations, and each EV has a limited battery capacity, before the depletion of which the EV has to reach a charging station to recharge. In practical situations, a VRP or variant can be stochastic and dynamic (SDVRP), i.e., its parameters (e.g., demand and travel time) are uncertain, and the requests are not known at the beginning but are revealed sequentially throughout the problem period.</p><p>The connection between VRP and ridesharing exists at both local and fundamental levels. As a subproblem in ride-pooling, the rerouting problem after a new passenger is matched to the vehicle is a TSP with pick-up and delivery (TSPPD), which is one-vehicle single-tour instance of CVRP with pick-up and delivery. At a fundamental level, (multi-vehicle) ride-pooling is a stochastic dynamic multi-vehicle CVRP with pick-up and delivery. Although there are no explicit time windows, cancellation may occur if waiting time is too long. Ride-hailing is also a special case where the vehicles all have unit capacity, and in this case, matching and routing merge into one single problem. So the ridesharing problem is an SDCVRP, except that repositioning is an intervention strategy not considered in SDCVRP.</p><p>The goal of this section is not to provide a complete survey of the VRP literature but rather to point out the representative or unique works that adopt RL to solve VRPs (see Table <ref type="table" target="#tab_8">6</ref>). A recent review of RL-based methods for solving stochastic dynamic VRP can be found in <ref type="bibr">[Hildebrandt, et al., 2021]</ref> and a more general one in <ref type="bibr" target="#b116">(Ulmer et al. 2020)</ref>.</p><p>Single-vehicle v.s. multi-vehicle problems CVRP may appear in different forms, and sometimes the subtle differences may not be stated clearly. Most papers solve the single-vehicle problem where there is only one active vehicle at any time. In the capacitated single vehicle problem, the vehicle can make multiple tours (i.e., passing through the depot multiple times) to fulfill all the demand, but the number of tours is not set in advance. In the multi-vehicle case, a fleet of K vehicles are active simultaneously. For static VRPs, if the number of tours in the single-vehicle case is fixed, then it is equivalent to the multi-vehicle counterpart by treating each tour as a separate vehicle. (For problems with time windows, this equivalence can be achieved by resetting the clock every time a new tour starts.) Otherwise, they are not equivalent in general because the number of tours in the optimal solution for the single-vehicle problem may not be N . For dynamic problems where the requests are not all known a-priori, it is not possible to generate the fixed number of tours in sequence, since one cannot insert a new request to a previous tour. In this case, equivalence can only be achieved by keeping each tour on the same clock and updating the routes with the newly appeared requests at each time step. As we will see below, this would render essentially a multi-vehicle algorithm.</p><p>The majority of the RL-based methods for VRP models the agent as a vehicle with the system-state visibility. The state thus consists of two types of information: the vehicle state, which includes the vehicle's current location and remaining capacity (for pick-up and delivery, e.g., <ref type="bibr" target="#b116">(Ulmer et al. 2020</ref><ref type="bibr" target="#b44">, James et al. 2019</ref><ref type="bibr" target="#b48">, Joe &amp; Lau 2020)</ref>) or inventory (for homogeneous goods delivery, e.g., <ref type="bibr" target="#b74">(Nazari et al. 2018</ref><ref type="bibr" target="#b53">, Kool et al. 2018</ref><ref type="bibr" target="#b23">, Delarue et al. 2020</ref>)); the system state, which contains the locations of the customer nodes, the demand at each node, and the unserved customers. For pick-up and delivery problems, the system state instead contains the pick-up and delivery locations of the orders. In the case of EVs, the vehicle state additionally contains the vehicle's battery level, and the system state also includes the locations of the charging stations and the number of vehicles available (not in charging). The action of the agent is to specify the next stop (pick-up/delivery location or charging station in the case EV) to visit for the current vehicle. The sequence of actions form a route for the vehicle. When multiple routes/tours are required, the different routes are separated by the insertion of the depot <ref type="bibr" target="#b74">(Nazari et al. 2018</ref><ref type="bibr" target="#b27">, Duan et al. 2020</ref><ref type="bibr" target="#b53">, Kool et al. 2018</ref><ref type="bibr" target="#b57">, Lin et al. 2021)</ref>. For (dynamic) multi-vehicle problem where decisions for all the vehicles are made at each time step, the agents would generate their actions sequentially to avoid conflicting actions <ref type="bibr" target="#b44">(James et al. 2019</ref><ref type="bibr" target="#b144">, Zhang, He, Zhang, Lin &amp; Li 2020)</ref>. Since the objective of VRPs is typically to minimize total travel distance, the reward is naturally defined as the negative travel distance. For problems with (soft) time window constraints, the negative penalty for constraint violation is added to the reward.</p><p>Typically, these methods adopt an encoder-decoder agent network architecture. The encoder is responsible for encoding part or all of the state information into an embedding vector (or context), which, potentially with additional input state features, is fed into the decoder to generate the action one at a time. <ref type="bibr" target="#b11">Bello et al. (2016)</ref> develop an policy network based on the pointer network <ref type="bibr" target="#b119">(Vinyals et al. 2015)</ref>, which consists of an RNN encoder and an RNN decoder. The major novelty over a sequence-to-sequence architecture is that the decoder uses attention mechanism to attend over the embeddings of the input nodes to generate the probability distribution over the input space, thus eliminating the distance disparity in the output with respect to the input, a feature that is particularly suitable for solving TSP and VRP. To reduce the complexity of the encoder and avoid imposing a sequence on the input state features (e.g., customer locations), which is unnecessary in routing problems, <ref type="bibr" target="#b74">Nazari et al. (2018)</ref> modify the pointer network with a non-sequential encoder which simply embeds each individual input node. They incorporate the policy network into an AC method and validate the design on a CVRP with split delivery. A few more recent works have adopted this network structure. <ref type="bibr" target="#b44">James et al. (2019)</ref> use structural graph embedding (Struct2Vec) for the encoder, since their agent's state additionally contains a vehicle tour graph. In <ref type="bibr" target="#b57">(Lin et al. 2021)</ref>, the encoder has 1D convolution and graph embedding for the input nodes, followed by an attention layer. <ref type="bibr" target="#b27">Duan et al. (2020)</ref> include edge features in the state besides the node features of the transportation network. Their encoder is based on graph convolution network with both node and edge inputs. Another work with significant novelty is <ref type="bibr" target="#b53">(Kool et al. 2018)</ref>, which develops a policy network with a transformer-based encoder and a self-attention-based decoder to use in a PG method (REINFORCE) with the baseline computed from deterministic greedy rollout. This training framework has also been adopted by <ref type="bibr" target="#b144">Zhang, He, Zhang, Lin &amp; Li (2020)</ref> for multi-vehicle VRP with soft time windows, <ref type="bibr" target="#b57">Lin et al. (2021)</ref> for EV VRP with time windows, and <ref type="bibr" target="#b27">Duan et al. (2020)</ref>, which jointly train an MLP-based binary classifier on edge encoding with the policy network output as labels. They have tested their method on a CVRP with 400 nodes, the largest among the reviewed works.</p><p>For SDVRP, <ref type="bibr" target="#b116">Ulmer et al. (2020)</ref> argue that it is a more convenient model, which also aligns better with popular approaches to this problem, that the action contains also the route plan information. They define a new variant of MDP, called route-based MDP, in which the state includes the route plan from the last epoch, and the action contains the updated route plan in addition to the next stop to visit. The 'immediate' reward becomes the difference in route value between the old and new plans. Following this line, Joe &amp; Lau (2020) model a system agent whose state includes the cost for the remaining route for each vehicle, and the agent assigns a new request to a vehicle at each decision epoch. The rerouting after matching is solved by simulated annealing for VRP. Under this framework, one only needs to learn an action-value function to generate the matching decisions. The algorithm is tested on a multi-vehicle SDVRP with pick-up/delivery and time windows. 6 In a somewhat similar spirit but for static CVRP, the MDP action in <ref type="bibr" target="#b23">(Delarue et al. 2020</ref>) is to generate one route (tour). The value network consists of dense layers and ReLU activation and is representable by mixed-integer linear constraints so that the action can be computed through solving a Prize Collecting TSP by MIP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Data Sets &amp; Environments</head><p>The problems in ridesharing are highly practice-oriented, and results from toy data sets or environments may present a very different picture from those in reality. Hence, real-world data sets and realistic simulators backed up by them are instrumental to research in RL algorithms for these problems.</p><p>The most commonly used data sets are those made available by NYC TLC (Taxi &amp; Limousine Commission) (TLC 2020). This large public data repository contains trip records from several different services, Yellow Taxi, Green Taxi, and FHV (For-Hire Vehicle), from 2009 to 2020. The Yellow Taxi data is the most frequently used for various studies. The FHV trip records are submissions from the TLC-licensed bases (e.g., <ref type="bibr">Uber, Lyft)</ref>   Although not yet open-sourced, this simulation environment supports both matching and vehicle repositioning tasks and accepts input algorithms through a Python API.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Challenges and Opportunities</head><p>Given the state of the current literature, we discuss a few challenges and opportunities that we feel crucial in advancing RL for ridesharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ride-pooling</head><p>As seen in Section 4.5, the reward function in ride-pooling is often a hand-tuned combination of multiple objectives. It is desirable to have a principled way to determine the best weighting scheme automatically, potentially leveraging inverse RL and multi-objective learning techniques <ref type="bibr" target="#b155">(Zou et al. 2021</ref><ref type="bibr" target="#b5">, Arora &amp; Doshi 2021)</ref> in a similar sense of the ride-hailing case <ref type="bibr" target="#b149">(Zhou et al. 2021)</ref>. Methods for learning to make matching decisions are still computationally intensive <ref type="bibr" target="#b90">(Shah et al. 2020</ref><ref type="bibr" target="#b142">, Yu &amp; Shen 2019)</ref>, in part due to the need to use VRP solver to determine feasible actions (combination of passengers). Moreover, all existing works assume that the action set is pre-determined, and some make only high-level decisions of reposition and serving new passengers or not. A more sophisticated agent may be called for to figure out, for example, how to dynamically determine the desirable passenger combination to match to a vehicle and the routes to take thereafter. Ride-pooling pricing (Ke, Yang, Li, Wang &amp; Ye 2020), a hard pricing problem itself, is tightly coupled with matching.</p><p>A joint pricing-matching algorithm for ride-pooling is therefore highly pertinent. As mentioned in Section 4.5, it is also highly anticipated to go beyond using generic routing algorithms and to tailor them to ride-pooling with RL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Joint Optimization</head><p>The rideshare platform is an integrated system, so joint optimization of multiple decision modules leads to better solutions that otherwise unable to realize under separate optimizations, ensuring that different decisions work towards the same goal. RL for joint optimization across multiple modules calls for research on reward function design, state-action representation that facilitates intermodule communication, and the training algorithms. Models and algorithms that allow decentralized execution by the different modules are highly preferred in practice. We have already seen development on RL for joint matching-reposition <ref type="bibr" target="#b40">(Holler et al. 2019</ref><ref type="bibr">, Jin et al. 2019</ref><ref type="bibr">, Tang et al. 2021</ref>) and with ride-pooling <ref type="bibr" target="#b34">(Guériau &amp; Dusparic 2018)</ref>, pricing-matching <ref type="bibr" target="#b19">(Chen, Jiao, Qin, Tang, Li, An, Zhu &amp; Ye 2019)</ref>, and pricing-reposition <ref type="bibr" target="#b114">(Turan et al. 2020</ref>). An RL-based method for fully joint optimization of all major modules is highly expected. Meanwhile, this also requires readiness from the rideshare platforms in terms of system architecture and organizational structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Heterogeneous Fleet</head><p>With the wide adoption of electric vehicles and the emergence of autonomous vehicles, we are facing an increasingly heterogeneous fleet on rideshare platforms. Electric vehicles have limited operational range per their battery capacities. They have to be routed to a charging station when the battery level is low (but sufficiently high to be able to travel to the station). Autonomous vehicles may run within a predefined service geo-fence due to their limited ability (compared to human drivers) to handle complex road situations. For an RL-based approach, a heterogeneous fleet means multiple types of agents with different state and action spaces. The adoption of autonomous vehicles also opens new operational paradigms. Dynamic fleet size inflation <ref type="bibr" target="#b10">(Beirigo et al. 2022)</ref>, for example, hires idle autonomous vehicles on demand to guarantee service quality contracts in a ridesharing marketplace. Specific studies are required to investigate how to make such a heterogeneous fleet cooperate well to complement each other and maximize the advantage of each type of vehicles to improve overall system efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Simulation &amp; Sim2Real</head><p>Simulation environments are fundamental infrastructure for successful development of RL methods. Despite those introduced in Section 4.7, simulation continues to be a significant engineering and research challenge. We have rarely seen comparable simulation granularity as that of the environments for traffic management, (e.g., SUMO <ref type="bibr" target="#b61">(Lopez et al. 2018)</ref>, Flow <ref type="bibr" target="#b130">(Wu et al. 2017</ref>)) or autonomous driving (e.g., SMARTS <ref type="bibr" target="#b151">(Zhou, Luo, Villela, Yang, Rusu, Miao, Zhang, Alban, Fadakar, Chen et al. 2020)</ref>, CARLA <ref type="bibr" target="#b26">(Dosovitskiy et al. 2017</ref>)). 7 The opportunity is an agent-based microscopic simulation environment for ridesharing that accounts for both ride-hailing and carpool, as well as driver and passenger behavior details, e.g., price sensitivity, cancellation behavior, driver entrance/exit behavior. None of the existing public/open-source simulators supports pricing decisions. Those simulators described in the pricing papers all have strong assumptions on passenger and driver price elasticities.</p><p>A better way might be to learn those behaviors from data through, e.g., generative adversarial imitation learning <ref type="bibr" target="#b92">(Shang et al. 2019)</ref> or inverse RL <ref type="bibr" target="#b70">(Mazumdar et al. 2017)</ref>.</p><p>No publicly known ridesharing simulation environment has sufficiently high fidelity to the real world to allow an agent trained entirely in it to deploy directly to production. Several deployed works <ref type="bibr" target="#b82">(Qin et al. 2020</ref><ref type="bibr" target="#b45">, Jiao et al. 2021)</ref> in Section 4 have all adopted offline RL for learning the state value functions and online planning. The robotics community has been extensively investigating ways to close the reality gap <ref type="bibr" target="#b112">(Traoré et al. 2019</ref><ref type="bibr" target="#b71">, Mehta et al. 2020</ref>). Sim2real transfer algorithms for ridesharing agents are urgently sought after.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Human Behavior</head><p>Central to ridesharing platforms are human participants (passengers and drivers). 8 The impact of human behavior is pervasive in the ridesharing marketplace, e.g., in request conversion, cancellation, idle driver diffusion, driver sign-in and sign-off, rider and driver responses to incentives. Human behavior is inherently stochastic and difficult to model, especially with limited data (in size and features), which introduces errors to optimization and simulation. Compared to traditional approaches from operations research, RL offers potential to better handle these stochasticity issues through its adaptability and data-driven nature.</p><p>Unlike cumulative effects induced by spatiotemporal transitions (e.g., matching), human-induced long-term effects from changes in habituation and sentiment on the marketplace are much harder to learn due to the much longer horizon such effects span over. To RL, this is dual challenge and opportunity. The challenge is the long feedback loop and very delayed reward signals, and the opportunities lie in engineering and capturing more refined system state features that capture human behavior characterization better and in designing a richer set of reward signals that facilitate the learning of policies for long-term optimality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Non-stationarity</head><p>We have seen in Sections 4.2 and 4.3 that RL algorithms deployed to real-world systems generally adopt offline training -once the value function or the policy is deployed, it is not updated until the next deployment. Value functions trained offline using a large amount of historical data are only able to capture recurring patterns resulted from day-on-day SD changes. However, the SD dynamics can be highly non-stationary in that one-time abrupt changes can easily occur due to various events and incidents, e.g., concerts, matches, and even road blocks by traffic accidents. To fully unleash the power of RL, practical mechanisms for real-time on-policy updates of the value function (e.g., <ref type="bibr" target="#b107">(Tang et al. 2021</ref><ref type="bibr">, Tong et al. 2021</ref><ref type="bibr" target="#b28">, Eshkevari et al. 2022</ref>)) is required. In view of the low risk tolerance of production systems in general, sample complexity, computational complexity, and robustness are the key challenges that such methods have to address.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Business Strategies</head><p>The research problems in the ridesharing domain are closely associated with how the ridesharing platforms run the operations. Innovation in product and business operations will continue to raise new challenging research problems. There can be multiple alternative product forms to achieve the same goals or address the same challenges, and they inherently define different optimization problems that RL can help tackle. Surge pricing, for example, is a pricing strategy during the peak hours to address the severe shortage of supply with respect to the surging demand. We have explained its motivation in Section 4.1. While surge pricing is a common practice nowadays, it is not the only strategy that the ridesharing platforms adopt. Passenger requests can be queued if there are no vacant vehicles around to immediately serve the requests <ref type="bibr" target="#b147">(Zhong et al. 2020)</ref>. The queuing mechanism is perceived in some markets as a more socially acceptable mechanism during the peak hours than surge pricing. Several operational decision questions immediately come up, e.g., how large an area each queue should cover, if the coverage should be dynamically updated, and when the incoming requests should start queuing. These potentially time-varying decisions in a highly stochastic environment are good candidates to be solved for by RL.</p><p>Ridesharing platforms often use incentives to stimulate growth on both sides of the marketplace. The forms of incentives are diverse and ever evolving: rider coupons, discounts, target-based challenges, driver bonuses with spatial and temporal constraints, etc. Each incentive strategy changes the behaviors of a certain segment of the marketplace participants in a certain way, and they inevitably interact with the other marketplace levers, e.g., dynamic pricing <ref type="bibr" target="#b136">(Yang, Shao, Wang &amp; Ye 2020)</ref>.</p><p>The collective effects of the evolving incentives convolute the environment and dynamics of the marketplace, posing significant challenges to RL and other optimization methods. How to represent and capture these factors or explicitly model them in joint optimization is key to tackle these challenges.</p><p>Third-party service integrator allows passengers to simultaneously request orders from multiple ridehailing platforms <ref type="bibr" target="#b153">(Zhou et al. 2022)</ref>. Service integrators offer the platforms more access to the demand but also bring competition more explicit by displaying the matching information (e.g., trip fare, pickup distance) side by side. Optimizing pricing and matching policies in a competitive environment with feedback from the service integrator on the competition landscape will be interestingly different from those without a service integrator or in a non-competitive environment. With the added environment complexity, these problems are challenging to solve by traditional methods and could be better tackled by RL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">General RL</head><p>RL provides the necessary tools for the methods reviewed in this survey. Hence, the problems of RL for ridesharing tie closely to the development in RL in general. In the context of ridesharing, we have seen from the literature review above that it is difficult for RL to learn combinatorial actions, e.g., the system matching actions. In the era of deep RL, model interpretability is a long-standing challenge, which hampers investigation of customer experience corner cases. For experience-critical service like ridesharing, policy exploration adds further complication, especially for real-world deployment. In view of these challenges, the future is probably that RL-based and traditional optimization approaches will be complementing each other for a long time. We have seen such combinations in the current literature as <ref type="bibr" target="#b133">(Xu et al. 2018</ref><ref type="bibr">, Qin, Zhu &amp; Ye 2021)</ref> for matching, <ref type="bibr" target="#b16">(Chaudhari et al. 2020a</ref><ref type="bibr" target="#b45">, Jiao et al. 2021)</ref> for repositioning, and <ref type="bibr" target="#b23">(Delarue et al. 2020)</ref> for VRP, that combine RL with combinatorial optimization, mixed-integer programming, and tree search. The breakthroughs of RL that we are seeing in other domains and the continued development of RL methodology for ridesharing certainly make it exciting to anticipate the future landscape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Closing Remarks</head><p>We have surveyed the RL literature for the core problems in ridesharing: pricing, dispatching, repositioning, routing, ride-pooling, and VRP. We have also discussed some open challenges and future opportunities pertinent to this area.</p><p>The ridesharing system is a complex multi-agent system with multiple decision levers. RL offers a powerful modeling vehicle for optimizing this system, but as we have seen from the current literature, challenges remain in tackling complexity in the learning algorithms, the coordination among the agents, and the joint optimization of multiple levers. Along tackling these challenges, we expect that domain knowledge in ridesharing as well as transportation in general will be increasingly instrumental to the successful adoption of RL. As one may have noticed, most of the literature has just appeared in the last four years, and we expect it to continue growing and updating rapidly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The order matching process from a single request's perspective.</figDesc><graphic coords="10,127.80,315.27,356.39,128.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of the (single-agent) taxi routing problem on a hexagon grid system. The vehicle at its origin position A has the option to reposition to one of the neighboring and current cells. The black arrows represent reposition (idle cruising), and in the two scenarios, the vehicle is matched to a trip request at B and C respectively. The orange arrows represent trip moves, and the orange flags are where the episodes terminate (for long-term horizons).</figDesc><graphic coords="14,207.00,92.71,198.00,198.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Illustration of system-level vehicle repositioning. The requests in the orange-circled and green-circled areas appear in the future w.r.t. the time of repositioning. The empty vehicles are existing ones in the orange-circled area. The orange and blue arrows represent potential reposition moves.</figDesc><graphic coords="14,167.40,406.59,277.21,237.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Illustration of the ride-pooling matching process adapted from (Alonso-Mora, Samaranayake, Wallar, Frazzoli &amp; Rus 2017). Stage A shows the state of the current vehicles and requests. Vehicle 1 has one passenger on board with her destination at the top right-hand corner, while vehicle 2 is empty. The feasible combinations of passengers and the vehicles feasible to serve them are determined at stage B. The corresponding assignment graph is set up and solved at stage C. Stage D shows the resulting routes to fulfill all four new requests as well as the existing trip.</figDesc><graphic coords="19,108.00,72.00,396.00,98.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc><ref type="bibr" target="#b1">Alabbasi et al. (2019)</ref>,<ref type="bibr" target="#b35">Haliem et al. (2020)</ref>,<ref type="bibr" target="#b100">Singh et al. (2021)</ref>,<ref type="bibr" target="#b36">Haliem et al. (2021)</ref> all attempt to minimize the SD mismatch, passenger wait time, reposition time, detour delay, and the number of vehicles used. In addition,<ref type="bibr" target="#b35">Haliem et al. (2020)</ref> consider the fleet profit, and Singh et al. (2021) study a more general form of ride-pooling, where a passenger can hop among different vehicles to complete a trip, with each vehicle completing one leg. They further consider the number of hops and the delay due to hopping. the link Similar approach to (Mainali et al. 2008), but incremental update is done by a step of SARSA. The updates are done in real time and value functions are sync-ed at each intersection, which is an independent traffic manageexperienced by the agent on a link offline batch RL: fitted Q-iterations with treebased function approximator (Extreme Randomized Trees). Compared with model-based Qexperienced by the agent on a link tabular Q-learning, global road network clustered into subnetworks by differential evolution-based clustering Top-level network contains only boundary nodes of the original network. Top network policy produces the 'destination' node for a subnetwork. Subnetwork policy provides linklevel guidance to reach its sub-destination. or travel distance on a link Bilevel optimization: Lower level -mean field MARL to solve for dynamic routing for travelers Upper level -Bayesian optimization to optimize controls by city planners SUMO with Manhattan network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,137.70,72.00,336.60,189.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of literature for Pricing.</figDesc><table><row><cell>Paper</cell><cell>Agent</cell><cell></cell><cell>State</cell><cell></cell><cell>Action</cell><cell>Reward</cell><cell></cell><cell>Algorithm</cell><cell>Environment</cell></row><row><cell>Wu et al. (2016)</cell><cell cols="2">global decision-</cell><cell cols="2">current trip price</cell><cell>price</cell><cell>profit</cell><cell></cell><cell>Q-learning</cell><cell>no spatiotempo-</cell></row><row><cell></cell><cell>maker</cell><cell></cell><cell cols="2">(same for all</cell><cell></cell><cell></cell><cell></cell><cell>ral dimensions</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">trips), SD info</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Chen, Jiao, Qin, Tang,</cell><cell cols="2">global decision-</cell><cell cols="2">features of the</cell><cell>discretized price</cell><cell>profit</cell><cell></cell><cell>contextual ban-</cell><cell>ride-hailing sim-</cell></row><row><cell>Li, An, Zhu &amp; Ye</cell><cell>maker</cell><cell></cell><cell cols="2">trip request</cell><cell>change percent-</cell><cell></cell><cell></cell><cell>dits with action</cell><cell>ulator with pric-</cell></row><row><cell>(2019)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>age</cell><cell></cell><cell></cell><cell>values</cell><cell>partly</cell><cell>ing module and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>computed</cell><cell>by</cell><cell>passenger elas-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CVNet</cell><cell>ticity model</cell></row><row><cell>Turan et al. (2020)</cell><cell cols="2">global decision-</cell><cell cols="2">electricity price</cell><cell>price for each</cell><cell>trip</cell><cell cols="2">revenue</cell><cell>PPO</cell><cell>simulator</cell></row><row><cell></cell><cell>maker</cell><cell>for</cell><cell cols="2">in each zone,</cell><cell>OD pair, repo-</cell><cell cols="3">-penalty for</cell></row><row><cell></cell><cell cols="2">pricing and EV</cell><cell cols="2">passenger queue</cell><cell>sition/ charging</cell><cell cols="3">queues -opera-</cell></row><row><cell></cell><cell>charging</cell><cell></cell><cell cols="2">length for each</cell><cell>for each vehicle</cell><cell cols="3">tional cost for</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">OD pair, number</cell><cell></cell><cell cols="2">charging</cell><cell>and</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">of vehicles in</cell><cell></cell><cell cols="2">reposition</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">each zone and</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>their</cell><cell>energy</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>levels</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Song et al. (2020)</cell><cell cols="2">global decision-</cell><cell cols="2">location, time</cell><cell>price for spatial-</cell><cell cols="3">trip price minus</cell><cell>Q-learning</cell><cell>case study: ride-</cell></row><row><cell></cell><cell>maker</cell><cell></cell><cell></cell><cell></cell><cell>temporal grid</cell><cell>penalty</cell><cell></cell><cell>for</cell><cell>hailing simula-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>cells</cell><cell cols="3">driver waiting</cell><cell>tion of Seoul</cell></row><row><cell>Mazumdar et al.</cell><cell>passenger</cell><cell></cell><cell cols="2">price multiplier,</cell><cell>wait, take cur-</cell><cell cols="3">trip price to pay</cell><cell>risk-sensitive in-</cell><cell>historical data</cell></row><row><cell>(2017)</cell><cell></cell><cell></cell><cell cols="2">time, if a ride has</cell><cell>rent ride</cell><cell></cell><cell></cell><cell>verse RL</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">completed</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Chen et al. (2021)</cell><cell cols="2">global decision-</cell><cell cols="2">number of open</cell><cell>joint actions of</cell><cell cols="3">profit: revenue</cell><cell>PPO</cell><cell>simulation based</cell></row><row><cell></cell><cell>maker</cell><cell></cell><cell cols="2">requests, vacant</cell><cell>price (per-km for</cell><cell cols="2">minus wage</cell><cell>on Hangzhou</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">vehicles, and oc-</cell><cell>excess mileage)</cell><cell></cell><cell></cell><cell>data from DiDi;</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">cupied vehicles</cell><cell>and wage (per-</cell><cell></cell><cell></cell><cell>modeling</cell><cell>on</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">in each grid cell</cell><cell>km rate) for each</cell><cell></cell><cell></cell><cell>both</cell><cell>supply</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">at time t, and de-</cell><cell>grid cell</cell><cell></cell><cell></cell><cell>and</cell><cell>demand</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">mand in time t −</cell><cell></cell><cell></cell><cell></cell><cell>elasticity</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Summary of literature for Online Matching.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc><ref type="bibr" target="#b97">Shou et al. (2020)</ref> have a boolean in the state to indicate if the driver is assigned to consecutive requests since its setting allows a driver to be matched before completing a trip request.<ref type="bibr" target="#b86">Rong et al. (2016)</ref>,<ref type="bibr" target="#b152">Zhou et al. (2018)</ref> have the direction from which the driver arrives at the current location. For deep RL-based approaches<ref type="bibr" target="#b127">(Wen et al. 2017</ref><ref type="bibr" target="#b45">, Jiao et al. 2021)</ref>, richer contextual information, such as SD distributions in the neighborhood, can go into the state.</figDesc><table /><note>The learning algorithms are fairly diverse but are all value-based. By estimating the various parameters (e.g., matching probability, passenger destination probability) to compute the transition probabilities,<ref type="bibr" target="#b86">Rong et al. (2016)</ref>,Yu et al. (2019),<ref type="bibr" target="#b97">Shou et al. (2020)</ref>,<ref type="bibr" target="#b152">Zhou et al. (2018)</ref> adopt a model-based approach and use value iterations to solve the MDP.<ref type="bibr" target="#b97">Shou et al. (2020)</ref> further use inverse RL to learn the unit-distance operational cost. Model-free methods are also common, e.g., Monte Carlo learning<ref type="bibr" target="#b118">(Verma et al. 2017</ref>), Q-learning(Han et al. 2016, Gao et al. 2018), and DQN  </note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc><ref type="bibr" target="#b58">Lin et al. (2018)</ref> develop contextual DQN and AC methods, in which coordination is achieved by masking the action space based on the state context and splitting the reward accrued in a grid cell among the multiple agents within the same cell.<ref type="bibr" target="#b76">Oda &amp; Joe-Wong (2018)</ref> treat the global state in grid as image input and develop an independent DQN method. They argue that independent learning, equipped with global state information, works quite well compared to an MPC-based approach. The zone structure in<ref type="bibr" target="#b60">(Liu et al. 2020</ref>) is constructed by clustering a road-connectivity graph. A single vehicle agent is trained with contextual deep RL and generates sequential actions for the vehicles.<ref type="bibr" target="#b145">Zhang, Wang, Li &amp; Xu (2020)</ref> also train a single DQN agent for all agents, but with global KL distance between the SD distributions similar to<ref type="bibr" target="#b150">(Zhou et al. 2019)</ref>. The DQN agent is put in tandem with QRewriter, another agent with a Q-table value function that converts the output of DQN to an improved action.<ref type="bibr" target="#b96">Shou &amp; Di (2020b)</ref> approach the MARL problem with bilevel optimization: The bottom level is a mean-field AC method<ref type="bibr" target="#b56">(Li et al. 2019)</ref> with the reward function coming from a platform reward design mechanism, which is tuned by the top level Bayesian optimization. Agent coordination is done by a central module in</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Summary of literature for Vehicle Repositioning (taxi routing).</figDesc><table><row><cell>Data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>taxi log data from</cell><cell>Singapore</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Beijing taxi data</cell><cell>2013</cell><cell></cell><cell></cell><cell></cell><cell>a year of taxi data</cell><cell>from a major city</cell><cell>in China</cell><cell>Shanghai taxi tra-</cell><cell>jectory data in</cell><cell>the morning of a</cell><cell>weekday</cell><cell>Beijing ride-</cell><cell>hailing trajectory</cell><cell>data from DiDi</cell><cell>over three week-</cell><cell>days</cell><cell>DiDi ride-hailing</cell><cell>data</cell></row><row><cell>Coordination</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>Sequentially make de-</cell><cell>cision for each driver.</cell><cell>Adjust the matching</cell><cell>prob of each driver's</cell><cell>MDP, and solve again.</cell><cell>-</cell></row><row><cell>Algorithm</cell><cell></cell><cell>model-based, VI to solve</cell><cell>MDP</cell><cell></cell><cell>Q-learning</cell><cell></cell><cell></cell><cell></cell><cell>MC learning</cell><cell></cell><cell>DQN with greedy action</cell><cell>Compared with MINLP and</cell><cell>SAR (simple anticipatory re-</cell><cell>balancing) on avg passenger</cell><cell>wait time.</cell><cell>MAB + MCTS</cell><cell></cell><cell>Q-learning</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>model-based, VI to solve</cell><cell>MDP</cell><cell>model-based, VI to solve</cell><cell>MDP; use parallel matrix op-</cell><cell>erations to accelerate compu-</cell><cell>tation</cell><cell>model-based, VI to solve</cell><cell>MDP; inverse RL to learn</cell><cell>unit distance op cost</cell><cell>offline CVNet + decision-</cell><cell>time action search</cell></row><row><cell>Episode</cell><cell></cell><cell>long-term</cell><cell></cell><cell></cell><cell>from idle to com-</cell><cell>pletion of next</cell><cell>trip(s) and being</cell><cell>idle again</cell><cell>up to the next</cell><cell>match</cell><cell>up to the next</cell><cell>match</cell><cell></cell><cell></cell><cell></cell><cell>up to the next</cell><cell>match</cell><cell>long-term (day)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>long-term</cell><cell>long-term (day)</cell><cell>long-term (day)</cell><cell>long-term</cell></row><row><cell>Reward</cell><cell></cell><cell>trip fare</cell><cell></cell><cell></cell><cell>trip fare -reposition cost</cell><cell></cell><cell></cell><cell></cell><cell>trip fare -reposition cost</cell><cell></cell><cell>saved idle time compared to</cell><cell>a counterfactual simulation</cell><cell>without reposition. Penal-</cell><cell>ized if no match after reposi-</cell><cell>tion.</cell><cell>idle cruising distance till the</cell><cell>next passenger</cell><cell>trip mileage/idle cruising</cell><cell>mileage</cell><cell>Problem objective: effec-</cell><cell>tive driving ratio = total trip</cell><cell>mileage / total idle cruising</cell><cell>mileage</cell><cell>trip fare</cell><cell>trip fare -operational cost</cell><cell>trip fare -operational cost</cell><cell>operational cost per unit dis-</cell><cell>tance learned through IRL</cell><cell>trip fare -reposition cost</cell></row><row><cell>Action</cell><cell></cell><cell>neighboring cells in a grid</cell><cell>system</cell><cell></cell><cell>neighboring cells in a grid</cell><cell>system</cell><cell></cell><cell></cell><cell>neighboring cells in a grid</cell><cell>system</cell><cell>neighboring cells in a grid</cell><cell>system</cell><cell></cell><cell></cell><cell></cell><cell>adjacent node or edge</cell><cell></cell><cell>idle cruise to a neighboring</cell><cell>cell in the grid system, carry-</cell><cell>ing passenger to destination,</cell><cell>waiting</cell><cell></cell><cell>adjacent node or edge</cell><cell>adjacent node or edge</cell><cell>neighboring cells in a grid</cell><cell>system</cell><cell>neighboring cells in a grid</cell><cell>system</cell></row><row><cell>Agent State (in addition to ST</cell><cell>info)</cell><cell>driver direction from which</cell><cell>driver arrives at the cur-</cell><cell>rent location</cell><cell>driver -</cell><cell></cell><cell></cell><cell></cell><cell>driver -</cell><cell></cell><cell>driver SD contextual info</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>driver current node on road</cell><cell>network</cell><cell>driver location, occupied,</cell><cell>parking, vacant</cell><cell></cell><cell></cell><cell></cell><cell>driver current road segment,</cell><cell>time, previous road seg-</cell><cell>ment</cell><cell>driver current node on road</cell><cell>network</cell><cell>driver boolean: whether or</cell><cell>not assigned to consec-</cell><cell>utive requests</cell><cell>vehicle SD contextual info in</cell><cell>the current cell</cell></row><row><cell>Type</cell><cell></cell><cell>taxi</cell><cell></cell><cell></cell><cell>taxi</cell><cell></cell><cell></cell><cell></cell><cell>taxi</cell><cell></cell><cell>taxi</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>taxi</cell><cell></cell><cell>taxi</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>taxi</cell><cell>taxi</cell><cell>taxi,</cell><cell>sys-</cell><cell>tem</cell><cell>taxi</cell></row><row><cell>Paper</cell><cell></cell><cell>Rong et al. (2016)</cell><cell></cell><cell></cell><cell>Han et al. (2016)</cell><cell></cell><cell></cell><cell></cell><cell>Verma et al. (2017)</cell><cell></cell><cell>Wen et al. (2017)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Garg &amp; Ranu (2018)</cell><cell></cell><cell>Gao et al. (2018)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Zhou et al. (2018)</cell><cell>Yu &amp; Shen (2019)</cell><cell>Shou et al. (2020)</cell><cell>Jiao et al. (2021)</cell></row></table><note>These are real-time</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Summary of literature for Vehicle Repositioning (system reposition).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>and have a flag indicating pooled trips offered by Uber Pool and Lyft Line. The pick-up and drop-off locations are represented by taxi zones. Manhattan, for example, is divided into 64 zones. There is no driver ID associated with the trip records, so reconstructing historical driver-based trajectories is not possible. An older version of the NYC data set<ref type="bibr" target="#b25">(Donovan &amp; Work 2016)</ref>, however, does include GPS coordinates for pick-up and drop-off locations, and car IDs can be used to track drivers within each year, allowing for more granular and diverse analyses. A similar subset of the NYC FHV data is also available at<ref type="bibr" target="#b49">(Kaggle 2017)</ref>, with GPS</figDesc><table><row><cell>Problem Size</cell><cell>Single-vehicle Capaci-</cell><cell>tated VRP with split de-</cell><cell>livery: one active vehi-</cell><cell>cle at a time</cell><cell>TSP and Capacitated</cell><cell>VRP with split delivery,</cell><cell>100 nodes</cell><cell></cell><cell></cell><cell></cell><cell>stochastic and dynamic</cell><cell>CVRP with pick-</cell><cell>up/delivery and time</cell><cell>windows, 8 x 8 map,</cell><cell>5 orders 3 pick-up</cell><cell>locations</cell><cell>multi-vehicle dynamic</cell><cell>VRP with pick-up and</cell><cell>delivery for EVs: 200</cell><cell>random requests, 100</cell><cell>vehicles</cell><cell></cell><cell>Multi-vehicle VRP</cell><cell>with soft time windows</cell><cell>(no split delivery): 150</cell><cell>nodes, 5 vehicles</cell><cell></cell><cell>SDVRP with pick-up</cell><cell>and delivery (DDARP)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Multi-vehicle dy-</cell><cell>namic VRP with</cell><cell>pick-up/delivery and</cell><cell>delivery windows: 48</cell><cell>nodes, 2 vehicles, avg</cell><cell>22 orders/day</cell><cell>CVRP: 51 nodes</cell><cell></cell><cell></cell><cell></cell><cell>CVRP: 400 nodes</cell><cell>EV with time window</cell><cell>and charging. Within</cell><cell>the planning horizon, a</cell><cell>vehicle can visit the de-</cell><cell>pot only once: C100,</cell><cell>S12, EV12</cell></row><row><cell>Algorithm</cell><cell>AC</cell><cell></cell><cell></cell><cell></cell><cell>REINFORCE with</cell><cell>greedy rollout baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>APE-X DQN (Horgan</cell><cell>et al. 2018), a variant of</cell><cell>a DQN that utilizes dis-</cell><cell>tributed prioritized ex-</cell><cell>perience replay</cell><cell></cell><cell>A3C</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>same as (Kool et al.</cell><cell>2018)</cell><cell></cell><cell></cell><cell></cell><cell>insert new request s</cell><cell>into the current route</cell><cell>and use variable neigh-</cell><cell>borhood search to im-</cell><cell>prove the route</cell><cell></cell><cell></cell><cell>NN-based TD learning</cell><cell>with experience replay</cell><cell>(like in (Tang et al.</cell><cell>2019)) to learn the</cell><cell>action-value function</cell><cell></cell><cell>MC policy iteration:</cell><cell>rollout N trajectories,</cell><cell>fit a new NN</cell><cell></cell><cell>REINFORCE with</cell><cell>greedy rollout baseline</cell><cell>(Kool et al. 2018) to</cell><cell>train the policy net-</cell><cell>work; Cross-entropy</cell><cell>loss to train the binary</cell><cell>classifier of route</cell><cell>edges with policy</cell><cell>output as labels</cell><cell>REINFORCE with</cell><cell>greedy rollout baseline</cell><cell>(Kool et al. 2018)</cell></row><row><cell>Network</cell><cell>Non-sequential encoder for the input with</cell><cell>RNN decoder that attends over over the input</cell><cell>space (Pointer network without an RNN en-</cell><cell>coder)</cell><cell>transformer encoder with input of coordinates</cell><cell>and demand of each node + self-attention-</cell><cell>based decoder with additional input of remain-</cell><cell>ing demands and capacity at time t</cell><cell></cell><cell></cell><cell>two dense layers NN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>pointer network for actor, another critic net-</cell><cell>work; Network architecture is similar to</cell><cell>NeurIPS-18 paper, but with structural graph</cell><cell>embedding (Struct2Vec) for the encoder.</cell><cell></cell><cell></cell><cell>same as (Kool et al. 2018)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>N.A.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>not specified</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Value network consists of dense layers + ReLU</cell><cell>activation (representable by mixed-integer lin-</cell><cell>ear constraints)</cell><cell></cell><cell>GCN-based encoder with both node and edge</cell><cell>features; GRU-based decoder similar to the</cell><cell>pointer network as policy network and MLP-</cell><cell>based decoder on the edge encoding as classi-</cell><cell>fier</cell><cell>Encoder with 1D conv layer and graph embed-</cell><cell>ding for the nodes and attention layer; LSTM-</cell><cell>based decoder</cell></row><row><cell>Reward</cell><cell>negative travel distance</cell><cell></cell><cell></cell><cell></cell><cell>negative travel distance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>value of delivered or-</cell><cell>der (accept, pickup, de-</cell><cell>liver) -cost (waiting,</cell><cell>traveling, penalty)</cell><cell></cell><cell></cell><cell>expected objective</cell><cell>value for a tour</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>negative travel distance</cell><cell>+ negative constraint vi-</cell><cell>olation penalty</cell><cell></cell><cell></cell><cell>difference in route</cell><cell>value between the old</cell><cell>route plan and the new</cell><cell>one</cell><cell></cell><cell></cell><cell></cell><cell>cost diff between two</cell><cell>consecutive decisions</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>negative tour distance</cell><cell></cell><cell></cell><cell></cell><cell>negative travel distance</cell><cell>negative travel distance</cell><cell>+ negative penalties for</cell><cell>constraint violations</cell></row><row><cell>Action</cell><cell>the next request to visit</cell><cell></cell><cell></cell><cell></cell><cell>next stop for a given ve-</cell><cell>hicle</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>accept an order, pick</cell><cell>up an accepted order,</cell><cell>wait</cell><cell></cell><cell></cell><cell></cell><cell>next stop for a given ve-</cell><cell>hicle; The vehicles gen-</cell><cell>erate actions sequen-</cell><cell>tially at each time step.</cell><cell></cell><cell></cell><cell>same as (Kool et al.</cell><cell>2018); The vehicles</cell><cell>generate actions se-</cell><cell>quentially at each time</cell><cell>step.</cell><cell>the next stop to visit</cell><cell>and the new route plan;</cell><cell>The paper defines a</cell><cell>new variant of MDP</cell><cell>called route-based</cell><cell>MDP.</cell><cell></cell><cell>matching a new order</cell><cell>to a vehicle. Rerouting</cell><cell>after matching is done</cell><cell>by simulated annealing</cell><cell>for VRP</cell><cell></cell><cell>to generate one route</cell><cell>(tour) through solving</cell><cell>a Prize Collecting TSP</cell><cell>by MIP</cell><cell>generate one node at a</cell><cell>time sequentially; The</cell><cell>resulting sequence may</cell><cell>have multiple depot oc-</cell><cell>currences for different</cell><cell>tours.</cell><cell>Next stop for the</cell><cell>current route; Unlike</cell><cell>(James et al. 2019) the</cell><cell>routes of the vehicles</cell><cell>are generated sequen-</cell><cell>tially. Every time the</cell><cell>depot appears in the</cell><cell>sequence, the system</cell><cell>time is reset to 0.</cell></row><row><cell>State</cell><cell>location and demand of</cell><cell>each request</cell><cell></cell><cell></cell><cell>coordinates and orig-</cell><cell>inal demand of each</cell><cell>node, remaining de-</cell><cell>mand of each node, re-</cell><cell>maining capacity of the</cell><cell>vehicle</cell><cell>current pickup loca-</cell><cell>tion, vehicle's location</cell><cell>and remaining capac-</cell><cell>ity, orders' locations,</cell><cell>statuses, waiting times,</cell><cell>and values</cell><cell>system state (available</cell><cell>requests, charging sta-</cell><cell>tion output, vehicles'</cell><cell>status(location, battery</cell><cell>levels, next stops)), ve-</cell><cell>hicle tour graph</cell><cell>same as (Kool et al.</cell><cell>2018)</cell><cell></cell><cell></cell><cell></cell><cell>vehicle location, time,</cell><cell>num of passengers</cell><cell>onboard, info of</cell><cell>in-process and out-</cell><cell>standing requests,</cell><cell>route plan from last</cell><cell>epoch</cell><cell>includes the cost for</cell><cell>the remaining route for</cell><cell>each vehicle</cell><cell></cell><cell></cell><cell></cell><cell>the remaining unvisited</cell><cell>nodes</cell><cell></cell><cell></cell><cell>nodes (location, de-</cell><cell>mand), edges (distance,</cell><cell>adjacency)</cell><cell>For time t, the state</cell><cell>of each vertex (loca-</cell><cell>tion, time window, re-</cell><cell>maining demand), and</cell><cell>global variables (time,</cell><cell>battery level of the ac-</cell><cell>tive vehicle, number of</cell><cell>EVs not in charging)</cell></row><row><cell>Type</cell><cell>single-vehicle</cell><cell></cell><cell></cell><cell></cell><cell>single-vehicle</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>single-vehicle</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>multi-vehicle</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>multi-vehicle</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>single-vehicle</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>multi-vehicle</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>single-vehicle</cell><cell></cell><cell></cell><cell></cell><cell>single-vehicle</cell><cell>multi-vehicle</cell></row><row><cell>Paper</cell><cell>Nazari et al. (2018)</cell><cell></cell><cell></cell><cell></cell><cell>Kool et al. (2018)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Balaji et al. (2019)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>James et al. (2019)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Zhang, He, Zhang, Lin</cell><cell>&amp; Li (2020)</cell><cell></cell><cell></cell><cell></cell><cell>Ulmer et al. (2020)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Joe &amp; Lau (2020)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Delarue et al. (2020)</cell><cell></cell><cell></cell><cell></cell><cell>Duan et al. (2020)</cell><cell>Lin et al. (2021)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Summary of literature for VRP.coordinates for pick-up and drop-off locations. In addition, travel time data between OD pairs can be obtained through Uber Movement (Uber 2021).Another taxi data set is the San Francisco data set<ref type="bibr" target="#b78">(Piorkowski et al. 2009)</ref>, which contains GPS coordinates of approximately 500 taxis collected over 30 days in the San Francisco Bay Area in May 2008. The average time interval between two consecutive location updates is less than 10s.A more recent rideshare (Transportation Network Providers, TNPs) data set is published by Chicago Data Portal<ref type="bibr" target="#b79">(Portal 2020)</ref>. This data set contains trips, drivers, and vehicles data reported by Transportation Network Providers (TNP, or rideshare companies) in Chicago from 2018. Trip origin and destinations are represented by census tracts. Times are rounded to the nearest 15 minutes. Fares are rounded to the nearest $2.50 and tips are rounded to the nearest $1.00. The driver and vehicle data are not joinable with the trip data.Developing ridesharing simulators has been a line of research itself.<ref type="bibr" target="#b138">Yao &amp; Bekhor (2021)</ref> offer a comprehensive review of recent works on ridesharing simulation models, most of them covering a subset of considerations on the number of the pre-/post-match passenger cancellation behaviors, and driver acceptance/rejection behaviors. In<ref type="bibr" target="#b138">(Yao &amp; Bekhor 2021)</ref>, a sophisticated eventbased simulation framework is proposed to capture all aspects of the behavior modeling. Although the 'ridesharing' in their paper is known as the hitch service, where the driver is on her own trip as well, the modeling framework is general and accommodates the ridesharing setting in this survey.<ref type="bibr" target="#b16">Chaudhari et al. (2020a)</ref> offer a Gym-compatible, open-source ride-hailing environment<ref type="bibr" target="#b17">(Chaudhari et al. 2020b)</ref> for training dispatching and repositioning agents. For large-scale simulation on transport networks, AMoDeus<ref type="bibr" target="#b87">(Ruch et al. 2018) and</ref> MATSim (W Axhausen et al. 2016) are well-established Java-based simulation frameworks that also come with graphical user interfaces and visualization tools. They are of more sophisticated engineering architectures albeit with higher programming bars for extension. The evaluation simulation environment for the KDD Cup 2020 competition is available for public access through the DiDi decision intelligence simulation platform<ref type="bibr" target="#b24">(DiDi 2021</ref>).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In this survey, we do not cover topics on hitch, in which the driver is on his/her own trip with a specific destination.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">This method can be regarded as one for the matching problem in ride-pooling described in Section 4.5.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7"><ref type="bibr" target="#b33">Guériau et al. (2020)</ref> evaluate the ridesharing algorithms in SUMO, but the environment is not public. 8 A partial exception is an autonomous ridesharing platform, where the supply side is powered by autonomous vehicles. However, such services are still prototypical at the time of writing and have very limited coverage.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Approximate dynamic programming for planning a ride-hailing system using autonomous fleets of electric vehicles</title>
		<author>
			<persName><forename type="first">L</forename><surname>Al-Kanj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">284</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1088" to="1106" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeppool: Distributed model-free algorithm for ride-sharing using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alabbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4714" to="4727" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On-demand highcapacity ride-sharing via dynamic trip-vehicle assignment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Alonso-Mora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samaranayake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wallar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frazzoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="462" to="467" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predictive routing for autonomous mobility-on-demand systems with ride-sharing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Alonso-Mora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wallar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)&apos;, IEEE</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3583" to="3590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Uber versus taxi: A driver&apos;s eye view</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Angrist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Caldwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Economic Journal: Applied Economics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="272" to="308" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey of inverse reinforcement learning: Challenges, methods and progress</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page">103500</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Orl: Reinforcement learning benchmarks for online stochastic optimization problems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bell-Masterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bilgin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maggiar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10641</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A hybrid evolutionary and multiagent reinforcement learning approach to accelerate the computation of traffic assignment</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Bazzan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1723" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A multiagent reinforcement learning approach to en-route trip building</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Bazzan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grunitzki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Joint Conference on Neural Networks (IJCNN)&apos;, IEEE</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5288" to="5295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Algorithms for trip-vehicle assignment in ride-sharing</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A business class for autonomous mobility-on-demand: Modeling service quality contracts in dynamic ridesharing systems</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Beirigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Negenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alonso-Mora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schulte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page">103520</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09940</idno>
		<title level="m">Neural combinatorial optimization with reinforcement learning&apos;, arXiv preprint</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Dota 2 with large scale deep reinforcement learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dębiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dennison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Farhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hashme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06680</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Dynamic pricing: A learning approach, in &apos;Mathematical and computational models for congestion charging</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Perakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="45" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial pricing in ride-sharing networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bimpikis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Candogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="744" to="769" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The Ride-Hail Utopia That Got Stuck in Traffic -WSJ</title>
		<author>
			<persName><forename type="first">E</forename><surname>Brown</surname></persName>
		</author>
		<ptr target="https://www.wsj.com/articles/the-ride-hail-utopia-that-got-stuck-in-traffic-11581742802" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learn to earn: Enabling coordination within a ride hailing fleet</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Byers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Terzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Big Data</title>
				<meeting>IEEE International Conference on Big Data</meeting>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Simulation code for &quot;learn to earn: Enabling coordination within a ride-hailing fleet</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Byers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Terzi</surname></persName>
		</author>
		<ptr target="https://transparent-framework.github.io/optimize-ride-sharing-earnings/" />
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatial-temporal pricing for ride-sourcing platform with reinforcement learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page">103272</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inbede: Integrating contextual bandit with td learning for joint pricing and dispatch of ride-hailing platforms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Data Mining (ICDM)&apos;, IEEE</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Dynamic pricing in a labor market: Surge pricing and flexible work on the uber platform</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sheldon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">455</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Can sophisticated dispatching strategy acquired by reinforcement learning?-a case study in dynamic courier dispatching system</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02716</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The truck dispatching problem</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Dantzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ramser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="80" to="91" />
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reinforcement learning with combinatorial actions: An application to vehicle routing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Delarue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tjandraatmadja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="609" to="620" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Didi decision intelligence simulation platform</title>
		<author>
			<persName><forename type="first">Didi</forename></persName>
		</author>
		<ptr target="https://outreach.didichuxing.com/Simulation" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">New york city taxi trip data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Donovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Work</surname></persName>
		</author>
		<idno type="DOI">10.13012/J8PN93H8</idno>
		<ptr target="https://doi.org/10.13012/J8PN93H8" />
		<imprint>
			<date type="published" when="2010">2016. 2010-2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Carla: An open urban driving simulator</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on robot learning&apos;, PMLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficiently solving the practical vehicle routing problem: A novel joint learning approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3054" to="3063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reinforcement learning in the wild: Scalable rl dispatching algorithm deployed in ridehailing marketplace</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Eshkevari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scalable deep reinforcement learning for ride-hailing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gluzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Control Systems Letters</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Optimize taxi driving strategies based on reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Geographical Information Science</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1677" to="1696" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Route recommendations for idle taxi drivers: Find me the shortest route to a customer!</title>
		<author>
			<persName><forename type="first">N</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ranu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1425" to="1434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Individual versus difference rewards on reinforcement learning for route choice</title>
		<author>
			<persName><forename type="first">R</forename><surname>Grunitzki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Oliveira Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L C</forename><surname>Bazzan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="253" to="258" />
		</imprint>
	</monogr>
	<note>Brazilian Conference on Intelligent Systems</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Shared autonomous mobility on demand: A learning-based approach and its performance in the presence of traffic congestion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guériau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cugurullo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Acheampong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dusparic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Transportation Systems Magazine</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="208" to="218" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Samod: Shared autonomous mobility-on-demand using decentralized reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guériau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dusparic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st International Conference on Intelligent Transportation Systems (ITSC)&apos;, IEEE</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1558" to="1563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A distributed model-free ride-sharing algorithm with pricing using deep reinforcement learning, in &apos;Computer Science in Cars Symposium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Haliem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bhargava</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A distributed model-free ride-sharing approach for joint matching, pricing, and dispatching using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Haliem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bhargava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7931" to="7942" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The fastest path through a network with random time-dependent travel times</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation science</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="182" to="188" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Routing an autonomous taxi with reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bressan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
				<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2421" to="2424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for intelligent transportation systems: A survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Haydari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for multi-driver vehicle dispatching and repositioning problem</title>
		<author>
			<persName><forename type="first">J</forename><surname>Holler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vuorio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Data Mining (ICDM)</title>
				<editor>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Shim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp; X</forename><surname>Wu</surname></persName>
		</editor>
		<meeting><address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1090" to="1095" />
		</imprint>
		<respStmt>
			<orgName>Institute of Electrical and Electronics Engineers</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00933</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Distributed prioritized experience replay</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Surge pricing and two-sided temporal responses in ride hailing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Manufacturing &amp; Service Operations Management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="109" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dynamic type matching</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Manufacturing &amp; Service Operations Management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="125" to="142" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Online vehicle routing with neural combinatorial optimization and deep reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3806" to="3817" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Real-world ride-hailing vehicle repositioning using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page">103289</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Coride: Joint order dispatching and fleet management for multi-scale ride-hailing platforms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1983" to="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Optimizing taxi carpool policies via reinforcement learning and spatio-temporal mining</title>
		<author>
			<persName><forename type="first">I</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nokleby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Big Data (Big Data)&apos;, IEEE</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1417" to="1426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning approach to solve dynamic vehicle routing problem with stochastic customers</title>
		<author>
			<persName><forename type="first">W</forename><surname>Joe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Automated Planning and Scheduling</title>
				<meeting>the International Conference on Automated Planning and Scheduling</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="394" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Uber pickups in new york city -trip data for over 20 million uber (and other for-hire vehicle) trips in nyc</title>
		<author>
			<persName><surname>Kaggle</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/fivethirtyeight/uber-pickups-in-new-york-city" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pricing and equilibrium in on-demand ride-pooling markets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part B: Methodological</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="411" to="431" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning to delay in ride-sourcing systems: a multi-agent deep reinforcement learning framework</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Optimal vehicle routing with real-time traffic information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="178" to="188" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Attention, learn to solve routing problems!</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hoof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08475</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dynamic ride-hailing with electric vehicles</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Kullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cousineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Goodson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Mendoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Science</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="775" to="794" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A contextual-bandit approach to personalized news article recommendation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
				<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Efficient ridesharing order dispatching with mean field multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 World Wide Web Conference on World Wide Web&apos;, International World Wide Web Conferences Steering Committee</title>
				<meeting>the 2019 World Wide Web Conference on World Wide Web&apos;, International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>To appear in</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for the electric vehicle routing problem with time windows</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghaddar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nathwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Efficient large-scale fleet management via multi-agent deep reinforcement learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1774" to="1783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Self-improving reactive agents based on reinforcement learning, planning and teaching</title>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="293" to="321" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Context-aware taxi dispatching at city-scale using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Microscopic traffic simulation using sumo</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Behrisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bieker-Walz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Erdmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-P</forename><surname>Flötteröd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hilbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lücken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rummel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 21st IEEE International Conference on Intelligent Transportation Systems</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Online spatio-temporal matching in stochastic and dynamic domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lowalekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Varakantham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jaillet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">261</biblScope>
			<biblScope unit="page" from="71" to="112" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Multi-agent actor-critic for mixed cooperative-competitive environments&apos;, Advances in neural information processing systems 30</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">I</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pieter Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Multi-objective online ride-matching</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-P</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Available at SSRN 3356823</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Spatio-temporal pricing for ridesharing platforms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Parkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGecom Exchanges</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="53" to="57" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Optimal route based on dynamic programming for road networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Mainali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mabu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hirasawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Advanced Computational Intelligence and Intelligent Informatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="546" to="553" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Dispatch of autonomous vehicles for taxi services: A deep reinforcement learning approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-J</forename><forename type="middle">M</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">102626</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A reinforcement learning framework for the adaptive routing problem in stochastic time-dependent network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="179" to="197" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Ride Sharing Market by Type (E-hailing, Station-Based, Car Sharing &amp; Rental), Car Sharing (P2P, Corporate)</title>
		<author>
			<persName><surname>Marketsandmarkets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Micro-Mobility (Bicycle, Scooter), Vehicle Type, and Region -Global Forecast to 2025</title>
				<meeting><address><addrLine>Service (Navigation, Payment, Information</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Gradient-based inverse risk-sensitive reinforcement learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mazumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Ratliff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fiez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 56th Annual Conference on Decision and Control (CDC)&apos;, IEEE</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5796" to="5801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Curriculum in gradient-based meta-reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Raparthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Paull</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07956</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Reinforcement learning for solving the vehicle routing problem</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nazari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oroojlooy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takác</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9839" to="9849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Algorithms for inverse reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Movi: A model-free approach to dynamic fleet management</title>
		<author>
			<persName><forename type="first">T</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Joe-Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM 2018-IEEE Conference on Computer Communications</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2708" to="2716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Dynamic matching for real-time ride sharing</title>
		<author>
			<persName><forename type="first">E</forename><surname>Özkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stochastic Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="70" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">CRAWDAD dataset epfl/mobility (v</title>
		<author>
			<persName><forename type="first">M</forename><surname>Piorkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sarafijanovic-Djukic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grossglauser</surname></persName>
		</author>
		<ptr target="https://crawdad.org/epfl/mobility/20090224" />
		<imprint>
			<date type="published" when="2009-02-24">2009. 2009-02-24</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Chicago transportation network providers (rideshare) data&apos;</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Portal</surname></persName>
		</author>
		<ptr target="https://data.cityofchicago.org" />
	</analytic>
	<monogr>
		<title level="m">/Transportation/Transportation-Network-Providers-Trips/ m6dm-c72p</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Approximate Dynamic Programming: Solving the curses of dimensionality</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Powell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">703</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Optimizing matching time intervals for ride-hailing services using reinforcement learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page">103239</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Ride-hailing order dispatching at didi via reinforcement learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INFORMS Journal on Applied Analytics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="272" to="286" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Reinforcement learning for ridesharing: A survey</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">&apos;appearing in Proceedings of the IEEE Intelligent Transportation Systems Conference</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Reinforcement learning applications in dynamic pricing of retail markets</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Narahari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ravikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on E-Commerce</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003. 2003. 2003</date>
			<biblScope unit="page" from="339" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Analysing the impact of travel information for minimising the regret of route choice</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D O</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Bazzan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Da Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="257" to="271" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">The rich and the poor: A markov decision process approach to optimizing taxi driver revenue efficiency</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shafiq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
				<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2329" to="2334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Amodeus, a simulation-based testbed for autonomous mobility-on-demand systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ruch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hörl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frazzoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st International Conference on Intelligent Transportation Systems (ITSC)&apos;, IEEE</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="3639" to="3644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Semi-markov reinforcement learning for stochastic resource collection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schmoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI)</title>
				<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Neural approximate dynamic programming for on-demand ride-pooling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lowalekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Varakantham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="507" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Partially observable environment estimation with uplift inference for reinforcement learning based recommendation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Environment reconstruction with hidden confounders for reinforcement learning based recommendation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="566" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Auxiliary-task based deep reinforcement learning for participant selection problem in mobile crowdsourcing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
				<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1355" to="1364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Operating electric vehicle fleet for ride-hailing services with reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Ioannou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4822" to="4834" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Multi-agent reinforcement learning for dynamic routing games: A unified paradigm</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10915</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Reward design for driver repositioning using multi-agent reinforcement learning&apos;, Transportation research part C: emerging technologies 119</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020b. 102738</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Optimal passenger-seeking policies on e-hailing platforms using markov decision process and imitation learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hampshire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="91" to="113" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Alphago: Mastering the ancient game of go with machine learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Research Blog 9</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">An approximate dynamic programming algorithm for large-scale fleet management: A case application</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Simao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gifford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nienow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Science</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="178" to="197" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">A distributed model-free algorithm for multi-hop ride-sharing using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Al-Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Here?s how long you have to wait for an Uber or Lyft in DC</title>
		<author>
			<persName><forename type="first">M</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="https://wtop.com/dc-transit/2019/12/how-long-you-have-to-wait-for-an-uber-or-lyft-in-d-c/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">An application of reinforced learningbased dynamic pricing for improvement of ridesharing platform service in seoul</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Hwang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">1818</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Model and analysis of labor supply for ride-sharing platforms in the presence of sample self-selection and endogeneity</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part B: Methodological</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="76" to="93" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Learning to predict by the methods of temporal differences</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="44" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">A deep value-network based approach for multi-driver order dispatching</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
				<meeting>the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1780" to="1790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Value function is all you need: A unified learning framework for ride hailing platforms</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining&apos;, KDD &apos;21</title>
				<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining&apos;, KDD &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3605" to="3615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Nyc taxi &amp; limousine commission trip record data</title>
		<ptr target="https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page" />
	</analytic>
	<monogr>
		<title level="j">TLC</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Combinatorial optimization meets reinforcement learning: Effective taxi order dispatching at large-scale</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">A unified approach to route planning for shared mobility</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the VLDB Endowment</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1633</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Spatial crowdsourcing: a survey</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shahabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="217" to="250" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Continual reinforcement learning deployed in real-life using policy distillation and sim2real transfer</title>
		<author>
			<persName><forename type="first">R</forename><surname>Traoré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Caselles-Dupré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lesort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Díaz-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Filliat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04452</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Aligning social welfare and agent preferences to alleviate traffic congestion</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">T</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agogino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems</title>
				<meeting>the 7th international joint conference on Autonomous agents and multiagent systems<address><addrLine>Citeseer</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="655" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Dynamic pricing and fleet management for electric autonomous mobility on demand systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Turan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pedarsani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page">102829</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Uber movement&apos;</title>
		<author>
			<persName><surname>Uber</surname></persName>
		</author>
		<ptr target="https://movement.uber.com/?lang=en-US" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">On modeling stochastic dynamic vehicle routing problems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Ulmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Goodson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Mattfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURO Journal on Transportation and Logistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">100008</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Learning ride-sourcing drivers&apos; customer-searching behavior: A dynamic discrete choice approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Urata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page">103293</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">Augmenting decisions of taxi drivers through reinforcement learning for improving revenues</title>
		<author>
			<persName><forename type="first">T</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Varakantham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Lau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<title level="m">Pointer networks, in &apos;Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">The multi-agent transport simulation MATSim</title>
		<author>
			<persName><forename type="first">K</forename><surname>W Axhausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Horni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nagel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Ubiquity Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Ridesourcing systems: A framework and review</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part B: Methodological</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="122" to="155" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Adaptive dynamic bipartite graph matching: A reinforcement learning approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 35th International Conference on Data Engineering (ICDE)&apos;, IEEE</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1478" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Sample efficient actor-critic with experience replay</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01224</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with knowledge transfer for online rides order dispatching</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Mining</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Hierarchical sarsa learning based route guidance algorithm</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Advanced Transportation</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Rebalancing shared mobility-on-demand systems: A reinforcement learning approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jaillet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)&apos;, Ieee</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="220" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">A cell-based logit-opportunity taxi customer-search model</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Szeto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="84" to="96" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title level="m" type="main">Flow: Architecture and benchmarking for reinforcement learning in traffic control</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kreidieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Parvate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vinitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bayen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05465</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">Automated pricing agents in the on-demand economy</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<pubPlace>Berkeley, CA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of California at Berkeley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">An efficient insertion operator in dynamic ridesharing services</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Largescale order dispatch in on-demand ride-hailing platforms: A learning and planning approach</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="905" to="913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Dynamic pricing and matching in ride-hailing platforms</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Korolko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Woodard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics (NRL)</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="705" to="724" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Optimizing matching time interval and matching radius in on-demand ride-sourcing markets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part B: Methodological</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page" from="84" to="105" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Integrated reward scheme and surge pricing in a ridesourcing market</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part B: Methodological</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="126" to="142" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Mean field multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5571" to="5580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title level="m" type="main">A ridesharing simulation platform that considers dynamic supplydemand interactions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bekhor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13463</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">A survey on reinforcement learning models and algorithms for traffic signal control</title>
		<author>
			<persName><forename type="first">K.-L</forename><forename type="middle">A</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Khoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Komisarczuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Q value-based dynamic programming with sarsa learning for real time route guidance in large scale road networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mabu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hirasawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2012 International Joint Conference on Neural Networks (IJCNN)&apos;, IEEE</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">A markov decision process approach to vacant taxi routing with e-hailing</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part B: Methodological</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="114" to="134" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">An integrated decomposition and approximate dynamic programming approach for on-demand ride pooling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3811" to="3820" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Beyond shortest paths: Route recommendations for ride-sharing</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ranu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bagchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2258" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Multi-vehicle routing problems with soft time windows: A multi-agent reinforcement learning approach</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page">102861</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Dynamic fleet management with rewriting deep reinforcement learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="143333" to="143341" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Order dispatch in price-aware ridesharing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
				<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="853" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<monogr>
		<title level="m" type="main">Queueing versus surge pricing mechanism: Efficiency, equity, and consumer welfare&apos;, Equity, and Consumer Welfare</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-J</forename><forename type="middle">M</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-09-24">2020. September 24. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">A reinforcement learning scheme for the equilibrium of the in-vehicle route choice problem based on congestion game</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Mathematics and Computation</title>
		<imprint>
			<biblScope unit="volume">371</biblScope>
			<biblScope unit="page">124895</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Multi-objective distributional reinforcement learning for large-scale order dispatching</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Data Mining (ICDM)&apos;, IEEE</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1541" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning for order-dispatching via order-vehicle distribution matching</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2645" to="2653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<monogr>
		<title level="m" type="main">Smarts: Scalable multi-agent reinforcement learning training school for autonomous driving</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Villela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fadakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09776</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Optimizing taxi driver profit efficiency: A spatial network-based markov decision process approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Khezerlou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shafiq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="158" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Competition and third-party platformintegration in ride-sourcing markets</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part B: Methodological</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="76" to="103" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">A mean-field markov decision process model for spatialtemporal subsidies in ride-sourcing markets</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part B: Methodological</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page" from="540" to="565" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Dynamic multiobjective optimization driven by inverse reinforcement learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">575</biblScope>
			<biblScope unit="page" from="468" to="484" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
