<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dissecting Hessian: Understanding Common Structure of Hessian in Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-24">October 24, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yikai</forename><surname>Wu</surname></persName>
							<email>yikai.wu@cs.princeton.edu</email>
						</author>
						<author>
							<persName><forename type="first">Xingyu</forename><surname>Zhu</surname></persName>
							<email>xingyu.zhu@duke.edu</email>
						</author>
						<author>
							<persName><forename type="first">Chenwei</forename><surname>Wu</surname></persName>
							<email>cwwu@cs.duke.edu</email>
						</author>
						<author>
							<persName><forename type="first">Annie</forename><surname>Wang</surname></persName>
							<email>annie.wang029@duke.edu</email>
						</author>
						<author>
							<persName><forename type="first">Rong</forename><surname>Ge</surname></persName>
							<email>rongge@cs.duke.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dissecting Hessian: Understanding Common Structure of Hessian in Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-24">October 24, 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">C464083EC23EA5A2817B0547B4FEFA9C</idno>
					<idno type="arXiv">arXiv:2010.04261v6[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-10-31T05:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hessian captures important properties of the deep neural network loss landscape. Previous works have observed low rank structure in the Hessians of neural networks. In this paper, we propose a decoupling conjecture that decomposes the layer-wise Hessians of a network as the Kronecker product of two smaller matrices. We can analyze the properties of these smaller matrices and prove the structure of top eigenspace random 2-layer networks. The decoupling conjecture has several other interesting implications -top eigenspaces for different models have surprisingly high overlap, and top eigenvectors form low rank matrices when they are reshaped into the same shape as the corresponding weight matrix. All of these can be verified empirically for deeper networks. Finally, we use the structure of layer-wise Hessian to get better explicit generalization bounds for neural networks.</p><p>* Equal contribution, listed in alphabetical order † Work done while Yikai Wu was undergraduate student in Duke University</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The loss landscape for neural networks is crucial for understanding training and generalization. In this paper we focus on the structure of Hessians, which capture important properties of the loss landscape. For optimization, Hessian information is used explicitly in second order algorithms, and even for gradient-based algorithms properties of the Hessian are often leveraged in analysis <ref type="bibr" target="#b40">(Sra et al., 2012)</ref>. For generalization, the Hessian captures the local structure of the loss function near a local minimum, which is believed to be related to generalization gaps <ref type="bibr" target="#b18">(Keskar et al., 2017)</ref>.</p><p>Several previous results including <ref type="bibr" target="#b34">Sagun et al. (2018)</ref>; <ref type="bibr" target="#b28">Papyan (2018)</ref> observed interesting structures in Hessians for neural networks -it often has around c large eigenvalues where c is the number of classes. In this paper we ask:</p><p>Why does the Hessian of neural networks have special structures in its top eigenspace?</p><p>A rigorous analysis of the Hessian structure would potentially allow us to understand what the top eigenspace of the Hessian depends on (e.g., the weight matrices or data distribution), as well as predicting the behavior of the Hessian when the architecture changes.</p><p>Towards this goal, we focus on the layer-wise Hessians in this paper. One difficulty in analyzing the layer-wise Hessian lies in its size -for a fully-connected layer with a n × n weight matrix, the layer-wise Hessian is a nn × nn matrix. We propose a decoupling conjecture that approximates this matrix by the Kronecker product of two smaller matrices -a n × n input autocorrelation matrix and a n × n output Hessian matrix. We then study the properties of these two smaller matrices, which together with the decoupling conjecture give an explanation of why there are just a few large eigenvalues, as well as a heuristic formula to efficiently compute the top eigenspace. We prove the decoupling conjecture and structure of the output Hessian matrix for a simple model of 2-layer network. We then empirically verify that these results extend to much more general settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Outline</head><p>Understanding Hessian Structure using Kronecker Factorization: In Section 3 We first formalize a decoupling conjecture that states the layer-wise Hessian can be approximated by the Kronecker product of the output Hessian and input auto-correlation.</p><p>The auto-correlation of the input is often very close to a rank 1 matrix, because the inputs for most layers have a nonzero expectation. We show that when the input auto-correlation component is approximately rank 1, top eigenspace of the layerwise Hessian is very similar to that of the output Hessian. On the contrary, when inputs have mean 0 (e.g., when the model is trained with batch normalization), the input auto-correlation matrix is much farther from rank 1 and the layer-wise Hessian often does not have the same low rank structure.</p><p>In Section 4 we prove that in an over-parametrized two-layer neural network on random data, the output Hessian is approximately rank c − 1. Further, we can compute the top c − 1 eigenspace directly from weight matrices. We show a similar low rank result for the layer-wise Hessian.</p><p>Implication on the Structure of Top Eigenspace for Hessians: The decoupling conjecture, together with our characterizations of its two components, have surprising implications to the structure of top-eigenspace for layer-wise Hessians. Since the eigenvector of a Kronecker product is just the outer product of eigenvectors of its components, if we express the top eigenvectors of a layer-wise Hessian as a matrix with the same dimensions as the weight matrix, then the matrix is approximately rank 1. In Fig. <ref type="figure">1</ref>.a we show the singular values of several such reshaped eigenvectors. Another more surprising phenomenon considers the overlap between top eigenspaces for different models. Figure <ref type="figure">1</ref>: Some interesting observations on the structure of layer-wise Hessians. The eigenspace overlap is defined in Definition 2.1 and the reshape operation is defined in Definition 2.2</p><p>Consider two neural networks trained with different random initializations and potentially different hyper-parameters; their weights are usually nearly orthogonal. One might expect that the top eigenspace of their layer-wise Hessians are also very different. However, empirically one observe that the top eigenspace of the layer-wise Hessians have a very high overlap, and the overlap peaks at the dimension of the layer's output (see Fig. <ref type="figure">1a</ref>). This is a direct consequence of the Kronecker product and the fact that the input auto-correlation matrix is close to rank 1.</p><p>Applications: As a direct application of our results, in Section 6 we show that the Hessian structure can be used to improve the PAC-Bayes bound computed in <ref type="bibr" target="#b1">Dziugaite &amp; Roy (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related Works</head><p>Hessian-based analysis for neural networks (NNs): Hessian matrices for NNs reflect the second order information about the loss landscape, which is important in characterizing SGD dynamics <ref type="bibr" target="#b14">(Jastrzebski et al., 2019)</ref> and related to generalization <ref type="bibr" target="#b24">(Li et al., 2020)</ref>, robustness to adversaries <ref type="bibr" target="#b43">(Yao et al., 2018)</ref> and interpretation of NNs <ref type="bibr" target="#b38">(Singla et al., 2019)</ref>. People have empirically observed several interesting phenomena of the Hessian, e.g., the gradient during training converges to the top eigenspace of Hessian <ref type="bibr" target="#b9">(Gur-Ari et al., 2018;</ref><ref type="bibr" target="#b4">Ghorbani et al., 2019)</ref>, and the eigenspectrum of Hessian contains a "spike" which has about c − 1 large eigenvalues and a continuous "bulk" <ref type="bibr" target="#b33">(Sagun et al., 2016;</ref><ref type="bibr" target="#b28">2018;</ref><ref type="bibr" target="#b28">Papyan, 2018)</ref>. People have developed different frameworks to explain the low rank structure of the Hessians including hierarchical clustering of logit gradients <ref type="bibr" target="#b29">(Papyan, 2019;</ref><ref type="bibr" target="#b30">2020)</ref>, independent Gaussian model for logit gradients <ref type="bibr" target="#b2">(Fort &amp; Ganguli, 2019)</ref>, and Neural Tangent Kernel <ref type="bibr" target="#b13">(Jacot et al., 2020)</ref>. A distinguishing feature of this work is that we are able to characterize the top eigenspace of the Hessian directly by the weight matrices of the network.</p><p>Layer-wise Kronecker factorization (K-FAC) for training NNs: The idea of using Kronecker product to approximate Hessian-like matrices is not new. <ref type="bibr" target="#b11">Heskes (2000)</ref> uses this idea to approximate Fisher Information Matrix (FIM). <ref type="bibr" target="#b26">Martens &amp; Grosse (2015)</ref> proposed Kronecker-factored approximate curvature which approximates the inverse of FIM using layer-wise Kronecker product. Kronecker factored eigenbasis has also been utilized in training <ref type="bibr" target="#b3">(George et al., 2018)</ref>. Our paper focuses on a different application with different matrix (Hessian vs. inverse FIM) and different ends of the spectrum (top vs. bottom eigenspace).</p><p>Theoretical Analysis for Hessians Eigenstructure: <ref type="bibr" target="#b16">Karakida et al. (2019b)</ref> showed that the largest c eigenvalues of the FIM for a randomly initialized neural network are much larger than the others. Their results rely on the eigenvalue spectrum analysis in <ref type="bibr" target="#b17">Karakida et al. (2019c;</ref><ref type="bibr">a)</ref>, which assumes the weights used during forward propagation are drawn independently from the weights used in back propagation <ref type="bibr" target="#b35">(Schoenholz et al., 2017)</ref>. More recently, <ref type="bibr">Singh et al. (2021)</ref> provided a Hessian rank formula for linear networks and <ref type="bibr">Liao &amp; Mahoney (2021)</ref> provided a characterization on the eigenspace structure of G-GLM models (including 1-layer NN). To our best knowledge, theoretical analysis on the Hessians of nonlinear deeper neural networks is still vacant.</p><p>PAC-Bayes generalization bounds: People have established generalization bounds for neural networks under PAC-Bayes framework by <ref type="bibr" target="#b27">McAllester (1999)</ref>. For neural networks, <ref type="bibr" target="#b1">Dziugaite &amp; Roy (2017)</ref> proposed the first nonvacuous generalization bound, which used PAC-Bayesian approach with optimization to bound the generalization error for a stochastic neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries and Notations</head><p>Basic Notations: In this paper, we generally follow the default notation suggested by <ref type="bibr" target="#b7">Goodfellow et al. (2016)</ref>.</p><p>Additionally, for a matrix M , let M F denote its Frobenius norm and M denote its spectral norm. For two matrices M ∈ R a1×b1 , N ∈ R a2×b2 , let M ⊗ N ∈ R (a1a2)×(b1b2) be their Kronecker product such that [M ⊗ N ] (i1−1)×a2+i2,(j1−1)×b2+j2 = M i1,i2 N j1,j2 .</p><p>Neural Networks: For a c-class classification problem with training samples S = {(x i , y i )} N i=1 where (x i , y i ) ∈ R d × {0, 1} c for all i ∈ [N ], assume S is i.i.d. sampled from the underlying data distribution D. Consider an L-layer fully connected ReLU neural network f θ : R d → R c . With σ(x) = x1 x≥0 as the Rectified Linear Unit (ReLU) function, the output of this network is a series of logits z ∈ R c computed recursively as z (p) := W (p) x (p) + b (p)  and x (p) := σ(z (p) )</p><p>Here we denote the input and output of the p-th layer as x (p) and z (p) , and set x (1) = x, z := f θ (x) = z (L) . We denote θ := (w (1) , b (1) , w (2) , b (2) , • • • , w (L) , b (L) ) ∈ R P the parameters of the network. For the i-th layer, w (i)  is the flattened weight matrix W (i) and b (i) is its corresponding bias vector. For convolutional networks, a similar framework is introduced in Appendix A.2. For a single input x ∈ R d with one-hot label y and logit output z, let n (p) and m (p) be the lengths of x (p) and z (p) . For convolutional layers, we consider the number of output channels as m (p) and width of unfolded input as n (p) . Note that x (1) = x, z (L) = z = f θ (x). We denote p := softmax(z) = e z / c i=1 e zi as the output confidence. With the cross-entropy loss function (p, y) = − c i=1 y i log(p i ) ∈ R + , the training process optimizes parameter θ to minimize the empirical training loss L(θ) := E(x,y)∈S <ref type="bibr">[ (z, y)</ref>] .</p><p>Hessians: Fixing the parameter θ, we use H (v, x) = ∇ 2 v (f θ (x), y) = ∇ 2 v (z, y) to denote the Hessian of some vector v with respect to scalar loss function at input x. Note that v can be any vector. For example, the full parameter Hessian is H (θ, x) where we take v = θ, and the layer-wise weight Hessian of the p-th layer is H (w (p) , x) where we take v = w (p) .</p><p>For simplicity, define E as the empirical expectation operator over the training sample S unless explicitly stated otherwise. We mainly focus on the layer-wise weight Hessians H L (w (p) ) = E[H (w (p) , x)] with respect to loss, which are diagonal blocks in the full Hessian H L (θ) = E[H (θ, x)] corresponding to the cross terms between the weight coefficients of the same layer. We define M (p)</p><p>x := H (z (p) , x) as the Hessian of output z (p) with respect to empirical loss. With the notations defined above, we have the p-th layer-wise Hessian for a single input as</p><formula xml:id="formula_0">H (w (p) , x) = ∇ 2 w (p) (z, y) = M (p)</formula><p>x ⊗ (x (p) x (p)T ).</p><p>(1)</p><p>It follows that</p><formula xml:id="formula_1">H L (w (p) ) = E M (p) x ⊗ x (p) x (p)T = E M ⊗ xx T .</formula><p>(2)</p><p>The subscription x and the superscription (p) will be omitted when there is no confusion, as our analysis primarily focuses on the same layer unless otherwise stated. We also define subspace overlap and layer-wise eigenvector matricization for our analysis.</p><p>Definition 2.1 (Subspace Overlap). For k-dimensional subspaces U , V in R d (d ≥ k) where the basis vectors u i 's and v i 's are column vectors, with φ as the size k vector of canonical angles between U and V , we define the subspace overlap of U and V as Overlap(U , V ) := U T V 2 F /k = cos φ 2 2 /k. Definition 2.2 (Layer-wise Eigenvector Matricization). Consider a layer with input dimension n and output dimension m. For an eigenvector h ∈ R mn of its layer-wise Hessian, the matricized form of h is Mat(h) ∈ R m×n where Mat(h) i,j = h (i−1)m+j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Decoupling Conjecture and Implications on the Structures of Hessian</head><p>The fact that layer-wise Hessian for a single sample can be decomposed into Kronecker product of two components naturally leads to the following informal conjecture:</p><p>Conjecture (Decoupling Conjecture). The layer-wise Hessian can be approximated by a Kronecker product of the expectation of its two components, that is</p><formula xml:id="formula_2">H L (w (p) ) = E[M ⊗ xx T ] ≈ E[M ] ⊗ E[xx T ].</formula><p>(3)</p><p>More specifically, we conjecture that</p><formula xml:id="formula_3">E[M ]⊗E[xx T ]−E[M ⊗xx T ] E[M ⊗xx T ]</formula><p>≤ , where is a small constant.</p><p>Note that this conjecture is certainly true when M and xx T are approximately statistically independent. One immediate implication is that the top eigenvalues and eigenspace of H L (w (p) ) is close to those of</p><formula xml:id="formula_4">E[M ] ⊗ E[xx T ].</formula><p>In Section 4 we prove that the eigenspaces are indeed close for a simple setting, and in Section 5.1 we show that this conjecture is empirically true in practice.</p><p>Assuming the decoupling conjecture, we can analyze the layer-wise Hessian by analyzing the two components separately. Note that E[M ] is the Hessian of the layer-wise output with respect to empirical loss, and E[xx T ] is the auto-correlation matrix of the layer-wise inputs. For simplicity we call E[M ] the output Hessian and E[xx T ] the input auto-correlation. For convolutional layers we can a similar factorization E[M ] ⊗ E[xx T ] for the layer-wise Hessian, but with a different M motivated by <ref type="bibr" target="#b8">Grosse &amp; Martens (2016)</ref>. (See Appendix A.2) We note that the off-diagonal blocks of the full Hessian can also be decomposed similarly, which in turn allows us to approximate the eigenvalues and eigenvectors of the full parameter Hessian. The details of this approximation is stated in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Structure of Input Auto-correlation Matrix E[xx T ] and output Hessian E[M ]</head><p>For the auto-correlation matrix, one can decompose it as</p><formula xml:id="formula_5">E[xx T ] = E[x]E[x] T + Var[x].</formula><p>A key observation is that the input x for most layers are outputs of a ReLU, hence it is nonnegative. For large networks the mean component For the output Hessian, we observe that E[M ] is approximately rank c−1 (with c−1 significantly large eigenvalues) in most cases. In Section 4, we show this is indeed the case in a simplified setting, and give a formula for computing the top c − 1 eigenspace using rows of weight matrices.</p><formula xml:id="formula_6">E[x]E[x]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implications on the eigenspectrum and eigenvectors of layer-wise Hessian</head><p>The eigenvectors of a Kronecker product is the tensor product of eigenvectors of its components. As a result, let h i be the i-th eigenvector of a layer-wise Hessian Another implication is related to eigenspace overlap for different models. Even though the output Hessians of two randomly trained models may be very different, the top eigenspace of the Hessian will be close to E[x] ⊗ I, so the top eigenspace of the two models will have a high overlap that peaks at the output dimension. See Section 5.3 for more details.</p><p>4 Hessian Structure for Infinite Width Two-Layer ReLU Neural Network</p><p>In this section, we show that for a simple setting of 2-layer networks, the layer-wise parameter Hessian has c − 1 large eigenvalues and its top c − 1 eigenspace is close to the top c − 1 eigenspace of the Kronecker product approximation.</p><p>Problem Setting and Notations Let bold non-italic letters such as v, M denote random vectors (lowercase) and matrices (uppercase). Consider a two layer fully connected ReLU activated neural network with input dimension d, hidden layer dimension n and output dimension c. In particular, let d = n 1+α for some constant α &gt; 0. Let the network has positive input from a rectified Gaussian x ∼ N R (0, I d ) where every entry is identically distributed as max{ x, 0} for x ∼ N (0, 1). Let W (1) ∈ R n×d and W (2) ∈ R c×n be the weight matrices. In this problem we consider a random Gaussian initialization that W (1) ∼ N (0,</p><formula xml:id="formula_7">1 d I dn ) and W (2) ∼ N (0, 1 n I nc ).</formula><p>Both weight matrices has expected row norm of 1. Let the loss objective be cross entropy . Training labels are irrelevant as they are independent from the Hessian at initialization. Denote the output of the first and second layer as y and z respectively. We have y = σ(W (1) x) and z = W (2) y.</p><p>Here σ is the element-wise ReLU function. Let D diag(I [y ≥ 0]) ∈ R n×n denote the 0/1 diagonal matrix representing the activation of σ that y = DW (1) x. Let p = softmax(z) and let A diag(p) − pp T . Note A is rank c − 1 with the null space of the all one vector. We give full details about our settings in Appendix B.1. By simple matrix calculus (see Appendix A.1), the output Hessian of M (1) and the full layer-wise Hessian has closed-form</p><formula xml:id="formula_8">M (1) = E x∼N R (0,I d ) DW (2)T AW (2) D , H (1) = E x∼N R (0,I d ) DW (2)T AW (2) D ⊗ xx T .<label>(4)</label></formula><p>Following the decoupling conjecture, the Kronecker approximation of the layer-wise Hessian is</p><formula xml:id="formula_9">H (1) E x∼N R (0,I d ) DW (2)T AW (2) D ⊗ E x∼N R (0,I d ) xx T .</formula><p>(5)</p><p>Since we are always taking the expectation over the input x, we will neglect the subscript and use E for expectation. Now we are ready to state our main theorem.</p><p>Theorem 4.1. For an infinite width two-layer ReLU activated neural network with Gaussian initialization as defined above, let V 1 and V 2 be the top c − 1 eigenspaces of H (1) and H (1) respectively, for all &gt; 0, lim n→∞ Pr W (1) ∼N (0,</p><formula xml:id="formula_10">1 d I nd ),W (2) ∼N (0, 1 n Icn) [Overlap (V 1 , V 2 ) &gt; 1 − ] = 1. Moreover H (1) has c − 1 large eigenvalues that, lim n→∞ Pr W (1) ∼N (0, 1 d I nd ),W (2) ∼N (0, 1 n Icn) λ c (H (1) ) λ c−1 (H (1) ) W (1) ,W (2) &lt; = 1.<label>(6)</label></formula><p>Instead of directly working on the layer-wise Hessian, we first show a similar theorem for the output Hessian M (1) . We will then show that the proof technique of the following theorem can be easily generalized to prove our main theorem.</p><p>Theorem 4.2. For the same network as in Theorem 4.1, let M * E D W (2)T AW (2) D where D is an independent copy of D and is independent of A. Let S 1 and S 2 be the top c − 1 eigenspaces of M (1) and M * respectively, S 2 is approximately R{W i } c i=1 \{1 T W } where R is the row span, and for all &gt; 0, lim n→∞ Pr W (1) ∼N (0,</p><formula xml:id="formula_11">1 d I nd ),W (2) ∼N (0, 1 n Icn) [Overlap (S 1 , S 2 ) &gt; 1 − ] = 1. Moreover, M has c − 1 large eigen- values that lim n→∞ Pr W (1) ∼N (0, 1 d I nd ),W (2) ∼N (0, 1 n Icn) λ c (M (1) ) λ c−1 (M (1) ) W (1) ,W (2) &lt; = 1.<label>(7)</label></formula><p>Remark. The closed form approximating of S 1 in Theorem 4.2 can be heuristically extended to the case with multiple layers, that the top eigenspace of the output Hessian of the k-layer would be approximately k+1) and R(S (k) ) is the row space of S (k) . Though our result was only proven for random initialization and random data, we observe that this subspace also has high overlap with the top eigenspace of output Hessian at the minima of models trained with real datasets. The corresponding empirical results are shown in Appendix G.1.</p><formula xml:id="formula_12">R(S (k) ) \ {1 T S (k) } where S (k) = W (n) W (n−1) • • • W (</formula><p>Proof Sketch for Theorem 4.2 For simplicity of notations, in this section we will use W to denote W (2) and M to denote M (1) unless specified otherwise. Our proof of Theorem 4.2 mainly consists of three parts. First we analyze the structure of M * and show that it is approximately rank c − 1. Then we show that M * and M are roughly equivalent via an approximate independence between D and A. Finally, by projecting both M and M * onto a c × c matrix using W , we can apply the approximate independence and prove that the top c − 1 eigenspace of M * is approximately that of M , which concludes the proof.</p><p>(1) Structure of M * When n → ∞, the output of the second layer y converges to a multivariate Gaussian (Lemma B.9), hence we can consider each diagonal entry of D as a p = 1 2 Bernoulli random variable. Since we assumed that D and A are independent, by some simple calculation,</p><formula xml:id="formula_13">M * = 1 4 W T E[A]W + diag(W T E[A]W ) .<label>(8)</label></formula><p>Here E[A] is rank c − 1 with the (c − 1)-th eigenvalue bounded below from 0 (Lemma B.12). Since the two terms in the sum has the same trace while</p><formula xml:id="formula_14">W T E[A]W is rank c − 1 compared to rank n of diag(W T E[A]W ),</formula><p>we can show that the top eigenspace is dominated by the eigenspace of</p><formula xml:id="formula_15">W T E[A]W , which is approximately R{W i } c i=1 \{1 T W }.</formula><p>(2) Approximate Independence Between A and D Intuitively, if D and A are independent, then M = M * . However, this is clearly not true -if the activations align with a row of W then the corresponding output is going to be large, which changes A significantly. To address this problem, we observe that the formula for M is only of degree 2 in D, so one can focus on conditioning on two of the activations -a negligible fraction in the limit.</p><p>More precisely, if one expand out the expression of each element squared in M , it is an homogeneous polynomial of the form p(A, D, Ā, D) = c i,j,k,l=1 n p,q=1 c ijklpq A ij Ākl D pp Dqq , where ( Ā, D) are independent copies of (A, D). The same element squared in M * is just going to be p(A, D , Ā, D ). By nice properties of the Gaussian initialized weight matrix, we show that as n → ∞, A is invariant when conditioning on two entries of D (Lemma B.11). Therefore, in the limit we have lim n→∞ E p(A, D, Ā, D) = E p(A, D , Ā, D ) (detailed proof in Appendix).</p><p>(3) Equivalence between M * and M Since the size of M also goes to infinity as we take the limit on n, it is technically difficult to directly compare their eigenspaces. In this case we utilize the fact that W has approximately orthogonal rows, and project M onto W M W T . In particular, by expanding out the Frobenious norms as polynomials and bounding the 1 norm of the coefficients, using Lemma B.11 we are able to show that . This result tells us that the projection does not lose information, and hence indirectly gives us the dominating eigenspace of M . This concludes our proof for Theorem 4.2 Proving Theorem 4.1 and Beyond To prove Theorem 4.1, we use a very similar strategy. We consider a re-scaled Hessian H 1 d H and show that in the independent setting</p><formula xml:id="formula_16">M 2 F ≈ W M W T 2 F ≈ W M * W T 2 F ≈ M * 2 F (Lemma B</formula><formula xml:id="formula_17">H * = 1 d E[D W AW D ⊗ x x T ] = M * ⊗ 1 d E[x x T ].</formula><p>We then generalize the conditioning technique to involve conditioning on two entries of x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Empirical Observation and Verification</head><p>In this section, we present some empirical observations that either verifies, or are induced by the decoupling conjecture. We conduct experiments on the CIFAR-10, CIFAR-100 <ref type="bibr" target="#b20">(Krizhevsky, 2009), and</ref><ref type="bibr">MNIST (LeCun et al., 1998)</ref> datasets as well as their random labeled versions, namely MNIST-R and CIFAR10-R. We used different fully connected (fc) networks (a fc network with m hidden layers and n neurons each hidden layer is denoted as F-n m ), several variations of <ref type="bibr">LeNet (LeCun et al., 1998)</ref>, VGG11 <ref type="bibr" target="#b36">(Simonyan &amp; Zisserman, 2015)</ref>, and ResNet18 <ref type="bibr" target="#b10">(He et al., 2016)</ref>. We use "layer:network" to denote a layer of a particular network. For example, conv2:LeNet5 refers to the second convolutional layer in LeNet5. More empirical results are included in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Kronecker Approximation of Layer-wise Hessian and Full Hessian</head><p>To verify the decoupling conjecture in practical settings, we compare the top eigenvalues and eigenspaces of the approximated Hessian and the true Hessian. We use subspace overlap (Definition 2.1) to measure the similarity between top eigenspaces. As shown in Fig. <ref type="figure">2</ref>, this approximation works reasonably well on the top eigenspace.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Low Rank Structure of E[M ] and H</head><p>Another way to empirically verify the decoupling conjecture is to show the similarity between the outliers in eigenspectrum of the layer-wise Hessian E[M ] and the output Hessian H L . Fig. <ref type="figure" target="#fig_4">3</ref> shows the similarity of eigenvalue spectrum between E[M ] and layer-wise Hessians in different situations, which agrees with our prediction. For (a) and (b) we are also seeing the eigengap at c − 1, which is consistent with our analysis and previous observations <ref type="bibr" target="#b34">(Sagun et al., 2018;</ref><ref type="bibr" target="#b29">Papyan, 2019)</ref>. However, the eigengap does not appear at minimum for random labeled data with a under-parameterized network, meaning that our theory may not generalize to all settings.   p) ).</p><p>The vertical axes denote the eigenvalues. Similarity between the two eigenspectra is a direct consequence of a low rank E[xx T ] and the decoupling conjecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Eigenspace Overlap of Different Models</head><p>Apart from the phenomena that are direct consequences of the decoupling conjecture, we observe another nontrivial phenomenon involving different minima. Consider models with the same structure, trained on the same dataset, but using different random initializations, despite no obvious correlation between their parameters, we observe surpisingly high overlap between the dominating eigenspace of some of their layer-wise Hessians.  It turns out that the nontrivial overlap is also a consequence of the decoupling conjecture, which arises when the output Hessian and autocorrelation are related in the following way: When the small eigenvalues of E[M ] ∈ R m×m approaches 0 slower than the small eigenvalues of E[xx T ], the top m eigenspace will then be approximately spanned by I m ⊗ E[x] T by the decoupling conjecture. Now suppose we have two different models with E[x] 1 and E[x] 2 respectively. Their top-m eigenspaces are approximately <ref type="bibr">E[x]</ref> 2 are the same for the input layer and all non-negative for other layers. While this particular relation between E[M ] and E[xx T ] are true in many shallow networks and in later layers of deeper networks, they are not satisfied for earlier layers of deeper networks. In Appendix G.3 we explain how one can still understand the overlap using correspondence matrices when the above simplified argument does not hold.</p><formula xml:id="formula_18">I m ⊗ E[x] 1 and I m ⊗ E[x] 2 . Thus the overlap at dimension m is approximately ( E[x] T 1 E[x] 2 ) 2 , which is large since E[x] 1 and</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Tighter PAC-Bayes Bound with Hessian Information</head><p>The PAC-Bayes bound is a commonly used bound for the generalization gap of neural networks. In this section we show how we can obtain tighter PAC-Bayes bounds using the Kronecker approximation of Hessian eigenbasis. Theorem 6.1 (PAC-Bayes Bound). <ref type="bibr" target="#b27">(McAllester, 1999;</ref><ref type="bibr" target="#b21">Langford &amp; Seeger, 2001)</ref> With the hypothesis space H parametrized by model parameters. For any prior distribution P in H that is chosen independently from the training set S, and any posterior distribution Q in H whose choice may inference S, with probability</p><formula xml:id="formula_19">1 − δ, D KL ( e(Q)||e(Q)) ≤ 1 |S|−1 D KL (Q||P ) + log |S| δ .</formula><p>Where e(Q) is the expected classification error for the posterior over the underlying data distribution and e(Q) is the classification error for the posterior over the training set.</p><p>Intuitively, if one can find a posterior Q that has low loss on the training set, and is close to the prior P , then the generalization error on Q must be small. <ref type="bibr" target="#b1">Dziugaite &amp; Roy (2017)</ref> uses optimization techniques to find an optimal posterior in the family of Gaussians with diagonal covariance. They showed that the bound can be nonvacuous for several neural network models.</p><p>We follow <ref type="bibr" target="#b1">Dziugaite &amp; Roy (2017)</ref> to set the prior P to be a multi-variant Gaussian. The covariance is invariant with respect to the change of basis since it is a multiple of identity. Thus, For the posterior, when the variance in one direction is larger, the distance with the prior decreases; however this also has the risk of increasing the empirical loss over the posterior. In general, one would expect the variance to be larger along a flatter direction in the loss landscape and smaller along a sharper direction. However, since the covariance matrix of Q is fixed to be diagonal in <ref type="bibr" target="#b1">Dziugaite &amp; Roy (2017)</ref>, the search of optimal deviation happens in standard basis vectors which are not aligned with the local loss landscape. Using the Kronecker factorization as in Equation 3, we can approximate the layer-wise Hessian's eigenspace. We set Q to be a Gaussian whose covariance is diagonal in the approximated eigenbasis of the layer-wise Hessians. Under this posterior change of basis, we can obtain tighter bounds compared to <ref type="bibr" target="#b1">Dziugaite &amp; Roy (2017)</ref>. In our experiments, the final posterior variance s is smaller along the direction of eigenvectors with larger eigenvalues (see Fig. <ref type="figure" target="#fig_38">31</ref>). This agrees with our presumption that the alignment of sharp and flat directions will result in a better optimized posterior Q and thus a tighter bound on classification error. Detailed algorithm description, experiment results, and plots are shown in Appendix H. A Detailed Derivations</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Derivation of Hessian</head><p>For an input x with label y, we define the Hessian of single input loss with respect to vector v as</p><formula xml:id="formula_20">H (v, x) = ∇ 2 v (f θ (x), y) = ∇ 2 v (z x , y).<label>(9)</label></formula><p>We define the Hessian of loss with respect to v for the entire training sample as</p><formula xml:id="formula_21">H L (v) = ∇ 2 v L(θ) = N i=1 ∇ 2 v (f θ (x i ), y i ) = N i=1 H (v, x i ) = E [H (v, x)] .<label>(10)</label></formula><p>We now derive the Hessian for a fixed input label pair (x, y). Following the definition and notations in Section 2, we also denote output as z = f θ (x). We fix a layer p for the layer-wise Hessian. Here the layer-wise weight Hessian is H (w (p) , x). We also have the output for the layer as z (p) . Since w (p) only appear in the layer but not the subsequent layers, we can consider z = f θ (x) = g θ (z (p) (w, x)) where g θ only contains the layers after the p-th layer and does not depend on w (p) . Thus, using the Hessian Chain rule <ref type="bibr" target="#b39">(Skorski, 2019)</ref>, we have</p><formula xml:id="formula_22">H (w (p) , x) = ∂z (p) ∂w (p) T H (z (p) , x) ∂z (p) ∂w (p) + m (p) i=1 ∂ (z, y) ∂z (p) i ∇ 2 w (p) z (p) i ,<label>(11) where z</label></formula><p>(p) i</p><p>is the ith entry of z (p) and m (p) is the number of neurons in p-th layer (size of z (p) ).</p><p>Since p) and w (p) = vec(W (p) ) we have</p><formula xml:id="formula_23">z (p) = W (p) x (p) + b (</formula><formula xml:id="formula_24">∂z (p) ∂w (p) = I m (p) ⊗ x (p)T .<label>(12)</label></formula><p>Since ∂z (p) ∂w (p) does not depend on w (p) , for all i we have ∇ 2</p><formula xml:id="formula_25">w (p) z (p) i = 0. Thus, H (w (p) , x) = I m (p) ⊗ x (p) H (z (p) , x) I m (p) ⊗ x (p)T . (<label>13</label></formula><formula xml:id="formula_26">)</formula><p>We define M (p)</p><p>x = H (z (p) , x) as in Section 2 so that</p><formula xml:id="formula_27">H (w (p) , x) = I m (p) ⊗ x (p) M (p) x I m (p) ⊗ x (p)T = M (p) x ⊗ x (p) x (p)T . (<label>14</label></formula><formula xml:id="formula_28">)</formula><p>We now look into M (p)</p><p>x = H (z (p) , x). Again we have z = g θ (z (p) ) and can use chain rule here,</p><formula xml:id="formula_29">H (z (p) , x) = ∂z ∂z (p) T H (z, x) ∂z ∂z (p) + c i=1 ∂ (z, y) ∂z i ∇ 2 z (p) z i<label>(15)</label></formula><p>By letting p := softmax(z) be the output confidence vector, we define the Hessian with respect to output logit z as A x and have</p><formula xml:id="formula_30">A x := H (z, x) = ∇ 2 z l(z, y) = diag(p) − pp T ,<label>(16)</label></formula><p>according to <ref type="bibr" target="#b38">Singla et al. (2019)</ref>.</p><p>We also define the Jacobian of z with respect to z (p) (informally logit gradient for layer p) as G (p)</p><p>x := ∂z ∂z (p) . For FC layers with ReLUs, we can consider ReLU after the p-th layer as multiplying z (p) by an indicator function 1 z (p) &gt;0 . To use matrix multiplication, we can turn the indicator function into a diagonal matrix and define it as D (p) where</p><formula xml:id="formula_31">D (p) := diag (1 z (p) &gt;0 ) .<label>(17)</label></formula><p>Thus, we have the input of the next layer as x (p+1) = D (p) z (p) . The FC layers can then be considered as a sequential matrix multiplication and we have the final output as</p><formula xml:id="formula_32">z = W (L) D (L−1) W (L−1) D (L−2) • • • D (p) z (p) .<label>(18)</label></formula><p>Thus,</p><formula xml:id="formula_33">G (p) x = ∂z ∂z (p) = W (L) D (L−1) W (L−1) D (L−2) • • • D (p) . (<label>19</label></formula><formula xml:id="formula_34">) Since G (p)</formula><p>x is independent of z (p) , we have</p><formula xml:id="formula_35">∇ 2 z (p) z i = 0, ∀i.<label>(20)</label></formula><p>Thus,</p><formula xml:id="formula_36">M (p) x = H (z (p) , x) = G (p)T x A x G (p) x .<label>(21)</label></formula><p>Moreover, loss Hessian with respect to the bias term b (p) equals to that with respect to the output of that layer z (p) . We thus have</p><formula xml:id="formula_37">H (b (p) , x) = M (p) x = G (p)T x A x G (p) x .<label>(22)</label></formula><p>The Hessians of loss for the entire training sample are simply the empirical expectations of the Hessian for single input. We have the formula as the following:</p><formula xml:id="formula_38">H L (w (p) ) = E H (w (p) , x) = E M (p) x ⊗ x (p) x (p)T ,<label>(23)</label></formula><formula xml:id="formula_39">H L (b (p) ) = H L (z (p) ) = E M (p) x = E G (p)T x A x G (p) x . (<label>24</label></formula><formula xml:id="formula_40">)</formula><p>Note that we can further decompose</p><formula xml:id="formula_41">A x = Q T x Q x , where Q x = diag ( √ p) I c − 1 c p T ,<label>(25)</label></formula><p>with 1 c is a all one vector of size c, proved in <ref type="bibr" target="#b29">Papyan (2019)</ref>.</p><p>We can further extend the close form expression to off diagonal blocks and the bias entries to get the full Gauss-Newton term of Hessian. Let</p><formula xml:id="formula_42">F T x =             G x (1)T ⊗ x (1) G x (1)T G x (2)T ⊗ x (2) G x (2)T . . . G x (L)T ⊗ x (n) G x (L)T             . (<label>26</label></formula><formula xml:id="formula_43">)</formula><p>The full Hessian is given by</p><formula xml:id="formula_44">H L (θ) = E F T x A x F x + E c i=1 ∂ (z, y) z i ∇ 2 θ z i .<label>(27)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Approximating Weight Hessian of Convolutional Layers</head><p>The approximation of weight Hessian of convolutional layer is a trivial extension from the approximation of Fisher information matrix of convolutional layer by <ref type="bibr" target="#b8">Grosse &amp; Martens (2016)</ref>.</p><p>Consider a two dimensional convolutional layer of neural network with m input channels and n output channels. Let its input feature map X be of shape (n, X 1 , X 2 ) and output feature map Z be of shape (m, P 1 , P 2 ). Let its convolution kernel be of size K 1 × K 2 . Then the weight W is of shape (m, n, K 1 , K 2 ), and the bias b is of shape (m). Let P be the number of patches slide over by the convolution kernel, we have P = P 1 P 2 .</p><p>Follow <ref type="bibr" target="#b0">Dangel et al. (2020)</ref>, we define Z ∈ R m×P as the reshaped matrix of Z and W ∈ R m×nK1K2 as the reshaped matrix of W. Define B ∈ R m×P by broadcasting b to P dimensions. Let X ∈ R nK1K2×P be the unfolded X with respect to the convolutional layer. The unfold operation <ref type="bibr" target="#b32">(Paszke et al., 2019)</ref> is commonly used in computation to model convolution as matrix operations.</p><p>After the above transformation, we have the linear expression of the p-th convolutional layer similar to FC layers:</p><formula xml:id="formula_45">Z (p) = W (p) X (p) + B (p)<label>(28)</label></formula><p>We still omit superscription of (p) for dimensions for simplicity. We also denote z (p) as the vector form of Z (p)  and has size mP . Similar to fully connected layer, we have analogue of Eq. ( <ref type="formula" target="#formula_27">14</ref>) for convolutional layer as</p><formula xml:id="formula_46">H (w (p) , X) = I m ⊗ X (p) M (p) x I m ⊗ X (p)T ,<label>(29)</label></formula><p>where M (p)</p><p>x = H (z (p) , X) and is a mP × mP matrix. Also, since convolutional layers can also be considered as linear operations (matrix multiplication with reshape) together with FC layers and ReLUs, Eq. (20) still holds. Thus, we still have</p><formula xml:id="formula_47">H (z (p) , X) = M (p) x = G (p)T x A x G (p) x ,<label>(30)</label></formula><p>where G (p)</p><p>x = ∂z ∂z (p) and has dimension c × mP , although is cannot be further decomposed as direct multiplication of weight matrices as in the FC layers.</p><p>However, for convolutional layers, X (p) is a matrix instead of a vector. Thus, we cannot make Eq. ( <ref type="formula" target="#formula_46">29</ref>) into the form of a Kronecker product as in Eq. ( <ref type="formula" target="#formula_27">14</ref>).</p><p>Despite this, it is still possible to have a Kronecker factorization of the weight Hessian in the form</p><formula xml:id="formula_48">H (w (p) , X) ≈ M (p) x ⊗ X (p) X (p)T ,<label>(31)</label></formula><p>using further approximation motivated by <ref type="bibr" target="#b8">Grosse &amp; Martens (2016)</ref>. Note that M (p)</p><formula xml:id="formula_49">x need to have a different shape (m × m) from M (p) x (mP × mP ), since H (w (p) , X) is mnK1K2 × mnK1K2 and X (p) X (p)T is nK1K2 × nK1K2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Since we can further decompose</head><formula xml:id="formula_50">A x = Q x T Q x , we then have M x (p) = G (p)T x A x G (p) x = Q x G x (p) T Q x G x (p) . (<label>32</label></formula><formula xml:id="formula_51">)</formula><p>We define N (p)</p><formula xml:id="formula_52">x = Q x G x (p) . Here Q x is c × c and G x (p) is c × mP so that N (p)</formula><p>x is c × mP . We can reshape N (p)</p><p>x into a cP × m matrix N (p)</p><p>x . We then reduce M (p)</p><p>x (mP × mP ) into a m × m matrix as</p><formula xml:id="formula_53">M (p) x = 1 P N (p)T x N (p) x .<label>(33)</label></formula><p>The scalar 1 P is a normalization factor since we squeeze a dimension of size P into size 1. Thus, we can have similar Kronecker factorization approximation as</p><formula xml:id="formula_54">H L (w (p) ) = E H (w (p) , X) = E I m ⊗ X (p) M (p) x I m ⊗ X (p)T (34) ≈ E M (p) x ⊗ X (p) X (p)T ≈ E M (p) x ⊗ E X (p) X (p)T .<label>(35)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Main Proof</head><p>This is the complete proof for the two main theorems sketched in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.1 Notations</head><p>In this section, we generally follow the notation standard by <ref type="bibr" target="#b7">Goodfellow et al. (2016)</ref>. We will use bold italic lowercase letters (v) to denote vectors, bold non-italic lowercase letters to denote random vectors (v), bold italic uppercase letters (A) to denote matrices, and bold italic uppercase letters (A) to denote random matrices.</p><p>Moreover, we use [n] for positive integer n to denote the set {1, • • • , n}, and M to denote the spectral norm of a matrix M . We use A, B F to denote the Frobenius inner product of two matrices A and B, namely A, B F i,j A i,j B i,j . We use tr(M ) to denote the trace of a matrix M , and we use 1 c to denote the all-one vector of dimension c (the subscript may be omitted when it's clear from the context).</p><p>For probability distributions, we use N R (µ, σ) to denote the rectified Gaussian distribution which has density function</p><formula xml:id="formula_55">f N R (x; µ, σ) = Φ µ σ δ(x) + 1 √ 2πσ 2 exp − (x − µ) 2 2σ 2 I [x &gt; 0] . (<label>36</label></formula><formula xml:id="formula_56">)</formula><p>Here Φ is the CDF of standard normal distribution, δ(x) is the Dirac delta function. Note that when µ = 0, the density function simplifies to</p><formula xml:id="formula_57">f N R (x; 0, σ) = 1 2 δ(x) + 1 √ 2πσ 2 exp − x 2 2σ 2 I [x &gt; 0] .<label>(37)</label></formula><p>We will use the same notation for multivariate rectified Gaussian distribution, which will be used to characterize the inputs of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.2 Problem Setting</head><p>Consider a two layer fully connected ReLU activated neural network with input dimension d, hidden layer dimension n and output dimension c. In particular, n goes to infinity, d = n 1+α for some α &gt; 0, and c is a finite constant.</p><p>Let network be trained with cross-entropy objective L. Let σ denote the element-wise ReLU activation function which acts as σ(x) = x • I x≥0 and the product here is applied element-wise. Let W (1) ∈ R n×d and W (2) ∈ R c×n denote the weight matrices of the first and second layer respectively.</p><p>We consider the case that the neural network has rectified standard Gaussian input x ∼ N R (0, I d ). Denote the output of the first and second layer as y and z respectively. We have y = σ(W (1) x) and z = W (2) y. Let p = softmax(z) denote the softmax output of the network and let A diag(p) − pp T .</p><p>In this problem, we look into the state of random Gaussian initialization, in which entries of both matrices are i.i.d. sampled from a standard normal distribution, and then re-scaled such that each row of W (1) and W (2) has norm 1. When taking n and d to infinity, with the concentration of norm in high-dimensional Gaussian random variables, we assume in this problem that entries of W (1) are iid sampled from a zero-mean distribution with variance 1/d, and entries of W (2) are iid sampled from a zero-mean distribution with variance 1/n. This initialization is standard in training neural networks. From the previous analysis of Hessian, the output Hessian corresponding to the first layer has closed form</p><formula xml:id="formula_58">M (1) E x∼N R (0,I d ) DW (2)T AW (2) D ,<label>(38)</label></formula><p>where D diag(I [y ≥ 0]) ∈ R n×n is the random 0/1 diagonal matrix representing the activations of ReLU function after the first layer. Note that the output Hessian of the second layer is simply</p><formula xml:id="formula_59">M (2) E [A].</formula><p>By the Kronecker decomposition, the closed form of the layer-wise Hessians of the first and the second layer are</p><formula xml:id="formula_60">H (1) E x∼N R (0,I d ) DW (2)T AW (2) D ⊗ xx T , H (2) E x∼N R (0,I d ) A ⊗ DW (1) xx T W (1)T D .</formula><p>Following the decoupling conjecture, let the Kronecker approximation of the Hessians above be</p><formula xml:id="formula_61">H (1) E x∼N R (0,I d ) DW (2)T AW (2) D ⊗ E x∼N R (0,I d ) xx T , H (2) E x∼N R (0,I d ) [A] ⊗ E x∼N R (0,I d ) DW (1) xx T W (1)T D .</formula><p>The decoupling conjecture is then equivalent to</p><formula xml:id="formula_62">H (1) ≈ H (1) , H (2) ≈ H (2) .</formula><p>Since our formulae for the Hessians are going to depend on the weight matrices, throughout the section we will condition on the value of W (1) and W (2) when we take expectation (i.e. the expectation is only taken over the input x ∼ N R (0, I d )). We will neglect this under-script of the expectation operator E as there will be no confusion.</p><p>When we are discussing the Hessians of a certain layer, we will also neglect the upper-script and just use H and M when there is no confusion. Moreover, we denote X E xx T as the autocorrelation of the input.</p><p>Furthermore, for simplicity of notations, we will sometimes use the verbal description "with probability 1 over</p><formula xml:id="formula_63">W (1) /W (2) , event E is true" to denote lim n→∞ Pr W (1) ∼N (0, 1 d I nd ),W (2) ∼N (0, 1 n Icn) [E] = 1.<label>(39)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Detailed Proof</head><p>First, we restate our main theorems:</p><p>Theorem 4.1 (Decoupling Theorem) Let V 1 and V 2 be the top c − 1 eigenspaces of H (1) and H (1) respectively, for all &gt; 0, lim n→∞ Pr</p><formula xml:id="formula_64">W (1) ∼N (0, 1 d I nd ),W (2) ∼N (0, 1 n Icn) [Overlap (V 1 , V 2 ) &gt; 1 − ] = 1. (<label>40</label></formula><formula xml:id="formula_65">)</formula><p>Moreover H (1) has c − 1 large eigenvalues that,</p><formula xml:id="formula_66">lim n→∞ Pr W (1) ∼N (0, 1 d I nd ),W (2) ∼N (0, 1 n Icn) λ c (H (1) ) λ c−1 (H (1) ) W (1) ,W (2) &lt; = 1. (41) Theorem 4.2 Let M * E D W (2)T AW (2) D</formula><p>where D is an independent copy of D and is independent of A. Let S 1 and S 2 be the top c − 1 eigenspaces of M (1) and M * respectively, for all &gt; 0, lim n→∞ Pr</p><formula xml:id="formula_67">W (1) ∼N (0, 1 d I nd ),W (2) ∼N (0, 1 n Icn) [Overlap (S 1 , S 2 ) &gt; 1 − ] = 1. (<label>42</label></formula><formula xml:id="formula_68">)</formula><p>Moreover,</p><formula xml:id="formula_69">lim n→∞ Pr W (1) ∼N (0, 1 d I nd ),W (2) ∼N (0, 1 n Icn) λ c (M ) λ c−1 (M ) W (1) ,W (2) &lt; = 1.<label>(43)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.1 Properties of Infinite Width Weight Matrices</head><p>We will first prove some simple properties of the Gaussian initialized weight matrices W (1) and W (2) that will facilitate our analysis. Recall that W (1) ∈ R d×n and W (2) ∈ R n×c where the output dimension c is a finite constant, the hidden layer width n goes to infinity, and the input dimension d = n 1+α for some constant α &gt; 0.</p><formula xml:id="formula_70">lemma B.1. For all i ∈ [c], for all &gt; 0, lim n→∞ Pr   n j=1 W (2) ij ≥   = 0. (<label>44</label></formula><formula xml:id="formula_71">) Proof of Lemma B.1. Since each entry of W (2) is initialized independently from N (0, 1 n ), by Central Limit Theorem we have n j=1 W (2) ij ∼ N (0, 1 n ).</formula><p>For any &gt; 0, fix . By Chebyshev's inequality,</p><formula xml:id="formula_72">lim n→∞ Pr   n j=1 W (2) ij ≥   &lt; lim n→∞ 1 n 2 = 0. (45) lemma B.2. (Laurent &amp; Massart, 2000) For X ∼ χ 2 n , Pr X − n ≥ 2 √ nt + 2t ≤ e −t , Pr X − n ≤ −2 √ nt ≤ e −t . (<label>46</label></formula><formula xml:id="formula_73">) lemma B.3. For all &gt; 0, lim n→∞ Pr | W (2) 2 F − c| ≥ = 0. (47) Beside, for all i ∈ [c], lim n→∞ Pr | W (2) i 2 − 1| ≥ = 0. (<label>48</label></formula><formula xml:id="formula_74">)</formula><p>Proof of Lemma B.3. For simplicity of notations, we will use W to denote</p><formula xml:id="formula_75">W (2) in this proof. Since each entry of W is initialized independently from N (0, 1 n ), we know that n W 2 F = c i=1 n j=1 nW i,j 2 follows a χ 2 cn -distribution. From Lemma B.2 we know that for large enough n, Pr |n W 2 F − cn| ≥ n ≥ Pr |n W 2 F − cn| ≥ 2 √ cn 3/4 + 2n 1/2 ≤ 2 exp(−n 1/2 ).<label>(49)</label></formula><p>In other words,</p><formula xml:id="formula_76">lim n→∞ Pr | W 2 F − c| ≥ = lim n→∞ Pr |n W 2 F − cn| ≥ n = 0.<label>(50)</label></formula><p>Similarly, for any</p><formula xml:id="formula_77">i ∈ [c], n W i 2 F follows a χ 2 n -distribution, so for large enough n, Pr |n W i 2 F − n| ≥ n ≤ Pr |n W 2 F − n| ≥ 2n 3/4 + 2n 1/2 ≤ 2 exp(−n 1/2 ),<label>(51)</label></formula><p>which indicates that</p><formula xml:id="formula_78">lim n→∞ Pr | W i 2 − 1| ≥ = lim n→∞ Pr |n W i 2 − n| ≥ n = 0. (<label>52</label></formula><formula xml:id="formula_79">)</formula><p>lemma B.4. Let w i denote the i-th column vector of W (1) . With probability 1 over W (1) ,</p><formula xml:id="formula_80">d max i=1 w i &lt; 5n − α 2 . (<label>53</label></formula><formula xml:id="formula_81">)</formula><p>Proof of Lemma B.4. Since entries of W (1) are i.i.d. sampled from N (0, 1 n ), each w i 2 obeys a χ 2 n scaled by 1+α) . Thus by the tail bound of Lemma B.2, setting t = n we have</p><formula xml:id="formula_82">1 d = n −(</formula><formula xml:id="formula_83">Pr w i 2 ≥ 5n −α = Pr d w i 2 ≥ n + 2 √ n 2 + 2n ≤ e −n .<label>(54)</label></formula><p>By a Union bound we have</p><formula xml:id="formula_84">Pr d max i=1 w i 2 ≥ 5n −α ≤ d i=1 Pr w i 2 ≥ 5n −α = de −n = n 1+α e −n . (<label>55</label></formula><formula xml:id="formula_85">)</formula><p>Since α is a constant, RHS converges to 0. Thus with probability 1 over W (1) , we have</p><formula xml:id="formula_86">d max i=1 w i 2 &lt; 5n −α . (<label>56</label></formula><formula xml:id="formula_87">)</formula><p>Taking square root on both sides completes the proof. lemma B.5. For any random matrix W For all &gt; 0,</p><formula xml:id="formula_88">lim n→∞ Pr W (1) W (1)T − I c ≥ = 0.<label>(57)</label></formula><p>Besides, for all i, j ∈</p><formula xml:id="formula_89">[c], lim n→∞ Pr |(W (1) W (1)T ) i,j − δ i,j | ≥ = 0 (<label>58</label></formula><formula xml:id="formula_90">)</formula><p>Here δ is the Kronecker delta function, i.e., δ i,j = I[i = j].</p><p>Proof of Lemma B.5. To prove this lemma we need the following tail bound: lemma B.6. <ref type="bibr" target="#b45">(Zhu, 2012)</ref> If S follows a Wishart distribution W d (n, C), with r = tr(C)/ C , for θ ≥ 0 the following inequality holds that</p><formula xml:id="formula_91">Pr 1 n S − C ≥ 2θ(r + 1) n + 2θr n C ≤ 2d exp(−θ).<label>(59)</label></formula><p>Since each entry of W (1) is initialized independently from N (0, 1 d ), we know that</p><formula xml:id="formula_92">W (1) W (1)T follows Wishart distribution W d (d, 1 d I n ). With r = tr( 1 d I n )/ 1 d I n = n and set θ = n α 2 , from Lemma B.6, for n ≥ 1 we get 2d exp(−n α 2 ) ≥ Pr 1 d W (1) W (1)T − 1 d I n ≥ 2θ(n + 1) d + 2θn d 1 d I n = Pr 1 d W (1) W (1)T − 1 d I n ≥ 2n α 2 (2n) n 1+α + 2n α 2 n n 1+α 1 d I n = Pr W (1) W (1)T − I n ≥ 2(n − α 4 + n − α 2 ) .<label>(60)</label></formula><p>Fix any &gt; 0, we may find</p><formula xml:id="formula_93">N ∈ N such that for all n &gt; N , 2(n − α 4 + n − α 2 ) &lt; . For any &gt; 0, we may find N such that 2d exp(−n α 2 ) = 2n 1+α exp(−n α 2 ) &lt; . Passing n to infinity we get lim n→∞ Pr W (1) W (1)T − I n &gt; = 0. (<label>61</label></formula><formula xml:id="formula_94">)</formula><p>Then we proceed to analyze the entries. For all i, j ∈ [n], we have</p><formula xml:id="formula_95">Pr |(W (1) W (1)T ) i,j − δ i,j | ≥ ≤ Pr   n i,j=1 Pr (W (1) W (1)T ) i,j − δ i,j 2 ≥ 2   = Pr W (1) W (1)T − I n 2 F ≥ 2 ≤ Pr W (1) W (1)T − I n ≥ √ n ,<label>(62)</label></formula><p>which implies that for all i, j ∈</p><formula xml:id="formula_96">[n], lim n→∞ Pr |(W (1) W (1)T ) i,j − δ i,j | ≥ = 0. (<label>63</label></formula><formula xml:id="formula_97">)</formula><p>For the second weight matrix W (2) , where W (2) W (2)T ∼ W c (n, 1 n I c ), we may prove an identical statement as shown in the corollary below. The proof proceeds identical as above since we only need the ratio between the width and the height of W , which is n/c in this case, to go to infinity.</p><formula xml:id="formula_98">corollary B.1. For all &gt; 0, lim n→∞ Pr W (2) W (2)T − I n ≥ = 0. (<label>64</label></formula><formula xml:id="formula_99">)</formula><p>Next we establish the approximate equivalence between the scatter matrix W (2)T W (2) and the projection matrix</p><formula xml:id="formula_100">P W (2) .</formula><p>lemma B.7. Let P W (2) be the projection matrix onto the row space of W (2) , then for all &gt; 0,</p><formula xml:id="formula_101">lim n→∞ Pr W (2)T W (2) − P W (2) 2 F &gt; = 0. (<label>65</label></formula><formula xml:id="formula_102">)</formula><p>Proof of Lemma B.7. For simplicity of notations, in this proof we will neglect the layer index superscript and use W to denote W (2) . Recall that W ∈ R n×c .</p><p>Fix ∈ (0, 1) without loss of generality. Let W i (i ∈ [c]) be the i-th row of W , and we will do the Gram-Schmidt process for the rows of W . Specifically, the Gram-Schmidt process is as following: Assume that the basis</p><formula xml:id="formula_103">{W i } k i=1 are already normalized, we set W k+1 W k+1 − k i=1 W k+1 , W i and W k+1 W k+1 / W k+1 . Finally,</formula><p>from the definition of projection matrix, we know that</p><formula xml:id="formula_104">P W = W T W . From Lemma B.3 we have for all i ∈ [c], lim n→∞ Pr | W i 2 − 1| ≥ = 0. (66) Let 2 / c 3 • 16 2c+1 , from Lemma B.5 we know that for all i, j ∈ [c], lim n→∞ Pr |W i W T j − δ i,j | ≥ = 0.<label>(67)</label></formula><p>Then we use induction to bound the difference between W and W . Specifically, we will show that for all</p><formula xml:id="formula_105">i ∈ [c], W i − W i ≤ 8 i .</formula><p>For simplicity of notations, in the following proof we will not repeat the probability argument and assume that for all i, j ∈</p><formula xml:id="formula_106">[c], |W i W T j − δ i,j | ≤ and for all i ∈ [c], | W i 2 − 1| ≤ .</formula><p>We will only use these inequalities finite times so applying a union bound will give the probability result.</p><p>For i = 1, we know that</p><formula xml:id="formula_107">W 1 = W 1 / W 1 and | W 1 − 1| ≤ , so W i − W i ≤ .</formula><p>If our inductive hypothesis holds for i ≤ k, then for i = k + 1, we have for all j ≤ k,</p><formula xml:id="formula_108">| W i , W j | ≤ | W i , W j | + | W i , W j − W j | ≤ + W i • W j − W j ≤ + (1 + )8 j ≤ (2 3j+1 + 1) .<label>(68)</label></formula><p>Therefore,</p><formula xml:id="formula_109">W i − W i ≤ j∈[k] | W i , W j | ≤ + j∈[k] (2 3j+1 + 1) ≤ (2 3k+2 − 1) ,<label>(69)</label></formula><p>and</p><formula xml:id="formula_110">| W i − 1| ≤ | W i − 1| + W i − W i ≤ 2 3k+2 .<label>(70)</label></formula><p>Thus,</p><formula xml:id="formula_111">W i − W i ≤ W i − W i + W i − W i ≤ | W i − 1| + W i − W i ≤ 8 k+1 ,<label>(71)</label></formula><p>which finishes the induction and implies that for all &gt; 0, for all i</p><formula xml:id="formula_112">∈ [c], W i − W i ≤ 8 i . Thus, W − W 2 F = i∈[c] W i − W i 2 ≤ c • 16 c . (<label>72</label></formula><formula xml:id="formula_113">)</formula><p>This means that</p><formula xml:id="formula_114">W T W − P W F = W T W − W T W F ≤ 2 W − W F W F + W − W 2 F ≤ 2c • √ c • 8 c √ + c • 16 c ≤ .<label>(73)</label></formula><p>For the final property of the weight matrices, we show that the maximum among all entry of the weight matrices are reasonably small with high probability.</p><p>lemma B.8. Fix any α &gt; 0, consider W ∈ R a×b for some b &gt; a 1+α such that each entry is sampled from a zero mean Gaussian N (0, 1 b ). The largest entry of W is reasonably small with high probability as b goes to infinity, namely,</p><formula xml:id="formula_115">lim b→∞ Pr max (i,j)∈[a]×[b] |W (2) ij | &gt; 2b − 1 3 = 0 (74)</formula><p>Proof of Lemma B.8. For i.i.d. random variables x 1 , . . . , x b ∼ N (0, 1), by concentration inequality on maximum of Gaussian random variables, for any t &gt; 0, we have</p><formula xml:id="formula_116">Pr max i∈[b] x i &gt; 2 log(2b) + t &lt; 2e − t 2 2 . (<label>75</label></formula><formula xml:id="formula_117">) For any i, j ∈ [a] × [b], since W ij are i.i.d. sampled from N (0, 1 b ), with rescaling of 1/ √ b we may substitute x j with W ij . It follows that Pr max (i,j)∈[a]×[b] W (2) ij &gt; 2 log(2ab) + t √ b &lt; 2e − t 2 2 . (<label>76</label></formula><formula xml:id="formula_118">) Taking t = b 1 6 , since a &lt; b, for large b we have 2 log(2ab) &lt; 2 log(2b 2 ) &lt; b 1 6</formula><p>. Thus for large b, Pr max</p><formula xml:id="formula_119">(i,j)∈[a]×[b] W ij &gt; 2b − 1 3 = Pr max (i,j)∈[a]×[b] W ij &gt; b 1 6 + b 1 6 √ n &lt; Pr max (i,j)∈[a]×[b] W ij &gt; 2 log(2b) + b 1 6 √ b &lt; 2e − b 1 3 2 . (<label>77</label></formula><formula xml:id="formula_120">)</formula><p>With the same argument, we have Pr min</p><formula xml:id="formula_121">(i,j)∈[a]×[b] W ij &lt; −2b − 1 3 &lt; 2e − b 1 3 2 . (<label>78</label></formula><formula xml:id="formula_122">)</formula><p>Passing b to infinity completes the proof.</p><p>From the above lemma, we can bound the maximum entry of W (1) and W (2) as follows:</p><p>corollary B.2. With probability 1 over W (1) and</p><formula xml:id="formula_123">W (2) , lim n→∞ Pr max (i,j)∈[n]×[d] |W (1) ij | &gt; 2d − 1 3 = 0, lim n→∞ Pr max (i,j)∈[c]×[n] |W (2) ij | &gt; 2n − 1 3 = 0.<label>(79)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.2 Approximate Independence Between Layer Inputs and Outputs</head><p>Let us first recall some definitions and notations of the inputs and outputs of layers. The input x follows the d-dimensional multivariate rectified Gaussian distribution with identity covariance for the pre-rectified Gaussian, namely x ∼ N R (0, I d ). The input propagates through the first layer to u W (1) x, and is multiplied element-wise by the ReLU activation to the input of the second layer y σ(u). Here we denote that activation of ReLU function by the random matrix D diag(I[u ≥ 0]) ∈ R n×n . Finally we get the logit output of the network z W (2) y.</p><p>The output Hessian of the last layer is</p><formula xml:id="formula_124">A = diag(p) − pp T ∈ R c×c .</formula><p>In this section we will show that when n goes to infinity, both y and z will converge in distribution to rectified Gaussian. Moreover, when we condition on two entries of x and two entries of y, the output Hessian A will be invariant in the limiting case. lemma B.9. When d → ∞, with probability 1 over W (1) ,</p><formula xml:id="formula_125">lim d→∞ y d − → N R 0, π − 1 2π I n .</formula><p>Proof of Lemma B.9. We will prove this lemma using the multivariate Lindeberg-Feller CLT. Given that x i 's are i.i.d. sampled from N R (0, 1) with bounded moments:</p><formula xml:id="formula_126">E[x i ] = 1 √ 2π , E[(x i − E[x i ]) 2 ] = π − 1 2π , E[(x i − E[x i ]) 4 ] = 6π 2 − 10π − 3 4π 2 &lt; 1. (80) For each i ∈ [d], let w (1) i ∈ R d denote the i-th column vector of W (1) . Let s i = w (1) i (x i − E[x i ]), then we have y = d i=1 w (1) i x i = d i=1 s i + d i=1 E[x i ]w (1) i = d i=1 s i + 1 √ 2π d i=1 w (1) i . (81) It follows that V ar[s i ] = V ar[w (1) i x i ] = π − 1 2π w (1) i w (1) T i . (<label>82</label></formula><formula xml:id="formula_127">) Let S = d i=1 V ar[s i ], S = π − 1 2π d i=1 w (1) i w (1) T i = π − 1 2π W (1) W (1)T . (83) As d → ∞, from Corollary B.1 we have W (1) W (1)T → I n in probability, therefore lim d→∞ S = π − 1 2π I n . (<label>84</label></formula><formula xml:id="formula_128">)</formula><p>We now verify the Lindeberg condition of independent random vectors {s 1 , . . . , s n }. First observe that the fourth moments of the s i 's are sufficiently small.</p><formula xml:id="formula_129">lim d→∞ d i=1 E s i 4 = lim d→∞ d i=1 E      n j=1 W (1) ji (x i − E[x i ]) 2   2    ≤ lim d→∞ d i=1 E   c 2 max j∈[n] W (1) ji 2 (x i − E[x i ]) 2 2   ≤ lim d→∞ c 2 max i∈[d],j∈[n] W (1) ji 4 d i=1 E (x i − E[x i ]) 4 . (<label>85</label></formula><formula xml:id="formula_130">) Since E[(x i − E[x i ]) 4 ] &lt; 1 and max i∈[d],j∈[n] |W (1) ji | &lt; 2d − 1 3 , with probability 1 over W (1) from Lemma B.8, it follows that lim d→∞ d i=1 E s i 4 ≤ c 2 lim d→∞ 2d − 1 3 4 d i=1 1 = c 2 lim d→∞ 16d − 4 3 n = 16c 2 lim d→∞ d − 1 3 = 0.<label>(86)</label></formula><p>For any &gt; 0, since s i &gt; in the domain of integration (when</p><formula xml:id="formula_131">I[ s i &gt; ]), lim d→∞ d i=1 E s i 2 I [ s i &gt; ] &lt; lim d→∞ d i=1 E s i 2 2 s i 2 I [ s i &gt; ] ≤ 1 2 lim d→∞ d i=1 E s i 4 = 0. (<label>87</label></formula><formula xml:id="formula_132">)</formula><p>As the Lindeberg Condition is satisfied, with lim d→∞ S = π−1 2π I n we have</p><formula xml:id="formula_133">lim d→∞ d i=1 s i d − → N 0, π − 1 2π I n .<label>(88)</label></formula><p>By Lemma B.1, we have lim d→∞ w</p><p>(1) i = − → 0 with probability 1 over W (1) , therefore plugging Eq. ( <ref type="formula" target="#formula_133">88</ref>) into Eq. ( <ref type="formula">81</ref>) we have lim</p><formula xml:id="formula_134">d→∞ y d − → N 0, π − 1 2π I n . (<label>89</label></formula><formula xml:id="formula_135">)</formula><p>Which completes the proof.</p><formula xml:id="formula_136">lemma B.10. lim n→∞ z d − → N (0, (π−1) 2 4π 2 I c ) with probability 1 over W (2) .</formula><p>Proof of Lemma B.10. The proof technique for z is identical to that of y. For completeness we will redo it for W (2) . From Lemma B.9, y i 's are i.i.d. from N R (0, π−1 2π ) with bounded moments:</p><formula xml:id="formula_137">E[y i ] = √ π − 1 2π , E[(y i − E[y i ]) 2 ] = (π − 1) 2 4π 2 , E[(y i − E[y i ]) 4 ] = (6π 2 − 10π − 3)(π − 1) 8π 3 &lt; 1. (<label>90</label></formula><formula xml:id="formula_138">)</formula><formula xml:id="formula_139">For each i ∈ [n], let w (2) i ∈ R c denote the i-th column vector of W (2) . Let v i = w (2) i (y i − E[y i ]), then we have z = n i=1 w (2) i y i = n i=1 v i + n i=1 E[y i ]w (2) i = n i=1 v i + √ π − 1 2π n i=1 w (2) i .<label>(91)</label></formula><p>It follows that</p><formula xml:id="formula_140">V ar[v i ] = V ar[w (2) i y i ] = (π − 1) 2 4π 2 w (2) i w (2)T i . (92) Let V = n i=1 V ar[v i ], V = (π − 1) 2 4π 2 n i=1 w (2) i w (2)T i = (π − 1) 2 4π 2 W (2) W (2)T . (93)</formula><p>As n → ∞, from Corollary B.1 we have W (2) W (2)T → I c in probability, therefore</p><formula xml:id="formula_141">lim n→∞ V = (π − 1) 2 4π 2 I c . (<label>94</label></formula><formula xml:id="formula_142">)</formula><p>We now verify the Lindeberg condition of independent random vectors {v 1 , . . . , v n }. First observe that the fourth moments of the v i 's are sufficiently small.</p><formula xml:id="formula_143">lim n→∞ n i=1 E v i 4 = lim n→∞ n i=1 E      c j=1 W (2) ji (y i − E[y i ]) 2   2    ≤ lim n→∞ n i=1 E   c 2 max j∈[c] W (2) ji 2 (y i − E[y i ]) 2 2   ≤ lim n→∞ c 2 max i∈[n],j∈[c] W (2) ji 4 n i=1 E (y i − E[y i ]) 4 . (<label>95</label></formula><formula xml:id="formula_144">) Since E[(y i − E[y i ]) 4 ] &lt; 1 and max i∈[n],j∈[c] |W (2) ji | &lt; 2n − 1 3 with probability 1 from Corollary B.2, it follows that lim n→∞ n i=1 E v i 4 ≤ c 2 lim n→∞ 2n − 1 3 4 n i=1 1 = c 2 lim n→∞ 16n − 4 3 n = 16c 2 lim n→∞ n − 1 3 = 0.<label>(96)</label></formula><p>For any &gt; 0, since v i &gt; in the domain of integration (when</p><formula xml:id="formula_145">I[ v i &gt; ]), lim n→∞ n i=1 E v i 2 I [ v i &gt; ] &lt; lim n→∞ n i=1 E v i 2 2 v i 2 I [ v i &gt; ] ≤ 1 2 lim n→∞ n i=1 E v i 4 = 0. (<label>97</label></formula><formula xml:id="formula_146">)</formula><p>As the Lindeberg Condition is satisfied, with lim n→∞ V = (π−1) 2 4π 2 I c we have</p><formula xml:id="formula_147">lim n→∞ n i=1 v i d − → N 0, (π − 1) 2 4π 2 I c .<label>(98)</label></formula><p>By Lemma B.1, we have lim n→∞ w</p><p>(2) i = − → 0 with probability 1 over W (2) , therefore plugging Eq. ( <ref type="formula" target="#formula_147">98</ref>) into Eq. ( <ref type="formula" target="#formula_139">91</ref>) we have</p><formula xml:id="formula_148">lim n→∞ z d − → N 0, (π − 1) 2 4π 2 I c .<label>(99)</label></formula><p>Which completes the proof. Now we will show a key lemma for proving the main theorem, which suggests that when reasonably conditioning on two entries of the input x and two entries of the activation D, the distribution of z converges in distribution to z without conditioning as n → ∞.</p><p>lemma B.11. With probability 1 over W (1) and W (2) , fix any β &lt; α 2 (recall that d = n 1+α ), fix any a, b ∈ (−n β , n β ), for any p, q ∈ [n] and k, l ∈ [d], we have the following convergence in distribution</p><formula xml:id="formula_149">z|(D pp = 1, D qq = 1, x k = a, x l = b) d − → z. (<label>100</label></formula><formula xml:id="formula_150">)</formula><p>Proof of Lemma B.11. For simplicity of notation, we will use subscript | x and | D to denote the conditions we impose. For example, we will denote z|(D pp = 1, D qq = 1, x k = a, x l = b) by z| D,x , and denote x|(x k = a, x l = b) by x| x etc.</p><p>First claim that with probability 1 over W (1) , u is invariant upon the conditioning on x. Let e (i) ∈ R d be the standard basis vector such that e (i)</p><formula xml:id="formula_151">j = I[i = j]. Then u − u| x = W (1) x − W (1) x| x = W (1) (x − x| x ) = W (1) ((x k − a)e (k) + (x l − b)e (l) ) = w (1) k |x k − a| + w (1) l |x l − b| ≤ 5n − α 2 (|x k | + |x l | + |a| + |b|) ≤ 5n − α 2 (x k + x l ) + 10n − α 2 n β 2 . (<label>101</label></formula><formula xml:id="formula_152">)</formula><p>The norms of w Note that conditioning on D pp = D qq = 1 is equivalent to conditioning on u p &gt; 0 and u q &gt; 0. Which is again equivalent to conditioning on y p and y q to be a half Gaussian distribution truncated at 0 instead of the rectified Gaussian. Recall that z = W (2) y = n i=1 w</p><p>(2) i y i . Since only y p and y q are affected by conditioning on D, we have</p><formula xml:id="formula_153">z − z| D = n i=1 w (2) i y i − n i=1 w (2) i (y| D ) i = w (2) p (y p − (y| D ) p ) + w (2) q (y q − (y| D ) q ) ≤ w (2) p |y p − (y| D ) p | + w (2) q |y q − (y| D ) q |.</formula><p>(102) Note that y p − (y| D ) p and y q − (y| D ) q are difference between a rectified Gaussian with finite variance and its corresponding truncated Gaussian, both are of bounded expectation and variance. Meanwhile, by Corollary B.2, for all i ∈ [n] we have that with probability 1 over</p><formula xml:id="formula_154">W (2) , w (2) i ≤ c max i∈[c],j∈[n] W (2) ij 2 &lt; 4cn − 2 3 .<label>(103)</label></formula><p>Since lim n→∞ 4cn − 2 3 = 0, as n goes to infinity we have</p><formula xml:id="formula_155">w (2) p |y p − (y| D ) p | + w (2) q |y q − (y| D ) q | d − → − → 0 .<label>(104)</label></formula><p>Therefore z| D d − → z, and hence</p><formula xml:id="formula_156">z|(D pp = 1, D qq = 1, x k = a, x l = b) d − → z. (<label>105</label></formula><formula xml:id="formula_157">)</formula><p>Given that p = softmax(z) and A = diag(p) − pp T , the mapping from z to A is bounded and continuous. Thus by the Portmanteau Theorem, we have the following corollary, corollary B.3. For any &gt; 0, with probability 1 over W (1) and</p><formula xml:id="formula_158">W (2) , fix any β &lt; α 2 (recall that d = n 1+α ), fix any a, b ∈ (−n β , n β ), for any p, q ∈ [n], k, l ∈ [d], and i, j ∈ [c], we have |E [A ij |(D pp = 1, D qq = 1, x k = a, x l = b)] − E [A ij ] | &lt; . (<label>106</label></formula><formula xml:id="formula_159">)</formula><p>By the proof of Lemma B.11, this property holds when dropping the conditioning on D or x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.3 Structure of A</head><p>In this section we will analyze properties of the second output Hessian A, which, despite being a R c×c "small" matrix, provides many important properties to the first output Hessian and the full layer-wise Hessians. lemma B.12. With probability 1 over W (1) and (2) , A lim n→∞ E[A] exist and is rank-(c − 1) .</p><p>Proof of Lemma B.12. Note that each entry of A is a quadratic function of p, and p is a continuous function of z. Therefore, we consider A as a function of z and write A(z) when necessary. From Lemma B.10 we know that lim n→∞ z follows a standard normal distribution N (0, γI c ) with probability 1 over W (1) and W (2) , where γ is some absolute constant. Therefore,</p><formula xml:id="formula_160">A lim n→∞ E[A] exist and it equals E[A(lim n→∞ z)] = E z∼N (0,γIc) [A(z)].</formula><p>For simplicity of notations, we will omit the statement "with probability 1 over W (1) and W (2) " when there is no confusion.</p><p>From the definition of A we know that A diag(p) − pp T where p is the vector obtained by applying softmax to z, so c i=1 p i = 1 and for all i ∈ [c], p i ∈ (0, 1). Therefore, for any vector p satisfying the previous conditions, we have</p><formula xml:id="formula_161">1 T A1 = c i=1   p i − c j=1 p i p j   = c i=1 (p i − p i ) = 0, (<label>107</label></formula><formula xml:id="formula_162">)</formula><p>where 1 is the all-one vector. Therefore, we know that A has an eigenvalue 0 with eigenvector c − 1 2 1. This means that E[A] also has an eigenvalue 0 with eigenvector c − 1 2 1. Thus, E[A] is at most of rank (c − 1).</p><p>Then we analyze the other</p><formula xml:id="formula_163">(c − 1) eigenvalues of A. Since A = QQ T where Q = diag( √ p)(I c − 1p T ), we know that A is always a positive semi-definite (PSD) matrix, which indicates that E[A] must also be PSD. Assume the c eigenvalues of A are λ 1 ≥ λ 2 ≥ • • • ≥ λ c−1 ≥ λ c = 0.</formula><p>Therefore, by definition, we have</p><formula xml:id="formula_164">λ c−1 = min v∈S, v =1 v T Av = E z∼N (0,γIc) min v∈S, v =1 v T Av ,<label>(108)</label></formula><p>where S R c \R{1 T } is the orthogonal subspace of the span of</p><formula xml:id="formula_165">1. v ∈ S implies that v ⊥ 1, i.e., c i=1 v i = 0. Direct computation gives us v T Av = c i=1 v 2 i p i − c i=1 v i p i 2 . (<label>109</label></formula><formula xml:id="formula_166">) Define two vectors a, b ∈ R c as for all i ∈ [c], with a i v i √ p i , b i √ p i , then b 2 = c i=1 p i = 1 and v T Av = a 2 − a, b 2 = a 2 • b 2 − a, b 2 . (110) Therefore, v T Av ≥ a 2 b 2 sin 2 θ(a, b),<label>(111)</label></formula><p>where θ(a, b) is the angle between a and b, i.e., θ(a, b) arccos a,b a b . Define p 0 min i∈[c] p i , then</p><formula xml:id="formula_167">a 2 = c i=1 v 2 i p i ≥ c i=1 v 2 i p 0 = p 0 v 2 = p 0 . (<label>112</label></formula><formula xml:id="formula_168">)</formula><p>Since b = 1, we have</p><formula xml:id="formula_169">sin 2 θ(a, b) = a − a, b • b 2 a 2 . (113) Besides, a − a, b • b 2 = c i=1   v i √ p i −   c j=1 v j p j   √ p i   2 = c i=1 p i   v i − c j=1 v j p j   2 ≥ p 0 c i=1   v i − c j=1 v j p j   2 . (<label>114</label></formula><formula xml:id="formula_170">) Define s arg max i∈[c] v i and t arg min i∈[c] v i , then c i=1   v i − c j=1 v j p j   2 ≥   v s − c j=1 v j p j   2 +   v t − c j=1 v j p j   2 ≥ (v s − v t ) 2 2 . (<label>115</label></formula><formula xml:id="formula_171">) From v = 1 we know that max i∈[c] |v i | ≥ c − 1 2 . Besides, since c i=1 v i = 0, we have v s &gt; 0 &gt; v t . Therefore, v s − v t &gt; max i∈[c] |v i | ≥ c − 1 2 . As a result, a − a, b • b 2 ≥ p 0 • (v s − v t ) 2 2 &gt; p 0 2c . (<label>116</label></formula><formula xml:id="formula_172">)</formula><p>Moreover,</p><formula xml:id="formula_173">a 2 = c i=1 v 2 i p i ≤ c i=1 p i = 1. (<label>117</label></formula><formula xml:id="formula_174">)</formula><p>Thus,</p><formula xml:id="formula_175">sin 2 θ(a, b) ≥ p0 2c 1 = p 0 2c ,<label>(118)</label></formula><p>which means that</p><formula xml:id="formula_176">v T Av ≥ p 0 • 1 • p 0 2c = p 2 0 2c . (<label>119</label></formula><formula xml:id="formula_177">)</formula><p>Now we analyze the distribution of p 0 . Since z follows a spherical Gaussian distribution N (0, γI c ), we know that the entries of z are totally independent. Besides, for each entry z i (i ∈ [c]), we have |z i | &lt; γ with probability ξ, where ξ ≈ 0.68 is an absolute constant. Therefore, with probability ξ c , forall entries</p><formula xml:id="formula_178">z i (i ∈ [c]), we have |z i | &lt; γ.</formula><p>In this case,</p><formula xml:id="formula_179">p 0 = exp(min i∈[c] z i ) c i=1 exp(z i ) ≥ exp(−γ) c exp(γ) .<label>(120)</label></formula><p>In other cases, we know that p 0 &gt; 0. Thus,</p><formula xml:id="formula_180">λ c−1 = E z∼N (0,γIc) min v∈S, v =1 v T Av ≥ ξ c • exp(−γ) c exp(γ) 2 2c .<label>(121)</label></formula><p>The right hand side is independent of n. Therefore, λ c−1 &gt; 0, which means that A has exactly (c − 1) positive eigenvalues and a 0 eigenvalue, and the eigenvalue gap between the smallest positive eigenvalue and 0 is independent of n.</p><p>Hence we complete the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.4 Projecting Hessians onto Finite Dimensions</head><p>In this section we will develop some technical tools for analyzing the eigenvalues and eigenvectors of the output Hessians and the full layer-wise Hessians. In particular, we will project both infinite dimensional matrices to c × c matrices.</p><p>First, we prove a technical lemma that will be very useful when we bound the Frobenius norm of the difference between infinite size matrices. lemma B.13. Let p(A, D, x) be a homogeneous polynomial of A, D, and x and is degree 1 in A, degree 2 in D, and degree 2 in x. Suppose the coefficients in p are upper bounded in 1 -norm by an absolute constant µ. Also let D be an independent copy of D and x be an independent copy of x independent to D and A. Then with probability 1 over W (1) and W (2) , we have</p><formula xml:id="formula_181">lim n→∞ E [p(A, D, x)] = E [p(A, D , x )]<label>(122)</label></formula><p>Proof of Lemma B.13. Fix any &gt; 0. Assume that the homogeneous polynomial is of the form</p><formula xml:id="formula_182">p(A, D, x) = m i=1 c i A s(i),t(i) D u(i),u(i) D v(i),v(i) x p(i) x q(i) ,<label>(123)</label></formula><p>for coefficients c i , then from linearity of expectation we know</p><formula xml:id="formula_183">E[p(A, D, x)] = m i=1 c i E[A s(i),t(i) D u(i),u(i) D v(i),v(i) x p(i) x q(i) ].<label>(124)</label></formula><p>Hence</p><formula xml:id="formula_184">|E [p(A, D, x)] − E [p(A, D , x )] | ≤ m i=1 c i |E[A s(i),t(i) D u(i),u(i) D v(i),v(i) x p(i) x q(i) ] − E[A s(i),t(i) D u(i),u(i) D v(i),v(i) x p(i) x q(i) ]|<label>(125)</label></formula><p>Since the entries of D can only be 0 or 1, we have</p><formula xml:id="formula_185">E[A s(i),t(i) D u(i),u(i) D v(i),v(i) x p(i) x q(i) ] = Pr D u(i),u(i) = D v(i),v(i) = 1 E[A s(i),t(i) x p(i) x q(i) |D u(i),u(i) = D v(i),v(i) = 1] = 1 4 E[A s(i),t(i) x p(i) x q(i) |D u(i),u(i) = D v(i),v(i) = 1].<label>(126)</label></formula><p>The last equality holds since u converges in distribution to a spherical Gaussian, and its entry-wise activations D follows a p = 1 2 Bernoulli distribution. Assume m i=1 |c i | ≥ µ, that the 1 norm of the coefficients is upper bounded by some constant µ. Set = µ . To prove this lemma it is sufficient to prove that each term of the polynomial are sufficiently small, namely, for any index,</p><formula xml:id="formula_186">|E[A s(i),t(i) x p(i) x q(i) |D u(i),u(i) = D v(i),v(i) = 1] − E[A s(i),t(i) x p(i) x q(i) |D u(i),u(i) = D v(i),v(i) = 1]| |E[A s(i),t(i) x p(i) x q(i) |D u(i),u(i) = D v(i),v(i) = 1] − E[A s(i),t(i) x p(i) x q(i) ]| &lt; . (<label>127</label></formula><formula xml:id="formula_187">)</formula><p>Fix a set of index s, t, p, q, u, v, for simplicity of notation, we use the abbreviation</p><formula xml:id="formula_188">E [A st x p x q | D ] to denote E[A s(i),t(i) x p(i) x q(i) |D u(i),u(i) = D v(i),v(i) = 1].</formula><p>Since x is of rectified Gaussian with the covariance of the initial Gaussian distribution being the identity, x p and x q shares the same density function when x &gt; 0, namely</p><formula xml:id="formula_189">f (x) = 1 √ 2π exp(−x 2 /2). Note that R + ×R + xy f (x)f (y) dx dy = E [x i x j ] = E [x i ] E [x j ] = 1 2π . (<label>128</label></formula><formula xml:id="formula_190">)</formula><p>Fix some β &lt; α 2 , we have</p><formula xml:id="formula_191">|E [A st x p x q | D ] − E A st x p x q | = R + ×R + E A st | D,xp=x,xq=y xy f (x)f (y) dx dy − R + ×R + E [A st ] xy f (x)f (y) dx dy ≤ R + ×R + |E A st | D,xp=x,xq=y − E [A st ] |xy f (x)f (y) dx dy = [0,n β ]×[0,n β ] |E A st | D,xp=x,xq=y − E [A st ] |xy f (x)f (y) dx dy + R + ×R + \([0,n β ]×[0,n β ]) |E A st | D,xp=x,xq=y − E [A st ] |xy f (x)f (y) dx dy. (129)</formula><p>From Corollary B.3 we have, for any indices s, t, for sufficiently large n, for any (x, y)</p><formula xml:id="formula_192">∈ [0, n β ] × [0, n β ], |E A st | D,xp=x,xq=y − E [A st ] | &lt; (130) Thus [0,n β ]×[0,n β ] |E A st | D,xp=x,xq=y − E [A st ] |xy f (x)f (y) dx dy ≤ [0,n β ]×[0,n β ] xy f (x)f (y) dx dy = 2π . (<label>131</label></formula><formula xml:id="formula_193">)</formula><p>Now we consider the other integral. First note that since A st is either p i − p 2 i or −p i p j for some i, j, and p i , p j , p i + p j ∈ (0, 1) as it is the output of the softmax function, we have</p><formula xml:id="formula_194">A st ∈ (− 1 4 , 1 4 ). It follows that |E A st | D,xp=x,xq=y − E [A st ] | ≤ 1 2 . Therefore R + ×R + \([0,n β ]×[0,n β ]) |E A st | D,xp=x,xq=y − E [A st ] |xy f (x)f (y) dx dy ≤ 1 2 R + ×R + \([0,n β ]×[0,n β ]) xy e −x 2 /2 √ 2π e −y 2 /2 √ 2π dx dy ≤ 1 2 • 1 2π ∞ n β e −x 2 /2 x dx R + e −y 2 /2 y dy + 1 2 • 1 2π R + e −x 2 /2 x dx ∞ n β e −y 2 /2 y dy = 1 2π e −n 2β ,<label>(132)</label></formula><p>which decreases below /2 for sufficiently large n. As both terms in Eq. ( <ref type="formula">129</ref>) are less than /2 as n → ∞, we have</p><formula xml:id="formula_195">|E [A st x p x q | D ] − E A st x p x q | &lt; .</formula><p>Which completes the proof of this lemma.</p><p>We then generalize this lemma for a degree ten homogeneous polynomial, in which the monomials are roughly multiplied with an independent copy of itself (except for A).</p><p>corollary B.4. Let p(A, D, x, Ā, D, x) be a homogeneous polynomial of A, D, x, Ā, D, and x. Let it be degree 1 in A, Ā, degree 2 in D, D, and degree 2 in x,x. Suppose the coefficients in p are upper bounded in 1 -norm by an absolute constant µ. Also let D be an independent copy of D and x be an independent copy of x independent to D and A. Morever let ( Ā, D, x, D , x ) be an independent copy of (A, D, x, D , x ). Then with probability 1 over W (1) and W (2) , we have</p><formula xml:id="formula_196">lim n→∞ E p(A, D, x, Ā, D, x) = E p(A, D , x , Ā, D , x ) .<label>(133)</label></formula><p>Proof of Corollary B.4. For simplicity of notations, denote</p><formula xml:id="formula_197">s ijuvrs = A ij D vv D ww x r x s , s ijuvrs = A ij D vv D ww x r x s .</formula><p>Similarly, denote t klpqtu = Ākl Dpp Dqq xt xu and t klpqtu = Ākl D pp D qq x t x u . As there is no confusion on indexing, we will also omit the subscripts and use s, t.</p><p>Fix any &gt; 0, Following the argument of the proof of Lemma B.13, it is sufficient to prove this corollary by showing for any indexing,</p><formula xml:id="formula_198">|E[A ij Ākl D vv D ww Dpp Dqq x r x s xt xu ] − E[A ij Ākl D vv D ww D pp D qq x r x s x t x u ]| = |E[st] − E[s t ]| &lt; µ . (<label>134</label></formula><formula xml:id="formula_199">)</formula><p>First note that since |A ij | &lt; 1 4 and |D ii | ≤ 1 for all i, j, we have</p><formula xml:id="formula_200">|E[s]| = |E[A ij D vv D ww x r x s ]| ≤ 1 4 |E[x r x s ]| = 1 8π . (<label>135</label></formula><formula xml:id="formula_201">)</formula><p>The same argument also applies to s , t, and t . Also, by Lemma B.13, for sufficiently large n we have</p><formula xml:id="formula_202">|E[s]−E[s ]| &lt; and |E[t] − E[t ]| &lt; .</formula><p>Since by construction s and t are independent, we have</p><formula xml:id="formula_203">|E[st] − E[s t ]| = |E[s]E[t] − E[s ]E[t ]| = |E[s]E[t] − E[s]E[t ] + E[s]E[t ] − E[s ]E[t ]| ≤ |E[s]||E[t] − E[t ]| + |E[t ]||E[s] − E[s ]| ≤ 1 8π + 1 8π &lt; ,<label>(136)</label></formula><p>which completes the proof of Corollary B.4. Now we formally begin our analysis. We will start from M (1) = E DW (2)T AW (2) D , the output Hessian of the first layer. The output Hessian of the second layer is just E [A], which had been analyzed in Appendix B.2.3. In this section we will neglect the superscript for M (1) and use M as there is no confusion. Also, we use W to denote W (2) unless specified otherwise. We first state our main lemma of projecting M . lemma B.14. With probability 1 over W (1) and W (2) ,</p><formula xml:id="formula_204">lim n→∞ W M W T 2 F M 2 F = 1. (<label>137</label></formula><formula xml:id="formula_205">)</formula><p>Proof of Lemma B.14. To prove the equivalence between W M W T 2 F and M 2 F , we need to introduce a bridging term</p><formula xml:id="formula_206">M * E[D W (2)T AW (2) D ] (<label>138</label></formula><formula xml:id="formula_207">)</formula><p>where D is an independent copy of D and also independent of A. Essentially M * is the matrix which has the same expression as M except that we assume D is independent of A in M * . Informally, the proof strategy of Lemma B.14 is</p><formula xml:id="formula_208">W M W T 2 F ≈ W M * W T 2 F ≈ M * 2 F ≈ M 2 F .<label>(139)</label></formula><p>We now formally establish this equivalence.</p><p>Then we look into the structures of the bridging matrix M * . It is simple to analyze as we assumed the independence between A and D . Formally, lemma B.15. With probability 1 over W (1) and W (2) ,</p><formula xml:id="formula_209">M * = 1 4 W T E[A]W + diag(W T E[A]W ) .<label>(140)</label></formula><p>Moreover, M * and M * 2 F are bounded below by some nonzero constant and bounded above by some constant.</p><p>Proof of Lemma B.15. First note that since D is the activation of u , which converges to a spherical Gaussian with probability 1 over W (1) and is independent with A, each diagonal entry of D is a Bernoulli random variable with p = 1 2 . For i, j ∈ [n], when i = j, we have</p><formula xml:id="formula_210">M * ij = E[D ii (W T AW ) ij D jj ] = E[D ii ]E[D jj ]E[(W T AW ) ij ] = 1 4 (W T E[A]W ) ij .<label>(141)</label></formula><p>When i = j,</p><formula xml:id="formula_211">M * i,i = E[D ii (W T AW ) ii D ii ] = E[D ii ]E[(W T AW ) ii ] = 1 2 (W T E[A]W ) i,j .<label>(142)</label></formula><p>Thus</p><formula xml:id="formula_212">M * = 1 4 W T E[A]W + diag(W T E[A]W ) . (<label>143</label></formula><formula xml:id="formula_213">)</formula><p>Now we show the lower bound and upper bound on norms of M * .</p><p>Since</p><formula xml:id="formula_214">E[W T AW ], diag(E[W T AW ]) ≥ 0, we have M * F ≥ E[W T AW ] F = W T AW F .<label>(144)</label></formula><p>Since W W T converges to I c in spectral norm from Lemma B.5, we have for sufficiently large n, the smallest singular value of W is larger than 1 2 . Moreover, since E[A] admits an eigenvalue that is bounded below by some constants η ξ c • exp(−γ) c exp(γ) 2 /2c where ξ ≈ 0.68 is an absolute constant and γ = (π−1) 2 4π 2 as shown in Lemma B.12, there exists an eigenvalue of M * = W T E[A]W that is larger than η 4 . Hence for large n, M * is bounded from below by η 4 , and hence M * 2 F .</p><p>Besides, since D is a diagonal matrix with 0/1 entries, and the absolute value of each entry of A is bounded by 1, we have</p><formula xml:id="formula_215">M F = E[DW T AW D] F ≤ E[W T AW ] F ≤ W 2 F A F ≤ c W 2 F .<label>(145)</label></formula><p>From Lemma B.3, we know that with probability 1, W 2 F ≤ 2c, therefore, M F is upper bounded by 2c 2 , which is independent of n. lemma B.16. With probability 1 over W (1) and W (2) ,</p><formula xml:id="formula_216">lim n→∞ M 2 F M * 2 F = 1. Proof of Lemma B.16. Recall that M * E[D W (2)T AW (2) D ]</formula><p>where D is an independent copy of D and also independent of A.</p><p>Since we will only explicitly use W (2) in this proof, for simplicity of notation, we will omit its superscript and use W . Let ( D, Ā) be an independent copy of (D, A), then</p><formula xml:id="formula_217">M 2 F = E[DW T AW D] 2 F = E DW T AW D, DW T ĀW D = E tr DW T AW D DW T ĀW D = E tr W DDW T AW D DW T Ā . (<label>146</label></formula><formula xml:id="formula_218">)</formula><p>Expressing the term inside the expectation as a polynomial of entries of A, D, Ā and D, we get</p><formula xml:id="formula_219">tr W DDW T AW D DW T Ā = c i=1 W DDW T AW D DW T Ā i,i = c i,j=1 W DDW T A i,j W D DW T Ā j,i = c i,j=1 c k=1 n l=1 W i,l W k,l D l,l D l,l A k,j c s=1 n t=1 W j,t W s,t Dt,t Dt,t A s,i = c i,j,k,s=1 n l,t=1 W i,l W k,l W j,t W s,t Āk,j A s,i Dl,l D l,l Dt,t D t,t .<label>(147)</label></formula><p>The monomials are Āk,j A s,i Dl,l D l,l Dt,t D t,t , and the corresponding coefficients are W i,l W k,l W j,t W s,t . Now we can bound the 1 norm of the coefficient of this polynomial as follows:</p><formula xml:id="formula_220">c i,j,k,s=1 n l,t=1 W i,l W k,l W j,t W s,t 1 ≤ c i,j,k,s=1 n l,t=1 |W i,l | • |W k,l | • |W j,t | • |W s,t | =   c i,k=1 n l=1 |W i,l | • |W k,l |     c j,s=1 n t=1 |W j,t | • |W s,t |   ≤   c i,k=1 n l=1 W 2 i,l + W 2 k,l 2     c j,s=1 n t=1 W 2 j,t + W 2 s,t 2   =   c i,k=1 W i 2 + W k 2 2     c j,s=1 W j 2 + W s 2 2   =(c W 2 F ) 2 = c 2 W 4 F .<label>(148)</label></formula><p>From Lemma B.3 we know that W 2 F = O(c) with probability 1 over W , so the coefficient of this polynomial is 1 -norm bounded. </p><formula xml:id="formula_221">lim n→∞ M 2 F / M * 2 F = 1. lemma B.17. For all i, j ∈ [c], lim n→∞ ((W M W T ) i,j − (W M * W T ) i,j ) = 0. Thus, lim n→∞ W M W T 2 F W M * W T 2 F = 1.</formula><p>Proof of Lemma B.17. This proof is very similar to that of Lemma B.16. First, we focus on a single entry of the matrix W M W T and express it as a polynomial of entries of A and D:</p><formula xml:id="formula_222">(W M W T ) i,j = E[(W DW T AW DW T ) i,j ] = E c k=1 (W DW T A) i,k (W DW T ) k,j = E c k=1 c s=1 n l=1 W i,l W s,l D l,l A s,k n t=1 W k,j W j,t D t,t = E   c k,s=1 n l,t=1 W i,l W s,l W k,t W j,t A s,k D l,l D t,t   . (<label>149</label></formula><formula xml:id="formula_223">)</formula><p>Then we bound the 1 norm of the coefficients of this polynomial as follows:</p><formula xml:id="formula_224">c k,s=1 n l,t=1 W i,l W s,l W k,t W j,t 1 ≤ c k,s=1 n l,t=1 |W i,l | • |W s,l | • |W k,t | • |W j,t | = c s=1 n l=1 |W i,l | • |W s,l | c k=1 n t=1 |W k,t | • |W j,t | ≤ c s=1 n l=1 W 2 i,l + W 2 s,l 2 c k=1 n t=1 W 2 k,t + W 2 j,t 2 = c W i 2 + W 2 F c W j 2 + W 2 F ≤(2c W 2 F ) 2 = 4c 2 W 4 F .<label>(150)</label></formula><p>Similar to Lemma B.16, this coefficient is 1 -norm bounded. The expression of each entry of W M * W T is just substituting D, D by D , D in the polynomial characterized by Eq. ( <ref type="formula" target="#formula_222">149</ref>). Therefore, using Lemma B.13, we have with probability 1 over W , for all i, j ∈ [c],</p><formula xml:id="formula_225">lim n→∞ ((W M W T ) i,j − (W M * W T ) i,j ) = 0. (<label>151</label></formula><formula xml:id="formula_226">)</formula><p>This completes the proof of the lemma as W M W T is of constant size.</p><p>lemma B.18. With probability 1 over W (1) and W (2) ,</p><formula xml:id="formula_227">lim n→∞ W M * W T 2 F M * 2 F = 1. (<label>152</label></formula><formula xml:id="formula_228">)</formula><p>Proof of Lemma B.18. The proof of this lemma will be divided into two parts. In the first part, we will estimate the Frobenius norm of M * , and in the second part we do the same thing for W M * W T .</p><p>Part 1: From Lemma B.15 we know that</p><formula xml:id="formula_229">M * = 1 4 W T E[A]W + diag(W T E[A]W ) . (<label>153</label></formula><formula xml:id="formula_230">) Denote A E[A], then E[W T AW ] = W T E[A]W = W T AW .<label>(154)</label></formula><p>From Lemma B.5, for all &gt; 0, with probability 1 over W we have W W T − I c ≤ . Besides, from <ref type="bibr" target="#b19">Kleinman &amp; Athans (1968)</ref> we know that for positive semi-definite matrices A and B we have λ min (A) tr(B) ≤ tr(AB) ≤ λ max (A) tr(B), so</p><formula xml:id="formula_231">W T AW 2 F − A 2 F = tr(W T AW W T AW ) − tr( A A) = tr(W W T AW W T A) − tr( A A) ≤ W W T − I c + 1 tr( AW W T A) − tr( A A) = W W T − I c + 1 tr(W W T A A) − tr( A A) ≤ W W T − I c + 1 2 tr( A A) − tr( A A) ≤ W W T − I c 2 A 2 F + 2 W W T − I c A 2 F .<label>(155)</label></formula><p>For any &gt; 0, set = min{ 4 , √ 2 } gives us with probability 1,</p><formula xml:id="formula_232">lim n→∞ W T AW 2 F − A 2 F A 2 F = 0,<label>(156)</label></formula><p>i.e.,</p><formula xml:id="formula_233">lim n→∞ W T AW 2 F A 2 F = 1.<label>(157)</label></formula><p>Besides, if we denote the i-th column of W by w i , then</p><formula xml:id="formula_234">diag(E[W T AW ]) 2 F = n i=1 (w T i Aw i ) 2 ≤ n i=1 w i 2 • A 2 = A 2 n i=1 w i 4 .<label>(158)</label></formula><p>Since E[n 2 w i 4 ] = c 2 + 2c, by the additive form of Chernoff bound we get</p><formula xml:id="formula_235">Pr n i=1 w i 4 ≥ c 2 + 3c n = Pr n i=1 n 2 w i 4 n − (c 2 + 2c) ≥ c ≤ e −2nc 2 . (<label>159</label></formula><formula xml:id="formula_236">)</formula><p>Therefore, when n → ∞, with probability 1 over W we have</p><formula xml:id="formula_237">diag(E[W T AW ]) 2 F ≤ A 2 n i=1 w i 4 ≤ A 2 • c 2 + 3c n .<label>(160)</label></formula><p>Thus, with probability 1 over W,</p><formula xml:id="formula_238">lim n→∞ diag E W T AW 2 F W T AW 2 F = 0,<label>(161)</label></formula><p>i.e.,</p><formula xml:id="formula_239">lim n→∞ 1 16 A 2 F M * 2 F = 1. (<label>162</label></formula><formula xml:id="formula_240">)</formula><p>Part 2: We now estimate the norm of W M * W . Plug equation Eq. ( <ref type="formula" target="#formula_209">140</ref>) into W M * W and we get</p><formula xml:id="formula_241">W M * W = 1 4 E[W W T AW W T ] + E[W diag(W T AW )W T ] .<label>(163)</label></formula><p>Similar to Part 1, when n → ∞, with probability 1, we have</p><formula xml:id="formula_242">lim n→∞ E[W W T AW W T ] 2 F A 2 F = 1.<label>(164)</label></formula><p>Besides, when n → ∞, with probability 1 we have</p><formula xml:id="formula_243">W diag(E[W T AW ])W T 2 F ≤ W 2 F A 2 n i=1 w i 4 ≤ A 2 • c 2 + 3c n W 2 F .<label>(165)</label></formula><p>As a result, with probability 1,</p><formula xml:id="formula_244">lim n→∞ W diag(E[W T AW ])W T 2 F W W T AW W T 2 F = 0,<label>(166)</label></formula><p>i.e.,</p><formula xml:id="formula_245">lim n→∞ 1 16 A 2 F W M * W T 2 F = 1. (<label>167</label></formula><formula xml:id="formula_246">)</formula><p>Combining the results of Part 1 and Part 2 proves this lemma.</p><p>Combining Lemma B.16, Lemma B.17, and Lemma B.18 directly finishes the proof of Lemma B.14.</p><p>After establishing the projection of M onto a c × c matrix, we may project the full layer-wise Hessian of the first layer, namely </p><formula xml:id="formula_247">H (1) = E[DW (2)T AW (2) D ⊗ xx T ]</formula><formula xml:id="formula_248">(2) , lim n→∞ V HV T 2 F H 2 F = 1.<label>(168)</label></formula><p>Proof of Lemma B.19. Similar to the proof for the output Hessian, we will introduce a "bridging term"</p><formula xml:id="formula_249">H * 1 d E[D W (2)T AW (2) D ⊗ x x T ] (<label>169</label></formula><formula xml:id="formula_250">)</formula><p>where D is an independent copy of D and also independent of A, and x is an independent copy of x which is independent to both D and A. Informally, we will show</p><formula xml:id="formula_251">V HV T 2 F ≈ V H * V T 2 F ≈ H * 2 F ≈ H 2 F .<label>(170)</label></formula><p>We first look into the structures of H * . lemma B.20. With probability 1 over W (1) and W (2) ,</p><formula xml:id="formula_252">H * = 1 4d W T E[A]W + diag(W T E[A]W ) ⊗ 1 2π 1 d 1 T d + π − 1 2π I d .<label>(171)</label></formula><p>Moreover, for large n, η/32 &lt; H * F &lt; 2c 2 .</p><p>Proof of Lemma B.20. By independence in construction, we have</p><formula xml:id="formula_253">H * = M * ⊗( 1 d E[x x T ]). Thus we only need to look into E[x x T ]. For i = j, we have E[x x T ] ii = E[x i x i ] = 1 2 while for i = j, E[x x T ] ij = E[x i x j ] = 1 2π . Thus E[x x T ] = 1 2π 1 d 1 T d + π − 1 2π I d .<label>(172)</label></formula><p>It follows that</p><formula xml:id="formula_254">lim d→∞ 1 d E[xx T ] F = lim d→∞ 1 d d 2 1 4π 2 + d (π − 1) 2 4π 2 = 1 2π &gt; 1 8 . (<label>173</label></formula><formula xml:id="formula_255">)</formula><p>Thus for large n we have </p><formula xml:id="formula_256">1 8 &lt; 1 d E[xx T ] F &lt; 1. Since H * F = 1 d M * ⊗ E[xx T ] F = M * F • 1 d E[xx</formula><formula xml:id="formula_257">d E[D W T AW D ⊗ x x T ].</formula><p>Let ( D, Ā, x) be an independent copy of (D, A, x),</p><formula xml:id="formula_258">H 2 F = 1 d E[DW T AW D ⊗ xx T ] 2 F = E 1 d 2 DW T AW D ⊗ xx T , DW T ĀW D ⊗ xx T = E 1 d 2 tr DW T AW D ⊗ xx T DW T ĀW D ⊗ xx T = E 1 d 2 tr DW T AW D DW T ĀW D tr xx T xx T = E 1 d 2 (x T xx T x) tr W DDW T AW D DW T Ā . (<label>174</label></formula><formula xml:id="formula_259">)</formula><p>Expressing the term inside the expectation as a polynomial of entries of A, D, Ā and D, we get</p><formula xml:id="formula_260">1 d 2 (x T xx T x) tr W DDW T AW D DW T Ā = 1 d 2 d p,q=1 x p xp x q xq c i=1 W DDW T AW D DW T Ā i,i = 1 d 2 d p,q=1 c i,j,k,s=1 n l,t=1 W i,l W k,l W j,t W s,t Āk,j A s,i Dl,l D l,l Dt,t D t,t x p xp x q xq .<label>(175)</label></formula><p>We skipped some derivations as they are identical to Eq. ( <ref type="formula" target="#formula_260">175</ref>). The monomials are Āk,j A s,i Dl,l D l,l Dt,t D t,t x p xp x q xq , and the corresponding coefficients are W i,l W k,l W j,t W s,t . The 1 norm of the coefficients is</p><formula xml:id="formula_261">1 d 2 d p,q=1 c i,j,k,s=1 n l,t=1 W i,l W k,l W j,t W s,t 1 = c i,j,k,s=1 n l,t=1 W i,l W k,l W j,t W s,t 1 ,<label>(176)</label></formula><p>which we know is upper bounded by some constant with probability 1 over W from Eq. ( <ref type="formula" target="#formula_220">148</ref>).</p><p>For any &gt; 0, fix . Note that H * 2 F is just substituting (D, D, x, x) by (D , D , x , x ) in the polynomial characterized by Eq. ( <ref type="formula" target="#formula_260">175</ref>). From Corollary B.4 we have the convergence of the difference of the expectation of the two polynomials, namely | H 2 F − H * 2 F | &lt; for sufficiently large n. Since the spectral norm of H * is bounded below from 0 by Lemma B.15, we have</p><formula xml:id="formula_262">lim n→∞ H 2 F / H * 2 F = 1. lemma B.22. For all i, j ∈ [c], lim n→∞ ((V HV T ) i,j − (V H * V T ) i,j ) = 0. Thus, lim n→∞ V HV T 2 F V H * V T 2 F = 1. (<label>177</label></formula><formula xml:id="formula_263">)</formula><p>Proof of Lemma B.22. This proof is very similar to that of Lemma B.21. First, we focus on a single entry of the matrix V HV T and express it as a polynomial of entries of A and D:</p><formula xml:id="formula_264">(V HV T ) i,j = E (W ⊗ U ) 1 d (DW T AW D ⊗ xx T )(W ⊗ U ) T i,j = E 1 d (W DW T AW DW T ) ⊗ (U xx T U T ) i,j = E 1 d • 1 d (1 T d xx T 1 d ) W DW T AW DW T i,j = E 1 d 2 d p,q=1 x p x q c k=1 (W DW T A) i,k (W DW T ) k,j = E   1 d 2 d p,q=1 c k,s=1 n l,t=1 W i,l W s,l W k,t W j,t A s,k D l,l D t,t x p x q   .<label>(178)</label></formula><p>We skipped some derivations as they are identical to Eq. ( <ref type="formula" target="#formula_222">149</ref>). The monomials are A s,k D l,l D t,t x p x q , and the corresponding coefficients are W i,l W s,l W k,t W j,t . Observe that the 1 norm of the coefficients satisfies</p><formula xml:id="formula_265">1 d 2 d p,q=1 c k,s=1 n l,t=1 W i,l W s,l W k,t W j,t 1 = c k,s=1 n l,t=1 W i,l W s,l W k,t W j,t 1 ,<label>(179)</label></formula><p>which we know is bounded above by some constant from Eq. ( <ref type="formula" target="#formula_224">150</ref>). Note that the expression of each entry of V H * W T is just substituting (D, D, x, x) by (D , D , x , x ) in the polynomial characterized by Eq. ( <ref type="formula" target="#formula_264">178</ref>). Therefore, using Lemma B.13, we have with probability 1 over W , for all i, j ∈</p><formula xml:id="formula_266">[c], lim n→∞ ((V HV T ) i,j − (V H * V T ) i,j ) = 0.<label>(180)</label></formula><p>This completes the proof of the lemma as V HV T is of constant size. lemma B.23. With probability 1 over W (1) and W (2) ,</p><formula xml:id="formula_267">lim n→∞ V H * V T 2 F H * 2 F = 1. (<label>181</label></formula><formula xml:id="formula_268">)</formula><p>Proof of Lemma B.23. This lemma is a direct corollary of Lemma B.18 for the output Hessian. Note that by the independence in construction,</p><formula xml:id="formula_269">V H * V T = 1 d (W ⊗ U ) E D W T AW D ⊗ x x T (W T ⊗ U T ) = 1 d (W ⊗ U ) M * ⊗ E x x T (W T ⊗ U T ) = 1 d W M * W T ⊗ U E x x T U T = W M * W T ⊗ 1 d 2 1 T d E x x T 1 d = 1 d 2 1 T d E x x T 1 d W M * W T . (<label>182</label></formula><formula xml:id="formula_270">)</formula><p>From Eq. ( <ref type="formula" target="#formula_253">172</ref>) we have</p><formula xml:id="formula_271">1 T d E x x T 1 d = d i,j=1 E xx T ij = 1 2π d 2 + π − 1 2π d.<label>(183)</label></formula><p>Thus</p><formula xml:id="formula_272">V H * V T 2 F = 1 2π + π − 1 2πd W M * W 2 F = 1 4π 2 + π − 1 2π 2 d + (π − 1) 2 4π 2 d 2 W M * W 2 F .<label>(184)</label></formula><p>Meanwhile note that</p><formula xml:id="formula_273">H * 2 F = 1 d 2 M * ⊗ E x x T 2 F = 1 d 2 M * 2 F ⊗ E x x T 2 F ,<label>(185)</label></formula><p>where</p><formula xml:id="formula_274">E x x T 2 F = d i,j=1 E xx T 2 ij = 1 4π 2 d 2 + π − 1 2π d. (186) Thus H * 2 F = 1 4π 2 + π − 1 2πd M * 2 F .<label>(187)</label></formula><p>Since d = n 1+α for some constant α &gt; 0, we have</p><formula xml:id="formula_275">lim n→∞ 1 4π 2 + π−1 2π 2 d + (π−1) 2 4π 2 d 2 1 4π 2 + π−1 2πd = 1. (<label>188</label></formula><formula xml:id="formula_276">)</formula><p>Thus combined with the result from Lemma B.18, we have Now we are done with the lemmas and will proceed to the proof of the main theorems.</p><formula xml:id="formula_277">lim n→∞ V H * V T 2 F H * 2 F = lim n→∞ 1 4π 2 + π−1 2π 2 d + (π−1) 2 4π 2 d 2 1 4π 2 + π−1 2πd lim n→∞ W M * W T 2 F M * 2 F = 1. (<label>189</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.5 Structure of Output Hessian of the First Layer</head><p>We first restate Theorem 4.2 here:</p><p>Theorem 4.2 Let M * E D W (2)T AW (2) D where D is an independent copy of D and is also independent of A. Let S 1 and S 2 be the top c − 1 eigenspaces of M (1) and M * respectively, for all &gt; 0, lim n→∞ Pr</p><formula xml:id="formula_278">W (1) ∼N (0, 1 d I nd ),W (2) ∼N (0, 1 n Icn) [Overlap (S 1 , S 2 ) &gt; 1 − ] = 1. (<label>190</label></formula><formula xml:id="formula_279">)</formula><p>Moreover, as</p><formula xml:id="formula_280">lim n→∞ Pr W (1) ∼N (0, 1 d I nd ),W (2) ∼N (0, 1 n Icn) λ c (M ) λ c−1 (M ) W (1) ,W (2) &lt; = 1.<label>(191)</label></formula><p>Proof of Theorem 4.2.</p><p>From Lemma B.14 we have</p><formula xml:id="formula_281">lim n→∞ W M W T 2 F M 2 F = 1. (<label>192</label></formula><formula xml:id="formula_282">)</formula><p>Then we consider</p><formula xml:id="formula_283">W T W M W T W 2 F . Note that W T W M W T W 2 F = tr(W T W M W T W W T W M W T W ) = tr(W W T W M W T W W T W M W T ).<label>(193)</label></formula><p>From Lemma B.5 we know that for all &gt; 0, lim n→∞ Pr( W W T − I c ≥ ) = 0. For notation simplicity, in this proof we will omit the limit and probability arguments which can be dealt with using union bound. Therefore, we will directly state W W T − I c ≤ . From <ref type="bibr" target="#b19">Kleinman &amp; Athans (1968)</ref> we know that for positive semi-definite matrices A and B we have λ min (A) tr(B) ≤ tr(AB) ≤ λ max (A) tr(B), so</p><formula xml:id="formula_284">| tr(W W T • W M W T W W T W M W T ) − tr(W M W T W W T W M W T )| ≤ max{1 − λ min (W W T ), λ max (W W T ) − 1} tr(W M W T W W T W M W T ) ≤ W W T − I c tr(W M W T W W T W M W T ) ≤ tr(W M W T W W T W M W T ).<label>(194)</label></formula><p>Similarly,</p><formula xml:id="formula_285">| tr(W M W T W W T W M W T ) − tr(W M W T W M W T )| =| tr(W W T • W M W T W M W T ) − tr(W M W T W M W T )| ≤ W W T − I c tr(W M W T W M W T ) ≤ tr(W M W T W M W T ).<label>(195)</label></formula><p>Therefore,</p><formula xml:id="formula_286">| W T W M W T W 2 F − W M W T 2 F | =| tr(W W T • W M W T W W T W M W T ) − tr(W M W T W M W T )| ≤| tr(W W T • W M W T W W T W M W T ) − tr(W M W T W W T W M W T )| + | tr(W M W T W W T W M W T ) − tr(W M W T W M W T )| ≤ tr(W M W T W W T W M W T ) + tr(W M W T W M W T ) ≤ (1 + ) tr(W M W T W M W T ) + tr(W M W T W M W T ) ≤(2 + ( ) 2 ) tr(W M W T W M W T ) = (2 + ( ) 2 ) W M W T 2 F .<label>(196)</label></formula><p>For all &gt; 0, select &lt; min{</p><formula xml:id="formula_287">√ 2 , 4 }, we have | W T W M W T W 2 F − W M W T 2 F | &lt; W M W T 2 F .<label>(197)</label></formula><p>In other words,</p><formula xml:id="formula_288">lim n→∞ W T W M W T W 2 F W M W T 2 F = 1. (<label>198</label></formula><formula xml:id="formula_289">)</formula><p>Hence we get</p><formula xml:id="formula_290">lim n→∞ W T W M W T W 2 F M 2 F = 1.<label>(199)</label></formula><p>Next, consider the orthogonal projection matrix P W W T W that projects vectors in R n into the subspace spanned by all rows of W . Here W is the orthogonolized W , which is explicitly defined in Lemma B.7. We will consider the matrix P W M P W . Define δ W T W − P W , then from Lemma B.7 we get δ 2 F ≤ . Therefore,</p><formula xml:id="formula_291">| W T W M W T W F − P W M P W F | ≤ P W M δ F + δM P W F + δM δ F ≤ M F Pr 2 P W F δ F + δ 2 F ≤ M F Pr 2 • 4c 2 + ( ) 2 . (200) For all &gt; 0, we choose &lt; min{ √ 2 , 16c 2 } and have | W T W M W T W F − P W M P W F | M F &lt; ,<label>(201)</label></formula><p>which means that</p><formula xml:id="formula_292">lim n→∞ | W T W M W T W F − P W M P W F | W T W M W T W F = lim n→∞ | W T W M W T W F − P W M P W F | M F = 0.<label>(202)</label></formula><p>Thus,</p><formula xml:id="formula_293">lim n→∞ P W M P W F M F = lim n→∞ P W M P W F W T W M W T W F = 1. (<label>203</label></formula><formula xml:id="formula_294">) Note that M 2 F = P W M P W 2 F + P W M P ⊥ W 2 F + P ⊥ W M P W 2 F + P ⊥ W M P ⊥ W 2</formula><p>F . It follows that,</p><formula xml:id="formula_295">lim n→∞ P W M P ⊥ W 2 F + P ⊥ W M P W 2 F + P ⊥ W M P ⊥ W 2 F M 2 F = lim n→∞ M 2 F − P W M P W 2 F M 2 F = 0.<label>(204)</label></formula><p>In other words,</p><formula xml:id="formula_296">lim n→∞ P W M P ⊥ W F M F = lim n→∞ P ⊥ W M P W F M F = lim n→∞ P ⊥ W M P ⊥ W F M F = 0.<label>(205)</label></formula><p>From Lemma B.15 we know that for large n, lim n→∞ M F is lower bounded by some constant that is independent of n, so lim n→∞</p><formula xml:id="formula_297">P W M P ⊥ W F = lim n→∞ P ⊥ W M P W F = lim n→∞ P ⊥ W M P ⊥ W F = 0.<label>(206)</label></formula><p>Note that</p><formula xml:id="formula_298">M = P W M P W + P W M P ⊥ W + P ⊥ W M P W + P ⊥ W M P ⊥ W .<label>(207)</label></formula><p>Thus,</p><formula xml:id="formula_299">lim n→∞ M − P W M P W F = 0. (<label>208</label></formula><formula xml:id="formula_300">)</formula><p>For any &gt; 0, set δ &lt; min{ η 8c 2 , √ η 2c }, from Lemma B.7, we know that with probability 1, P W − W T W F ≤ δ. Therefore,</p><formula xml:id="formula_301">P W M P W − W T W M W T W F ≤ P W − W T W 2 F M F + 2 P W − W T W F M F P W F ≤δ 2 • 2c 2 + 2δ • 2c 2 &lt; . (209)</formula><p>In other words,</p><formula xml:id="formula_302">lim n→∞ P W M P W − W T W M W T W F = 0. (<label>210</label></formula><formula xml:id="formula_303">)</formula><p>Now we conclude that</p><formula xml:id="formula_304">lim n→∞ M − W T W M W T W F = 0.<label>(211)</label></formula><p>From Lemma B.17 we know that</p><formula xml:id="formula_305">lim n→∞ W M W T − W M * W T F = 0. (<label>212</label></formula><formula xml:id="formula_306">) Since W T W M W T W − W T W M * W T W F ≤ W 2 F W M W T − W M * W T F ,<label>(213)</label></formula><p>from Lemma B.3 which bounds the Frobenius norm of W we know that</p><formula xml:id="formula_307">lim n→∞ W T W M W T W − W T W M * W T W F = 0. (214) Thus, lim n→∞ M − W T W M * W T W F = 0. (<label>215</label></formula><formula xml:id="formula_308">) Note that M * = 1 4 E[W T AW ] + diag(E[W T AW ]) , so 4W T W M * W T W = W T W W T AW W T W + W T W diag(E[W T AW ])W T W . (<label>216</label></formula><formula xml:id="formula_309">)</formula><p>We will first analyze the second term on the RHS of equation Eq. ( <ref type="formula" target="#formula_308">216</ref>). For all &gt; 0, set = √ c , and from Lemma B.5 we know that W W T − I c &lt; with probability 1, which means that | W W T F − c| &lt; with probability 1. Set = c, we know that W W T F &lt; 2c with probability 1. Note that</p><formula xml:id="formula_310">W T W diag(E[W T AW ])W T W F ≤ W T W 2 F diag(E[W T AW ]) F = W W T 2 F diag(E[W T AW ]) F ≤ 4c 2 diag(E[W T AW ]) F . (217)</formula><p>Combine this with equation Eq. ( <ref type="formula" target="#formula_238">161</ref>) and we have</p><formula xml:id="formula_311">lim n→∞ W T W diag(E[W T AW ])W T W F W T AW F = 0. (<label>218</label></formula><formula xml:id="formula_312">)</formula><p>From Lemma B.15 we know that W T AW F ≥ η 4 with probability 1, so</p><formula xml:id="formula_313">lim n→∞ 4W T W M * W T W − W T W W T AW W T W F = 0. (<label>219</label></formula><formula xml:id="formula_314">)</formula><p>Similarly, define δ W W T − I c , then</p><formula xml:id="formula_315">W T W W T AW W T W − W T AW F ≤ W T δ AδW F + 2 W T Aδ F ≤ W 2 F δ 2 F A F + 2 W F δ F A F .<label>(220)</label></formula><p>Set &lt; min{ 8c 2 , 8c 3 }, then from Lemma B.5 we know that δ F &lt; with probability 1, and from Lemma B.3 we have W F ≤ 2c with probability 1. We also have A F ≤ c since each entry of A is bounded by 1 in absolute value. Therefore,</p><formula xml:id="formula_316">W T W W T AW W T W − W T AW F ≤ 4c 2 ( ) 2 • c + 2 • 2c • c &lt; 2 + 2 = ,<label>(221)</label></formula><p>which means that</p><formula xml:id="formula_317">lim n→∞ W T W W T AW W T W − W T AW F = 0. (<label>222</label></formula><formula xml:id="formula_318">)</formula><p>From Eq. ( <ref type="formula" target="#formula_313">219</ref>) and Eq. ( <ref type="formula" target="#formula_317">222</ref>) we get</p><formula xml:id="formula_319">lim n→∞ 1 4 W T AW − W T W M * W T W F = 0. (<label>223</label></formula><formula xml:id="formula_320">)</formula><p>Combining with Eq. ( <ref type="formula" target="#formula_307">215</ref>) we have</p><formula xml:id="formula_321">lim n→∞ M − 1 4 W T AW F = 0. (<label>224</label></formula><formula xml:id="formula_322">)</formula><p>Besides, from equation Eq. ( <ref type="formula" target="#formula_112">72</ref>) in Lemma B.7 we know that for any &gt; 0,</p><formula xml:id="formula_323">W − W 2 F = i∈[c] W i − W i 2 &lt; , (<label>225</label></formula><formula xml:id="formula_324">)</formula><p>where W is the orthogonal version of W , i.e., we run the Gram-Schmidt process for the rows of W . Define δ W − W , for any &gt; 0, set = min{ 8c 2 , 2c }, we have with probability 1,</p><formula xml:id="formula_325">W T AW − W T AW F ≤ 2 δ F A F W F + δ 2 F A F ≤ 4c 2 + c( ) 2 &lt; . (226) Therefore, lim n→∞ W T AW − W T AW F = 0,<label>(227)</label></formula><p>which implies</p><formula xml:id="formula_326">lim n→∞ M − 1 4 W T AW F = 0. (<label>228</label></formula><formula xml:id="formula_327">)</formula><p>From Lemma B.12 we know that with probability 1, A is of rank (c − 1). Since A • 1 = 0 is always true, the top (c − 1) eigenspace of A is R c \{1}. Note that the rows in W are of unit norm and orthogonal to each other, we conclude that W T AW is of rank (c − 1) and the corresponding eigenspace is R{W i } c i=1 \{1 T W }. Moreover, the minimum positive eigenvalue of W T AW is lower bounded by η 4 .</p><p>As for the top</p><formula xml:id="formula_328">c − 1 eigenvectors of M , define δ M − 1 4 W T AW , then M = 1 4 W T AW + δ.</formula><p>Define S 1 as the top c − 1 eigenspaces for M , and S 2 to be the top c − 1 eigenspaces for 1 4 W T AW . Then from Davis-Kahan Theorem we know that sin</p><formula xml:id="formula_329">Θ(S 1 , S 2 ) F ≤ δ F λ c−1 ( 1 4 W T AW ) . (<label>229</label></formula><formula xml:id="formula_330">)</formula><p>Here Θ(S 1 , S 2 ) is a (c − 1) × (c − 1) diagonal matrix whose i-th diagonal entry is the i-th canonical angle between S 1 and S 2 . Since lim n→∞ δ F = 0, and with probability 1, λ c−1 ( 1 4 W T AW ) ≥ η which is independent of n, we have with probability 1, lim</p><formula xml:id="formula_331">n→∞ sin Θ(S 1 , S 2 ) F = 0,<label>(230)</label></formula><p>which indicates that the top c − 1 eigenspaces for M and 1 4 W T AW are the same when n → ∞.</p><p>Here we note that the top c − 1 eigenspace of W T AW is R{W i } c i=1 \{1 T W } since A has its null space spanned by the all-one vector, so M will also have the same top c − 1 eigenspaces. Besides, from equation Eq. ( <ref type="formula" target="#formula_112">72</ref>) we know that lim n→∞ W − W F = 0, so R{W i } c i=1 \{1 T W } are the same as R{W i } c i=1 \{1 T W }. This completes the proof of this theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.6 Structure of Full Hessian of the First Layer</head><p>We first restate Theorem 4.1 here: Theorem 4.1: Let V 1 and V 2 be the top c − 1 eigenspaces of H and H respectively, for all &gt; 0, lim n→∞ Pr</p><formula xml:id="formula_332">W (1) ∼N (0, 1 d I nd ),W (2) ∼N (0, 1 n Icn) [Overlap (V 1 , V 2 ) &gt; 1 − ] = 1. (231)</formula><p>Moreover,</p><formula xml:id="formula_333">lim n→∞ Pr W (1) ∼N (0, 1 d I nd ),W (2) ∼N (0, 1 n Icn) λ c (H) λ c−1 (H) W (1) ,W (2) &lt; = 1. (<label>232</label></formula><formula xml:id="formula_334">)</formula><p>Before proceeding to the main theorem, we will first look into the eigenspectrum of the scaled auto-correlation matrix X 1 d E xx T and the top eigenspace of H. Also recall some useful notations including</p><formula xml:id="formula_335">U = 1 √ d 1 T d and V W ⊗ U . lemma B.24. λ 1 ( X) = 1 2π + π−1 2πd with eigenvector 1 √ d 1 d . λ 2 ( X) = • • • = λ d ( X) = π−1 2πd .</formula><p>Proof of Lemma B.24. From Eq. ( <ref type="formula" target="#formula_253">172</ref>) we know that</p><formula xml:id="formula_336">X = 1 d E xx T = 1 2πd 1 d 1 T d + π − 1 2πd I d . (<label>233</label></formula><formula xml:id="formula_337">)</formula><formula xml:id="formula_338">For unit vector v = 1 √ d 1 d , it satisfies Xv = 1 2πd 1 d 1 T d 1 √ d 1 d + π − 1 2πd I d 1 √ d 1 d = 1 2π + π − 1 2πd 1 √ d 1 d . (<label>234</label></formula><formula xml:id="formula_339">)</formula><p>Hence the all one vector has eigenvalue 1 2π + π−1 2πd . For any unit vector v ⊥ 1 d , it satisfies</p><formula xml:id="formula_340">Xv = 1 2πd 1 d 1 T d v + π − 1 2πd I d v = π − 1 2πd v. (<label>235</label></formula><formula xml:id="formula_341">) Which means λ 2 = λ 3 = • • • = λ d = π−1 2πd . corollary B.5.</formula><p>With probability 1 over W (1) and W (2) , the overlap between the top c − 1 eigenspace of H and</p><formula xml:id="formula_342">R{V i } c i=1 \{V • 1} converges to 1 as n → ∞.</formula><p>Proof of Corollary B.5. First note that by simple linear algebra,</p><formula xml:id="formula_343">R{V i } c i=1 \{1 T V } = (R{W i } c i=1 \{W • 1}) ⊗ U . (<label>236</label></formula><formula xml:id="formula_344">)</formula><p>While from Theorem 4.2 we know the overlap between the top c − 1 eigenspace of M and R{W i } c i=1 \{1 T W } converges to 1. Thus for proving this corollary it is sufficient to show that the top c − 1 eigenspace of</p><formula xml:id="formula_345">1 d H = M ⊗ X</formula><p>is the Kronecker product of the top c − 1 eigenspace of M and U .</p><p>From Theorem 4.2 we know, with probability 1 over W (1) and W (2) , for large n, λ c−1 (M ) &gt; η/4 and λ 1 (M ) &lt; 2c 2 where η and c are absolute constants. Thus for large n we have</p><formula xml:id="formula_346">lim n→∞ λ 1 ( X)λ c−1 (M ) = lim d→∞ 1 2π + π − 1 2πd λ c−1 (M ) ≥ 1 2π η 4 . (<label>237</label></formula><formula xml:id="formula_347">) while lim n→∞ λ 2 ( X)λ 1 (M ) = lim d→∞ π − 1 2πd λ c−1 (M ) ≤ lim d→∞ π − 1 2πd 2c 2 = 0. (<label>238</label></formula><formula xml:id="formula_348">)</formula><p>Since for large n, λ 1 ( X)λ c−1 (M ) &gt; λ 2 ( X)λ 1 (M ), the top c − 1 eigenspace of 1 d H is the top c − 1 eigenspace of M Kronecker with the first eigenvector of X, which is exactly U from Lemma B.24. This completes the proof of this corollary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now we proceed to prove the main theorem</head><p>Proof of Theorem 4.1. We will conduct the proof on H = 1 d H as the properties to be proved are invariant to scalar multiplication. From Corollary B.5 we know the overlap between the top c − 1 eigenspace of H and R{V i } c i=1 \{1 T V } converges to 1. Thus we only need to show the overlap between the top c − 1 eigenspace of H and R{V i } c i=1 \{1 T V } converges to 1.</p><p>The proof strategy for the full layerwise Hessian is exactly the same as the proof for the output Hessian in Appendix B.2.3. In particular, the proof is nearly identical when we change the projection matrix from P W to P V where V W ⊗ U .</p><p>Therefore, instead of rewriting the entire proof, we may neglect some repeating arguments by verifying the equivalent lemmas for the full layer-wise Hessian. With V as defined, we have V F = W F , V V T = W W T , and </p><formula xml:id="formula_349">V T V F = W T W F ,</formula><formula xml:id="formula_350">lim n→∞ Pr V T V − P V 2 F &gt; = 0. (<label>239</label></formula><formula xml:id="formula_351">)</formula><p>Proof of Lemma B.25. Since Kronecker product with the constant 1 × d matrix U preserves the orthogonality of vectors, doing Gram-Schmit on V is equivalent to doing Gram-Schmit on W then Kronecker with U , which results in V by construction. Therefore P V is a valid projection matrix.</p><p>Also note that for any W ,</p><formula xml:id="formula_352">V T V − P V 2 F = (W ⊗ U ) T (W ⊗ U ) − V T V 2 F = (W T W ) ⊗ (U T U ) − (W T W ) ⊗ (U T U ) 2 F = W T W − W T W 2 F U T U 2 F = W T W − W T W 2 F 1 d 1 d 1 T d 2 F = W T W − P W 2 F .<label>(240)</label></formula><p>From Lemma B.7 we have</p><formula xml:id="formula_353">lim n→∞ Pr V T V − P V 2 F &gt; = lim n→∞ Pr W T W − P W 2 F &gt; = 0. (<label>241</label></formula><formula xml:id="formula_354">)</formula><p>Note that the equivalent lemmas of Lemma B. </p><formula xml:id="formula_355">H − V T V H * V T V F = 0. (<label>242</label></formula><formula xml:id="formula_356">)</formula><p>Now claim an equivalent argument of Eq. ( <ref type="formula" target="#formula_319">223</ref>), that</p><formula xml:id="formula_357">lim n→∞ 1 8π V T AV − V T V H * V T V F = 0. (<label>243</label></formula><formula xml:id="formula_358">)</formula><p>Observe that</p><formula xml:id="formula_359">V T AV = (W ⊗ U ) T A(W ⊗ U ) = W T AW ⊗ U T U (244)</formula><p>and</p><formula xml:id="formula_360">V T V H * V T V = (W ⊗ U ) T (W ⊗ U )(M * ⊗ X)(W ⊗ U ) T (W ⊗ U ) = W T W M * W T W ⊗ U T U XU T U . (<label>245</label></formula><formula xml:id="formula_361">)</formula><p>We have</p><formula xml:id="formula_362">1 8π V T AV − V T V H * V T V F = 1 8π W T AW ⊗ U T U − W T W M * W T W ⊗ U T U XU T U F ≤ 1 8π W T AW ⊗ U T U − 1 2π W T W M * W T W ⊗ U T U F + 1 2π W T W M * W T W ⊗ U T U − W T W M * W T W ⊗ U T U XU T U F = 1 2π 1 4 W T AW − W T W M * W T W F U T U XU T U F + W T W M * W T W F 1 2π U T U − U T U XU T U F .<label>(246)</label></formula><p>Let's first consider the second term. Note that from Lemma B.24,</p><formula xml:id="formula_363">1 2π U T U − U T U XU T U F = 1 2πd 1 d 1 T d − 1 d 2   d i,j=1 1 d E xx T ij   1 d 1 T d F = 1 2πd − 1 d 3 2πd 2 2π + (π − 1)d 2π 1 d 1 T d F = π − 1 2πd 2 d = π − 1 2πd . (<label>247</label></formula><formula xml:id="formula_364">)</formula><p>Which converges to 0 as n → ∞ (since d = n 1+α ). Since W T W M * W T W F is bounded above from Lemma B.3 and Lemma B.15. We have</p><formula xml:id="formula_365">lim n→∞ W T W M * W T W F 1 2π U T U − U T U XU T U F = 0. (<label>248</label></formula><formula xml:id="formula_366">)</formula><p>For the first term, since for all d,</p><formula xml:id="formula_367">U T U XU T U F = 1 d 2   d i,j=1 1 d E xx T ij   1 d 1 T d F = 1 d 2 2πd 2 2π + (π − 1)d 2π &lt; 1 2 ,<label>(249)</label></formula><p>Combined with Eq. ( <ref type="formula" target="#formula_319">223</ref>) we have</p><formula xml:id="formula_368">lim n→∞ 1 2π 1 4 W T AW − W T W M * W T W F U T U XU T U F = 0. (<label>250</label></formula><formula xml:id="formula_369">)</formula><p>Plug Eq. ( <ref type="formula" target="#formula_365">248</ref>) and Eq. ( <ref type="formula" target="#formula_368">250</ref>) into Eq. ( <ref type="formula" target="#formula_362">246</ref>) gives us Eq. ( <ref type="formula" target="#formula_357">243</ref>).</p><p>Now substitute 1 4 W T AW in Appendix B.2.5 to 1 8π V T AV , following the arguments after Eq. ( <ref type="formula" target="#formula_319">223</ref>) completes the remaining proof for this theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Structure of Dominating Eigenvectors of the Full Hessian.</head><p>Although it is not possible to apply Kronecker factorization to the full Hessian directly, we can construct an approximation of the top eigenvectors and eigenspace using similar ideas and our findings.In this section, we will always have superscript (p) for all layer-wise matrices and vectors in order to distinguish them from the full versions. As shown in Eq. ( <ref type="formula" target="#formula_44">27</ref>) of Appendix A.1, we have the full Hessian of fully connected networks as</p><formula xml:id="formula_370">H L (θ) = E F T x A x F x + E c i=1 ∂ (z, y) z i ∇ 2 θ z i ,<label>(251)</label></formula><p>where</p><formula xml:id="formula_371">F T x =             G x (1)T ⊗ x (1) G x (1)T G x (2)T ⊗ x (2) G x (2)T . . . G x (L)T ⊗ x (n) G x (L)T             . (<label>252</label></formula><formula xml:id="formula_372">)</formula><p>In order to simplify the formula, we define</p><formula xml:id="formula_373">x (p) = x (p) 1 (253)</formula><p>to be the extended input of the p-th layer. Thus, the terms in the Hessian attributed to the bias can be included in the Kronecker product with the extended input, and F T x can be simplified as</p><formula xml:id="formula_374">F T x =       G x (1)T ⊗ x (1) G x (2)T ⊗ x (2) . . . G x (L)T ⊗ x (n)       . (<label>254</label></formula><formula xml:id="formula_375">)</formula><p>As discussed in several previous works <ref type="bibr" target="#b33">(Sagun et al., 2016;</ref><ref type="bibr" target="#b28">Papyan, 2018;</ref><ref type="bibr">2019;</ref><ref type="bibr" target="#b2">Fort &amp; Ganguli, 2019)</ref>, the full Hessian can be decomposed in to the G-term and the H-term. Specifically, the G-term is</p><formula xml:id="formula_376">E F T x A x F x , and the H-term is E c i=1 ∂ (z,y) zi ∇ 2 θ z i in Eq. (251).</formula><p>Empirically, the G-term usually dominates the H-term, and the top eigenvalues and eigenspace of the Hessian are mainly attributed to the G-term. Since we focus on the top eigenspace, we can approximate our full Hessian using the G-term, as</p><formula xml:id="formula_377">H L (θ) ≈ E F T x A x F x .<label>(255)</label></formula><p>In our approximation of the layer-wise Hessian H L (w (p) ) Eq. ( <ref type="formula" target="#formula_405">2</ref>), the two parts of the Kronecker factorization are the layer-wise output Hessian E[M</p><formula xml:id="formula_378">(p)</formula><p>x ] and the auto-correlation matrix of the input E[x (p) x (p)T ]. Although we cannot apply Kronecker factorization to E F T</p><p>x A x F x , we can still approximate its eigenspace using the eigenspace of the full output Hessian.</p><p>Note here that the full output Hessian is not a common definition. Let m = L p=1 m (p) be the sum of output dimension of each layer. We define a full output vector z ∈ R m by concatenating all the layerwise outputs together,</p><formula xml:id="formula_379">z :=      z (1) z (2)</formula><p>. . .</p><formula xml:id="formula_380">z (L)      . (<label>256</label></formula><formula xml:id="formula_381">)</formula><p>We then define the full output Hessian is the Hessian w.r.t. z. Let the full output Hessian for a single input x be M x ∈ R m× m . Similar to Eq. ( <ref type="formula" target="#formula_38">23</ref>), it can be expressed as</p><formula xml:id="formula_382">M x := H ( z, x) = G T x A x G x ,<label>(257)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Computation of Hessian Eigenvalues and Eigenvectors</head><p>For Hessian approximated using Kronecker factorization, we compute E ] has n eigenvectors, we can approximate all mn eigenvectors for the layer-wise Hessian. All these calculation can be done directly.</p><p>However, it is almost prohibitive to calculate the true Hessian explicitly. Thus, we use numerical methods with automatic differentiation <ref type="bibr" target="#b31">(Paszke et al., 2017)</ref> to calculate them. The packages we use is <ref type="bibr" target="#b6">Golmant et al. (2018)</ref> and we use the Lanczos method in most of the calculations. We also use package in <ref type="bibr" target="#b44">Yao et al. (2019)</ref> as a reference.</p><p>For layer-wise Hessian, we modified the <ref type="bibr" target="#b6">Golmant et al. (2018)</ref> package. In particular, the package relies on the calculation of Hessian-vector product Hv, where v is a vector with the same size as parameter θ. To calculate eigenvalues and eigenvectors for layer-wise Hessian at the p-th layer, we cut the v into different layers. Then, we only leave the part corresponding to weights of the p-th layer and set all other entries to 0. Note that the dimension does not change. We let the new vector be v (p) and get the value of u = Hv (p) using auto differentiation. Then, we do the same operation to u and get u (p) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Detailed Experiment Setup E.1 Datasets</head><p>We conduct experiment on CIFAR-10, CIFAR-100 (MIT) <ref type="bibr" target="#b20">(Krizhevsky, 2009)</ref> (https://www.cs.toronto. edu/ ˜kriz/cifar.html), and MNIST (CC BY-SA 3.0) <ref type="bibr" target="#b23">(LeCun et al., 1998)</ref> (http://yann.lecun. com/exdb/mnist/). The datasets are downloaded through torchvision <ref type="bibr" target="#b32">(Paszke et al., 2019)</ref> (https:// pytorch.org/vision/stable/index.html). We used their default splitting of training and testing set.</p><p>To compare our work on PAC-Bayes bound with the work of <ref type="bibr" target="#b1">Dziugaite &amp; Roy (2017)</ref>, we created a custom dataset MNIST-2 by setting the label of images 0-4 to 0 and 5-9 to 1. We also created random-labeled datasets MNIST-R and CIFAR10-R by randomly labeling the images from the training set of MNIST and CIFAR10. The dataset information is summarized in All the datasets (MNIST, CIFAR-10, and CIFAR-100) we used are publicly available. According to their descriptions on the contents and collection methods, they should not contain any personal information or offensive content.</p><p>MNIST is a remix of datasets from the National Institute of Standards and Technology (NIST), which obtained consent for collecting the data. However, we also note that CIFAR-10 and CIFAR-100 are subsets of the dataset 80 Million Tiny Image <ref type="bibr" target="#b41">(Torralba et al., 2008)</ref> (http://groups.csail.mit.edu/vision/TinyImages/), which used automatic collection and includes some offensive images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Network Structures</head><p>Fully Connected Network: We used several different fully connected networks varying in the number of hidden layers and the number of neurons for each hidden layer. The output of all layers except the last layer are passed into ReLU before feeding into the subsequent layer. As described in Section 5.1, we denote a fully connected network with m hidden layers and n neurons each hidden layer by F-n m . For networks without uniform layer width, we denote them by a sequence of numbers (e.g. for a network with three hidden layers, where the first two layers has 200 neurons each and the third has 100 neurons, we denote it as F-200 2 -100). For example, the structure of F-200 2 is shown in Table <ref type="table" target="#tab_7">3</ref>.</p><p>LeNet5: We adopted the LeNet5 structure proposed by <ref type="bibr" target="#b23">LeCun et al. (1998)</ref> for MNIST, and slightly modified the input convolutional layers to adapt the input of CIFAR-10 dataset. The standard LeNet5 structure we used in the experiments is shown in Table <ref type="table" target="#tab_8">4</ref>. We further modified the dimension of fc1 and conv2 to create several variants for the experiment in Section 5.3. Take the model whose first fully connected layer is adjusted to have 80 neurons as an example, we denote it as LeNet5-(fc1-80).</p><p>Networks with Batch Normalization: In Appendix G.4 we conducted several experiments regarding the effect of batch normalization on our results. For those experiments, we use the existing structures and add batch normalization Linear(84, 10) 84 10 output layer for each intermediate output after it passes the ReLU module. In order for the Hessian to be well-defined, we fix the running statistics of batch normalization and treat it as a linear layer during inference. We also turn off the learnable parameters θ and β <ref type="bibr" target="#b12">(Ioffe &amp; Szegedy, 2015)</ref> for simplicity. For network structure X, we denote the variant with batch normalization after all hidden layers X-BN. For example, the detailed structure LeNet5-BN is shown in Table <ref type="table" target="#tab_9">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variants of VGG11:</head><p>To verify that our results apply to larger networks, we trained a number of variant of VGG11 (originally named VGG-A in the paper, but commonly refered as VGG11) proposed by <ref type="bibr" target="#b36">Simonyan &amp; Zisserman (2015)</ref>. For simplicity, we removed the dropout regularization in the original network. To adapt the structure, which is originally designed for the 3 × 224 × 224 input of ImageNet, to 3 × 32 × 32 input of CIFAR-10.</p><p>Since the original VGG11 network is too large for computing the top eigenspace up to hundreds of dimensions, we reduce the number of output channels of each convolution layer in the network to 32, 48, 64, 80, and 200. We denote the small size variants as VGG11-W32, VGG11-W48, VGG11-W64, VGG11-W80, and VGG11-W200 respectively. We use conv1 -conv8 and fc1 to denote the layers of VGG11 where conv1 is closest to the input feature and fc1 is the classification layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variants of ResNet18:</head><p>We also trained a number of variant of ResNet18 proposed by <ref type="bibr" target="#b10">He et al. (2016)</ref>. As batch normalization will change the low rank structure of the auto correlation matrix and reduce the overlap, we removed all batch normalization operations. Following the adaptation of ResNet to CIFAR dataset as in https://github.com/kuangliu/pytorch-cifar, we changed the input size to 3 × 32 × 32 and added a 1x1 convolutional layer for each shortcut after the first block. Similar to VGG11, we reduce the number of output channels of each convolution layer in the network to 48, 64, 80. We denote the small size variants as ResNet18-W48, ResNet18-W64, and ResNet18-W80 respectively. We use conv1 -conv17 and fc1 to denote the layers of the ResNet18 backbone where conv1 is closest to the input feature and fc1 is the classification layer. For the 1x1 convolutional layers in the shortcut, we denote them by sc-conv1sc-conv3. where sc-conv1 is the convolutional layer on the shortcut of the second ResNet block and sc-conv3 is the convolutional layer on the shortcut of the fourth ResNet block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Training Process and Hyperparameter Configuration</head><p>For all datasets, we used the default splitting of training and testing set. All models (except explicitly stated otherwise) are trained using batched stochastic gradient descent (SGD) with batch-size 128 and fixed learning rate 0.01 for 1000 epochs. No momentum and weight decay regularization were used. The loss objective converges by the end of training, so we may assume that the final models are at local minima. For generality we also used a training scheme with fixed learning rate at 0.001, and a training scheme with fixed learning rate at 0.01 with momentum of 0.9 and weight-decay factor of 0.0005. Models trained with these settings will be explicitly stated. Otherwise we assume they were trained with the default scheme mentioned above. Follow the default initialization scheme of PyTorch <ref type="bibr" target="#b32">(Paszke et al., 2019)</ref>, the weights of linear layers and convolutional layers are initialized using the Xavier method <ref type="bibr" target="#b5">(Glorot &amp; Bengio, 2010)</ref>, and bias of each layer are initialized to be zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional Empirical Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Low Rank Structure of Auto-correlation Matrix E[xx T ]</head><p>We have briefly discussed about the autocorrelation matrix E[xx T ] being approximately rank 1 in Section 3.1 in the main text. In particular, we claimed that the mean of layer input dominate the covariance, that</p><formula xml:id="formula_383">E[xx T ] ≈ E[x]E[x T ].</formula><p>In this section we provide some additional empirical results supporting that claim.</p><p>We use two metrics to quantify the quality of this approximation: the squared dot product between normalized E[x] and the first eigenvector of E[xx T ] and the ratio between the first and second eigenvalue of E[xx T ]. Intuitively if the first quantity is close to 1 and the second quantity is large, then the approximation is accurate. Formally, for fully connected layers, define E[x] as the normalized expectation of the layer input x, namely E[x]/ E[x] . For convolutional layers, following the notations in Appendix A.2, define E[x] as the first left singular vector of E[X] where E[x] ∈ R nK1K2 . Abusing notations for simplicity, we use</p><formula xml:id="formula_384">E[xx T ] to denote the nK 1 K 2 × nK 1 K 2 matrix E[XX T ].</formula><p>In this section we consider the squared dot product between E[x] and the first eigenvector</p><formula xml:id="formula_385">v 1 of E[xx T ], namely (v T 1 E[x]) 2 .</formula><p>For the spectral ratio, let λ 1 be the first eigenvalue of E[xx T ] and λ 2 be the second. We have</p><formula xml:id="formula_386">λ 1 λ 2 ≥ E[x]E[x] T − Σ x Σ x = E[x]E[x] T Σ x − 1,<label>(264)</label></formula><p>where Σ x is the covariance of x. Thus, the spectral norm of E[x]E[x] T divided by that of Σ x gives a lower bound to λ 1 /λ 2 . In our experiments, we usually have</p><formula xml:id="formula_387">λ 1 /λ 2 ≥ E[x]E[x] T / Σ x .</formula><p>As we can see from Table <ref type="table" target="#tab_10">6 and Table 7</ref>, in a variety of settings, E[x]E[x] T indeed dominated the autocorrelation matrix E[xx T ] for fully connected layers. Similar phenomenon also holds for convolutional layers in the modern architectures, but the spectral gap are generally smaller compared to that of the fully connected layers.</p><p>Table <ref type="table">6</ref>: Squared dot product (v T 1 E[x]) 2 and spectral ratio λ 1 /λ 2 for fully connected layers in a selection of network structures and datasets. We independently trained 5 runs for each instance and compute the mean, minimum, and maximum of the two quantities over all layers (except the first layer which takes the input with mean-zero) in all runs. </p><formula xml:id="formula_388">(v T 1 E[x]) 2 λ1/</formula><formula xml:id="formula_389">(v T 1 E[x]</formula><p>) 2 and spectral ratio λ 1 /λ 2 for convolutional layers in the selection of network structures and datasets in Table <ref type="table">6</ref>. </p><formula xml:id="formula_390">(v T 1 E[x]) 2 λ1/</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Eigenspace Overlap Between Different Models</head><p>The non trivial overlap between top eigenspaces of layer-wise Hessians is one of our interesting observations that had been discusses in Section 5.3. Here we provide more related empirical results. Some will further verify our claim in Section 5.3 and some will appear to be challenge that. Both results will be explained discussed more extensively in Appendix G.</p><p>F.2.1 Overlap preserved when varying hyper-parameters:</p><p>We first verify that the overlap also exists for a set of models trained with the different hyper-parameters. Using the LeNet5 (defined in Table <ref type="table" target="#tab_8">4</ref>) as the network structure. We train 6 models using the default training scheme (SGD, lr=0.01, momentum=0), 5 models using a smaller learning rate (SGD, lr=0.001, momentum=0), and 5 models using a combination of optimization tricks (SGD, lr=0.01, momentum=0.9, weight decay=0.0005). With these 16 models, we compute the pairwise eigenspace overlap of their layer-wise Hessians (120 pairs in total) and plot their average in Fig. <ref type="figure" target="#fig_12">6</ref>. The shade areas in the figure represents the standard deviation. The pattern of overlap is clearly preserved, and the position of the peak roughly agrees with the output dimension m, demonstrating that the phenomenon is caused by a common structure instead of similarities in training process. Note that for fc3 (the final output layer), we are not observing a linear growth starting from 0 like other layers. This can be explained by the lack of neuron permutation. Related details will be discussed along with the reason for the linear growth pattern for other layers in Appendix G.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2.2 Eigenspace overlap for convolutional layers in large models:</head><p>Even though the exact Kroneckor Factorization for layer-wise Hessians is only well-defined for fully connected layers, we also observe similar nontrivial eigenspace overlap for convolutional layers in larger and deeper networks including variants of VGG11 and ResNet18 on datasets CIFAR10 and CIFAR100. Some representative results are shown in Fig. <ref type="figure">7</ref> and Fig. <ref type="figure" target="#fig_13">8</ref>. For each model on each dataset, we independently train 5 models and compute the average pairwise eigenspace overlap. The shade areas represents the standard deviation.</p><p>For most of the convolutional layers, the eigenspace overlap peaks around the dimension which is equal to the number of output channels of that layer, which is similar to the layers in LeNet5 as in Fig. <ref type="figure" target="#fig_12">6</ref>. The eigenspace overlap of the final fully connected-layer also behaves similar to fc3:LeNet5, which remains around a constant then drops after exceeding the dimension of final output. However, there are also layers whose overlap does not peak around the output dimensions, (e.g. conv2 of Fig. <ref type="figure">7</ref>(a) and conv7 of Fig. <ref type="figure" target="#fig_13">8(a)</ref>). We will discuss these special cases in the following paragraph. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2.3 Failed cases for eigenspace overlap</head><p>As seen in Fig. <ref type="figure">7</ref> and Fig. <ref type="figure" target="#fig_13">8</ref>, there is a small portion of layers, usually closer to the input, whose eigenspace overlap does peak around the output dimensions. These layers can be clustered into the following two general cases.</p><p>Early Peak of Low Overlap For layers shown in Fig. <ref type="figure" target="#fig_14">9</ref>. The overlap of dominating eigenspaces are significantly lower than the other layers. Also there exists a small peak at very small dimensions. Delayed Peak / Peak Doesn't Decline For layers shown in Fig. <ref type="figure" target="#fig_15">10</ref>, the top eigenspaces has a nontrivial overlap, but the peak dimension is larger than predicted output dimension. However, the existence of such failure cases does not undermine the theory of Kronecker factorization approximation.</p><p>In fact, both appear because the top hessian eigenspace is not completely spanned by E[x], and can be predicted by computing the auto correlation matrices and the output Hessians. The details will also be elaborated in Appendix G.3 with the help of correspondence matrices.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Additional Explanations G.1 Heuristic Approximating of the Top Eigenspace of Output Hessians</head><p>As briefly mentioned in Section 4, the closed form approximating of S 1 in Theorem 4.2 can be heuristically extended to the case with multiple layers, that the top eigenspace of the output Hessian of the k-layer would be approximately k+1) and R(S (k) ) is the row space of S (k) .</p><formula xml:id="formula_391">R(S (k) ) \ {1 T S (k) } where S (k) = W (n) W (n−1) • • • W (</formula><p>Though our result was only proven for random initialization and random data, we observe that this subspace also has high overlap with the top eigenspace of output Hessian at the minima of models trained with real datasets. In Table <ref type="table">8</ref> we show the overlap of R(S (k) ) \ {1 T S (k) } and the top c − 1 dimension eigenspace of E[M (k) ] of different layers at minima. Table <ref type="table">8</ref> Note that the overlap can be low for random-label datasets which do not have a clear eigengap (as in Fig. <ref type="figure" target="#fig_4">3</ref>).</p><formula xml:id="formula_392">: Overlap of R(S (k) ) \ {1 T S (k) } and the top c − 1 dimension eigenspace of E[M (k) ] of different layers. Dataset MNIST MNIST-R CIFAR10 CIFAR10-R Network F-1500 3 LeNet5 F-1500 3 LeNet5 F-1500 3 LeNet5 F-</formula><p>Understanding how the data could change the behavior of the Hessian is an interesting open problem. Other papers including Papyan (2019) have given alternative explanations which are not directly comparable to ours, however ours is the only one that gives a closed-form formula for top eigenspace. In Appendix G.5 we will discuss the other explanations in more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Dominating Eigenvectors of Layer-wise Hessian are Low Rank</head><p>A natural corollary for the Kronecker factorization approximation of layer-wise Hessians is that the eigenvectors of the layer-wise Hessians are low rank. Let h i be the i-th eigenvector of a layer-wise Hessian. The rank of Mat(h i ) can be considered as an indicator of the complexity of the eigenvector. Consider the case that h i is one of the top eigenvectors. From Section 5.3, we have  </p><formula xml:id="formula_393">h i ≈ u i ⊗ E[x]. Thus, Mat(h i ) ≈ u i E[x]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 Eigenspace Overlap of Different Models</head><p>From the experiment results in Appendix F together with Fig. <ref type="figure" target="#fig_6">4</ref>, we can see that our approximation and explanation stated in Section 5.3 of the main text is approximately correct but may not be so accurate for some layers. We now present a more general explanation which addresses why the overlap before rank-m grows linearly. We will also explain some exceptional cases as shown in Appendix F.2 and possible discrepancies of our approximation.</p><p>Let h i be the i-th eigenvector of the layer-wise Hessian H L (w (p) ), under the assumption that the autocorrelation matrix</p><formula xml:id="formula_394">E[xx T ] is approximately rank 1 that E[xx T ] ≈ E[x]E[x] T , for all i ≤ m, we can approximate the h i as u i ⊗ (E[x]/ E[x]</formula><p>) where u i is the i-th eigenvector of E[M ]. Formally, the trend of top eigenspace can be characterized by the following theorem. For simplicity of notations, we abuse the superscript within parentheses to refer the two models instead of layer number in this section.</p><p>Theorem G.1. Consider 2 different models with the same network structure trained on the same dataset. Fix the p-th hidden layer with input dimension n and output dimension m. For the first model, denote its output Hessian as E[M ] (1) with eigenvalues τ</p><formula xml:id="formula_395">(1) 1 ≥ τ (1) 2 ≥ • • • ≥ τ (1)</formula><p>m ≥ 0 and eigenvectors r</p><p>(1)</p><formula xml:id="formula_396">1 , • • • , r<label>(1)</label></formula><p>m ∈ R m ; denote its autocorrelation matrix as E[xx T ] (1) , with eigenvalues γ</p><p>(1)</p><formula xml:id="formula_397">1 ≥ γ (1) 2 ≥ • • • ≥ γ (1) m ≥ 0 and eigenvectors t (1) 1 , • • • , t<label>(1)</label></formula><p>n ∈ R n . The variables for the second matrices are defined identically by changing 1 in the superscript parenthesis to 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assume the Kronecker factorization approximation is accurate that H</head><formula xml:id="formula_398">L (w (p) ) (1) ≈ E[M ] (1) ⊗ E[xx T ] (1) and H L (w (p) ) (2) ≈ E[M ] (2) ⊗ E[xx T ] (2)</formula><p>. Also assume the autocorrelation matrices of two models are sufficiently close to rank 1 in the sense that τ</p><formula xml:id="formula_399">(1) m γ (1) 1 &gt; τ (1) 1 γ (1) 2 and τ (2) m γ (2) 1 &gt; τ (2) 1 γ (2)</formula><p>2 . Then for all k ≤ m, the overlap of top k eigenspace between their layerwise Hessians H L (w (p) ) (1) and H L (w (p) ) (2) will be approximately</p><formula xml:id="formula_400">k m (t (1) 1 • t (2)</formula><p>1 ) 2 . Consequently, the top eigenspace overlap will show a linear growth before it reaches dimension m. The peak at m is approximately</p><formula xml:id="formula_401">(t 1 • t 2 ) 2 . Proof of Theorem G.1. Let h (2)</formula><p>i be the i-th eigenvector of the layer-wise Hessian for the first model H L (w (p) ) (1) , and g i be that of the second model H L (w (p) ) (2) . Consider the first model. By the Kronecker factorization approximation, since τ </p><formula xml:id="formula_402">τ (1) 1 , • • • , γ (1) 1 τ (1) m .</formula><p>Consequently, for all i ≤ m we have h i ≈ r</p><p>(1)T i ⊗ t</p><p>(1) 1 . Thus, for any k ≤ m, we have its top k eigenspace as</p><formula xml:id="formula_403">V (1) k ⊗ t (1) 1 , where V (1) k ∈ R m×k has column vectors r (1) 1 , . . . , r<label>(1)</label></formula><p>k . Similarly, for the second model we have h</p><formula xml:id="formula_404">(2) i ≈ r (2) i ⊗ t (2)</formula><p>1 and the top k eigenspace as</p><formula xml:id="formula_405">V (2) k ⊗ t (2) 1 , where V (2) k has column vectors r (2) 1 , . . . , r<label>(2)</label></formula><p>k . The eigenspace overlap of the 2 models at dimension k is thus</p><formula xml:id="formula_406">Overlap V (1) k ⊗ t (1) 1 , V (2) k ⊗ t (2) 1 = 1 k V (1)T k V (2) k ⊗ t (1)T 1 t (2) 1 2 F = t (1) 1 • t (2) 1 2 Overlap V (1) k , V<label>(2) k</label></formula><p>.</p><p>Note that for all i ≤ m, r</p><p>i , r</p><p>(2) i ∈ R n , which is the space corresponding to the neurons. Since for hidden layers, the output neurons (channels for convolutional layers) can be arbitrarily permuted to give equivalent models while changing eigenvectors. For h i ≈ r i ⊗ t 1 , permuting neurons will permute entries in r i . Thus, we can assume that for two models, r</p><p>(1) i and r</p><p>(2) i are not correlated and thus have an expected inner product of 1/m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>It follows from Definition</head><formula xml:id="formula_409">2.1 that E[Overlap(V (1) k , V (2) k )] = k i=1 E[(r (1) i • r (2) i ) 2 ] = k( 1 m ) = k m<label>(268)</label></formula><p>and thus the eigenspace overlap of at dimension k would be approximately k m (t</p><formula xml:id="formula_410">(1) 1 • t (2)</formula><p>1 ) 2 . This explains the peak at dimension m and the linear growth before it.</p><p>From our results on autocorrelation matrices in Section 3.1 and Appendix F.1, we have E</p><formula xml:id="formula_411">[x] (1) ≈ t (1) 1 and E[x] (2) ≈ t (2) 1</formula><p>where E is the normalized expectation. Hence when k = m, the overlap is approximately</p><formula xml:id="formula_412">( E[x] (1) • E[x] (2) ) 2 . Since E[x]</formula><p>(1) and E[x] (2) are the identical for the input layers, the overlap is expected to be very high at dimension m for input layers. For other hidden layers in a ReLU network, x are output of ReLU and thus non-negative. Two non-negative vectors E[x] (1) and E[x] (2) still have relatively large dot product, which contributes to the high overlap peak.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3.1 The Decreasing Overlap After Output Dimension</head><p>Consider the (m + 1)-th eigenvector h   </p><formula xml:id="formula_413">m ⊗ t (2) 1 since ( E[x] (1) • E[x] (2) ) 2 is large.</formula><p>Moreover, since the remaining eigenvectors of the autocorrelation matrix no longer has the all positive property as the first eigenvector and structure of the convariance Σ x is directly associated with the ordering of the input neurons which are randomly permuted across different models, the overlap between other eigenvectors of the autocorrelation matrix across different models will be close to random, hence the overlap after the top m dimension will decrease until the eigenspaces has sufficiently many basis vectors to make the random overlap large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3.2 The Output Layer</head><p>Note that for the last layer satisfying the assumptions in Theorem G.1, the overlap will stay high before dimension m and be approximately (t 1 • t 2 ) 2 since the output neurons directly correspondence to classes, and hence neurons cannot be permuted. In this case, the overlap will be approximately (t 1 • t 2 ) 2 for all dimension k ≤ m. This is consistent with our observations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3.3 Explaining "Failed Cases" of Eigenspace Overlap</head><p>As shown in Fig. <ref type="figure" target="#fig_14">9</ref> and Fig. <ref type="figure" target="#fig_15">10</ref>, the nontrivial top eigenspace overlap does not necessarily peak at the output dimension for all layers. Some layers has a low peak at very small dimensions and others has a peak at a larger dimension. With the more complete analysis provided above, we now proceed to explain these two phenomenons. The major reason for such phenomenons is that the assumption of autocorrelation matrix being sufficiently close to rank 1 is not always satisfied. In particular, following the notations in Theorem G.1, for these exceptional layers we have τ m γ 1 &lt; τ 1 γ 2 . We first consider the first phenomenon (early peak of low overlap) and take fc2:F-200 2 (MNIST) in as an example. Here Fig. <ref type="figure" target="#fig_4">23</ref> As reflected by the first row of <ref type="bibr">Fig. 23(c) and Fig. 23(d)</ref>, for i ≤ 9 we have h i ≈ r i ⊗ E[x], which falls in the regime of Theorem G.1. Hence we are seeing an linearly growing pattern of the overlap for dimension less than 10 and reaches a mean overlap of around 0.012 by dimension 9. If following this linear trend, the overlap would be close to 0.25 by the output dimension of 200. However, since the 10-th eigenvalue of the output Hessian is significantly smaller, little of the 10-19 dimensional eigenspace were contributed by E[x], hence the overlap of dimension larger than 10 falls into the regime discussed in Appendix G.3.1, for which we see a sharp decrease of overlap after dimension 9. Note that this example shows that Kronecker factorization can be used to predict when our conditions in Theorem G.1 fails and also predict the condition can be satisfied up to which dimension. As shown in Fig. <ref type="figure" target="#fig_6">24</ref>, similar explanation also applies to convolutional layers in larger networks.</p><p>We then consider the second phenomenon (delayed peak) and take conv2:VGG11-W200 (CIFAR10) in as an example. Here Fig. <ref type="figure" target="#fig_28">25</ref>(a) is identical to Fig. <ref type="figure" target="#fig_15">10(d</ref>), which has the overlap peak later than the output dimension 200. In this case, the second eigenvalue of the auto correlation matrix is still not negligible compared to the top eigenvalue. What differentiate this case from the first phenomenon is that the eigenvalues of the output Hessian no longer has a significant peak -instead it has a heavy tail which is necessary for high overlap.</p><p>Towards dimension m there gradually exhibits higher correspondence to later eigenvectors of the input autocorrelation matrix and hence less correspondence to E[x]. This eventually results in the delayed and flattened peak. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4 Batch Normalization and Zero-mean Input</head><p>In this section, we show the results on networks with using Batch normalization (BN) <ref type="bibr" target="#b12">(Ioffe &amp; Szegedy, 2015)</ref>. For layers after BN, we have E[x] ≈ 0 so that E[x]E[x] T no longer dominates Σ x and the low rank structure of E[xx T ] should disappear. Thus, we can further expect that the overlap between top eigenspace of layer-wise Hessian among different models will not have a peak.</p><p>Table <ref type="table" target="#tab_14">9</ref> shows the same experiments done in Table <ref type="table">6</ref>. The values for each network are the average of 3 different models. It is clear that the high inner product and large spectral ratio both do not hold here, except for the first layer where there is no normalization applied. Note that we had channel-wise normalization (zero-mean for each channel but not zero-mean for x) for conv1 in LeNet5 so that the spectral ratio is also small.      with BN. The approximation using Kronecker factorization is also worse than the case without BN (Fig. <ref type="figure">2</ref>). However, the approximation still gives meaningful information as the overlap of top eigenspace is still highly nontrivial. apply to all models at the minima, even when the models have around c significant large eigenvalues. As shown in Fig. <ref type="figure" target="#fig_36">30</ref>, the class clustering is very weak but there are still around c significant large eigenvalues. We conjecture that the class clustering of logit gradients may be a sufficient but not necessary condition for the Hessian to be low rank at minima.   set η = 10. We also tried η with the same decay schedule as learning rate (multiply η by 10 every time the learning rate is multiplied by 0.1) and the results are similar to those without decay. We also used the same Monte Carlo method as in <ref type="bibr" target="#b1">Dziugaite &amp; Roy (2017)</ref> to calculate the final PAC-Bayes bound. Except that we used 50000 iterations instead of 150000 iterations because extra iterations do not further tighten the bound significantly. We use sample frequency 100 and δ = 0.01 as in that paper.</p><p>The complete experiment results are listed in Table <ref type="table" target="#tab_15">10</ref>. We follow the same naming convention as in <ref type="bibr" target="#b1">Dziugaite &amp; Roy (2017)</ref>  We also plotted the final posterior variance, s. Fig. <ref type="figure" target="#fig_38">31</ref> shown below is for T-200 2 10 . For posterior variance optimized with our algorithms (APPR, ITER, and ITER.M) we can see that direction associated with larger eigenvalue has a smaller variance. This agrees with our presumption that top eigenvectors are aligned with sharper directions and should have smaller variance after optimization. The effect is more significant and consistent for Iterative Hessian, where the PAC-Bayes bound is also tighter.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Overlap between dominate eigenspace of layer-wise Hessian at different minima for fc1:LeNet5 (left) with output dimension 120 and conv11:ResNet18-W64 (right) with output dimension 64. 10 singular values of the top 4 eigenvectors of the layer-wise Hessian of fc1:LeNet5 after reshaped as matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Top eigenspace of layer-wise Hessians</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 2: Comparison between the approximated and true layer-wise Hessian of F-200 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Eigenspectrum of the layer-wise output Hessian E[M ] and the layer-wise weight Hessian H L (w (p) ).The vertical axes denote the eigenvalues. Similarity between the two eigenspectra is a direct consequence of a low rank E[xx T ] and the decoupling conjecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Overlap between the top k dominating eigenspace of different independently trained models. The overlap peaks at the output dimension m. The eigenspace overlap is defined in Definition 2.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Lemma B.4. Note that as n → ∞ we have n − α 2 and n − α−β 2 converging to 0 as we set β &lt; α. Since x is of bounded expectation and variance, 5n − α 2 (x k + x l ) converges in distribution to 0. Therefore u − u| x d − → 0 and hence y| x d − → y. Since z is determined by y, to prove z| D,x d − → z, we now only ne ed to show z| D d − → z.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>For any &gt; 0, fix . Note that M * 2 F is just substituting D, D by D , D in the polynomial characterized by Eq. (175). From Corollary B.4 we have the convergence of the difference of the expectation of the two polynomials, namely | M 2 F − M * 2 F | &lt; for sufficiently large n. Since the spectral norm of M * is on the order of constant from Lemma B.15, we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>) Combining Lemma B.21, Lemma B.22, and Lemma B.23 completes the proof of Lemma B.19.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>so we can directly apply the exact same result of the two norm bounds (Lemma B.3, Corollary B.1) on V . Now we prove Lemma B.25 as the equivalent of Lemma B.7. lemma B.25. Let V W ⊗ U , then P V V T V is the projection matrix from R nd onto the subspace spanned by all rows of V = W ⊗ U . Moreover, for all &gt; 0,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>[M ] and E[xx T ] explicitly. Let m and v be an eigenvector of E[M ] and E[xx T ] respectively, with corresponding eigenvalues λ m and λ v . Since both matrices are positive semi-definite, m ⊗ v is an eigenvector of E[M ] ⊗ E[xx T ] with eigenvalue λ m λ v . In this way, since E[M ] has m eigenvectors and E[xx T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Eigenspace overlap of different models of LeNet5 trained with different hyperparameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 :</head><label>8</label><figDesc>Figure 7: Top Eigenspace overlap for varients of VGG11 on CIFAR10 and CIFAR100</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Top eigenspace overlap for layers with an early low peak. Figures in the second row are the zoomed in versions of the figures in the first row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Top eigenspace overlap for layers with a delayed peak.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Correspondence with E[M ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Eigenvector Correspondence for fc2:LeNet5. (m=84)</figDesc><graphic coords="59,83.76,75.97,210.05,88.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Eigenvector Correspondence for conv1:LeNet5. (m=6)</figDesc><graphic coords="59,310.54,354.12,210.05,67.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 15 :Figure 16 :Figure 17 :Figure 18 :Figure 19 :</head><label>1516171819</label><figDesc>Figure 15: Eigenvector Correspondence for conv2:LeNet5. (m=16)</figDesc><graphic coords="59,83.76,354.12,210.05,67.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Top eigenvalues of E[M ] along training (fc1:LeNet5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Ratio between top singular value and Frobenius norm of matricized dominating eigenvectors. (LeNet5 on CIFAR10). The horizontal axes denote the index i of eigenvector h i , and the vertical axes denote Mat(h i ) / Mat(h i ) F .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>top m eigenvalues of the layer-wise Hessian are γ (1) 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>m+1 of the first model. Following the Kronecker factorization approximation and assumptions in Theorem G.1, we have h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head></head><label></label><figDesc>top m eigenspace of the first model is approximately I m ⊗ t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head></head><label></label><figDesc>be orthogonal to the top m eigenspace of the first model. It will also have low overlap with I</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 22 :</head><label>22</label><figDesc>Figure 22: Top eigenspace overlap for the final fully connected layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 23 :Figure 24 :</head><label>2324</label><figDesc>Figure 23: Eigenspace overlap, eigenspectrum, and cropped (upper 20 × 20 block) eigenvector correspondence matrices for fc2:F-200 2 (MNIST)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 25 :</head><label>25</label><figDesc>Figure 25: Eigenspace overlap, eigenspectrum, and cropped (upper 50 × 50 block)eigenvector correspondence matrices for conv2:VGG11-W200 (CIFAR10)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Fig. 27 Figure 27 :</head><label>2727</label><figDesc>Fig. 27(b) shows the eigenvector correspondence matrix of True Hessian with E[xx T ] for fc1:LeNet5. Because E[xx T ] is no longer close to rank 1, only very few eigenvectors of the layer-wise Hessian will have high correspondence with the top eigenvector of E[xx T ], as expected. This directly leads to the disappearance of peak in top eigenspace overlap of different models, as shown in Fig. 27. The peak still exists in conv1 because BN is not applied to the input.</figDesc><graphic coords="66,314.26,151.29,93.02,93.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head></head><label></label><figDesc>Top eigenvalues of approximated and exact layer-wise Hessian for fc2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head></head><label></label><figDesc>Top eigenspace overlap between approximated and true layer-wise Hessian.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure 28 :</head><label>28</label><figDesc>Figure 28: Comparison between the true and approximated layer-wise Hessians for LeNet5-BN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Fig. 28</head><label>28</label><figDesc>Fig.28compare the eigenvalues and top eigenspaces of the approximated Hessian and the true Hessian for LeNet5 with BN. The approximation using Kronecker factorization is also worse than the case without BN (Fig.2). However, the approximation still gives meaningful information as the overlap of top eigenspace is still highly nontrivial.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head></head><label></label><figDesc>Clustering of Γ with logits at initialization. Clustering of ∆ with logits at initialization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>Figure 29 :</head><label>29</label><figDesc>Figure 29: Logit clustering behavior of ∆ and Γ at initialization (fc1:T-200 2 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Figure 30 :</head><label>30</label><figDesc>Figure 30: Class clustering behavior of ∆ and Γ at minimum. (fc1:T-200 2 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head></head><label></label><figDesc>except adding T-200 2 we introduced in Section 3. T-600 10 , T-600 2 10 , and T-200 2 10 are trained on standard MNIST with 10 classes, and others are trained on MNIST-2 (see Appendix E.1), in which we combined class 0-4 and class 5-9. In Table 10, Prev means the previous results in Dziugaite &amp; Roy (2017), APPR means Approximated Hessian, ITER means Iterative Hessian, ITER (D) means Iterative Hessian with decaying η, ITER.M means Iterative Hessian with approximated output Hessian. BASE are Base PAC-Bayes optimization as in the previous paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head>Figure 31 :</head><label>31</label><figDesc>Figure 31: Optimized posterior variance, s. (fc1:T-200 2 , trained on MNIST), the horizontal axis is ordered with decreasing eigenvalues.</figDesc><graphic coords="71,319.30,341.11,88.19,76.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Optimized PAC-Bayes bounds using different methods. T-n m and R-n m represents network F-n m trained with true/random labels. TESTER. gives the empirical generalization gap. BASE represents the bound given by<ref type="bibr" target="#b1">Dziugaite &amp; Roy (2017)</ref>. OURS represents the bound we get.</figDesc><table><row><cell>Model</cell><cell cols="7">T-600 T-1200 T-300 2 T-600 2 R-600 T-60010 T-200 2 10</cell></row><row><cell cols="2">TestEr. 0.015</cell><cell>0.016</cell><cell>0.015</cell><cell>0.015</cell><cell>0.493</cell><cell>0.018</cell><cell>0.021</cell></row><row><cell>BASE</cell><cell>0.154</cell><cell>0.175</cell><cell>0.169</cell><cell>0.192</cell><cell>0.605</cell><cell>0.287</cell><cell>0.417</cell></row><row><cell>OURS</cell><cell>0.120</cell><cell>0.142</cell><cell>0.125</cell><cell>0.146</cell><cell>0.568</cell><cell>0.213</cell><cell>0.215</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>With probability 1 over W(1) and W (2) , Proof of Lemma B.21. Unsurprisingly, this proof will be very similar to the proof of Lemma B.16. Recall that H *</figDesc><table><row><cell>lim n→∞</cell><cell>H 2 F F H *  2</cell><cell>= 1.</cell></row><row><cell>1</cell><cell></cell><cell></cell></row></table><note>T ] F and we know that η 4 &lt; H * F &lt; 2c 2 from Lemma B.15. We can conclude that for large n, η/32 &lt; H * F &lt; 2c 2 . lemma B.21.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 Table 2</head><label>22</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>: Datasets</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2"># Data Points</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Train</cell><cell>Test</cell><cell>Input Size</cell><cell># Classes</cell><cell>Label</cell></row><row><cell>CIFAR10</cell><cell cols="3">50000 10000 3 × 32 × 32</cell><cell>10</cell><cell>True</cell></row><row><cell cols="4">CIFAR10-R 50000 10000 3 × 32 × 32</cell><cell>10</cell><cell>Random</cell></row><row><cell>CIFAR100</cell><cell cols="3">50000 10000 3 × 32 × 32</cell><cell>100</cell><cell>True</cell></row><row><cell>MNIST</cell><cell cols="2">60000 10000</cell><cell>28 × 28</cell><cell>10</cell><cell>True</cell></row><row><cell>MNIST-2</cell><cell cols="2">60000 10000</cell><cell>28 × 28</cell><cell>2</cell><cell>True</cell></row><row><cell>MNIST-R</cell><cell cols="2">60000 10000</cell><cell>28 × 28</cell><cell>10</cell><cell>Random</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Structure of F-200 2 on MNIST</figDesc><table><row><cell cols="2"># Name Module</cell><cell cols="2">In Shape Out Shape</cell></row><row><cell>1</cell><cell>Flatten</cell><cell>(28,28)</cell><cell>784</cell></row><row><cell>2 fc1</cell><cell>Linear(784, 200)</cell><cell>784</cell><cell>200</cell></row><row><cell>3</cell><cell>ReLU</cell><cell>200</cell><cell>200</cell></row><row><cell>4 fc2</cell><cell>Linear(200, 200)</cell><cell>200</cell><cell>200</cell></row><row><cell>5</cell><cell>ReLU</cell><cell>200</cell><cell>200</cell></row><row><cell>6 fc3</cell><cell>Linear(200, 10)</cell><cell>200</cell><cell>10</cell></row><row><cell></cell><cell>output</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Structure of LeNet5 on CIFAR-10</figDesc><table><row><cell># Name</cell><cell>Module</cell><cell>In Shape</cell><cell>Out Shape</cell></row><row><cell>1 conv1</cell><cell>Conv2D(3, 6, 5, 5)</cell><cell>(3, 32, 32)</cell><cell>(6, 28, 28)</cell></row><row><cell>2</cell><cell>ReLU</cell><cell>(6, 28, 28)</cell><cell>(6, 28, 28)</cell></row><row><cell cols="2">3 maxpool1 MaxPooling2D(2,2)</cell><cell>(6, 28, 28)</cell><cell>(6, 14, 14)</cell></row><row><cell>4 conv2</cell><cell>Conv2D(6, 16, 5, 5)</cell><cell>(6, 14, 14)</cell><cell>(16, 10, 10)</cell></row><row><cell>5</cell><cell>ReLU</cell><cell cols="2">(16, 10, 10) (16, 10, 10)</cell></row><row><cell cols="3">6 maxpool2 MaxPooling2D(2,2) (16, 10, 10)</cell><cell>(16, 5, 5)</cell></row><row><cell>7</cell><cell>Flatten</cell><cell>(16, 5, 5)</cell><cell>400</cell></row><row><cell>8 fc1</cell><cell>Linear(400, 120)</cell><cell>400</cell><cell>120</cell></row><row><cell>9</cell><cell>ReLU</cell><cell>120</cell><cell>120</cell></row><row><cell>10 fc2</cell><cell>Linear(120, 84)</cell><cell>120</cell><cell>84</cell></row><row><cell>11</cell><cell>ReLU</cell><cell>84</cell><cell>84</cell></row><row><cell>12 fc3</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Structure of LeNet5-BN on CIFAR-10</figDesc><table><row><cell># Name</cell><cell>Module</cell><cell>In Shape</cell><cell>Out Shape</cell></row><row><cell>1 conv1</cell><cell>Conv2D(3, 6, 5, 5)</cell><cell>(3, 32, 32)</cell><cell>(6, 28, 28)</cell></row><row><cell>2</cell><cell>ReLU</cell><cell>(6, 28, 28)</cell><cell>(6, 28, 28)</cell></row><row><cell>3</cell><cell>BatchNorm2D</cell><cell>(6, 28, 28)</cell><cell>(6, 28, 28)</cell></row><row><cell cols="2">4 maxpool1 MaxPooling2D(2,2)</cell><cell>(6, 28, 28)</cell><cell>(6, 14, 14)</cell></row><row><cell>5 conv2</cell><cell>Conv2D(6, 16, 5, 5)</cell><cell>(6, 14, 14)</cell><cell>(16, 10, 10)</cell></row><row><cell>6</cell><cell>ReLU</cell><cell cols="2">(16, 10, 10) (16, 10, 10)</cell></row><row><cell>7</cell><cell>BatchNorm2D</cell><cell cols="2">(16, 10, 10) (16, 10, 10)</cell></row><row><cell cols="3">8 maxpool2 MaxPooling2D(2,2) (16, 10, 10)</cell><cell>(16, 5, 5)</cell></row><row><cell>9</cell><cell>Flatten</cell><cell>(16, 5, 5)</cell><cell>400</cell></row><row><cell>10 fc1</cell><cell>Linear(400, 120)</cell><cell>400</cell><cell>120</cell></row><row><cell>11</cell><cell>ReLU</cell><cell>120</cell><cell>120</cell></row><row><cell>12</cell><cell>BatchNorm1D</cell><cell>120</cell><cell>120</cell></row><row><cell>13 fc2</cell><cell>Linear(120, 84)</cell><cell>120</cell><cell>84</cell></row><row><cell>14</cell><cell>ReLU</cell><cell>84</cell><cell>84</cell></row><row><cell>15</cell><cell>BatchNorm1D</cell><cell>84</cell><cell>84</cell></row><row><cell>16 fc3</cell><cell>Linear(84, 10)</cell><cell>84</cell><cell>10</cell></row><row><cell></cell><cell>output</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Squared dot product</figDesc><table><row><cell>λ2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>T , which is approximately rank 1. Experiments shows that first singular values of Mat(h i ) divided by its Frobenius Norm are usually much larger than 0.5, indicating the top eigenvectors of the layer-wise Hessians are very close to rank 1. Fig.21shows first singular values of Mat(h i ) divided by its Frobenius Norm for i from 1 to 200. We can see that the top eigenvectors of the layer-wise Hessians are very close to rank 1.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Structure of E[xx T ] for BN networks</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(v T 1 E[x]) 2</cell><cell></cell><cell>λ1/λ2</cell></row><row><cell>Dataset</cell><cell>Network</cell><cell cols="2"># fc mean</cell><cell>min</cell><cell>max</cell><cell>mean min max</cell></row><row><cell>MNIST</cell><cell>F-200 2 -BN</cell><cell>2</cell><cell cols="3">0.062 0.001 0.260</cell><cell>1.16</cell><cell>1.04 1.30</cell></row><row><cell></cell><cell>F-600 2 -BN</cell><cell>2</cell><cell cols="3">0.026 0.000 0.063</cell><cell>1.13</cell><cell>1.02 1.26</cell></row><row><cell></cell><cell>F-600 4 -BN</cell><cell>4</cell><cell cols="3">0.027 0.000 0.146</cell><cell>1.11</cell><cell>1.03 1.19</cell></row><row><cell cols="2">CIFAR10 LeNet5-BN</cell><cell>3</cell><cell cols="3">0.210 0.001 0.803</cell><cell>1.54</cell><cell>1.20 1.89</cell></row></table><note>Fig. 27(a)  shows that E[xx T ] is no longer close to rank 1 when having BN. This is as expected. However, E[xx T ] still has a few large eigenvalues.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Full PAC-Bayes bound optimization results</figDesc><table><row><cell></cell><cell></cell><cell>PAC-Bayes</cell><cell>KL</cell><cell>SNN</cell><cell></cell><cell>Test</cell></row><row><cell cols="2">Network Method</cell><cell>Bound</cell><cell>Divergence</cell><cell>loss</cell><cell>λ (prior)</cell><cell>Error</cell></row><row><cell>T-600</cell><cell>PREV</cell><cell>0.161</cell><cell>5144</cell><cell>0.028</cell><cell>-</cell><cell>0.017</cell></row><row><cell></cell><cell>BASE</cell><cell>0.154</cell><cell>4612.6</cell><cell cols="3">0.03373 -1.3313 0.0153</cell></row><row><cell></cell><cell>APPR</cell><cell>0.1432</cell><cell>3980.6</cell><cell cols="3">0.03417 -1.6063 0.0153</cell></row><row><cell></cell><cell>ITER</cell><cell>0.1198</cell><cell>3766.1</cell><cell cols="3">0.02347 -1.2913 0.0153</cell></row><row><cell></cell><cell>ITER(D)</cell><cell>0.1199</cell><cell>3751.1</cell><cell cols="3">0.02366 -1.2913 0.0153</cell></row><row><cell></cell><cell>ITER.M</cell><cell>0.1255</cell><cell>3929.9</cell><cell cols="3">0.02494 -1.3213 0.0153</cell></row><row><cell>T-600 2</cell><cell>PREV</cell><cell>0.186</cell><cell>6534</cell><cell>0.028</cell><cell>-</cell><cell>0.016</cell></row><row><cell></cell><cell>BASE</cell><cell>0.1921</cell><cell>6966.6</cell><cell cols="3">0.03262 -1.4163 0.0148</cell></row><row><cell></cell><cell>APPR</cell><cell>0.1658</cell><cell>5176.1</cell><cell cols="3">0.03468 -2.0963 0.0148</cell></row><row><cell></cell><cell>ITER</cell><cell>0.1456</cell><cell>5086.5</cell><cell cols="3">0.02473 -1.7963 0.0148</cell></row><row><cell></cell><cell>ITER(D)</cell><cell>0.1443</cell><cell>4956.8</cell><cell cols="3">0.02523 -1.7963 0.0148</cell></row><row><cell></cell><cell>ITER.M</cell><cell>0.1502</cell><cell>5024.5</cell><cell cols="3">0.02767 -1.8363 0.0148</cell></row><row><cell>T-1200</cell><cell>PREV</cell><cell>0.179</cell><cell>5977</cell><cell>0.027</cell><cell>-</cell><cell>0.016</cell></row><row><cell></cell><cell>BASE</cell><cell>0.1754</cell><cell>5917.6</cell><cell cols="3">0.03295 -1.5463 0.0161</cell></row><row><cell></cell><cell>APPR</cell><cell>0.1725</cell><cell>5318.8</cell><cell cols="3">0.03701 -1.8313 0.0161</cell></row><row><cell></cell><cell>ITER</cell><cell>0.1417</cell><cell>5071</cell><cell cols="3">0.02292 -1.4763 0.0161</cell></row><row><cell></cell><cell>ITER(D)</cell><cell>0.1413</cell><cell>5021.1</cell><cell cols="3">0.02316 -1.4763 0.0161</cell></row><row><cell></cell><cell>ITER.M</cell><cell>0.1493</cell><cell>5185.4</cell><cell cols="3">0.02576 -1.5363 0.0161</cell></row><row><cell>T-300 2</cell><cell>PREV</cell><cell>0.17</cell><cell>5791</cell><cell>0.027</cell><cell>-</cell><cell>0.015</cell></row><row><cell></cell><cell>BASE</cell><cell>0.1686</cell><cell>5514.9</cell><cell cols="2">0.03329 -1.1513</cell><cell>0.015</cell></row><row><cell></cell><cell>APPR</cell><cell>0.1434</cell><cell>4105.4</cell><cell cols="2">0.03296 -1.8063</cell><cell>0.015</cell></row><row><cell></cell><cell>ITER</cell><cell>0.1249</cell><cell>3873.2</cell><cell cols="2">0.02514 -1.4763</cell><cell>0.015</cell></row><row><cell></cell><cell>ITER(D)</cell><cell>0.1244</cell><cell>3833.7</cell><cell cols="2">0.02526 -1.4763</cell><cell>0.015</cell></row><row><cell></cell><cell>ITER.M</cell><cell>0.1308</cell><cell>3987.2</cell><cell cols="2">0.02721 -1.5713</cell><cell>0.015</cell></row><row><cell>R-600</cell><cell>PREV</cell><cell>1.352</cell><cell>201131</cell><cell>0.112</cell><cell>-</cell><cell>0.501</cell></row><row><cell></cell><cell>BASE</cell><cell>0.6046</cell><cell>1144.8</cell><cell>0.507</cell><cell cols="2">-1.8263 0.4925</cell></row><row><cell></cell><cell>APPR</cell><cell>0.5653</cell><cell>390.25</cell><cell>0.5066</cell><cell cols="2">-2.4713 0.4925</cell></row><row><cell></cell><cell>ITER(D)</cell><cell>0.5681</cell><cell>431.62</cell><cell>0.5066</cell><cell cols="2">-2.4513 0.4925</cell></row><row><cell></cell><cell>ITER.M</cell><cell>0.5616</cell><cell>340.62</cell><cell>0.5065</cell><cell cols="2">-2.5263 0.4925</cell></row><row><cell>T-200 2 10</cell><cell>BASE</cell><cell>0.4165</cell><cell>21896</cell><cell cols="3">0.04706 -1.1513 0.0208</cell></row><row><cell></cell><cell>APPR</cell><cell>0.2621</cell><cell>11068</cell><cell>0.0366</cell><cell cols="2">-1.4213 0.0208</cell></row><row><cell></cell><cell>ITER</cell><cell>0.2145</cell><cell>9821</cell><cell cols="3">0.02229 -1.1513 0.0208</cell></row><row><cell></cell><cell>ITER(D)</cell><cell>0.2311</cell><cell>9758.5</cell><cell cols="3">0.03071 -1.1513 0.0208</cell></row><row><cell></cell><cell>ITER.M</cell><cell>0.2728</cell><cell>13406</cell><cell cols="3">0.02605 -1.1513 0.0208</cell></row><row><cell>T-600 10</cell><cell>BASE</cell><cell>0.2879</cell><cell>12674</cell><cell cols="2">0.03854 -1.1513</cell><cell>0.018</cell></row><row><cell></cell><cell>APPR</cell><cell>0.2424</cell><cell>9095.8</cell><cell cols="2">0.04159 -1.6013</cell><cell>0.018</cell></row><row><cell></cell><cell>ITER</cell><cell>0.2132</cell><cell>8697.9</cell><cell cols="2">0.02947 -1.3063</cell><cell>0.018</cell></row><row><cell></cell><cell>ITER.M</cell><cell>0.2227</cell><cell>8870.9</cell><cell cols="2">0.03294 -1.4613</cell><cell>0.018</cell></row><row><cell>T-600 2 10</cell><cell>BASE</cell><cell>0.3472</cell><cell>17212</cell><cell cols="3">0.03884 -1.1513 0.0186</cell></row><row><cell></cell><cell>APPR</cell><cell>0.2896</cell><cell>11618</cell><cell cols="3">0.04723 -2.0563 0.0186</cell></row><row><cell></cell><cell>ITER</cell><cell>0.2431</cell><cell>10568</cell><cell cols="3">0.03057 -1.5713 0.0186</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>where</p><p>similar to Eq. ( <ref type="formula">254</ref>). The full output Hessian for the entire training sample is thus</p><p>We can then approximate the eigenvectors of the full Hessian H L (θ) using the eigenvectors of E[M x ]. Let the i-th eigenvector of H L (θ) be v i and that of E[M x ] be u i . We may then break up u i into segments corresponding to different layers as in</p><p>where for all layer p, u p)  . Motivated by the relation between G x and F x , the i-th eigenvector of H L (θ) can be approximated as the following. Let</p><p>We then have</p><p>We can then use the Gram-Schmidt process to get the basis vectors of the approximated eigenspace.</p><p>Another reason for this approximation is that the expectation is the input of each layer E[x (p) ] dominates its covariance as shown in Appendix F.1. Thus, the approximate is accurate for top eigenvectors and also top eigenspace. For latter eigenvectors, the approximation would not be as accurate since this approximate loses all information in the covariance of the inputs.</p><p>We also approximated the eigenvalues using this approximation. Let the i-th eigenvalue of H L (θ) be λ i and that of E[M x ] be σ i . We have</p><p>Below we show the approximation of the eigenvalues top eigenspace using this method. The eigenspace overlap is defined as in Definition 2.1. We experimented on several fully connected networks, the results shown below are for F-200 2 (same as Fig.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Eigenvector Correspondence</head><p>In this section, we leverage the idea of eigenvector matricization (Definition 2.2) and analyze the validity of the decoupling conjecture using a matrix which we defined as the eigenvector corresponding matrix. First let us recall the definition of eigenvector matricization Definition 2.2 Consider a layer with input dimension n and output dimension m. For an eigenvector h ∈ R mn of its layer-wise Hessian, the matricized form of h is Mat(h) ∈ R m×n where Mat(h) i,j = h (i−1)m+j .</p><p>Suppose the i-th eigenvector for E[xx T ] is v i and the j-th eigenvector for E[M ] is u j . Then the Kronecker product</p><p>] has an eigenvector u j ⊗ v i . Therefore if the decoupling conjecture is true, one would expect that the top eigenvector of the layer-wise Hessian have a clear correspondence with the top eigenvectors of its two components. Note that u ⊗ v is just the flattened matrix uv T .</p><p>More concretely, to demonstrate the correspondence between the eigenvectors of the layerwise hessian and the eigenvectors of matrix E[M ] and E[xx T ], we introduce "eigenvector correspondence matrices" as shown in Fig. <ref type="figure">11</ref>. Definition F.1 (Eigenvector Correspondence Matrices). For layer-wise Hessian matrix H ∈ R mn×mn with eigenvectors h 1 , • • • , h mn , and its corresponding auto-correlation matrix</p><p>The correspondence between v i and h j can be defined as</p><p>For the output Hessian matrix</p><p>We may then define the eigenvector correspondence matrix between H and E[xx T ] as a n × mn matrix whose i, j-th entry is Corr(v i , h j ), and the eigenvector correspondence matrix between H and E[M ] as a m × mn matrix whose i, j-th entry is Corr(u i , h j ).</p><p>Intuitively, if the i, j-th entry of the corresponding matrix is close to 1, then the eigenvector h j is likely to be the Kronecker product of v i (or u i ) with some vector. Note that if the decoupling conjecture holds absolutely, every eigenvector of the layer-wise Hessian (column of the correspondence matrices) should have a perfect correlation of 1 with exactly one of v i and one of u i . In Fig. <ref type="figure">11</ref> we can see that the correspondence matrices for the true layer-wise Hessian approximately satisfies this property for top eigenvectors. The similarity between the correspondence patterns for the true and approximated Hessian also verifies the validity of the Kronecker approximation for dominating eigenspace.</p><p>In   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.5 Outliers in Hessian Eigenspectrum</head><p>One characteristic of Hessian that has been mentioned by many is the outliers in the spectrum of eigenvalues. <ref type="bibr" target="#b34">Sagun et al. (2018)</ref> suggests that there is a gap in Hessian eigenvalue distribution around the number of classes c in most cases, where c = 10 in our case. A popular theory to explain the gap is the class / logit clustering of the logit gradients <ref type="bibr" target="#b2">(Fort &amp; Ganguli, 2019;</ref><ref type="bibr" target="#b29">Papyan, 2019;</ref><ref type="bibr" target="#b30">2020)</ref>. Note that these explanations can be consistent with our heuristic formula for the top eigenspace of output Hessian at initialization-in the two-layer setting we considered the logit gradients are indeed clustered.</p><p>In the layer-wise setting, the clustering claim can be formalized as follows: For each class k ∈ [c] and logit entry l ∈ [c], with Q be defined as in Eq. ( <ref type="formula">25</ref>), and (x, y) as the input, label pair, let</p><p>Then at the initialization, for each logit entry j, {∆ i,j } i∈[c] is clustered around the "logit center"</p><p>With the decoupling conjectures, we may also consider similar claims for output Hessians, where</p><p>A natural extension of the clustering phenomenon on output Hessians is then as follows: At the initialization, for each logit entry j,</p><p>. Note that we have the layer-wise Hessian and layer-wise output Hessian</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low-rank Hessian at Random Initialization and Logit Gradient Clustering</head><p>We first briefly recapture our explanation on the low-rankness of Hessian at random initialization. In Section 4 and Appendix B.2, we have shown that for a two layer ReLU network with Gaussian random initialization and Gaussian random input, the output hessian of the first layer M (1) is approximately 1 4 W (2)T AW (2) . We then heuristically extend this approximation to a randomly initialized L-layer network, that with S (p) = W (L) W (L−1) • • • W (p+1) , the output Hessian of the p-th layer H (p) can be approximated by M (p) where</p><p>Since A is strictly rank c − 1 with null space of the all-one vector, H (p) is strictly rank c − 1. Thus H (p) is approximately rank c − 1, and so is the corresponding layerwise Hessian according to the decoupling conjecture.</p><p>Now we discuss the connection between our analysis with the theory of logit gradient clustering. As previously observed by <ref type="bibr" target="#b29">Papyan (2019)</ref>, for each logit entry l, {∆ i,j } l∈[c] are clustered around the logit gradients</p><p>Similar clustering effects for {Γ i,j } l∈[c] were also empirically observed by our experiments. Moreover, through the approximation above and the decoupling conjecture, for each logit entry j, the cluster centers Γ j and ∆ j can be approximated by</p><p>Following <ref type="bibr" target="#b29">Papyan (2019)</ref>, we used t-SNE (Van der <ref type="bibr" target="#b42">Maaten &amp; Hinton, 2008)</ref> to visualize the logit gradients. As we see in Fig. <ref type="figure">29</ref>, the "logit centers" of the clustering directly corresponds to the approximated dominating eigenvectors of the Hessian, which is consistent with our analysis.</p><p>Gradient Clustering at Minima Currently our theory does not provide an explanation to the low rank structure of Hessian at the minima. However we have observed that the class clustering of logit gradients does not universally</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Computing PAC-Bayes Bounds with Hessian Approximation</head><p>Given a model parameterized with θ and an input-label pair [ l(θ,</p><p>as the expected and empirical classification error of θ, respectively. We define the measurable hypothesis space of parameters H := R P . For any probabilistic measure P in H, let e(P ) = E θ∼P e(θ), e(P ) = E θ∼P e(θ), and ȇ(P ) = E θ∼P L(θ). Here ȇ(P ) serves as a differentiable convex surrogate of e(P ).</p><p>Theorem H.1 (Pac-Bayes Bound). (McAllester, 1999) <ref type="bibr" target="#b21">(Langford &amp; Seeger, 2001)</ref> For any prior distribution P in H that is chosen independently from the training set S, and any posterior distribution Q in H whose choice may inference S, with probability 1 − δ,</p><p>Fix some constant b, c ≥ 0 and θ 0 ∈ H as a random initialization, <ref type="bibr" target="#b1">Dziugaite &amp; Roy (2017)</ref> shows that when setting Q = N (w, diag(s)), P = N (θ 0 , λI P ), where w, s ∈ H and λ = c exp (−j/b) for some j ∈ N, and solve the optimization problem</p><p>with initialization w = θ, s = θ 2 , one can achieved a nonvacous PAC-Bayes bound by Eq. ( <ref type="formula">275</ref>).</p><p>In order to avoid discrete optimization for j ∈ N , Dziugaite &amp; Roy (2017) uses the B RE term to replace the bound in Eq. ( <ref type="formula">275</ref>). The B RE term is defined as</p><p>where Q = N (w, diag(s)), P = N (θ 0 , λI P ). The optimization goal actually used in the implementation is thus</p><p>Algorithm 1 shows the algorithm for Iterative Hessian (ITER) PAC-Bayes Optimization. If we set η = T , the algorithm will be come Approximate Hessian (APPR) PAC-Bayes Optimization. In the algorithm, HESSIANCALC(w) is the process to calculate Hessian information with respect to the posterior mean w in order to produce the Hessian eigenbasis to perform the change of basis. For very small networks, we can calculate Hessian explicitly but it is prohibitive for most common networks. However, efficient approximate change of basis can be performed using our approximated layer-wise Hessians. In this case, we would just need to calculate the full eigenspace of E[M ] and that of E[xx T ] for each layer. For pth layer, we denote them as U (p) and V (p) respectively with eigenvectors as columns. We can also store the corresponding eigenvalues by doing pairwise multiplications between eigenvalues of E[M ] and E[xx T ].</p><p>After getting the eigenspaces, we can perform the change of basis. Note that we perform change of basis on vectors with the same dimensionality as the parameter vector (or the posterior mean). TOHESSIAN(u) is the process to put a vector u in the standard basis to the Hessian eigenbasis. We first break u into different layers and let u (p) be the vector for the pth layer. We then define Mat (p) as the reshape of a vector to the shape of the parameter matrix W (p) of that layer. We have the new vector v (p) in Hessian basis as </p><p>end for 16: return w, s(ς), λ( ) 17: end procedure</p><p>The new vector v = TOHESSIAN(u) is thus the concatenation of all the v (p) . TOSTANDARD(v) is the process to put a vector v in the Hessian eigenbasis to the standard basis. It is the reverse process to TOHESSIAN. We also break v into layers and let the vector for the pth layer be v (p) . Then, the new vector u (p) is u</p><p>The new vector u = TOSTANDARD(v) is thus the concatenation of all u (p) .</p><p>After getting optimized w, s, λ, we compute the final bound using Monte Carlo methods same as in <ref type="bibr" target="#b1">Dziugaite &amp; Roy (2017)</ref>.</p><p>Note that the prior P is invariant with respect to the change of basis, since its covariance matrix is a multiple of identity λI P . Thus, the KL divergence can be calculate in the Hessian eigenbasis without changing the value of λ. In the Iterative Hessian with approximated output Hessian (ITER.M), we use M to approximate E[M ], as in Eq. (272).</p><p>We followed the experiment setting proposed by <ref type="bibr" target="#b1">Dziugaite &amp; Roy (2017)</ref> in general. In all the results we present, we first trained the models from Gaussian random initialization w 0 to the initial posterior mean estimate w using SGD (lr=0.01) with batch-size 128 and epoch number 1000.</p><p>We then optimize the posterior mean and variance with layer-wise Hessian information using Algorithm 1, where δ = 0.025, b = 100, and c = 0.1. We train for 2000 epochs, with learning rate τ initialized at 0.001 and decays with ratio 0.1 every 400 epochs. For Approximated Hessian algorithm, we set η = 1. For Iterative Hessian algorithm, we</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modular block-diagonal curvature approximations for feedforward architectures</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dangel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hennig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="799" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI</title>
				<meeting>the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Emergent properties of the local geometry of neural loss landscapes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05929</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast approximate natural gradient descent in a kronecker factored eigenbasis</title>
		<author>
			<persName><forename type="first">T</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bouthillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9550" to="9560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An investigation into neural net optimization via hessian eigenvalue density</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2232" to="2241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
				<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">pytorch-hessian-eigentings: efficient pytorch hessian eigendecomposition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Golmant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<ptr target="https://github.com/noahgolmant/pytorch-hessian-eigenthings" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><surname>Learning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A kronecker-factored approximate fisher matrix for convolution layers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="573" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04754</idno>
		<title level="m">Gradient descent happens in a tiny subspace</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On &quot;natural&quot; learning and pruning in multilayered perceptrons</title>
		<author>
			<persName><forename type="first">T</forename><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="881" to="901" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The asymptotic spectrum of the hessian of DNN throughout training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hongler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the relation between the sharpest directions of DNN loss and the SGD step length</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The normalization method for alleviating pathological sharpness in wide neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Karakida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akaho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="6406" to="6416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Karakida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akaho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05992</idno>
		<title level="m">Pathological spectra of the fisher information metric and its variants in deep neural networks</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Universal statistics of fisher information in deep neural networks: Mean field approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Karakida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akaho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019c</date>
			<biblScope unit="page" from="1032" to="1041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T P</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations, ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The design of suboptimal linear time-varying systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kleinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Athans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="159" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bounds for averaging classifiers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptive estimation of a quadratic functional by model selection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Massart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="1302" to="1338" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hessian based analysis of sgd for deep nets: Dynamics and generalization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 SIAM International Conference on Data Mining</title>
				<meeting>the 2020 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="190" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hessian eigenspectra of more realistic nonlinear models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optimizing neural networks with kronecker-factored approximate curvature</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2408" to="2417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Some pac-bayesian theorems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="355" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The full spectrum of deepnet hessians at scale: Dynamics with sgd training and sample size</title>
		<author>
			<persName><forename type="first">V</forename><surname>Papyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07062</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Measurements of three-level hierarchical structure in the outliers in the spectrum of deepnet hessians</title>
		<author>
			<persName><forename type="first">V</forename><surname>Papyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5012" to="5021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Traces of class/cross-class structure pervade deep learning spectra</title>
		<author>
			<persName><forename type="first">V</forename><surname>Papyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.11865</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07476</idno>
		<title level="m">Eigenvalues of the hessian in deep learning: Singularity and beyond</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Empirical analysis of the hessian of overparametrized neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">U</forename><surname>Güney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018, Workshop Track Proceedings</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep information propagation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR</title>
				<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Analytic insights into structure and rank of neural network hessian maps</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bachmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Understanding impacts of high-order loss approximations and features in deep learning interpretation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feizi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5848" to="5856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Chain rules for hessian and higher derivatives made easy by tensor calculus</title>
		<author>
			<persName><forename type="first">M</forename><surname>Skorski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.13292</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Optimization for machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Mit Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hessian-based analysis of large batch training and robustness to adversaries</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4949" to="4959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><surname>Pyhessian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.07145</idno>
		<title level="m">Neural networks through the lens of the hessian</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A short note on the tail bound of wishart distribution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5860</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
