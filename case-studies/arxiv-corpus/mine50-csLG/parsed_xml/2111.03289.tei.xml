<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Regret Analysis for Variance-Adaptive Linear Bandits and Horizon-Free Linear Mixture MDPs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-20">20 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yeoneung</forename><surname>Kim</surname></persName>
							<email>yeoneung@gachon.ac.kr</email>
						</author>
						<author>
							<persName><forename type="first">Insoon</forename><surname>Yang</surname></persName>
							<email>insoonyang@snu.ac.kr</email>
						</author>
						<author>
							<persName><forename type="first">Kwang-Sung</forename><surname>Jun</surname></persName>
							<email>kjun@cs.arizona.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Gachon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Arizona</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Regret Analysis for Variance-Adaptive Linear Bandits and Horizon-Free Linear Mixture MDPs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-20">20 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">8F208B263F01AA75D744318AAFE9059A</idno>
					<idno type="arXiv">arXiv:2111.03289v3[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-10-31T05:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In online learning problems, exploiting low variance plays an important role in obtaining tight performance guarantees yet is challenging because variances are often not known a priori. Recently, considerable progress has been made by Zhang  et al. (2021)  where they obtain a variance-adaptive regret bound for linear bandits without knowledge of the variances and a horizon-free regret bound for linear mixture Markov decision processes (MDPs). In this paper, we present novel analyses that improve their regret bounds significantly. For linear bandits, we achieve</p><p>where d is the dimension of the features, K is the time horizon, and σ 2 k is the noise variance at time step k, and Õ ignores polylogarithmic dependence, which is a factor of d 3 improvement. For linear mixture MDPs with the assumption of maximum cumulative reward in an episode being in [0, 1], we achieve a horizon-free regret bound of Õ(d</p><p>where d is the number of base models and K is the number of episodes. This is a factor of d 3.5 improvement in the leading term and d 7 in the lower order term. Our analysis critically relies on a novel peeling-based regret analysis that leverages the elliptical potential 'count' lemma.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In online learning, variance often plays an important role in achieving low regret bounds. For example, for the prediction with expert advice problem, Hazan and Kale <ref type="bibr" target="#b10">[11]</ref> proposed an algorithm that achieves a regret bound of O( √ VAR K ) where VAR K is a suitably-defined variance of the loss function up to time step K, without knowing VAR K ahead of time. The implication is that when the given sequence of loss functions has a small variance, one can perform much better than the previously known regret bound O( √ K). For multi-armed bandits, Audibert et al. <ref type="bibr" target="#b1">[2]</ref> proposed an algorithm that achieves regret bounds that depends on the variances of the arms, which means that, again, the regret bound becomes smaller as the variances become smaller.</p><p>It is thus natural to obtain similar variance-adaptive bounds for other problems. For example, in d-dimensional stochastic contextual bandit problems, the optimal worst-case regret bound is Õ(σd √ K) where Õ hides polylogarithmic dependencies and σ 2 is a uniform upper bound on the noise variance. Following the developments in other online learning problems, it is natural to ask if we can develop a similar variance-adaptive regret bound. The recent work by Zhang et al. <ref type="bibr" target="#b30">[31]</ref> has provided an affirmative answer. Their algorithm called VOFUL achieves a regret bound of Õ(d 4.5 </p><formula xml:id="formula_0">∑ K k=1 σ 2 k + d 5 )</formula><p>where σ 2 k is the (unknown) noise variance at time step k. This implies that, indeed, it is possible to adapt to the variance and suffer a much lower regret. Furthermore, they show that a similar variance-adaptive analysis can be used to solve linear mixture Markov decision processes (MDPs) with the unit cumulative rewards assumption :</p><formula xml:id="formula_1">h r k h ∈ [0, 1], ∀k<label>(1)</label></formula><p>where r k h is the reward received at episode k and horizon h. They show a regret bound of Õ(d 4.5 √ K + d 9 ), which does not depend on the planning horizon length H up to polylogarithmic factors. We elaborate more on the linear bandit and linear mixture MDP problems in Section 2.</p><p>However, the regret rates of these problems have a large gap between the known lower and the upper bounds. For example, in linear bandits, it is well-known that the regret bound has to be Ω(d √ K) <ref type="bibr" target="#b5">[6]</ref>,</p><p>which rejects the possibility of obtaining o(d</p><formula xml:id="formula_2">∑ K k=1 σ 2 k</formula><p>), yet the best upper bound obtained so far is O(d 4.5 ∑ K k=1 σ 2 k ). Thus, the gap is a factor of d 3.5 , which is quite large. In this paper, we reduce such gaps significantly by obtaining much tighter regret upper bounds. Specifically, we show that a slight variation of VOFUL <ref type="bibr" target="#b30">[31]</ref> for linear bandits has a regret bound of Õ(min{d √ K, d 1.5 ∑ K k=1 σ 2 k )} without knowledge of the variances. This reduces the gap between the upper and lower bounds to only √ d for the leading term in the regret. Furthermore, we employ a similar technique to show that the algorithm VARLin <ref type="bibr" target="#b30">[31]</ref> for linear mixture MDPs with unit cumulative rewards has a regret bound of Õ(d</p><formula xml:id="formula_3">√ K + d 2 )</formula><p>. At the heart of our analysis is a direct peeling of the instantaneous regret terms using an elliptical potential 'count' lemma (EPC). EPC bounds, given q &gt; 0, how many times</p><formula xml:id="formula_4">x k 2 V −1 k−1 ≥ q happens from time k = 1 to ∞ where V k−1 = ∑ k−1 s=1 x s x ⊺</formula><p>s . Our lemma is an improved and generalized version of <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">Exercise 19.3]</ref>, which was originally used for improving the regret bound of linear bandit algorithms. We provide the proofs of our main results for linear bandits and linear mixture MDPs in Section 3 and Section 4 respectively. Finally, we conclude the paper with exciting future directions.</p><p>Related work. There are numerous works on linear bandit problems such as <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b16">17]</ref> where the information of variance is not used. On the other hand, variance can be exploited to obtain better regret <ref type="bibr" target="#b1">[2]</ref>. Recently, works by <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33]</ref> proposed ways to infuse the variance information in the regret analysis which improves the standard regret bound. Reinforcement learning with linear function approximation has been widely studied to develop efficient learning methods that work for large state-action space <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28]</ref>. To our knowledge, all aforementioned works derived a regret bound that depends on the planning horizon H polynomially. It was Zhang et al. <ref type="bibr" target="#b30">[31]</ref> who first remove the polynomial dependence of H in the linear mixture MDP problem, achieving a bound of Õ(d 4.5 √ K + d 9 ). In contrast, our analysis shows that their algorithm in fact achieves significantly better bound of Õ(d √ K + d 2 ). Note that these results assume the unit cumulative rewards assumption (1) and time-homogeneous transition models. In a similar setup where r h,k ∈ [0, 1] with time-inhomogeneous transition models, Zhou et al. <ref type="bibr" target="#b32">[33]</ref> achieve the regret Õ( √</p><formula xml:id="formula_5">d 2 H + dH 3 √ HK + d 2 H 3 + d 3 H 2 )</formula><p>and show a lower bound of Ω(dH 3 2 √ K). These problem setups are incompatible to the setup of <ref type="bibr" target="#b30">[31]</ref> and ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Definition</head><p>Notations. We denote d-dimensional ℓ 2 ball by</p><formula xml:id="formula_6">B d 2 (R) ∶= {x ∈ R d ∶ x 2 ≤ R} and define B d 1 (R) similarly for the ℓ 1 ball. Let [N ] ∶= {1, 2, . . . . , N } for N ∈ N. Given ℓ ∈ R and x ∈ R d ,</formula><p>we define the clipping operator as follows (take 0 0 = 0):</p><formula xml:id="formula_7">(x) ℓ ∶= min x 2 , 2 −ℓ ⋅ x x 2 . (<label>2</label></formula><formula xml:id="formula_8">)</formula><p>Linear bandits. The linear bandit problem has the following protocol. At time step k, the learner observes an arm set X k ⊆ B d 2 (1), chooses an arm x k ∈ X k , pulls it. The learner then receives a stochastic reward</p><formula xml:id="formula_9">r k = x ⊺ k θ * + ǫ k where θ * ∈ B d 2<label>(1</label></formula><p>) is an unknown parameter and ǫ k is a zeromean stochastic noise. Following <ref type="bibr" target="#b30">[31]</ref>, we assume that</p><formula xml:id="formula_10">(i) ∀k ∈ [K], r k ≤ [− 1 2 , 1 2 ] almost surely, (ii) E[ǫ k F k ] = 0 where F k = σ(x 1 , ǫ 1 , ..., x k−1 , ǫ k−1 , x k ), and (iii) E[ǫ 2 k F k ] = σ 2 k . Note that the bound on r k implies that ǫ k ≤ 1 almost surely. Our goal is to minimize the regret R K = ∑ K k=1 max x∈X k x ⊺ θ * − x ⊺ k θ * .</formula><p>Linear mixture MDPs. We consider an episodic Markov Decision Process (MDP) with a tuple (S, A, r(s, a), P (s ′ s, a), K, H) where S is the state space, A is the action space, r ∶ S × A → [0, 1] is the reward function, P (s ′ s, a) is the transition probability, K is the number of episodes, and H is the planning horizon. A policy is defined as π = {π h ∶ S → D(A)} H h=1 where D(A) is a set of all distributions over A. For each episode k ∈ [K], the learner chooses a policy π k , and then the environment executes π k on the MDP by successively following</p><formula xml:id="formula_11">a k h ∼ π k h (s k h ) and s k h+1 ∼ P (⋅ s k h , a k h ).</formula><p>Then, the learner observes the rewards {r k h ∈ [0, 1]} k,h and moves onto the next episode. The key modeling assumption of linear mixture MDPs is that the transition probability P is a linear combination of a known set of models {P i }, namely,</p><formula xml:id="formula_12">P = ∑ d i=1 θ * i P i where θ * ∈ B d 1<label>(1</label></formula><p>) is an unknown parameter. We follow <ref type="bibr" target="#b30">[31]</ref> and make the following assumptions:</p><p>• The reward at each time step h and episode k is</p><formula xml:id="formula_13">r k h = r(s k h , a k h ) for some known function r ∶ S × A → [0, 1]. • Unit cumulative rewards:∑ H h=1 r k h ∈ [0, 1] for any policy π k . For a policy π, V π h (s) ∶= max a∈A Q π h (s, a) where Q π h (s, a) = r(s, a) + E s ′ ∼P (⋅ s,a) V π h+1 (s ′ ) and V π H+1 (s) ∶= 0. Denoting V π (s 1 ) = V π 1 (s 1 ) and V * (s 1 ) = V π * (s 1</formula><p>), our goal is to minimize the regret</p><formula xml:id="formula_14">R K = K k=1 V * (s k 1 ) − V k (s k 1 ) .</formula><p>3 Variance-Adaptive Linear Bandits</p><p>In this section, we show that VOFUL of Zhang et al. <ref type="bibr" target="#b30">[31]</ref> has a tighter regret bound than what was reported in their work. Our version of VOFUL, which we call VOFUL2, has slightly different confidence set for ease of exposition. Specifically, we use a confidence set that works for every µ ∈ B d 2 (2) rather than over an ǫ-net of B d 2 (2) (but we do use an ǫ-net for the proof of the confidence set).</p><p>The full pseudocode can be found in Algorithm 1. VOFUL2 follows the standard optimism-based arm selection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b0">1]</ref>. Let ǫ s (θ) ∶= r s − x ⊺ s θ and ǫ 2 s (θ) ∶= (ǫ s (θ)) 2 . With L and ι defined in Algorithm 1, we define our confidence set after k time steps as</p><formula xml:id="formula_15">Θ k ∶= ∩ L ℓ=1 Θ ℓ k (3) where Θ ℓ k ∶= ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ θ ∈ B d 2 (1) ∶ k s=1 x ⊺ s µ ℓ ǫ s (θ) ≤ k s=1 x ⊺ s µ 2 ℓ ǫ 2 s (θ)ι + 2 −ℓ ι, ∀µ ∈ B d 2 (2) ⎫ ⎪ ⎪ ⎬ ⎪ ⎪ ⎭</formula><p>and the clipping operator (z) ℓ is defined in <ref type="bibr" target="#b1">(2)</ref>. The role of clipping is two-fold: (i) it allows us</p><formula xml:id="formula_16">to factor out ∑ s=1 ǫ 2 s (θ) by ∑ s x ⊺ s µ 2 ℓ ǫ 2 s (θ) ≤ (2 −ℓ ) 2 ∑ s=1 ǫ 2 s (θ)</formula><p>and (ii) the lower order term is reduced to the order of 2 −ℓ . Both properties are critical in obtaining variance-adaptive regret bounds as discussed in <ref type="bibr" target="#b30">[31]</ref>. The true parameter is contained in our confidence set with high probability as follows.</p><p>Lemma 1. (Confidence set) Let L, ι, and δ be given as those in Algorithm 1. Then,</p><formula xml:id="formula_17">P(E 1 ∶= {∀k ∈ [K], θ * ∈ Θ k }) ≥ 1 − δ .</formula><p>In fact, in our algorithm, we use the confidence set of ∩ k−1 s=1 Θ s at time step k for a technical reason. VOFUL2 has the following regret bound.</p><p>Theorem 1. VOFUL2 satisfies, with probability at least 1 − 2δ,</p><formula xml:id="formula_18">R K = Õ ⎛ ⎜ ⎝ d 1.5 K k=1 σ 2 k ln(1 δ) + d 2 ln(1 δ) ⎞ ⎟ ⎠ Algorithm 1 VOFUL2 1: Initialize: L = 1 ∨ ⌊log 2 (K)⌋ where ι = 128 ln((12K2 L ) d+2 δ) and δ ≤ e −1 . 2: for k = 1, 2, . . . , K do 3:</formula><p>Observe a decision set X k ⊆ B d 2 (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Compute the optimistic arm as following: x k = arg max x∈X k max θ∈∩ k−1 s=1 Θs x ⊺ θ where Θ s is defined in (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Receive a reward r k . 6: end for where Õ hides poly-logarithmic dependence on {d, K,</p><formula xml:id="formula_19">∑ K k=1 σ 2 k , ln(1 δ)}.</formula><p>Note that one can also show that VOFUL2 can be slightly modified to achieve the regret bound</p><formula xml:id="formula_20">of Õ min d K ln(1 δ), d 1.5 ∑ K k=1 σ 2 k ln(1 δ) + d 2 ln(1 δ</formula><p>) , thus being no worse than OFUL. We postpone the proof of this to Section A.4 to avoid clutter.</p><p>Properties of the confidence sets and implications on the regret. Before presenting the proof of Theorem 1, we provide some key properties of our confidence set (Lemma 3) and the intuition behind our regret bound. First, let us describe a few preliminaries. Define</p><formula xml:id="formula_21">W ℓ,k−1 (µ) ∶= 2 −ℓ I + k−1 s=1 ⎛ ⎝ 1 ∧ 2 −ℓ x ⊺ s µ ⎞ ⎠ x s x ⊺ s .</formula><p>Let θ k be the maximizer of the optimization problem at line 4 of Algorithm 1 and define µ k = θ k −θ * . For brevity, we use a shorthand of</p><formula xml:id="formula_22">W ℓ,k−1 ∶= W ℓ,k−1 (µ k ) = 2 −ℓ I + k−1 s=1 ⎛ ⎝ 1 ∧ 2 −ℓ x ⊺ s µ k ⎞ ⎠ x s x ⊺ s .</formula><p>Finally, we need to define the following event regarding the concentration of the empirical variance around the true variance:</p><formula xml:id="formula_23">E 2 ∶= ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ ∀k ∈ [K], k s=1 ǫ 2 s (θ * ) ≤ k s=1 8σ 2 s + 4 log( 4K(log 2 (K) + 2) δ ) ⎫ ⎪ ⎪ ⎬ ⎪ ⎪ ⎭ ,</formula><p>which is true with high probability as follows.</p><p>Lemma 2. We have</p><formula xml:id="formula_24">P(E 2 ) ≥ 1 − δ Proof.</formula><p>The proof is a direct consequence of Lemma 13 in our appendix.</p><p>Let ℓ k be the integer ℓ such that</p><formula xml:id="formula_25">x ⊺ k µ k ∈ (2 ⋅ 2 −ℓ , 2 ⋅ 2 −ℓ+1 ] and define A k ∶= ∑ k s=1 σ 2</formula><p>s . Lemma 3 below states the properties of our confidence set. Lemma 3. Suppose the events E 1 and E 2 are true. Then, for any k with ℓ k = ℓ, (i) For some absolute constant c 1 ,</p><formula xml:id="formula_26">µ k 2 W ℓ,k−1 ≤ 2 −ℓ 128A k−1 ι + 11 ⋅ 2 −ℓ ι ≤ c 1 2 −ℓ ( A k−1 ι + ι), (ii) There exists an absolute constant c 2 such that x k µ k ≤ c 2 x ⊺ k 2 W −1 ℓ,k−1 √ A k−1 ι + ι .</formula><p>The key difference between Lemma 3 and the results of Zhang et al. <ref type="bibr" target="#b30">[31]</ref> is that we use the norm notations, although the norm involves a rather complicated matrix W ℓ,k−1 . This opens up possibilities of analyzing the regret of VOFUL2 with existing tools such as applying Cauchy-Schwarz inequality and the elliptical potential lemma <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16]</ref>. In particular, Lemma 3(ii) seems useful because if we had such a result with W ℓ,k−1 replaced by</p><formula xml:id="formula_27">V k−1 = λI + ∑ k−1 s=1 x s x ⊺</formula><p>s , then we would have, ignoring the additive term ι,</p><formula xml:id="formula_28">x ⊺ k µ k ≤ x k 2 V −1 k−1 k−1 s=1 σ 2 s ι .</formula><p>Together with the optimism and the standard elliptical potential lemma (see Section 3.1 for details), this leads to</p><formula xml:id="formula_29">R K ≤ K k=1 x ⊺ k µ k ≤ c 2 K k=1 x k 2 V −1 k−1 k−1 s=1 σ 2 s ι ≤ c 2 ⋅ O(d log(T d)) ⋅ K s=1 σ 2 s ι .</formula><p>Since ι is linear in d, we would get the regret bounded by the order of d 1.5 ∑ K k=1 σ 2 k , roughly speaking. However, the discrepancy between W ℓ,k−1 and V k−1 is not trivial to resolve, especially due to the fact that Lemma 3(ii) has µ k on both left and the right hand side. That is, µ k is the key quantity that we need to understand, but we are bounding x k µ k as a function of µ k . The novelty of our analysis of regret is exactly at relating W ℓ,k−1 to V k−1 via a novel peeling-based analysis, which we present below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Proof of Theorem 1</head><p>Throughout the proof, we condition E 1 and E 2 where each one is true with probability at least 1 − δ, as shown in Lemma 1 and 2 respectively. For our regret analysis, it is critical to use Lemma 4 below, which we call the elliptical 'count' lemma. This lemma is a generalization of Lattimore and Szepesvári <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">Exercise 19.3]</ref>, which was originally used therein to improve the dependence of the range of the expected rewards in the regret bound. Similar lemmas have been used in parallel studies <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22]</ref>. In particular, He et al. <ref type="bibr" target="#b11">[12]</ref> employ a lemma similar to elliptical potential count and peeling technique for the regret analysis for the linear MDP as well, which we compare in detail in Section C due to space constraint. We remark that a similar strategy appears in disguise in Russo and Van Roy [20, Proposition 3] as well.</p><formula xml:id="formula_30">Lemma 4. (Elliptical potential count) Let x 1 , . . . , x k ∈ R d be a sequence of vectors with x s 2 ≤ X for all s ∈ [k]. Let V k = τ I + ∑ k s=1 x s x ⊺ s for some τ &gt; 0. Let J ⊆ [k] be the set of indices where x s 2 V −1 s−1 ≥ q. Then, J ≤ 2 ln(1 + q) d ln 1 + 2 e ln(1 + q) X 2 τ .</formula><p>As the name explains, the lemma above bounds how many times</p><formula xml:id="formula_31">x s 2 V −1 s−1</formula><p>can go above a given value q &gt; 0, which is different from existing elliptical potential lemmas that bound the sum of</p><formula xml:id="formula_32">x s 2 V −1 s−1</formula><p>. Let θ k be the θ that maximizes the optimization problem at line 4 of Algorithm 1. We start by the usual optimism-based bounds: due to E 1 , we have</p><formula xml:id="formula_33">R K = K k=1 (max x∈X k (x ⊺ θ * − x ⊺ k θ * )) ≤ K k=1 ( max x∈X k ,θ∈Θ k x ⊺ θ − x ⊺ k θ * ) ≤ k=1 x ⊺ k (θ k − θ * ) = k=1 x ⊺ k µ k .</formula><p>We now take a peeling-based regret analysis that is quite different from existing analysis techniques:</p><formula xml:id="formula_34">R K ≤ K k=1 x ⊺ k (θ k − θ * ) ≤ 2 −L K + L ℓ=1 2 −ℓ+2 K k=1 ½ x ⊺ k µ k ∈ (2 ⋅ 2 −ℓ , 2 ⋅ 2 −ℓ+1 ] ,</formula><p>where L is defined in Algorithm 1. Given ℓ and k, let n k,ℓ be n such that max v∶k≤v≤K,ℓv</p><formula xml:id="formula_35">=ℓ x ⊺ k µ v ∈ (2 −ℓ+n , 2 −ℓ+n+1 ] if such n satisfies n ≥ 1. Otherwise, set n k,ℓ = 0, which means max v∶k≤v≤K,ℓv =ℓ x ⊺ k µ v ≤ 2 −ℓ+n+1 with n = 0. We then define G ℓ,n ∶= {s ∈ [K −1] ∶ ℓ s = ℓ, n s,ℓ = n} and let G ℓ,n [k] ∶= G ℓ,n ∩ [k]. Then, K k=1 ½ x ⊺ k µ k ∈ (2 ⋅ 2 −ℓ , 2 ⋅ 2 −ℓ+1 ] = K k=1 ½ {ℓ k = ℓ} ≤ 1 + n s∈G k,n 1 . Letting V ℓ,n,k−1 ∶= 2 −ℓ I + ∑ s∈G ℓ,n [k−1] x s x ⊺</formula><p>s , a comparison between two matrices W and V is given as follows for every v ∈ {k, . . . , K}:</p><formula xml:id="formula_36">W ℓ,k−1 (µ v ) = 2 −ℓ I + k−1 s=1 ⎛ ⎝ 1 ∧ 2 −ℓ x ⊺ s µ v ⎞ ⎠ x s x ⊺ s ⪰ 2 −ℓ I + s∈G ℓ,n [k−1] ⎛ ⎝ 1 ∧ 2 −ℓ 2 −ℓ+n+1 ⎞ ⎠ x s x ⊺ s ⪰ c ⋅ 2 −n V ℓ,n,k−1 .<label>(4)</label></formula><p>For <ref type="bibr" target="#b3">4)</ref> and Lemma 3(i))</p><formula xml:id="formula_37">k ∈ G ℓ,n [K − 1] and u = arg max v∶k≤v≤K,ℓv =ℓ x ⊺ k µ v , we have 2 −ℓ+n &lt; x k µ u ≤ x k W −1 ℓ,k−1 (µu) µ u W ℓ,k−1 (µu) ≤ x k W −1 ℓ,k−1 (µu) µ u W ℓ,u−1 (µu) (u ≥ k) ≤ c √ 2 n x k V −1 ℓ,n,k−1 2 −ℓ ( A K ι + ι) . (<label>(</label></formula><p>Consequently,</p><formula xml:id="formula_38">x k 2 V −1 ℓ,n,k−1 ≥ c 2 −ℓ+n √ A K ι+ι . Thus, using Lemma 4 with τ = 2 −ℓ , 1 + ℓ n=0 k∈G k,n 1 ≤ 1 + n k∈G k,n ½ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ x k 2 V −1 ℓ,n,k−1 ≥ c 2 −ℓ+n √ A K ι + ι ⎫ ⎪ ⎪ ⎬ ⎪ ⎪ ⎭ ≤ 1 + c n 2 ℓ−n ( A K ι + ι)d ln 1 + c4 ℓ ( A K ι + ι) ≤ c2 ℓ ( A K ι + ι)d ln 1 + c4 ℓ ( A K ι + ι) .</formula><p>where we use the fact that 1 ln(1 + q) ≤ c q for an absolute constant c if q is bounded by an absolute constant. Finally,</p><formula xml:id="formula_39">K k=1 x ⊺ k µ k ≤ 2 −L K + c L ℓ=1 2 −ℓ 2 ℓ ( A K ι + ι)d ln 1 + c4 ℓ ( A K ι + ι) = 2 −L K + cL( A K ι + ι)d ln 1 + c4 L ( A K ι + ι) . We choose L = 1 ∨⌊log 2 (K)⌋, which leads to R K ≤ c √ A K−1 ι + ι d ln 2 1 + cK 2 ( √ A K−1 ι + ι) . This concludes the proof. s k h ,a k h (V k h+1 ) 2 m</formula><p>] ⊺ and let L, ι, and δ be given as define Algorithm 2.</p><p>Let</p><formula xml:id="formula_40">ǫ m v,u (θ) ∶= θ ⊺ x m v,u −(V v u+1 (s v u+1 )) 2 m for (v, u) ∈ [K]×[H], m ∈ {0, 1, ..., L} where L is defined in Algorithm 2.</formula><p>We construct our confidence set as</p><formula xml:id="formula_41">Θ k ∶= L ⋂ m=0 ⋂ i∈[L] ⋂ ℓ∈[L] Θ m,i,ℓ k (5)</formula><p>where we define Θ m,i,ℓ k below, based on the data collected up to episode k − 1 . First, let</p><formula xml:id="formula_42">η m k,h ∶= max θ∈Θ k {θ ⊺ x m+1 k,h − (θ ⊺ x m k,h ) 2 } and T m,i k,h ∶= {(v, u) ∈ ([k] × [H]) ∪ ({k} × [h]) ∶ η m v,u ∈ (2 −i , 2 1−i ])} . We naturally define T m,L+1 k,h ∶= {(v, u) ∈ T m,i k,h ∶ η m v,u ≤ 2 −L }. With ι defined in Algorithm 2, define Θ m,i,ℓ k−1 ∶= ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ θ ∈ B d 1 (1) ∶ (v,u)∈T m,i k−1,H (x m v,u ) ⊺ µ ℓ ǫ m v,u (θ) ≤ Algorithm 2 VARLin2 1: Initialize: L = ⌊log 2 HK⌋ + 1, ι = 3 ln((2HK) 2(d+3) δ), δ ≤ e −1 . 2: for k = 1, 2, . . . , K do 3:</formula><p>for h = H, ..., 1 do</p><formula xml:id="formula_43">4: For each (s, a) ∈ S × A, define Q k h (s, a) = min{1, r(s, a) + max θ∈Θ k−1 ∑ d i=1 θ i P i s,a V k h+1 } where Θ k−1 is defined in Lemma 5 5: For each state s, V k h (s) = max a∈A Q k h (s, a). 6:</formula><p>end for 7:</p><p>for h = 1, ..., H do 8:</p><p>Choose</p><formula xml:id="formula_44">a k h = arg max a∈A Q k h (s k h , a). 9:</formula><p>Observe a reward r k h and the next state s k h+1 .</p><p>10:</p><p>end for 11: end for</p><formula xml:id="formula_45">4 (v,u)∈T m,i k−1,H (x m v,u ) ⊺ µ 2 ℓ η m v,u ι + 4 ⋅ 2 −ℓ ι, ∀µ ∈ B d 1 (2) ⎫ ⎪ ⎪ ⎬ ⎪ ⎪ ⎭<label>(6)</label></formula><p>We show that the confidence set is correct w.h.p. in the following lemma.</p><p>Lemma 5. (Confidence set for MDP) P(∀k</p><formula xml:id="formula_46">∈ [K], θ * ∈ Θ k ) ≥ 1 − δ.</formula><p>The consequence is that the Q values computed in VarLin2 is optimistic with high probability due to the following property:</p><formula xml:id="formula_47">Lemma 6. For every k ≥ 1, θ * ∈ Θ k ⇒ ∀h, s, a ∶ Q k h (s, a) ≥ Q * (s, a).</formula><p>Now with the confidence set defined above we state our main result.</p><p>Theorem 2. With probability at least 1-δ,</p><formula xml:id="formula_48">R K = K k=1 [V * (s k 1 ) − V k (s k 1 )] = Õ(d K log 2 (1 δ) + d 2 log(1 δ)) .</formula><p>where Õ hides poly-logarithmic dependence on {d, K, H, ln(1 δ)}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Proof of Theorem 2</head><p>The main idea of the proof is to infuse a peeling-based argument together with elliptical potential count lemma to both the planning horizon and episode. Noting that the regret of the predicted variance is controlled by the variance of variance, one can expect to reduce the total regret using this information, as done in <ref type="bibr" target="#b30">[31]</ref>. We begin by introducing relevant quantities that are parallel with those in linear bandits. Let us first introduce the following lemma.</p><formula xml:id="formula_49">Lemma 7. Let x 1 , . . . , x T ∈ R d be a sequence of vectors with x t 2 ≤ X for all t ∈ [T ]. Let V t = λI +∑ t s=1 x s x ⊺ s . Let 0 = τ 0 &lt; τ 1 &lt; τ 2 &lt; . . . &lt; τ z =</formula><p>T where τ i marks the last time step of the i-th block formed by {τ i−1 + 1, . . . , τ i } for all i ∈ {0, . . . , z}. Let anc(t) be the 'anchor' of t, the last time step of the i-th block such that the (i + 1)-th block contains t:</p><formula xml:id="formula_50">anc(t) = max{τ i ∶ i ∈ {0, . . . , z}, τ i &lt; t}. Let r &gt; 0. Define J ⊆ [T ] to be the set of indices t such that x t 2 V −1 anc(t) &gt; r x t 2 V −1 t−1</formula><p>is true for the first time in the block containing t. Then,</p><formula xml:id="formula_51">J ≤ 2 ln(r) d ln 1 + 2 e ln(r) X 2 λ .</formula><p>To keep track of episode-horizon index pairs concisely, we use a flat index t ∈ [T ] where T ∶= HK. Specifically, an episode k and a horizon h corresponds to the flat index t</p><formula xml:id="formula_52">= (k − 1)H + h. Let t(k, h) ∶= (k − 1)H + h. Let k(t)</formula><p>and h(t) be the mapping from t to its corresponding episode and horizon index respectively so that k = k(t(k, h)) and h = h(t(k, h)). By taking τ k in Lemma 7 as t(k, H), we have that anc(t) ∶= t(t − 1, H). We define</p><formula xml:id="formula_53">T m,i t ∶= {t(k ′ , h ′ ) ∶ (k ′ , h ′ ) ∈ T m,i k,h } and µ m t ∶= µ m k(t),h(t)</formula><p>. Similarly, we define x m t , etc., by replacing the subscript k, h with t. Hereafter, any appearance of subscript k, h can be replaced with t such that t = t(k, h) without changing the meaning.</p><p>Given m, k and h, we define ℓ m k,h as the integer ℓ such that</p><formula xml:id="formula_54">(x m k,h ) ⊺ µ m k,h ∈ (2 ⋅ 2 −ℓ , 2 ⋅ 2 −ℓ+1 ] where µ m k,h ∶= θ m k,h − θ * and θ m t = arg max θ∈Θ k−1 {θ ⊺ x m+1 t − (θ ⊺ x m t ) 2 } . For simplicity, we abbreviate ℓ m k,h by ℓ. Define W m,i,ℓ t (µ) ∶= 2 −ℓ I + ∑ s∈T m,i t 1 ∧ 2 −ℓ (x m s ) ⊺ µ x m s (x m s )</formula><p>⊺ and introduce a shorthand W m,i,ℓ anc(t) ∶= W m,i,ℓ anc(t) (µ m t ) as before. With the definition above we have the following:</p><formula xml:id="formula_55">2 −ℓ µ m t 2 + s∈T m,i anc(t) (x m s ) ⊺ µ m t ℓ (x m s ) ⊺ µ m t = µ m t 2 W m,i,ℓ anc(t)</formula><p>.</p><p>We now show the key result of the confidence set of VarLin2 that parallels Lemma 3 for bandits.</p><formula xml:id="formula_56">Lemma 8. Fix m ∈ {0, . . . , L m } and i ∈ [L]. Let t ∈ T m,i T . Then, with ℓ = ℓ m t , µ m t 2 W m,i,ℓ anc(t) ≤ c M ⋅ max{ √ 2 −i ι, √ 2 −ℓ ι} where c M &gt; 0 is an absolute constant.</formula><p>What is different from the linear bandit problem is that we do not update θ until the planning horizon is over and an additional layer for peeling is imposed on variance. In <ref type="bibr" target="#b30">[31]</ref>, the authors introduce an indicator I k h to characterize episode-horizon pairs for which growth of the norm of µ with respect to W t is controlled by the norm with respect to W anc(t) with d 2 growth rate, i.e.,</p><formula xml:id="formula_57">I k h ∶= ½{ µ t W m,i,ℓ t ≤ 4(d + 2) 2 µ t W m,i,ℓ anc(t) } .</formula><p>where t = t(k, h). Our novelty novelty lies in being able to replace 4(d + 2) 2 above by a constant rate r that is set to 2 later (modulo some differences due to technical reasons). To distinguish we denote such a set by I k,h . See B.5 for the definition of I k,h and the proof of the following lemma.</p><formula xml:id="formula_58">Lemma 9. ∑ K k=1 ∑ H−1 h=1 I k,h − I k,h+1 ≤ O( d ln(r) ) log(dHK(1 + d 2 ln(r))) for r &gt; 0.</formula><p>Note here that once we fix r such as r = 2, the bound can be replaced by O(d log 5 (dHK)). We now use the following regret decomposition due to <ref type="bibr" target="#b30">[31]</ref> which just come from replacing I k h by I k,h . Lemma 10. (Zhang et al. <ref type="bibr" target="#b30">[31]</ref>)</p><formula xml:id="formula_59">R K ≤ Reg 1 + Reg 2 + Reg 3 + ∑ K k=1 ∑ H−1 h (I k,h −I k,h+1 ) where Reg 1 = ∑ k,h (P s k h ,a k h V k h+1 − V k h+1 (s k h+1 ))I k,h , Reg 2 = ∑ k,h (V k h (s k h ) − r k h − P s k h ,a k h V k h+1 )I k,h , and Reg 3 = ∑ K k=1 (∑ H h=1 r k h − V π k 1 (s k 1 )). Let xm k,h ∶= x m k,h I k,h and define R m , M m as R m ∶= k,h (x m k,h ) ⊺ µ m k,h , and M m ∶= k,h (P s k h ,a k h (V k h+1 ) 2 m − (V k h+1 (s k h+1 )) 2 m )I k,h .</formula><p>We have that Reg 1 = M 0 and Reg 2 ≤ R 0 since</p><formula xml:id="formula_60">Q k h (s, a) − r(s, a) − P s,a V k h+1 ≤ max θ∈Θ k x 0 k,h (θ − θ * ).</formula><p>To proceed, we first note that ∑ k,h (I k,h − I k,h+1 ) and Reg 3 are bounded by O(d log 5 (dHK)) and O( K log(1 δ)) respectively from Lemma 9 and Lemma 16. Since Reg 1 + Reg 2 ≤ R 0 + M 0 , it remains to find a bound on R 0 +M 0 . This, however, involves solving a series of recursive inequalities. We leave the details in the appendix and provide a high-level description below.</p><p>Let us begin with Lemma 15 in the appendix that shows</p><formula xml:id="formula_61">M m ≤ Õ( M m+1 + d + 2 m+1 (K +R 0 ) log(1 δ) + log(1 δ))<label>(7)</label></formula><p>where the RHS is a function of √ M m+1 and √ R 0 . Taking Proposition 1 below for granted and combining it with the relation from Zhang et al. <ref type="bibr" target="#b30">[31,</ref><ref type="bibr">Eq. (57)</ref>] showing</p><formula xml:id="formula_62">k,h η m k,h I k,h ≤ M m+1 + O(d log 5 (dHK)) + 2 m+1 (K + R 0 )) + R m+1 + 2R m , one arrives at R m ≤ Õ d 1 2 (M m+1 +2 m+1 (K +R 0 )+R m+1 + 2R m +d).</formula><p>This bound is the key improvement we obtain via our peeling-based regret analysis. Specifically, the bound on R m obtained by <ref type="bibr" target="#b30">[31]</ref> has d 4 and d 6 in place of d 1 2 and d above.</p><p>We first show how our regret bound helps in obtaining the stated regret bound and then present Proposition 1. Noting that both R L and M L are trivially bounded by HK, one can solve the series of inequalities on R m and M m to obtain a bound on R 0 :</p><formula xml:id="formula_63">R 0 ≤ Õ d 2 log(1 δ) + d 2 (K + R 0 ) log(1 δ) .<label>(8)</label></formula><p>Solving it for R 0 , we obtain R 0 ≤ Õ d 2 log(1 δ) + d 2 K log(1 δ) . One can now plug in R 0 to the bound ( <ref type="formula" target="#formula_61">7</ref>) and obtain a bound on M 0 in a similar way as follows, which concludes the proof:</p><formula xml:id="formula_64">M 0 ≤ Õ(d K log 2 (1 δ) + d 2 log(1 δ)).</formula><p>We now show the key proposition that allows us to improve the bound on R m . In the paper by Zhang et al. <ref type="bibr" target="#b30">[31]</ref>, d 4 was derived while we propose the following.</p><formula xml:id="formula_65">Proposition 1. Let ηm k,h ∶= η m k,h I k,h . Then, we have R m ≤ O(d 0.5 log 2.5 (HK) (1 + k,h ηm k,h )ι log(dι) + d log 3 (HK)ι log(dι)) Proof. Define T m,i,ℓ ∶= {t ∈ T m,i T ∶ (x m t ) ⊺ µ m t ∈ (2 ⋅ 2 −ℓ , 2 ⋅ 2 1−ℓ ]} and split the time steps T m,i,ℓ by T m,i,ℓ,⟨1⟩ ∶= ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ t ∈ T m,i,ℓ ∶ µ m k,h W m,i,ℓ anc(a) ≤ c M √ 2 −i ι ⎫ ⎪ ⎪ ⎬ ⎪ ⎪ ⎭ and T m,i,ℓ,⟨2⟩ ∶= T m,i,ℓ ∖T m,i,ℓ,⟨1⟩ .</formula><p>Having defined I k,h with r = 2, we also denote</p><formula xml:id="formula_66">T m,i,ℓ,⟨z⟩ ∶= T m,i,ℓ,⟨z⟩ ∩ t ∈ [T ] ∶ I t = 1 . Now we decompose R m as R m = t∈[T ] (x m t ) ⊺ µ m t = i,ℓ t∈ T m,i,ℓ,⟨1⟩ (x m t ) ⊺ µ m t + i,ℓ t∈ T m,i,ℓ,⟨2⟩ (x m t ) ⊺ µ m t .</formula><p>Fix m, i and ℓ and focus on ∑ t∈ T m,i,ℓ,⟨z⟩ (x m t ) ⊺ µ m t for z = 1, 2. Hereafter, we omit the superscripts and subscripts of (m, i, ℓ) to avoid clutter, unless there is a need. Note that for t ∈ T ⟨1⟩,n and b such that t &lt; b ∈ T ⟨1⟩ ,</p><formula xml:id="formula_67">W anc(t) (µ b ) = 2 −ℓ I + t ′ ∈T m,i,ℓ anc(t) (1 ∧ 2 −ℓ x ⊺ t ′ µ b )x t ′ x ⊺ t ′ ⪰ 2 −ℓ I + t ′ ∈T ⟨1⟩,n anc(t) (1 ∧ 2 −ℓ 2 −ℓ+n+1 )x t ′ x ⊺ t ′ ⪰ 2 −ℓ I + 2 −n−1 t ′ ∈T ⟨1⟩,n anc(t) x t ′ x ⊺ t ′ ⪰ c2 −n V ⟨1⟩,n anc(t) .</formula><p>For the same t,</p><formula xml:id="formula_68">letting b = arg max t≤b ′ ∈ T ⟨1⟩ x ⊺ t µ b ′ , 2 −ℓ+n ≤ x ⊺ t µ b ≤ x t W −1 anc(t) (µ b ) µ b W anc(t) (µ b ) ≤ √ 2 n x t (V ⟨1⟩,n anc(t) ) −1 µ b W anc(b) (µ b ) ≤ c √ r √ 2 n x t (V ⟨1⟩,n t−1 ) −1 √ 2 −i ι (by b ∈ T ⟨1⟩ ) This implies that x t 2 (V ⟨1⟩,n t−1 ) −1 ≥ c 2 −2ℓ+n r2 −i ι . Thus, t∈ T ⟨1⟩ x ⊺ t µ t ≤ c2 −ℓ t∈ T ⟨1⟩ 1 ≤ c2 −ℓ T ⟨1⟩ t∈ T ⟨1⟩ 1 ≤ c2 −ℓ T ⟨1⟩ ℓ n=0 t∈ T ⟨1⟩,n 1 ≤ c2 −ℓ T ⟨1⟩ ℓ n=0 t∈ T ⟨1⟩,n ½ x t 2 (V ⟨1⟩,n t−1 ) −1 ≥ c 2 −2ℓ+n r2 −i ι ≤ c2 −ℓ T ⟨1⟩ ℓ n=0 t∈T ⟨1⟩,n ½ x t 2 (V ⟨1⟩,n t−1 ) −1 ≥ c 2 −2ℓ+n r2 −i ι ( T ⟨1⟩,n ⊆ T ⟨1⟩,n ) ≤ c2 −ℓ T ⟨1⟩ ℓ n=0 r2 −i ι 2 −2ℓ+n d ln 1 + c r2 −i ι 2 −2ℓ+n 2 −ℓ ≤ c dr T ⟨1⟩ 2 −i ι ln 1 + crι8 ℓ ≤ c dr(1 + t∈ T ⟨1⟩ η m t )ι ln 1 + crι8 ℓ</formula><p>where we use the fact that T ⟨1⟩ ⋅2 −i ≤ O(1+∑ t∈ T ⟨1⟩ η m t ), which is straightforward by the definition. The summation over T (2) can be handled in a similar way and the details of the proof is provided in Section B.7 in our appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we have made significant improvements in the regret upper bounds for linear bandits and linear mixture MDPs by employing a novel peeling-based regret analysis based on the elliptical potential count lemma. Our study opens up numerous future research directions. First, the optimal regret rates are still not identified for these problems. It would be interesting to close the gap between the upper and lower bound. Second, our algorithms are not computationally tractable. We believe computationally tractable algorithms, even at the price of increased regret, may lead to practical algorithms. Finally, characterizing variance-dependent uncertainty in the linear regression setting without prior knowledge of variances is an interesting statistical problem on its own. Identifying novel estimators for it and proving their optimal coverage would be interesting. Proof.</p><formula xml:id="formula_69">Let W t = V 0 + ∑ s∈J,s≤t x s x ⊺ s . Then, dτ + X 2 J d d ≥ tr(W t ) d d ≥ W t (AM-GM ineq.) = V 0 s∈J (1 + x s 2 W −1 s−1</formula><p>) (rank-1 update equality for det.)</p><formula xml:id="formula_70">≥ V 0 s∈J (1 + x s 2 V −1 s−1 ) (W s−1 ⪯ V s−1 ) ≥ τ d 2 J ⇒ J ≤ d ln(2) ln 1 + X 2 J dτ</formula><p>Let us generalize it so that we compute the number of times</p><formula xml:id="formula_71">x s 2 V −1 t−1 ≥ q is true rather than x s 2 V −1 t−1 ≥ 1 in which case we have J ≤ d ln(1 + q) ln 1 + X 2 J dτ =∶ A ln(1 + B J )<label>(9)</label></formula><p>We want to solve it for J . We observe the following:</p><formula xml:id="formula_72">J ≤ A ln(1 + B J ) = A ⎛ ⎝ ln J 2A + ln 2A( 1 J + B) ⎞ ⎠ (<label>10</label></formula><formula xml:id="formula_73">) ≤ J 2 + A ln ⎛ ⎝ 2A e 1 J + B ⎞ ⎠ (11) ⇒ J ≤ 2A ln ⎛ ⎝ 2A e 1 J + B ⎞ ⎠ = 2 ln(1 + q) d ln ⎛ ⎝ 2d e ln(1 + q) 1 J + X 2 dτ ⎞ ⎠<label>(12)</label></formula><p>We fix c &gt; 0 and consider two cases:</p><p>• Case 1: J &lt; cd In this case, from ( <ref type="formula" target="#formula_71">9</ref>), we have J ≤ d ln(1+q) ln 1</p><formula xml:id="formula_74">+ cX 2 τ • Case 2: J ≥ cd</formula><p>In this case, from <ref type="bibr" target="#b11">(12)</ref> we have J ≤ 2 ln(1+q) d ln τ . We remark that one can make the constant in front of the log to be d ln(1+q) by plugging this bound into the RHS of (9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Lemma 1</head><p>Proof. Let ǫ s ∶= ǫ s (θ * ) = r s − x ⊺ θ * . It suffices to show that the following is true w.p. at least</p><formula xml:id="formula_75">1 − δ, ∀ℓ ∈ [L], k ∈ [K], µ ∈ B d 2 (2), k s=1 x ⊺ s µ ℓ ǫ s ≤ k s=1 x ⊺ s µ 2 ℓ ǫ 2 s ι + 2 −ℓ ι .</formula><p>To show this, we define Bℓ to be a ξ ℓ -net over B d 2 <ref type="bibr" target="#b1">(2)</ref>. with cardinality at most 12</p><formula xml:id="formula_76">ξ ℓ d</formula><p>. Such a net exists due to Lemma 4.1 <ref type="bibr" target="#b18">[19]</ref>. Let us assume the following event, which happens with probability at least 1 − 6K log 2 (K) ∑ L ℓ=1 Bℓ by Lemma 12:</p><formula xml:id="formula_77">k s=1 x ⊺ s µ ′ ℓ ǫ s ≤ 8 k s=1 x ⊺ s µ ′ 2 ℓ ǫ 2 s ln(1 δ) + 16 ⋅ 2 −ℓ ln(1 δ) . (E) Let us fix ℓ ∈ [L], k ∈ [K], and µ ∈ B d 2 (2). Choose µ ′ ∈ Bℓ such that µ − µ ′ 2 ≤ ξ ℓ . Then, k s=1 x ⊺ s µ ǫ s ≤ k s=1 ( x ⊺ s µ − x ⊺ s µ ′ )ǫ s + k s=1 x ⊺ s µ ′ ǫ s ≤ k s=1 x ⊺ s µ − x ⊺ s µ ′ + k s=1 x ⊺ s µ ′ ǫ s ( ǫ s ≤ 1) (a) ≤ kξ ℓ + k s=1 x ⊺ s µ ′ ǫ s ≤ kξ ℓ + 8 k s=1 x ⊺ s µ ′ 2 ǫ 2 s ln(1 δ) + 16 ⋅ 2 −ℓ ln(1 δ) (by (E)) ≤ kξ ℓ + 8 2 k s=1 x ⊺ s µ 2 + ξ 2 ℓ ǫ 2 s ln(1 δ) + 16 ⋅ 2 −ℓ ln(1 δ) ≤ kξ ℓ + ξ ℓ ⋅ 8 2k ln(1 δ) + 8 2 k s=1 x ⊺ s µ 2 ǫ 2 s ln(1 δ) + 16 ⋅ 2 −ℓ ln(1 δ) ≤ 2 −ℓ + 2 −ℓ ⋅ 8 2 ln(1 δ) + 8 2 k s=1 x ⊺ s µ 2 ǫ 2 s ln(1 δ) + 16 ⋅ 2 −ℓ ln(1 δ) (choose ξ ℓ = 2 −ℓ K) ≤ 8 2 k s=1 x ⊺ s µ 2 ǫ 2 s ln(1 δ) + 32 ⋅ 2 −ℓ ln(1 δ) (by 1 ≤ ln(1 δ))</formula><p>where (a) follows from the fact that x ⊺ s (µ − µ ′ ) ≤ ǫ and the observation that the clipping operation applied to two real values z and z ′ only makes them closer. It remains to adjust the confidence level. Note that</p><formula xml:id="formula_78">L ℓ=1 Bℓ K = L ℓ=1 (12K2 ℓ ) d K ≤ 2(12K) d ⋅ 2 Ld 2 d ⋅ K ≤ (12K2 L ) d+1 . Thus, 6 log 2 (K) L ℓ=1 Bℓ K ≤ (12K2 L ) d+2 .</formula><p>Replacing δ with δ (12K2 L ) d+2 and setting ι = 128 ln((12K2 L ) d+2 δ), we conclude the proof. We remark that we did not optimize the constants in this proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Proof of Lemma 3</head><p>Proof. Throughout the proof, every clipping operator (.) is a shorthand of (.) ℓ . For (i), we note that</p><formula xml:id="formula_79">2 −ℓ µ k 2 + k s=1 x ⊺ s µ k ℓ x ⊺ s µ k = 2 −ℓ µ k 2 + k s=1 ⎛ ⎜ ⎝ ⎛ ⎝ 1 ∧ 2 −ℓ x ⊺ s µ k ⎞ ⎠ x s ⎞ ⎟ ⎠ ⊺ µ k x ⊺ s µ k = µ ⊺ k ⎛ ⎜ ⎝ 2 −ℓ λI+ k s=1 ⎛ ⎝ 1 ∧ 2 −ℓ x ⊺ s µ k ⎞ ⎠ x s x ⊺ s ⎞ ⎟ ⎠ µ k = µ k 2 W ℓ,k−1 . Then, µ k 2 (W ℓ,k−1 −2 −ℓ I) = k−1 s=1 x ⊺ s µ k x ⊺ s µ k = k−1 s=1 x ⊺ s µ k (x s θ k − r k + r k − x s θ * ) = k−1 s=1 x ⊺ s µ k (−ǫ s (θ k ) + ǫ s (θ * )) ≤ k−1 s=1 x ⊺ s µ k 2 ǫ 2 s (θ k )ι + 2 −ℓ ι + k−1 s=1 x ⊺ s µ k 2 ǫ 2 s (θ * )ι + 2 −ℓ ι (a) ≤ k−1 s=1 x ⊺ s µ k 2 2(x ⊺ s µ k ) 2 ι + 2 k−1 s=1 x ⊺ s µ k 2 2ǫ 2 s (θ * )ι + 2 ⋅ 2 −ℓ ι ≤ k−1 s=1 x ⊺ s µ k 2 2(x ⊺ s µ k ) 2 ι + 2 −ℓ 4 ⎛ ⎝ k−1 s=1 8σ 2 s + 4 ln( 4K(log 2 (K) + 2) δ ) ⎞ ⎠ ι + 2 ⋅ 2 −ℓ ι (By E 2 ) ≤ k−1 s=1 x ⊺ s µ k 2 2(x ⊺ s µ k ) 2 ι + 2 −ℓ 32 k−1 s=1 σ 2 s ι + 3 ⋅ 2 −ℓ ι ≤ 4 k−1 s=1 2 −ℓ x ⊺ s µ k (x ⊺ s µ k )ι + 2 −ℓ 32 k−1 s=1 σ 2 s ι + 3 ⋅ 2 −ℓ ι ( x ⊺ s µ k ≤ 2, (⋅) ≤ 2 −ℓ ) = 4 ⋅ 2 −ℓ µ 2 (W ℓ,k−1 −2 −ℓ I) ι + 2 −ℓ 32 k−1 s=1 σ 2 s ι + 3 ⋅ 2 −ℓ ι where (a) follows from ǫ 2 s (θ k ) = (r s − x s θ k ) 2 = (x ⊺ s (θ * − θ k ) + ǫ 2 s (θ * )) ≤ 2(x ⊺ s µ k ) 2 + 2ǫ 2 s . We now have µ 2 (W ℓ,k−1 −2 −ℓ I) on both sides. Using X ≤ A+ √ BX ≤ A+(B 2)+(X 2) ⇒ X ≤ 2A+B, we have µ k 2 (W ℓ,k−1 −2 −ℓ I) ≤ 2 −ℓ 128 k−1 s=1 σ 2 s ι + 8 ⋅ 2 −ℓ ι ⇒ µ k 2 W ℓ,k−1 ≤ 4 ⋅ 2 −ℓ + 2 −ℓ 128 k−1 s=1 σ 2 s ι + 8 ⋅ 2 −ℓ ι. Since 1 ≤ ln(1 δ), we have 4 ⋅ 2 −ℓ ≤ 4 ⋅ 2 −ℓ ln(1 δ) ≤ 2 −ℓ</formula><p>ι, which concludes the proof of (i).</p><p>For (ii), let c be an absolute constant that may be different every time it is used. We apply Cauchy-Schwarz inequality to obtain</p><formula xml:id="formula_80">(x ⊺ k µ k ) 2 ≤ x k 2 W −1 ℓ,k−1 µ k 2 W ℓ,k−1 ≤ x k 2 W −1 ℓ,k−1 ⋅ c ⋅ ⎛ ⎜ ⎝ 2 −ℓ k−1 s=1 σ 2 s ι + 2 −ℓ ι ⎞ ⎟ ⎠ ≤ x k 2 W −1 ℓ,k−1 ⋅ c ⋅ x ⊺ k µ k ⎛ ⎜ ⎝ k−1 s=1 σ 2 s ι + ι ⎞ ⎟ ⎠ (2 −ℓ ≤ x ⊺ k µ k ≤ 2 −ℓ+1 )</formula><p>Dividing both sides by x ⊺ k µ k concludes the proof.</p><formula xml:id="formula_81">A.4 d √ K regret bound of VOFUL2</formula><p>Let us slightly modify the algorithm so we now add Θ ℓ k with ℓ = 0:</p><formula xml:id="formula_82">Θ k = ∩ L ℓ=0 Θ ℓ k .</formula><p>Let us call this algorithm VOFUL3. Note that this slight change will not alter the order of the regret bound of VOFUL2 reported in Theorem 1.</p><p>Let λ &gt; 0 be an analysis parameter to be determined later. Let X k ∈ R k×d be the design matrix where row s is x ⊺ s and define</p><formula xml:id="formula_83">y k ∶= (r 1 , . . . , r k ) ⊺ , η k ∶= (ǫ 1 , . . . , ǫ k ) ⊺ . Let V k ∶= λI + X ⊺ k X k and θk ∶= V −1 k X ⊺ k y k (13) We claim that Θ k ⊆ θ ∈ B d 2 (1) ∶ θk − θ 2 V k ≤ β k =∶ Θk for some β k = Õ(d+ln(1 δ))</formula><p>. This suffices to show that the VOFUL2 has regret bound of Õ(d √ K) since the proof technique of OFUL <ref type="bibr" target="#b0">[1]</ref> can be immediately applied since the UCB computed based on Θ k is bounded above by the UCB computed with Θk . Thus, VOFUL3 has regret bound of</p><formula xml:id="formula_84">R K = Õ ⎛ ⎜ ⎝ d K ln(1 δ), d 1.5 K k=1 σ 2 k ln(1 δ) + d 2 ln(1 δ) ⎞ ⎟ ⎠ .</formula><p>To see why the claim above is true, let θ ∈ Θ k . Then, using ǫ 2 s (θ) ≤ 4, we have,</p><formula xml:id="formula_85">∀µ ∈ B d 2 (2), s µ ⊺ x s (y s − x ⊺ s θ) ≤ µ ∑ k s=1 xsx ⊺ s ⋅ √ 4ι + ι</formula><p>Let us drop the subscript k from θk , V k , X k , y k , η k for brevity. The display above can be rewritten as</p><formula xml:id="formula_86">µ ⊺ (X ⊺ y − X ⊺ Xθ) = µ ⊺ V ( θ − θ) + λµ ⊺ θ ≤ µ X ⊺ X ⋅ √ 4ι + ι ⇒ µ ⊺ V ( θ − θ) ≤ µ V ⋅ √ 4ι + ι + 2λ ( θ 2 ≤ 1, µ ⊺ θ ≤ 2)</formula><p>We can choose µ = 1 2 ( θ − θ) since 1 2 θ − θ 2 ≤ 1 2 ( θ 2 + θ 2 ) ≤ 2 by Lemma 11 and the choice of λ therein. Then,</p><formula xml:id="formula_87">θ − θ 2 V ≤ θ − θ V √ 4ι + 2ι + 4λ ⇒ θ − θ 2 V ≤ 8ι + 8λ (AM-GM ineq. on θ − θ V √ 4ι) = Õ(d + ln(1 δ))</formula><p>Lemma 11. Take the assumptions for the linear bandit problem in Section 2. Consider θk defined in <ref type="bibr" target="#b12">(13)</ref> with λ = d ln(1 + K d ) + 2 ln(1 δ). Let K ≥ (e − 1)d. Then, with probability at least 1 − δ, we have, ∀k ≤ K, θk 2 ≤ 3.</p><p>Proof. Let us drop the subscript k from θk , V k , X k , y k , η k . Then,</p><formula xml:id="formula_88">θ 2 ≤ θ − θ * 2 + θ * 2 . Note that θ * 2 ≤ 1.</formula><p>We can further bound the first term:</p><formula xml:id="formula_89">θ − θ * 2 = V −1 (X ⊺ η − λθ * ) 2 ≤ V −1 X ⊺ η 2 + λ V −1 θ * 2 . Note that λ V −1 θ * 2 = λ θ * ⊺ V −2 θ * ≤ λ θ * ⊺ ( 1 λ 2 I)θ * ≤ θ * 2 ≤ 1 .</formula><p>It remains to bound V −1 X ⊺ η 2 . To see this,</p><formula xml:id="formula_90">V −1 X ⊺ η 2 = η ⊺ XV −2 X ⊺ η = η ⊺ XV −1 ( 1 λ ⋅ λI)V −1 X ⊺ η ≤ η ⊺ XV −1 ( 1 λ ⋅ V )V −1 X ⊺ η = 1 √ λ X ⊺ η V −1 ≤ 1 √ λ ⎛ ⎜ ⎝ d ln(1 + k dλ ) + 2 ln(1 δ) ⎞ ⎟ ⎠ (Abbasi-Yadkori et al. [1, Theorem 1]) (a) ≤<label>1</label></formula><p>where (a) is by choosing λ = d ln(1+ K d )+2 ln(1 δ) and then using λ ≥ 1 (due to K ≥ (e−1)d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Miscellaneous Lemmas</head><p>For completeness, we state the lemmas borrowed from prior work.</p><p>Lemma 12. (Zhang et al. <ref type="bibr" target="#b30">[31,</ref><ref type="bibr">Lemma 9]</ref>) Let {F i } n i=0 be a filtration. Let {X i } n i=1 be a sequence of real-valued random variables such that X i is F i -measurable. We assume that E X i F i−1 = 0 and that X i ≤ b almost surely. For δ &lt; e −1 , we have</p><formula xml:id="formula_91">P ⎛ ⎜ ⎝ n i=1 X i ≤ 8 n i=1 X 2 i ln(1 δ) + 16b ln(1 δ) ⎞ ⎟ ⎠ ≥ 1 − 6δ log 2 (n)</formula><p>Lemma 13. (Zhang et al. <ref type="bibr" target="#b30">[31,</ref><ref type="bibr">Lemma 10]</ref>) Let {F i } i≥0 be a filtration. Let {X i } n i=1 be a sequence of random variables such that X i ≤ 1 almost surely, that X i is F i -measurable. For every δ ∈ (0, 1), we have</p><formula xml:id="formula_92">P ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ n i=1 X 2 i ≥ n i=1 8E X 2 i F i−1 + 4 ln 4 δ ⎤ ⎥ ⎥ ⎥ ⎥ ⎦ ⩽ ⌈log 2 n⌉ + 1 δ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proofs for VARLin2</head><p>Throughout the proof, we use c as absolute constant that can be different every single time it is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Proof of Lemma 6</head><p>Proof. Assume that</p><formula xml:id="formula_93">θ * ∈ Θ k for all k ∈ [K]. Since θ * ∈ Θ k , Q k h (s, a) = min{r(s, a) + max θ∈Θ k d i=1 θ i P i s,a V k h+1 } ≥ min{1, r(s, a) + d i=1 θ * i P i s,a V k h+1 } ≥ min{1, r(s, a) + d i=1 θ * i P i s,a V * h+1 } = Q * h (s, a)</formula><p>, so the statement follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Proof of Lemma 5</head><p>Proof. Similar to the linear bandit case, let Bℓ be a ξ ℓ -net over B d 1 (2) with cardinality at most ( 12 ξ ℓ ) d and pick µ ∈ B d 1 (2) and µ ′ ∈ Bℓ such that the distance between them is at most ξ ℓ . Let us define the conditional variance</p><formula xml:id="formula_94">V v,u [ǫ m v,u ] ∶= V[ǫ m v,u F v u ]</formula><p>where F v u denotes history up to (and including) episode v and time horizon u.</p><formula xml:id="formula_95">Noticing V v,u [ǫ m v,u ] = E v,u [(ǫ m v,u ) 2 ] = ((θ * ) ⊺ x m v,u ) 2 − 2((θ * ) ⊺ x m v,u ) ⋅ E v,u [(V v u+1 (s v u+1 )) 2 m ] + E v,u [ (V v u+1 (s v u+1 )) 2 m 2 ], E v,u [(V v u+1 (s v u+1 )) 2 m ] = (θ * ) ⊺ x m v,u , and E v,u [ (V v u+1 (s v u+1 )) 2 m 2 ] = (θ * ) ⊺ x m+1 v,u , we have V v,u [ǫ m v,u ] = (θ * ) ⊺ x m+1 v,u − ((θ * ) ⊺ x m v,u ) 2 .</formula><p>We apply Lemma 14 with ǫ = 1, b = 2 −ℓ to obtain</p><formula xml:id="formula_96">(v,u)∈T m,i k,H (x m v,u ) ⊺ µ ′ ℓ ǫ m v,u ≤ 4 (v,u)∈T m,i k,H (x m v,u ) ⊺ µ ′ 2 ℓ V[ǫ m v,u F v u ] + 4 ⋅ 2 −ℓ ln(1 δ)</formula><p>with probability at least 1 − δ(1 + log 2 (HK)) and repeat the similar procedure by taking the union bound. We drop ℓ from the clipping notation for the sake of brevity.</p><formula xml:id="formula_97">(v,u)∈T m,i k,H (x m v,u ) ⊺ µ ǫ m v,u = (v,u)∈T m,i k,H ( (x m v,u ) ⊺ µ − (x m v,u ) ⊺ µ ′ )ǫ m v,u + (v,u)∈T m,i k,H (x m v,u ) ⊺ µ ′ ǫ m v,u ≤ (v,u)∈T m,i k,H (x m v,u ) ⊺ µ − (x m v,u ) ⊺ µ ′ + (v,u)∈T m,i k,H (x m v,u ) ⊺ µ ′ ǫ m v,u ( ǫ s ≤ 1) ≤ HKξ ℓ + 4 (v,u)∈T m,i k,H (x m v,u ) ⊺ µ ′ 2 V[ǫ m v,u F v u ] ln(1 δ) + 4 ⋅ 2 −ℓ ln(1 δ) ≤ HKξ ℓ + 4 2 (v,u)∈T m,i k,H { (x m v,u ) ⊺ µ 2 + ξ 2 ℓ }V[ǫ m v,u F v u ] ln(1 δ) + 4 ⋅ 2 −ℓ ln(1 δ) ≤ HKξ ℓ + 4ξ ℓ 2HK ln(1 δ) + 4 2 (v,u)∈T m,i k,H (x m v,u ) ⊺ µ 2 V[ǫ m v,u F v u ] ln(1 δ) + 4 ⋅ 2 −ℓ ln(1 δ) ≤ (4 √ 2 + 5) ⋅ 2 −ℓ ln(1 δ) + 4 2 (v,u)∈T m,i k,H (x m v,u ) ⊺ µ 2 V[ǫ m v,u F v u ] ln(1 δ) (choose ξ ℓ = 2 −ℓ (HK)) ≤ 4 ⋅ 2 −ℓ ln(1 δ ′ ) + 4 (v,u)∈T m,i k,H (x m v,u ) ⊺ µ 2 η m v,u ln(1 δ ′ ) (setting δ = δ ′ 1 3 )</formula><p>We then take union bounds over m ∈ {1, 2, ..., L}, i, ℓ ∈ [L], k ∈ [K], and µ ′ ∈ Bℓ , which invoke applying Lemma 14 (2HK) </p><formula xml:id="formula_98">Bℓ = L 0 LK ℓ (HK2 ℓ ) d ≤ (HK) 2 (HK) d 2 Ld 2 d ≤ (HK2 L ) d+2 ≤ (2HK) 2(d+2) .</formula><p>Hence, the display above holds with probability at least d+3) . Replacing δ with 1 (2HK) 2(d+3) ⋅ δ and setting ι = 3 ⋅ ln((2HK) 2(d+3) δ) the result follows.</p><formula xml:id="formula_99">1 − δ(1 + log 2 (HK))(2HK) 2(d+2) ≥ 1 − δ(2HK) 2(</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Proof of Lemma 7</head><p>Proof. Lemma 12 of Abbasi-Yadkori et al. <ref type="bibr" target="#b0">[1]</ref> shows the following in its proof: Let A, B, and C be positive semi-definite (PSD) matrices such that A = B + C. Then we have that</p><formula xml:id="formula_100">sup x≠0 x 2 A x 2 B ≤ det(A) det(B) .</formula><p>Note that V −1 anc(t) = V −1 t−1 + C for some PSD matrix C (to see this, apply a series of rank-one update formula for the covariance matrix). Thus, we have that if t ∈ J, then r &lt;</p><formula xml:id="formula_101">x t 2 V −1 anc(t) x t 2 V −1 t−1 ≤ V −1 anc(t) V −1 t−1 = V t−1 V anc(t) . Let J[t] = J ∩ [t] with [0] ∶= ∅. Let prev(t) be the time step in J immediately prior to t: prev(t) ∶= max{s ∈ {0} ∪ J ∶ s &lt; t} Define W t = V 0 + ∑ s∈J[t] x s x ⊺ s . dλ + X 2 J d d ≥ tr(W T ) d d ≥ W T (AM-GM ineq.) = V 0 t∈J W t W prev(t) ≥ V 0 t∈J W t W anc(t) (prev(t) ≤ anc(t)) = V 0 r J ≥ λ d r J ⇒ J ≤ d ln(r) ln 1 + X 2 J dλ Then, J ≤ d ln(r) ln 1 + X 2 J dλ =∶ A ln(1 + B J )<label>(14)</label></formula><p>We want to solve it for J . Do the following:</p><formula xml:id="formula_102">J ≤ A ln(1 + B J ) = A ⎛ ⎝ ln J 2A + ln 2A( 1 J + B) ⎞ ⎠<label>(15)</label></formula><formula xml:id="formula_103">≤ J 2 + A ln ⎛ ⎝ 2A e 1 J + B ⎞ ⎠<label>(16)</label></formula><formula xml:id="formula_104">⇒ J ≤ 2A ln ⎛ ⎝ 2A e 1 J + B ⎞ ⎠ = 2 ln(r) d ln ⎛ ⎝ 2d e ln(r) 1 J + X 2 dλ ⎞ ⎠<label>(17)</label></formula><p>We fix c &gt; 0 and consider two cases:</p><p>• Case 1: J &lt; cd In this case, from ( <ref type="formula" target="#formula_101">14</ref> ) 2 We have</p><formula xml:id="formula_105">µ m t 2 (W m,i,ℓ anc(t) −2 −ℓ I) = b∈T m,i anc(t) (x m b ) ⊺ µ m t (x m b )µ m t = b∈T m,i anc(t) (x m b ) ⊺ µ m t (−ǫ m b (θ m t ) + ǫ m b (θ * )) ≤ b∈T m,i anc(t) (x m b ) ⊺ µ m t 2 η m b (θ m t )ι + 4 ⋅ 2 −ℓ ι + 4 b∈T m,i anc(t) (x m b ) ⊺ µ m t 2 η m b (θ * )ι + 4 ⋅ 2 −ℓ ι ≤ C b∈T m,i anc(t) (x m b ) ⊺ µ m t 2 ⋅ 2 −i ι + 8 ⋅ 2 −ℓ ι ≤ C b∈T m,i anc(t) (x m b ) ⊺ µ m t (x m b ) ⊺ µ m t ⋅ 2 −i ι + 8 ⋅ 2 −ℓ ι ≤ C µ m t W m,i,ℓ anc(t) √ 2 −i ι + 8 ⋅ 2 −ℓ ι Solving for µ m t W m,i,ℓ anc(t)</formula><p>, we get the desired bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 The Exact Definition of I k,h and Proof of Lemma 9</head><p>Fix m and i. Let t ∈ T m,i T and ℓ = ℓ m t . We recall Lemma 8 that yields µ m t W m,i,ℓ anc(t)</p><formula xml:id="formula_106">≤ c M ⋅ max{ √ 2 −i ι, √ 2 −ℓ ι} ⪰ 2 −ℓ I + t ′ ∈T m,i,ℓ,⟨1⟩,n anc(t) (1 ∧ 2 −ℓ x t ′ µ b )x t ′ x ⊺ t ′ ⪰ 2 −ℓ I + t ′ ∈T m,i,ℓ,⟨1⟩,n anc(t) (1 ∧ 2 −ℓ 2 −ℓ+n+1 )x t ′ x ⊺ t ′ ⪰ 2 −ℓ I + 2 −n−1 t ′ ∈T m,i,ℓ,⟨1⟩,n anc(t) x t ′ x ⊺ t ′ ⪰ c2 −n V ⟨1⟩,n anc(t) .</formula><p>For t ∈ T ⟨1⟩,n , let b = arg max t≤b ′ ∈ T ⟨1⟩ x ⊺ t µ b ′ . Then,</p><formula xml:id="formula_107">2 −ℓ+n ≤ x ⊺ t µ b (def'n of b) ≤ x t W −1 anc(t) (µ b ) µ b W anc(t) (µ b ) ≤ √ 2 n x t (V ⟨1⟩,n anc(t) ) −1 µ b W anc(b) (µ b ) ≤ c √ r √ 2 n x t (V ⟨1⟩,n t−1 ) −1 √ 2 −i ι .</formula><p>(by b ∈ T ⟨1⟩ )</p><p>This implies that</p><formula xml:id="formula_108">x t 2 (V ⟨1⟩,n t−1 ) −1 ≥ c ⟨1⟩ 2 −2ℓ+n r2 −i ι .</formula><p>for some absolute constant c ⟨1⟩ &gt; 0.</p><p>Thus,  <ref type="bibr" target="#b30">[31,</ref><ref type="bibr">Lemma 12]</ref>) For λ i &gt; 0, i ∈ {1, 2, 4} and λ 3 ≥ 1, let κ = max{log 2 (λ 1 ), 1}. Assume that 0 ≤ a i ≤ λ 1 and a i ≤ λ 2 a i + a i+1 + 2 i+1 λ 3 +λ 4 for i ∈ {1, 2, ..., κ}.</p><formula xml:id="formula_109">t∈ T ⟨1⟩ x t µ t ≤ c2 −ℓ t∈ T ⟨1⟩ 1 ≤ c2 −ℓ T ⟨1⟩ t∈ T ⟨1⟩ 1 ≤ c2 −ℓ T ⟨1⟩ ℓ n=0 t∈ T ⟨1⟩,n<label>1</label></formula><formula xml:id="formula_110">≤ c2 −ℓ T ⟨1⟩ ℓ n=0 t∈ T ⟨1⟩,n ½ x t 2 (V ⟨1⟩,n t−1 ) −1 ≥ c ⟨1⟩ 2 −2ℓ+n r2 −i ι ≤ c2 −ℓ T ⟨1⟩ ℓ n=0 t∈T ⟨1⟩,n ½ x t 2 (V ⟨1⟩,n t−1 ) −1 ≥ c ⟨1⟩ 2 −2ℓ+n r2 −i ι ( T ⟨1⟩,n ⊆ T ⟨1⟩,n ) ≤ c2 −ℓ T ⟨1⟩ ℓ n=0 r2 −i ι 2 −2ℓ+n d ln 1 + c r2 −i ι 2 −2ℓ+n 2 −ℓ (<label>Lemma</label></formula><p>Then, we have a 1 ≤ 22λ 2 2 + 6λ 4 + 4λ 2 2λ 3 Lemma 18. (Zhang et al. <ref type="bibr" target="#b29">[30,</ref><ref type="bibr">Lemma 2]</ref>) Let λ 1 , λ 2 , λ 4 ≥ 0 and λ 3 ≥ 1 with i ′ = log 2 (λ 1 ). We have a sequence {a i } i for i ∈ {1, 2, ..., i ′ } satisfying a i ≤ λ 1 and a i ≤ λ 2 a i+1 + 2 i+1 λ 3 + λ 4 . Then, a 1 ≤ max (λ 2 + λ 2 2 + λ 4 ) 2 , λ 2 8λ 3 + λ 4</p><p>C Comparison with He et al. <ref type="bibr" target="#b11">[12]</ref> At a high-level, He et al. <ref type="bibr" target="#b11">[12]</ref> apply a similar strategy to ours. One immediate difference is that we do not incur an extra dependence on d inside the logarithm, but it could be due to the fact that their lemma is applying the count on a different quantity from ours. Note that our EPC is still novel and tighter than the bound appeared in a concurrent work of Wagenmaker et al. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr">Lemma 6</ref>.2] for a large enough K (i.e., time horizon).</p><p>Note that the core of our novelty is the point of view introduced by the the matrix norm with respect to W ℓ,k−1 (µ). This enables the connection to the elliptical potential lemma and the peeling technique. Such a viewpoint is exactly what the paper of VOFUL <ref type="bibr" target="#b30">[31]</ref> did not seem to have realized. Indeed, the proof of VOFUL <ref type="bibr" target="#b30">[31]</ref> does not use peeling on x ⊺ k µ k as we do. We also like to highlight that Lemma 7 is still novel and is one of the main contributors to the improved regret bound for the linear mixture MDP.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Checklist 1 .</head><label>1</label><figDesc>For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] (c) Did you discuss any potential negative societal impacts of your work? [No] There is no negative impacts, to our knowledge. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [Yes] (b) Did you include complete proofs of all theoretical results? [Yes] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [N/A] (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [N/A] (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A] (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [N/A] 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [N/A] (b) Did you mention the license of the assets? [N/A] (c) Did you include any new assets either in the supplemental material or as a URL? [N/A] (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] A Proofs for VOFUL2 A.1 Proof of Lemma 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 e ln(1+q) 1 c + X 2 τset c = 2 e</head><label>122</label><figDesc>We ln(1+q) to obtain J ≤2 ln(1+q) d ln 1 + 2 e ln(1+q) X 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 c + X 2 λ 8 − (θx m t ) 2 }</head><label>1282</label><figDesc>), we have J ≤ d ln(r) ln 1 + cL 2 λ • Case 2: J ≥ cd In this case, from (17) we have J ≤ 2 ln(r) d ln 2 e ln(r) We set c = 2 e ln(r) to obtain J ≤ 2 ln(r) d ln 1 + 2 e Recall that θ m t = arg max θ∈Θ k−1 {θx m+1 t and η m b (θ) = θx m+1 b − (θx m+1 b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4 )≤ 1 √ 2 − 2 ⋅ K + 1 2 ⋅ d 2</head><label>412222</label><figDesc>c dr T ⟨1⟩ 2 −i ι ln 1 + crι8 ℓ ≤ c dr(1 + t∈ T ⟨1⟩ η t )ι ln 1 + crι8 ℓFor the other case involving T ⟨2⟩ , we use the same logic as above. Let t ∈ T ⟨1⟩,n . Then, for any b such that a ≤ b, one can show thatW anc(t) (µ b ) ⪰ c2 −n V ⟨2⟩,n anc(t) . Then, once again letting b = arg max t&lt;b∈ T ⟨2⟩ x ⊺ t µ b for t ∈ T ⟨2⟩,n one can show that 2 −ℓ+n ≤ c √ r √ 2 n x t (V ⟨2⟩,n t−1 ) −ℓ ι .where the last inequality uses √ AB ≤ A+B 2 to obtain the following:K + R 0 ≤ Õ(K + d 2 log 7 (dHK) log(1 δ) + Kd 2 log 7 (dHK) log(1 δ)) ≤ Õ K + d 2 log 7 (dHK) log(1 δ) + 1 log 7 (dHK) log(1 δ) .Altogether, we obtain b 0 = Õ( Kd 2 log 7 (dHK) log 2 (1 δ) + d 2 log 7 (dHK) log(1 δ)) This concludes the proof.B.8 Miscellaneous lemmasLemma 14. (Zhang et al.<ref type="bibr" target="#b31">[32,</ref> Lemma 11]) Let (M n ) n≥0 be a martingale such that M 0 = 0 andM n − M n−1 ≤ b almost surely for n ≥ 1. For each n ≥ 0, let F n = σ(M 1 , ..., M n ).Then for any n ≥ 1 and ǫ, δ &gt; 0, we haveP ⎛ ⎜ ⎝ M n ≥ 2 n i=1 E[(M i − M i−1 ) 2 F i−1 ] ln(1 δ) + 2 ǫ ln(1 δ) + 2b ln(1 δ) ⎞ ⎟ ⎠ ≤ 2(log 2 (b 2 n ǫ)+1)δLemma15. (Zhang et al. [31, Lemma 25]) M m ≤ O M m+1 + O(d log 5 (dHK)) + 2 m+1 (K + R 0 ) log(1 δ) + log(1 δ) Lemma 16. (Zhang et al. [31, Lemma 6]) Reg 3 ≤ O( K log(1 δ)). Lemma 17. (Zhang et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>2(d+2) times. It follows from</figDesc><table /><note>i,ℓ,k</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Linear Mixture MDPAs linear bandits and linear mixture MDPs have quite a similar nature, we bring the techniques in our analysis of VOFUL2 to improve the regret bound of VARLin of Zhang et al.<ref type="bibr" target="#b30">[31]</ref>. A key feature of linear mixture MDP setting is that one can estimate the upper bound of the variance as it is a quadratic function of θ * while linear bandits do not have a structural assumption on the variance. Thanks to such a structural property, we obtain a slightly better dependence on the dimension d. The confidence set derived for our proposed algorithm is slightly different from that of VARLin as ours is defined with ∀µ ∈ B d 1 (2) rather than an ǫ-net. Our version of VARLin, which we call VARLin2, is described in Algorithm 2. Given s k h and a k h , let us defineP s k h ,a k h (V k h+1 ) ∶= E s ′ ∼P s k h ,a k h [V k h+1 (s ′ )]and x m k,h ∶= [P 1 s k h ,a k h (V k h+1 ) 2 m , ..., P d</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>The authors thank Liyu Chen for finding an error in our earlier version. Insoon Yang is supported in part by the National Research Foundation of Korea (MSIT2020R1C1C1009766), the Information and Communications Technology Planning and Evaluation (IITP) grants (MSIT2022-0-00124, MSIT2022-0-00480), and Samsung Electronics. Kwang-Sung Jun is supported by Data Science Academy and Research Innovation &amp; Impact at University of Arizona.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Let us define</p><p>} and split the time steps T m,i,ℓ as follows:</p><p>where we set n = 0 if the maximum above is less or equal to</p><p>for some r &gt; 1 to be specified later. With this definition, one can see that</p><p>is the number of bad episodes where there exists h ∈ [H] such that</p><p>. Define similar quantities for ⟨2⟩ as well.</p><p>Define</p><p>. We now prove Lemma 9.</p><p>Note that</p><p>,</p><p>We now assume z = 1 without loss of generality. The display above can be written as .</p><p>Recall that the inner sum above is the count of the 'bad' episodes. We invoke Lemma 7 2L 0 (L) 3  times with X = √ d as x t 1 ≤ √ d to finish the proof as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Proof of Proposition 1</head><p>To proceed we safely choose r = 2 and inherit all notations from Section B.5. Let us define</p><p>Let us fix m, i, and ℓ and focus on controlling ∑ t∈ T m,i,ℓ,⟨z⟩ (x m t ) ⊺ µ m t for z ∈ <ref type="bibr" target="#b1">[2]</ref>. Hereafter, we omit the superscripts of (m, i, ℓ) to avoid clutter, unless there is a need.</p><p>Note that for t ∈ T ⟨1⟩,n and b such that t ≤ b,</p><p>This implies that</p><p>Invoking the elliptical potential count lemma together with r = 2,</p><p>where ηm t ∶= η m t I t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.7 Proof of Theorem 2</head><p>Proof. We continue from the proof in the main paper where it remains to bound R 0 + M 0 . Using the relation (equation ( <ref type="formula">56</ref>) and (57) in <ref type="bibr" target="#b30">[31]</ref>),</p><p>one has, using Proposition 1,</p><p>The strategy is to solve the recursive inequalities with respect to R m and M m to obtain a bound on R 0 and M 0 . By Lemma 15, we have We can solve it for R 0 to obtain R 0 ≤ Õ( Kd 2 log 7 (dHK) log(1 δ) + d 2 log 7 (dHK) log(1 δ)).</p><p>Next, we apply Lemma 18 to (18) with λ 2 = Θ(1), λ 3 = (K + R 0 ) ln(1 δ), and λ 4 = Θ(d 0.5 log 2.5 (dHK) + ln(1 δ)) to obtain M 0 ≤ O( (K + R 0 ) ln(1 δ) + d 0.5 log 2.5 (dHK) + ln(1 δ)) ≤ Õ( K log(1 δ) + d 2 log 7 (dHK) log 2 (1 δ) + d 0.5 log 2.5 (dHK) + ln(1 δ))</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improved Algorithms for Linear Stochastic Bandits</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Abbasi-Yadkori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Use of variance estimation in the multi-armed bandit problem. NeurIPS Workshop on On-line Trading of Exploration and Exploitation Workshop</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using Confidence Bounds for Exploitation-Exploration Trade-offs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="397" to="422" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Nonstochastic Multiarmed Bandit Problem</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="77" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Prediction, learning, and games</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stochastic Linear Optimization under Bandit Feedback</title>
		<author>
			<persName><forename type="first">V</forename><surname>Dani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Learning Theory (COLT)</title>
				<meeting>the Conference on Learning Theory (COLT)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="355" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On Oracle-Efficient PAC RL with Rich Observations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Provably Efficient Q-learning with Function Approximation via Distribution Shift Error Checking Oracle</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8058" to="8068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Agnostic q-learning with function approximation in deterministic systems: Near-optimal bounds on approximation error and sample complexity</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Provably Efficient Exploration for Reinforcement Learning Using Unsupervised Learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="22492" to="22504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extracting certainty from uncertainty: Regret bounded by variation in costs</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="165" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Logarithmic regret for reinforcement learning with linear function approximation</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4171" to="4180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Contextual decision processes with low bellman rank are pac-learnable</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1704" to="1713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Provably efficient reinforcement learning with linear function approximation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2137" to="2143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pac reinforcement learning with rich observations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bandit Algorithms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lattimore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nearly Minimax-Optimal Regret for Linearly Parameterized Bandits</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Learning Theory (COLT)</title>
				<meeting>the Conference on Learning Theory (COLT)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2173" to="2174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kinematic state abstraction and provably efficient rich-observation reinforcement learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
				<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6961" to="6971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Empirical processes: theory and applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pollard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSF-CBMS regional conference series in probability and statistics</title>
				<imprint>
			<publisher>JSTOR</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page">86</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Eluder dimension and the sample complexity of optimistic exploration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2256" to="2264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Model-based rl in contextual decision processes: Pac bounds and exponential improvements over model-free approaches</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Learning Theory (COLT)</title>
				<meeting>the Conference on Learning Theory (COLT)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2898" to="2933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">First-order regret in reinforcement learning with linear function approximation: A robust estimation approach</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Wagenmaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Simchowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jamieson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="22384" to="22429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On Reward-Free Reinforcement Learning with Linear Function Approximation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="17816" to="17826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Provably Efficient Reinforcement Learning with General Value Function Approximation. CoRR, abs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005.1, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Optimism in reinforcement learning with generalized linear function approximation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient exploration and value function generalization in deterministic systems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sample-optimal parametric q-learning using linearly additive features</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6995" to="7004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10746" to="10756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning near optimal policies with low inherent bellman error</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zanette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kochenderfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10978" to="10989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Is reinforcement learning more difficult than bandits? a nearoptimal algorithm escaping the curse of horizon</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4528" to="4531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Variance-Aware Confidence Set: Variance-Dependent Bound for Linear Bandits and Horizon-Free Bound for Linear Mixture MDP</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Model-free reinforcement learning: from clipped pseudo-regret to sample complexity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12653" to="12662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Nearly minimax optimal reinforcement learning for linear mixture markov decision processes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4532" to="4576" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
