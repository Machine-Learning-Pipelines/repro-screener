<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Simple and General Debiased Machine Learning Theorem with Finite Sample Guarantees</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-21">21 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Victor</forename><surname>Chernozhukov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Whitney</forename><forename type="middle">K</forename><surname>Newey</surname></persName>
							<email>wnewey@mit.edu</email>
						</author>
						<author>
							<persName><forename type="first">Rahul</forename><surname>Singh</surname></persName>
							<email>rahul.singh@mit.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">MIT Economics</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">MIT Economics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Simple and General Debiased Machine Learning Theorem with Finite Sample Guarantees</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-21">21 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">FF47A3742FD65A72400BA293CCA64221</idno>
					<idno type="arXiv">arXiv:2105.15197v3[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-10-31T05:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Debiased machine learning is a meta algorithm based on bias correction and sample splitting to calculate confidence intervals for functionals, i.e. scalar summaries, of machine learning algorithms. For example, an analyst may desire the confidence interval for a treatment effect estimated with a neural network. We provide a nonasymptotic debiased machine learning theorem that encompasses any global or local functional of any machine learning algorithm that satisfies a few simple, interpretable conditions. Formally, we prove consistency, Gaussian approximation, and semiparametric efficiency by finite sample arguments. The rate of convergence is n −1/2 for global functionals, and it degrades gracefully for local functionals. Our results culminate in a simple set of conditions that an analyst can use to translate modern learning theory rates into traditional statistical inference. The conditions reveal a general double robustness property for ill posed inverse problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of this paper is to provide a useful technical result for analysts who desire confidence intervals for functionals, i.e. scalar summaries, of machine learning algorithms. For example, the functional of interest could be the average treatment effect of a medical intervention, and the machine learning algorithm could be a neural network trained on medical scans. Alternatively, the functional of interest could be the price elasticity of consumer demand, and the machine learning algorithm could be a kernel ridge regression trained on economic transactions. Treatment effects and price elasticities for a specific demographic are examples of localized functionals. In these various applications, confidence intervals are essential.</p><p>We provide a simple set of conditions that can be verified using the kind of rates provided by statistical learning theory. Unlike previous work, we provide a finite sample analysis for any global or local functional of any machine learning algorithm, without bootstrapping, subject to these simple and interpretable conditions. The machine learning algorithm may be estimating a nonparametric regression, a nonparametric instrumental variable regression, or some other nonparametric quantity. We provide conceptual and statistical contributions for the rapidly growing literature on debiased machine learning.</p><p>Conceptually, our result unifies, refines, and extends existing debiased machine learning theory for a broad audience. We unify finite sample results that are specific to particular functionals or machine learning algorithms. General asymptotic theory with abstract conditions already exists, which we refine to finite sample theory with simple conditions. In doing so, we uncover a new notion of double robustness for exactly identified ill posed inverse problems. A virtue of finite sample analysis is that it handles the case where the functional involves localization. We show how learning theory delivers inference.</p><p>Statistically, we provide results for the class of global functionals that are mean square continuous, and their local counterparts, using algorithms that have sufficiently fast finite sample learning rates. Formally, we prove (i) consistency, Gaussian approximation, and semiparametric efficiency for global functionals; and (ii) consistency and Gaussian approximation for local functionals. The analysis explicitly accounts for each source of error in any finite sample size. The rate of convergence is the parametric rate of n −1/2 for global functionals, and it degrades gracefully to nonparametric rates for local functionals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>By focusing on functionals of nonparametric quantities, this paper continues the tradition of classic semiparametric statistics <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b1">2]</ref>. Whereas classic semiparametric theory studies functionals of densities or regressions over low dimensional domains, we study functionals of machine learning algorithms over arbitrary domains. In classic semiparametric theory, an object called the Riesz representer appears in efficient influence functions and asymptotic variance calculations <ref type="bibr" target="#b27">[28]</ref>. For the same reasons, it appears in debiased machine learning confidence intervals.</p><p>In asymptotic inference, the Riesz representer is inevitable. A growing literature directly incorporates the Riesz representer into estimation, which amounts to debiasing known estimators. Doubly robust estimating equations serve this purpose <ref type="bibr" target="#b32">[33]</ref>. A geometric perspective emphasizes Neyman orthogonality: by debiasing, the learning problem for the functional becomes orthogonal to the learning problem for the nonparametric object <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21</ref>]. An analytic perspective emphasizes the mixed bias property: by debiasing, the functional has bias equal to the product of certain learning rates <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35]</ref>. In this work, we focus on debiased machine learning with doubly robust estimating equations.</p><p>With debiasing alone, a key challenge remains: for inference, the function class in which the nonparametric quantity is learned must be Donsker <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b31">32]</ref>, or it must have slowly increasing entropy <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41]</ref>. However, popular nonparametric settings in machine learning may not satisfy this property. A solution to this challenging issue is to combine debiasing with sample splitting <ref type="bibr" target="#b25">[26]</ref>. The targeted <ref type="bibr" target="#b45">[46]</ref> and debiased <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13]</ref> machine learning literatures provide this insight. In particular, debiased machine learning delivers sufficient conditions for asymptotic inference on functionals in terms of learning rates of the underlying nonparametric quantity and the Riesz representer. We complement prior results with a finite sample analysis. This paper subsumes <ref type="bibr" target="#b37">[38,</ref><ref type="bibr">Section 4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Framework and examples</head><p>The general inference problem is to find a confidence interval for some scalar θ 0 in R where θ 0 = E{m(W, γ 0 )}, γ 0 is in Γ, and m : W × L 2 → R is an abstract formula. W in W is a concatenation of random variables in the model excluding the outcome Y in Y ⊂ R. L 2 is the space of functions of the form γ : W → R that are square integrable with respect to measure pr. Γ is a linear subset of L 2 known by the analyst, which may be L 2 itself.</p><p>Note that γ 0 may be the conditional expectation function γ 0 (w) = E(Y | W = w) or some other nonparametric quantity. For example, it could be the function defined as the solution to the ill posed inverse problem E(Y | W 2 = w 2 ) = E{γ(W 1 ) | W 2 = w 2 } where W 1 , W 2 ⊂ W . Such a function is called a nonparametric instrumental variable regression in econometrics <ref type="bibr" target="#b29">[30]</ref>. We study the exactly identified case, which amounts to assuming completeness when Γ = L 2 <ref type="bibr" target="#b11">[12]</ref>. If W 1 = W 2 then nonparametric instrumental variable regression simplifies into nonparametric regression.</p><p>A local functional θ lim 0 in R is a scalar that takes the form</p><formula xml:id="formula_0">θ lim 0 = lim h→0 θ h 0 , θ h 0 = E{m h (W, γ 0 )} = E{ℓ h (W j )m(W, γ 0 )}, γ 0 in Γ,</formula><p>where ℓ h is a Nadaraya Watson weighting with bandwidth h and W j is a scalar component of W . θ lim 0 is a nonparametric quantity. However, it can be approximated by the sequence (θ h 0 ). Each θ h 0 can be analyzed like θ 0 above as long as we keep track of how certain quantities depend on h. By this logic, finite sample semiparametric theory for θ h 0 translates to finite sample nonparametric theory for θ lim 0 up to some approximation error. In this sense, our analysis encompasses both global and local functionals.</p><p>To illustrate, we consider some classic functionals.</p><p>Example 3.1 (Heterogeneous treatment effect estimated by neural network). Let Y be a health outcome. Let W = (D, V, X) concatenate binary treatment D, covariate of interest V such as age, and other covariates X such as medical scans. Let</p><formula xml:id="formula_1">γ 0 (d, v, x) = E(Y | D = d, V = v, X = x)</formula><p>be a function estimated by a neural network. Under the assumption of selection on observables, the heterogeneous treatment effect is</p><formula xml:id="formula_2">CATE(v) = E{γ 0 (1, V, X) − γ 0 (0, V, X) | V = v} = lim h→0 E[ℓ h (V ){γ 0 (1, V, X) − γ 0 (0, V, X)}],</formula><p>where</p><formula xml:id="formula_3">ℓ h (V ) = (hω) −1 K {(V − v)/h}, ω = E[h −1 K {(V − v)/h}],</formula><p>and K is a bounded and symmetric kernel that integrates to one.</p><p>The heterogeneous treatment effect is defined with respect to some interpretable, low dimensional characteristic V such as age, race, or gender <ref type="bibr" target="#b0">[1]</ref>. The same functional without the localization ℓ h is the classic average treatment effect. See <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b18">[19]</ref> for other meaningful localizations of average treatment effect. </p><formula xml:id="formula_4">RDD = lim d↓0 E{γ 0 (d, X)} − lim d↑0 E{γ 0 (d, X)} = lim h→0 E{ℓ + h (D)γ 0 (D, X) − ℓ − h (D)γ 0 (D, X)}, where ℓ + h (D) = (hω + ) −1 K {(2D − h)/(2h)}, ω + = E h −1 K {(2D − h)/(2h)} , ℓ − h (D) = (hω − ) −1 K {(−2D − h)/(2h)}, ω − = E h −1 K {(−2D − h)/(2h)} , and K vanishes outside of the interval (−1/2, 1/2).</formula><p>The expressions for fuzzy regression discontinuity, exact kink, and fuzzy kink designs are similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 3.3 (Demand elasticity estimated by kernel instrumental variable regression).</head><p>Let Y be log quantity demanded of some good. Let W = (D, X, Z) concatenate log price D, covariates X, and cost shifter Z. Let γ 0 (d, x) be defined as the solution to E(Y | X = x, Z = z) = E{γ(D, X) | X = x, Z = z} estimated by a kernel instrumental variable regression <ref type="bibr" target="#b38">[39]</ref>. The demand elasticity is</p><formula xml:id="formula_5">ELASTICITY = E ∂ ∂d γ 0 (D, X) .</formula><p>In Supplement 2, we present the additional example of heterogeneous average derivative estimated by lasso, which is useful when an analyst has access to data on household spending behavior.</p><p>For our simple and general theorem, we require that the formula m is mean square continuous.</p><p>Assumption 3.1 (Linearity and mean square continuity). Assume that the functional γ → E{m(W, γ)} is linear, and that there exist</p><formula xml:id="formula_6">Q &lt; ∞ and q &gt; 0 such that E{m(W, γ) 2 } ≤ Q[E{γ(W ) 2 }] q for all γ in Γ.</formula><p>This condition will be key in Section 5, where we reduce the problem of inference for θ 0 into the problem of learning (γ 0 , α min 0 ), where α min 0 is introduced below. It is a powerful condition satisfied by many functionals of interest, or at least satisfied by their approximating sequences. Though the local functional θ lim 0 does not satisfy Assumption 3.1, each approximating θ h 0 does. In particular, for each m h there exists some Qh that depends on h. We keep track of Q in our analysis and subsequently consider Q = Qh . See Theorem 5.2 below for conditions that characterize Qh in local functionals, including Examples 3.1 and 3.2.</p><p>The restriction that γ 0 is in Γ ⊂ L 2 , where Γ is some linear function space, is called a restricted model in semiparametric statistical theory. In learning theory, mean square rates are adaptive to the smoothness of γ 0 , encoded by γ 0 in Γ. We quote a general Riesz representation theorem for restricted models. Lemma 3.1 (Riesz representation <ref type="bibr" target="#b15">[16]</ref>). Suppose Assumption 3.1 holds. Further suppose γ 0 is in Γ. Then there exists a Riesz representer α 0 in L 2 such that for all γ in Γ, E{m(W, γ)} = E{α 0 (W )γ(W )}. There exists a unique minimal Riesz representer α min 0 in closure(Γ) that satisfies this equation, obtained by projecting any α 0 onto Γ. Moreover, denoting by M the operator norm of γ → E{m(W, γ)}, we have that</p><formula xml:id="formula_7">[E{α min 0 (W ) 2 }] 1/2 = M ≤ Q1/2 &lt; ∞.</formula><p>The condition M &lt; ∞ is enough for the conclusions of Lemma 3.1 to hold. Since M ≤ Q1/2 , Q &lt; ∞ in Assumption 3.1 is a sufficient condition. Nonetheless, we assume Q &lt; ∞ because mean square continuity plays a central role in the main results of Section 5. In Examples 3.1 and 3.2, with propensity score π 0 (v, x),</p><formula xml:id="formula_8">α 0 (d, v, x) = ℓ h (v) d π 0 (v, x) − 1 − d 1 − π 0 (v, x) ; α + 0 (d, x) = ℓ + h (d), α − 0 (d, x) = ℓ − h (d).</formula><p>Riesz representation delivers a doubly robust formulation of the target θ 0 in R. For the case where γ 0 (w) is defined as a nonparametric regression in Γ or projection onto Γ, consider the estimating equation</p><formula xml:id="formula_9">θ 0 = E[m(W, γ 0 ) + α min 0 (W ){Y − γ 0 (W )}].</formula><p>This formulation is doubly robust since it remains valid if either γ 0 or α min 0 is correct: for all (γ, α) in Γ,</p><formula xml:id="formula_10">θ 0 = E[m(W, γ 0 ) + α(W ){Y − γ 0 (W )}] = E[m(W, γ) + α min 0 (W ){Y − γ(W )}].</formula><p>The term α(w){y − γ(w)} serves as a bias correction for the term m(w, γ). We view (γ 0 , α min 0 ) as nuisance parameters that we must learn in order to learn and infer θ 0 . While any Riesz representer α 0 will suffice for valid learning and inference of θ 0 = E{m(W, γ 0 )} under correct specification of γ 0 as the regression E(Y | W = w) in Γ, the minimal Riesz representer α min 0 confers specification robust inference and semiparametric efficiency for estimating θ 0 = E{m(W, γ 0 )} when γ 0 is only the projection of E(Y | W = w) onto Γ; see <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">Theorem 4.2]</ref>.</p><p>If γ 0 (w) is defined as the solution to an ill posed inverse problem, then the appropriate Riesz representer is defined as the solution to another ill posed inverse problem <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b22">23]</ref>. The relevant nuisance parameters are (γ 0 , α min 0 ) defined as unique solutions (γ, α) to </p><formula xml:id="formula_11">E(Y | W 2 = w 2 ) = E{γ(W 1 ) | W 2 = w 2 }, η min 0 (w 1 ) = E{α(W 2 ) | W 1 = w 1 },</formula><formula xml:id="formula_12">θ 0 = E[m(W 1 , γ 0 ) + α min 0 (W 2 ){Y − γ 0 (W 1 )}].</formula><p>A new insight of this work is that, for any mean square continuous functional, n −1/2 Gaussian approximation is still possible if either γ 0 or α min 0 is the solution to a mildly, rather than severely, ill posed inverse problem; the doubly robust formulation confers double robustness to ill posedness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Algorithm</head><p>Our goal is general purpose learning and inference for the target parameter θ 0 in R that is a mean square continuous functional of γ 0 in Γ. Lemma 3.1 demonstrates that any such θ 0 has a unique minimal representer α min 0 in Γ. In this section, we describe a meta algorithm to turn estimators γ of γ 0 and α of α min 0 into an estimator θ of θ 0 such that θ has a valid and practical confidence interval. Recall that γ may be any machine learning algorithm. To preserve this generality, we do not instantiate a choice of γ; we treat it as a black box. In subsequent analysis, we will only require that γ converges to γ 0 in mean square error. This mean square rate is guaranteed by existing statistical learning theory.</p><p>The target estimator θ as well as its confidence interval will depend on nuisance estimators γ and α. We refrain from instantiating the estimator α for α min 0 . As we will see in subsequent analysis, the general theory only requires that α converges to α min 0 in mean square error. A recent literature provides α estimators with fast rates inspired by the Dantzig selector <ref type="bibr" target="#b15">[16]</ref>, lasso <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b3">4]</ref>, adversarial neural networks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25]</ref>, and kernel ridge regression <ref type="bibr" target="#b37">[38]</ref>. 1. For each fold ℓ, estimate γℓ and αℓ from observations in I c ℓ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Estimate θ</head><formula xml:id="formula_13">0 as θ = n −1 L ℓ=1 i∈I ℓ [m(W i , γℓ ) + αℓ (W i ){Y i − γℓ (W i )}].</formula><p>3. Estimate its (1 − a)100% confidence interval as θ ± c a σn −1/2 , where c a is the 1 − a/2 quantile of the standard Gaussian and</p><formula xml:id="formula_14">σ2 = n −1 L ℓ=1 i∈I ℓ [m(W i , γℓ ) + αℓ (W i ){Y i − γℓ (W i )} − θ] 2 .</formula><p>This meta algorithm can be seen as an extension of classic one step corrections <ref type="bibr" target="#b30">[31]</ref> amenable to the use of modern machine learning, and it has been termed debiased machine learning <ref type="bibr" target="#b12">[13]</ref>. It departs from targeted machine learning inference with a finite sample <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b10">11]</ref> in a few ways. On the one hand, it avoids iteration and bootstrapping, thereby simplifying computation. On the other hand, it does not involve substitution, which would ensure that the estimator obeys additional meaningful constraints. See <ref type="bibr" target="#b17">[18]</ref> for an algorithm that combines the two approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Validity of confidence interval</head><p>We write this section at a high level of generality so it can be used by analysts working on a variety of problems. We assume a few simple and interpretable conditions and consider black box estimators (γ, α). We prove by finite sample arguments that θ defined by Algorithm 4.1 is consistent, and that its confidence interval is valid and semiparametrically efficient. Towards this end, define the oracle moment function 4 }. Write the Berry Esseen constant as c BE = 0.4748 <ref type="bibr" target="#b36">[37]</ref>. The result will be in terms of abstract mean square rates. Definition 5.1 (Mean square error). Write the mean square error R(γ ℓ ) and the projected mean square error P(γ ℓ ) of γℓ trained on observations indexed by</p><formula xml:id="formula_15">ψ 0 (w) = ψ(w, θ 0 , γ 0 , α min 0 ), ψ(w, θ, γ, α) = m(w, γ) + α(w){y − γ(w)} − θ. Its moments are σ 2 = E{ψ 0 (W ) 2 }, κ 3 = E{|ψ 0 (W )| 3 }, and ζ 4 = E{ψ 0 (W )</formula><formula xml:id="formula_16">I c ℓ as R(γ ℓ ) = E[{γ ℓ (W ) − γ 0 (W )} 2 | I c ℓ ], P(γ ℓ ) = E([E{γ ℓ (W 1 ) − γ 0 (W 1 ) | W 2 , I c ℓ }] 2 | I c ℓ )</formula><p>. Likewise define R(α ℓ ) and P(α ℓ ).</p><p>Statistical learning theory provides rates of this form, where I c ℓ is a training set and W is a test point. In the case of nonparametric regression, R(γ ℓ ) or R(α ℓ ) typically has a fast rate between n −1/2 and n −1 . In the case of nonparametric instrumental variable regression, R(γ ℓ ) and R(α ℓ ) typically have rates slower than n −1/2 due to ill posedness, but P(γ ℓ ) or P(α ℓ ) may have a fast rate <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b19">20]</ref>. Our main result is a finite sample Gaussian approximation. Theorem 5.1 (Finite sample Gaussian approximation). Suppose Assumption 3.</p><formula xml:id="formula_17">1 holds, E[{Y − γ 0 (W )} 2 | W ] ≤ σ2 , and α min 0 ∞ ≤ ᾱ. Then with probability 1 − ǫ, sup z∈R pr n 1/2 σ ( θ − θ 0 ) ≤ z − Φ(z) ≤ c BE κ σ 3 n −1/2 + ∆ (2π) 1/2 + ǫ,</formula><p>where Φ(z) is the standard Gaussian cumulative distribution function and</p><formula xml:id="formula_18">∆ = 3L ǫσ ( Q1/2 + ᾱ){R(γ ℓ )} q/2 + σ{R(α ℓ )} 1/2 + {nR(γ ℓ )R(α ℓ )} 1/2 .</formula><p>If in addition αℓ ∞ ≤ ᾱ′ then the same result holds updating ∆ to be</p><formula xml:id="formula_19">4L ǫ 1/2 σ ( Q1/2 + ᾱ + ᾱ′ ){R(γ ℓ )} q/2 + σ{R(α ℓ )} 1/2 + 1 σ [{nP(γ ℓ )R(α ℓ )} 1/2 ∧{nR(γ ℓ )P(α ℓ )} 1/2 ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For local functionals, further suppose approximation error of size</head><formula xml:id="formula_20">∆ h = n 1/2 σ −1 h |θ h 0 − θ lim 0 |. Then the same result holds replacing ( θ, θ 0 , ∆) with ( θh , θ lim 0 , ∆ + ∆ h ).</formula><p>Theorem 5.1 is a finite sample Gaussian approximation for debiased machine learning with black box (γ ℓ , αℓ ). It degrades gracefully if the parameters ( Q, σ, ᾱ, ᾱ′ ) diverge relative to n and the learning rates. Note that ᾱ′ is a bound on the chosen estimator αℓ that can be imposed by censoring extreme evaluations. Theorem 5.1 is a finite sample refinement of the asymptotic black box result in <ref type="bibr" target="#b13">[14]</ref>.</p><p>In the bound ∆, the expression {nR(γ ℓ )R(α ℓ )} 1/2 allows a tradeoff: one of the learning rates may be slow, as long as the other is sufficiently fast to compensate. It is easily handled in the case of nonparametric regression, where R(γ ℓ ) or R(α ℓ ) typically has a fast rate. However, the expression may diverge in the case of nonparametric instrumental variable regression, where both rates may be slow due to ill posedness.</p><p>The refined bound provides an alternative path to Gaussian approximation, replacing {nR(γ ℓ )R(α ℓ )} 1/2 with the minimum of {nP(γ ℓ )R(α ℓ )} 1/2 and {nR(γ ℓ )P(α ℓ )} 1/2 . Importantly, the projected mean square error P(γ ℓ ) can have a fast rate even when the mean square error R(γ ℓ ) has a slow rate because its definition sidesteps ill posedness. Moreover, the analyst only needs P(γ ℓ ) fast enough to compensate for the ill posedness encoded in R(α ℓ ), or P(α ℓ ) fast enough to compensate for the ill posedness encoded in R(γ ℓ ). This general and finite sample characterization of double robustness to ill posedness appears to be new. In independent work, <ref type="bibr" target="#b24">[25]</ref> document an asymptotic special case of this result for a specific global functional and specific nuisance estimators; see Supplement 3.</p><p>By Theorem 5.1, the neighborhood of Gaussian approximation scales as σn −1/2 . If σ is a constant, then the rate of convergence is n −1/2 , i.e. the parametric rate. If σ is a diverging sequence, then the rate of convergence degrades gracefully to nonparametric rates. A precise characterization of σ is possible, which we provide in Supplement 2 and summarize here. It turns out that global functionals have σ that is constant, while local functionals have σ = σ h that is a diverging sequence. We emphasize which quantities are diverging sequences for local functionals by indexing with the bandwidth h. Theorem 5.2 (Characterization of key quantities). If noise has finite variance then σ2 &lt; ∞. Suppose bounded moment and heteroscedasticity conditions defined in Supplement 2 hold. Then for</p><formula xml:id="formula_21">global functionals κ/σ σ ≍ M &lt; ∞; κ, ζ M 2 ≤ Q &lt; ∞; and ᾱ &lt; ∞.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Suppose bounded moment, heteroscedasticity, density, and derivative conditions defined in Supplement 2 hold. Then for local functionals κ</head><formula xml:id="formula_22">h /σ h h −1/6 , σ h ≍ Mh ≍ h −1/2 , κ h h −2/3 , ζ h h −3/4 , Qh h −2 , ᾱh h −1 , and ∆ h n 1/2 h v+1/2</formula><p>where v is the order of differentiability defined in Supplement 2.</p><p>For global functionals, ( Q, ᾱ) are finite constants that depend on the problem at hand. For example, for treatment effects a sufficient condition is that the propensity score is bounded away from zero and one. For derivatives, a sufficient condition is that Γ satisfies Sobolev conditions. For local functionals, we handle ( Qh , ᾱh ) on a case by case basis. See Supplement 2 for interpretable and complete characterizations.</p><p>Observe that the finite sample Gaussian approximation in Theorem 5.1 is in terms of the true asymptotic variance σ 2 . We now provide a guarantee for its estimator σ2 .</p><formula xml:id="formula_23">Theorem 5.3 (Variance estimation). Suppose Assumption 3.1 holds, E[{Y − γ 0 (W )} 2 | W ] ≤ σ2 , and αℓ ∞ ≤ ᾱ′ . Then with probability 1 − ǫ ′ , |σ 2 − σ 2 | ≤ ∆ ′ + 2(∆ ′ ) 1/2 {(∆ ′′ ) 1/2 + σ} + ∆ ′′ , where ∆ ′ = 4( θ − θ 0 ) 2 + 24L ǫ ′ { Q + (ᾱ ′ ) 2 }R(γ ℓ ) q + σ2 R(α ℓ ) , ∆ ′′ = 2 ǫ ′ 1/2 ζ 2 n −1/2 .</formula><p>Theorem 5.3 is a finite sample variance estimation guarantee. It degrades gracefully if the parameters ( Q, σ, ᾱ′ ) diverge relative to n and the learning rates. Theorems 5.1 and 5.3 immediately imply simple, interpretable conditions for validity of the confidence interval. We conclude by summarizing these conditions. Corollary 5.1 (Confidence interval). Suppose Assumption 3.1 holds as well as the following regularity and learning rate conditions, as n → ∞ and as h → 0:</p><formula xml:id="formula_24">E[{Y − γ 0 (W )} 2 | W ] ≤ σ2 , α min 0 ∞ ≤ ᾱ, αℓ ∞ ≤ ᾱ′ , (κ/σ) 3 + ζ 2 n −1/2 → 0; 1. Q1/2 + ᾱ/σ + ᾱ′ {R(γ ℓ )} q/2 = o p (1); 2. σ{R(α ℓ )} 1/2 = o p (1); 3. [{nR(γ ℓ )R(α ℓ )} 1/2 ∧ {nP(γ ℓ )R(α ℓ )} 1/2 ∧ {nR(γ ℓ )P(α ℓ )} 1/2 ]/σ = o p (1).</formula><p>Then the estimator θ in Algorithm 4.1 is consistent and asymptotically Gaussian, and the confidence interval in Algorithm 4.1 includes θ 0 with probability approaching the nominal level. Formally,</p><formula xml:id="formula_25">θ = θ 0 + o p (1), σ −1 n 1/2 ( θ − θ 0 ) N (0, 1), pr θ 0 in θ ± c a σn −1/2 → 1 − a.</formula><p>For local functionals, if ∆ h → 0, then the same result holds replacing ( θ, θ 0 ) with ( θh , θ lim 0 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Simulations</head><p>We present simulations for Example 3.1: heterogeneous treatment effect estimated by neural network. In addition, we present results for heterogeneous treatment effect estimated by random forest and lasso.</p><p>Recall that the localized functional is</p><formula xml:id="formula_26">CATE(v) = lim h→0 E[ℓ h,v (V ){γ 0 (1, V, X) − γ 0 (0, V, X)}],</formula><p>where</p><formula xml:id="formula_27">ℓ h,v (V ) = (hω) −1 K {(V − v)/h} and ω = E[h −1 K {(V − v)/h}].</formula><p>V is an interpretable, low dimensional characteristic such as age, race, or gender. We implement the heterogeneous treatment effect design of <ref type="bibr" target="#b0">[1]</ref>, where CATE(v 2 and v is a value in the interval (−0.5, 0.5). A single observations consists of the tuple (Y i , D i , V i , X i ) for outcome, treatment, covariate of interest, and other covariates. In this design,</p><formula xml:id="formula_28">) = v(1 + 2v) 2 (v − 1)</formula><formula xml:id="formula_29">Y i , D i , V i are in R and X i is in R 3 .</formula><p>A single observation is generated as follows. Draw the latent variables (ǫ ij ) (j = 1, ..., 4) independently and identically from the uniform distribution U(−0.5, 0.5). Then set the covariates (V i , X i ) according to</p><formula xml:id="formula_30">V i = ǫ i1 , X i1 = 1 + 2V i + ǫ i2 , X i2 = 1 + 2V i + ǫ i3 , X i3 = (V i − 1) 2 + ǫ i4 .</formula><p>Under the assumption of selection on observables, treatment assignment is as good as random conditional on (V i , X i ). Draw the treatment D i from the Bernoulli distribution with parameter</p><formula xml:id="formula_31">Λ{1/2(V i + X i1 + X i2 + X i3 )} where Λ is the logistic link function. Finally, calculate outcome Y i as 0 if D i = 0 and V i X i1 X i2 X i3 + ν i if D i = 1</formula><p>, where the response noise ν i is independently drawn from the Gaussian distribution N (0, 1/16). A random sample consists of n = 100 such observations</p><formula xml:id="formula_32">(Y i , D i , V i , X i ) (i = 1, ..., n).</formula><p>We implement different variations of Algorithm 4.1 with L = 5 folds. Across variations, we use a lasso estimator α for the minimal Riesz representer α min 0 <ref type="bibr" target="#b17">[18]</ref>. We consider different estimators γ for the nonparametric regression γ 0 : neural network, random forest, and lasso. We consider both low dimensional and high dimensional variations. In the low dimensional variation, the estimators</p><formula xml:id="formula_33">(α ℓ , γℓ ) use (D i , V i , X i ) (i in I c</formula><p>ℓ ) as well as their interactions. In the high dimensional variation, the estimators (α ℓ , γℓ ) use fourth order polynomials of (D i , V i , X i ) (i in I c ℓ ). Some tuning choices are necessary. We follow the default hyperparameter settings to tune the lasso Riesz representer and lasso regression from <ref type="bibr" target="#b17">[18]</ref>. We implement the neural network with a single hidden layer of eight neurons and the random forest with 1000 trees as in <ref type="bibr" target="#b12">[13]</ref>. Finally, to tune the bandwidth, we use the heuristic h = c h σv n −0.2 <ref type="bibr" target="#b18">[19]</ref>, where σ2 v is the sample variance of (V i ) (i = 1, ..., n). The bandwidth hyperparameter c h is chosen by the analyst. We evaluate robustness of coverage with respect to hyperparameter values c h = 0.25, 0.50, 1.00 below. Empirically, we find that c h = 0.25 and c h = 0.50 work well.</p><p>For each choice of nonparametric regression estimator γ, whether neural network, random forest, or lasso, and for each choice of specification, whether low or high dimensional, we report a coverage table summarizing 500 simulations. The initial columns denote the grid value v, the corresponding heterogeneous treatment effect CATE(v), and the bandwidth hyperparamter value c h . The subsequent columns calculate the average point estimate and the average standard error across the 500 simulations for this choice of {v, CATE(v), c h }. The final columns report what percentage of the 500 confidence intervals contain the true value CATE(v) compared to the theoretical benchmarks of 80% and 95%, respectively.</p><p>For the low dimensional regime, Tables <ref type="table" target="#tab_1">1, 2</ref>, and 3 summarize results for neural network, random forest, and lasso, respectively. With bandwidth hyperparameter values c h = 0.25 and c h = 0.50, coverage is close to the nominal level across γ estimators and across grid values v = −0.25, 0.00, 0.25. Neural network and random forest have comparable performance. Lasso has higher bias and compensates with higher variance for the grid value v = 0.25.   For the high dimensional regime, Tables <ref type="table" target="#tab_4">4, 5</ref>, and 6 summarize results for neural network, random forest, and lasso, respectively. With bandwidth hyperparameter values c h = 0.25 and c h = 0.50, coverage is close to the nominal level across γ estimators and for grid values v = −0.25 and v = 0.00. Across γ estimators, the grid value v = 0.25 is more challenging. Compared to the low dimensional regime, each estimator in the high dimensional regime has higher bias and compensates with higher variance for the grid value v = 0.25.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Characterization of key parameters B.1 Additional example</head><p>We present an additional example useful in commercial applications where an analyst has access to data on household spending behavior. See <ref type="bibr" target="#b14">[15]</ref> for a motivating economic model and further interpretation.</p><p>Example B.1 (Heterogeneous average derivative estimated by lasso). Let Y be share of household expenditure on some good. Let W = (D, V, X) concatenate log price of the good D, covariate of interest V such as household size, and other covariates X such as log prices of other goods and log total household expenditure. Let</p><formula xml:id="formula_34">γ 0 (d, v, x) = E(Y | D = d, V = v, X =</formula><p>x) be a function estimated by lasso. The heterogeneous average derivative is</p><formula xml:id="formula_35">DERIV(v) = E ∂ ∂d γ 0 (D, V, X) | V = v = lim h→0 E ℓ h (V ) ∂ ∂d γ 0 (D, V, X) , where ℓ h (V ) = (hω) −1 K {(V − v)/h}, ω = E[h −1 K {(V − v)/h}],</formula><p>and K is a bounded and symmetric kernel that integrates to one.</p><p>The heterogeneous average derivative is defined with respect to some interpretable, low dimensional characteristic V such as household size <ref type="bibr" target="#b0">[1]</ref>. The same functional without the localization ℓ h is the classic average derivative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Riesz representers</head><p>We begin by deriving explicit expressions for Riesz representer α 0 . Recall that the unique minimal Riesz representer α min 0 is the projection of any valid Riesz representer α 0 onto Γ. The explicit expressions for α 0 help to articulate simple sufficient conditions for existence of α min 0 . Lemma B.1. In Examples 3.1, 3.2, 3.3, and B.1, the minimal representer α min 0 (w) can be obtained by projecting the following Riesz representer α 0 (w) onto Γ.</p><formula xml:id="formula_36">1. Example 3.1. Denote the propensity score π 0 (v, x) = pr(D = 1 | V = v, X = x). Then α 0 (d, v, x) = ℓ h (v) d π 0 (v, x) − 1 − d 1 − π 0 (v, x)</formula><p>.</p><p>Hence the minimal Riesz representer α min 0 exists if π 0 (v, x) is bounded away from zero and one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Example 3.Denote the Riesz representer for the first term by α +</head><p>0 and the Riesz representer for the second term by α − 0 . Then</p><formula xml:id="formula_37">α + 0 (d, x) = ℓ + h (d), α − 0 (d, x) = ℓ − h (d).</formula><p>Hence the minimal Riesz representer α min 0 exists. </p><formula xml:id="formula_38">α 0 (d, v, x) = −ℓ h (v)∂ d log f (d | v, x).</formula><p>Next, we characterize key quantities ( Q, σ, ᾱ, ᾱ′ ) and moments (σ, κ, ζ) that appear in Theorems 5.1 and 5.3. Recall that</p><formula xml:id="formula_39">E[{Y − γ 0 (W )} 2 | W ] ≤ σ2 , αℓ ∞ ≤ ᾱ′ ,</formula><p>so σ is a constant if noise has bounded variance and ᾱ′ can be imposed by trimming the algorithm αℓ . We therefore focus on ( Q, ᾱ) and (σ, κ, ζ).</p><p>We consider two cases: global functionals, with weighting ℓ h that has a fixed bandwidth; and local functionals, with weighting ℓ h that depends on some vanishing bandwidth h → 0. To lighten notation, let X pr,q = {E(|X| q )} 1/q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Global functionals</head><p>Lemma B.2 (Oracle moments for global functionals). Suppose the weighting is bounded, namely ℓ h ≤ C &lt; ∞, and Γ ⊂ L 2 . Suppose α min 0 exists. Further suppose there exist (c, c, c) bounded away from zero and above such that the following conditions hold.</p><p>1. Control of representer moments. For q = 3, 4 α min 0 pr,q ≤ c α min 0 2 pr,2 ∨ 1 .</p><p>2. Bounded moments. For q = 2, 3, 4,</p><formula xml:id="formula_40">U 1 = m(W, γ 0 ) − E{m(W, γ 0 )}, U 1 pr,q ≤ c.</formula><p>3. Bounded heteroscedasticity. For q = 2, 3, 4,</p><formula xml:id="formula_41">U 2 = Y − γ 0 (W ), c ≤ U 2 | W pr,q ≤ c. Then c M ≤ σ ≤ c 1 + M 2 , κ, ζ ≤ c{1 + c( M 2 ∨ 1)}.</formula><p>In summary,</p><formula xml:id="formula_42">κ σ σ ≍ M &lt; ∞, κ, ζ M 2 &lt; ∞.</formula><p>Clearly Assumption 3.1 depends on the functional of interest. Towards a characterization of Q and q in Examples 3. 1. Each γ in Γ is twice continuously differentiable.</p><p>2. For each γ in Γ, k γ pr,2 &lt; ∞ where</p><formula xml:id="formula_43">k γ (d, x) = {−∂ d log f (d | x)}{∂ d γ(d, x)} − ∂ 2 d γ(d, x). Then E[{∂ d γ(D, X)} 2 ] ≤ k γ pr,2 [E{γ(D, X) 2 }] 1/2 .</formula><p>Furthermore, sup γ∈Γ k γ pr,2 &lt; ∞ if either of the following conditions hold.  </p><formula xml:id="formula_44">1. ∂ d log f (D | X) pr,2 &lt; ∞ and for all γ in Γ, ∂ d γ ∞ &lt; ∞ and ∂ 2 d γ pr,2 &lt; ∞; 2. −∂ d log f (d | x)</formula><formula xml:id="formula_45">[{∂ d γ(D, V, X)} 2 ] &lt; ∞ and E[{∂ 2 d γ(D, V, X)} 2 ] &lt; ∞.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Local functionals</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Given a local functional</head><formula xml:id="formula_46">θ h 0 = E{m h (W, γ 0 )} = E{ℓ h (V )m(W, γ 0</formula><p>)}, we will now refer to the Riesz representer of m h by α h 0 and the Riesz representer of m by α 0 for this subsection. The latter objects correspond to the global setting where the weighting is bounded. To lighten notation, we write ℓ = ℓ h . Lemma B.6 (Oracle moments for local functionals). Suppose α min 0 exists and Γ = L 2 . Further suppose there exist (α, α, c, c, f, f, f ′ , h 0 ) bounded away from zero and above such that the following conditions hold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Control of representer absolute value:</head><formula xml:id="formula_47">0 &lt; α ≤ α 0 (w) ≤ α.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Valid neighborhood. There exists</head><formula xml:id="formula_48">N h0 (v) = (v ′ : |v ′ − v| ≤ h) ⊂ V.</formula><p>3. Bounded moments. For all h ≤ h 0 and for q = 2, 3, 4,</p><formula xml:id="formula_49">U 1 = m h (W, γ 0 ) − E{m h (W, γ 0 )}, U<label>1</label></formula><p>pr,q ≤ c ℓ pr,q .</p><p>4. Bounded heteroscedasticity. For q = 2, 3, 4,</p><formula xml:id="formula_50">U 2 = Y − γ 0 (W ), c ≤ U 2 | W pr,q ≤ c.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Bounded density. The density</head><formula xml:id="formula_51">f V obeys, for all v ′ in N h0 (v), 0 &lt; f ≤ f V (v ′ ) ≤ f, |∂f V (v ′ )| ≤ f ′ .</formula><p>Then finite sample bounds in the proof hold. In summary,</p><formula xml:id="formula_52">κ h σ h h −1/6 σ h ≍ Mh ≍ h −1/2 → ∞, κ h h −2/3 → ∞, ζ h h −3/4 → ∞.</formula><p>The conditions of Lemma B.5 suffice for 0 &lt; α ≤ α 0 (w) ≤ α in Examples 3.1 and 3.2.</p><p>As before, Assumption 3.1 depends on the functional of interest. Lemma B.7 (Mean square continuity for local functionals). Suppose α min 0 exists and Γ ⊂ L 2 . Further suppose there exist ( f, f, f ′ , h 0 ) bounded away from zero and above such that the following conditions hold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Valid neighborhood. There exists</head><formula xml:id="formula_53">N h0 (v) = (v ′ : |v ′ − v| ≤ h) ⊂ V.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Bounded density. The density f</head><formula xml:id="formula_54">V obeys, for all v ′ in N h0 (v), 0 &lt; f ≤ f V (v ′ ) ≤ f, |∂f V (v ′ )| ≤ f ′ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The conditions of Lemma B.4 hold. Then the finite sample bound in the proof holds for Examples 3.1 and 3.2. In summary,</head><formula xml:id="formula_55">Qh h −2 → ∞.</formula><p>As before, the assumption of α min,h 0 ∞ ≤ ᾱ depends on the functional of interest. Lemma B.8 (Bounded Riesz representer for local functionals). Suppose α min 0 exists and Γ ⊂ L 2 . Further suppose there exist ( f, f, f ′ , h 0 , K) bounded away from zero and above such that the following conditions hold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Valid neighborhood. There exists</head><formula xml:id="formula_56">N h0 (v) = (v ′ : |v ′ − v| ≤ h) ⊂ V.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Bounded density. The density</head><formula xml:id="formula_57">f V obeys, for all v ′ in N h0 (v), 0 &lt; f ≤ f V (v ′ ) ≤ f, |∂f V (v ′ )| ≤ f ′ . 3. Bounded kernel. |K(u)| ≤ K.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The conditions of Lemma B.5 hold, allowing bandwidth to vanish.</head><p>Then the finite sample bound in the proof holds. In summary,</p><formula xml:id="formula_58">ᾱh h −1 → ∞.</formula><p>The main results are in terms of abstract mean square rates R(α h ℓ ) and P(α h ℓ ) for the local Riesz representer α min,h 0 of the functional m h . A growing literature proposes machine learning estimators α with rates R(α ℓ ) and P(α ℓ ) for the global Riesz representer α min 0 of the functional m.</p><p>A natural choice of estimator αh for α min,h 0 is the localization ℓ h times an estimator α for α min 0 . We prove that this choice allows an analyst to translate global Riesz representer rates into local Riesz representer rates under mild regularity conditions. In Supplement 1, we confirm that this choice performs well in simulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma B.9 (Translating global rates to local rates). Suppose the conditions of Lemma B.8 hold with</head><formula xml:id="formula_59">Γ = L 2 . Then R(α h ℓ ) h −2 R(α ℓ ), P(α h ℓ ) h −2 P(α ℓ ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Approximation error</head><p>Finally, we characterize the finite sample approximation error</p><formula xml:id="formula_60">∆ h = n 1/2 σ −1 |θ h 0 − θ lim 0 | where θ lim 0 = lim h→0 θ h 0 , θ h 0 = E{m h (W, γ 0 )} = E{ℓ h (V )m(W, γ 0 )}.</formula><p>∆ h is bias from using a semiparametric sequence to approximate a nonparametric quantity.</p><p>We define</p><formula xml:id="formula_61">m(v) = E[m(W, γ 0 ) | V = v] to lighten notation.</formula><p>Lemma B.10 (Approximation error from localization <ref type="bibr" target="#b15">[16]</ref>). Suppose there exist constants (h 0 , K, v, ḡv , fv , f, ḡ) bounded away from zero and above such that the following conditions hold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Valid neighborhood. There exists</head><formula xml:id="formula_62">N h0 (v) = (v ′ : |v ′ − v| ≤ h) ⊂ V.</formula><p>2. Differentiability. On N h0 (v), m(v ′ ) and f V (v ′ ) are differentiable to the integer order sm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Bounded derivatives. Let</head><formula xml:id="formula_63">v = sm ∧ o where o is the order of the kernel K. Let ∂ v d denote the v order derivative ∂ v /(∂d) v . Assume sup v ′ ∈N h 0 (v) ∂ v v (m(v ′ )f V (v ′ )) op ≤ ḡv , sup v ′ ∈N h 0 (v) ∂ v v f V (v ′ ) op ≤ fv , inf v ′ ∈N h 0 (v) f V (v ′ ) ≥ f. 4. Bounded conditional formula. m(v)f V (v) ≤ ḡ.</formula><p>Then there exist constants (C, h 1 ) depending only on (h 0 , K, v, ḡv , fv , f, ḡ) such that for all h 1 in (h, h 0 ),</p><formula xml:id="formula_64">|θ h 0 − θ lim 0 | ≤ Ch v . In summary, ∆ h n 1/2 h v+1/2 .</formula><p>To summarize the characterizations in this Supplement, we provide a corollary for local functionals. Let R(α ℓ ) and P(α ℓ ) be defined as in Lemma B.9. As n → ∞ and h → 0, suppose the regularity condition on moments n −1/2 h −3/2 → 0 as well as the following learning rate conditions:</p><formula xml:id="formula_65">1. h −1 + ᾱ′ {R(γ ℓ )} q/2 = o p (1); 2. σh −1 {R(α ℓ )} 1/2 = o p (1); 3. h −1/2 [{nR(γ ℓ )R(α ℓ )} 1/2 ∧ {nP(γ ℓ )R(α ℓ )} 1/2 ∧ {nR(γ ℓ )P(α ℓ )} 1/2 ] = o p (1).</formula><p>Finally suppose the approximation error condition ∆ h → 0. Then the estimator θh in Algorithm 4.1 is consistent and asymptotically Gaussian, and the confidence interval in Algorithm 4.1 includes θ lim 0 with probability approaching the nominal level. Formally,</p><formula xml:id="formula_66">θh = θ lim 0 + o p (1), σ −1 h n 1/2 ( θh − θ lim 0 ) N (0, 1), pr θ lim 0 in θh ± c a σn −1/2 → 1 − a.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Discussion</head><p>In independent work, <ref type="bibr" target="#b24">[25,</ref><ref type="bibr">Theorem 9]</ref> present an asymptotic Gaussian approximation result for a particular global functional: average treatment effect identified by negative controls. This functional fits within our framework because it is a mean square continuous functional of a nonparametric instrumental variable regression. To verify mean square continuity, see Example 3.1 in Lemma B.4.</p><p>In their analysis, the authors write the sufficient condition</p><formula xml:id="formula_67">min(τ γ,n , τ α,n )ι γ,n ι α,n = o(n −1/2 ), τ γ,n = sup γ∈Gn {R(γ)} 1/2 {P(γ)} 1/2 , τ α,n = sup α∈An {R(α)} 1/2 {P(α)} 1/2</formula><p>where (τ γ,n , τ α,n ) are ratio measures of ill posedness and (ι γ,n , ι α,n ) are critical radii for the sequence of function classes (G n , A n ) used in adversarial estimation procedures for (γ, α). In particular (ι γ,n , ι α,n ) appear in the authors' bounds for {P(γ)} 1/2 and {P(α)} 1/2 , respectively.</p><p>For comparison, our analogous condition in Theorem 5.1 is that</p><formula xml:id="formula_68">min {P(γ ℓ )R(α ℓ )} 1/2 , {R(γ ℓ )P(α ℓ )} 1/2 = o(σn −1/2 ).</formula><p>By contrast, our result is (i) for the entire class of mean square continuous functionals, (ii) for black box estimators (γ, α), and (iii) finite sample, so it also handles local functionals in which σ diverges.</p><p>Critical radius arguments are one way to prove mean square rates for certain machine learning estimators, but not the only way. This distinction is important, since many existing statistical learning theory rates, whether for neural networks as in Example 3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Proof of main result D.1 Gateaux differentiation</head><p>Recall the notation ψ 0 (w) = ψ(w, θ 0 , γ 0 , α min 0 ), ψ(w, θ, γ, α) = m(w, γ) + α(w){y − γ(w)} − θ, where γ → m(w, γ) is linear. For readability, we introduce the following notation for Gateaux differentiation.</p><p>Definition D.1 (Gateaux derivative). Let u(w), v(w) be functions and let τ, ζ in R be scalars. The Gateaux derivative of ψ(w, θ, γ, α) with respect to its argument γ in the direction u is</p><formula xml:id="formula_69">{∂ γ ψ(w, θ, γ, α)}(u) = ∂ ∂τ ψ(w, θ, γ + τ u, α) τ =0</formula><p>.</p><p>The cross derivative of ψ(w, θ, γ, α) with respect to its arguments (γ, α) in the directions (u, v) is</p><formula xml:id="formula_70">{∂ 2 γ,α ψ(w, θ, γ, α)}(u, v) = ∂ 2 ∂τ ∂ζ ψ(w, θ, γ + τ u, α + ζv) τ =0,ζ=0</formula><p>.</p><p>Proposition D.1 (Calculation of derivatives).</p><formula xml:id="formula_71">{∂ γ ψ(w, θ, γ, α)}(u) = m(w, u) − α(w)u(w); {∂ α ψ(w, θ, γ, α)}(v) = v(w){y − γ(w)}; {∂ 2 γ,α ψ(w, θ, γ, α)}(u, v) = −v(w)u(w).</formula><p>Proof. For the first result, write</p><formula xml:id="formula_72">ψ(w, θ, γ + τ u, α) = m(w, γ) + τ m(w, u) + α(w){y − γ(w) − τ u(w)} − θ.</formula><p>For the second result, write</p><formula xml:id="formula_73">ψ(w, θ, γ, α + ζv) = m(w, γ) + α(w){y − γ(w)} + ζv(w){y − γ(w)} − θ.</formula><p>For the final result, write</p><formula xml:id="formula_74">ψ(w, θ, γ + τ u, α + ζv) = m(w, γ) + τ m(w, u) + α(w){y − γ(w) − τ u(w)} + ζv(w){y − γ(w) − τ u(w)} − θ.</formula><p>Finally, take scalar derivatives with respect to (τ, ζ).</p><p>By using the doubly robust moment function, we have the following helpful property.</p><p>Proposition D.2 (Mean zero derivatives). For any (u, v),</p><formula xml:id="formula_75">E{∂ γ ψ 0 (W )}(u) = 0, E{∂ α ψ 0 (W )}(v) = 0.</formula><p>Proof. We appeal to Proposition D.1. For the first result, write</p><formula xml:id="formula_76">E{∂ γ ψ 0 (W )}(u) = E{m(W, u) − α min 0 (W )u(W )}.</formula><p>In the case of nonparametric regression or projection, appeal to the definition of minimal Riesz representer α min 0 . In the case of nonparametric instrumental variable regression,</p><formula xml:id="formula_77">E{m(W 1 , u) − α min 0 (W 2 )u(W 1 )} = E[{η min 0 (W 1 ) − α min 0 (W 2 )}u(W 1 )] = E([η min 0 (W 1 ) − E{α min 0 (W 2 ) | W 1 }]u(W 1 )) = 0.</formula><p>For the second result, write</p><formula xml:id="formula_78">E{∂ α ψ 0 (W )}(v) = E[v(W ){Y − γ 0 (W )}].</formula><p>In the case of nonparametric regression, γ 0 (w) = E(Y | W = w) and we appeal to law of iterated expectations. In the case of nonparametric projection, the desired result holds by orthogonality of the projection residual. In the case of nonparametric instrumental variable regression,  </p><formula xml:id="formula_79">E[v(W 2 ){Y − γ 0 (W 1 )}] = E(v(W 2 )[E(Y | W 2 ) − E{γ 0 (W 1 ) | W 2 }]) = 0.</formula><formula xml:id="formula_80">∆ 1ℓ = n 1/2 ℓ E ℓ {m(W, u) − α min 0 (W )u(W )}; ∆ 2ℓ = n 1/2 ℓ E ℓ [v(W ){Y − γ 0 (W )}]; ∆ 3ℓ = n 1/2 ℓ 2 E ℓ {−u(W )v(W )}.</formula><p>Proof. An exact Taylor expansion gives</p><formula xml:id="formula_81">ψ(w, θ 0 , γℓ , αℓ ) − ψ 0 (w) = {∂ γ ψ 0 (w)}(u) + {∂ α ψ 0 (w)}(v) + 1 2 {∂ 2 γ,α ψ 0 (w)}(u, v).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Averaging over observations in</head><formula xml:id="formula_82">I ℓ θℓ − θℓ = E ℓ {ψ(W, θ 0 , γℓ , αℓ )} − E ℓ {ψ 0 (W )} = E ℓ {∂ γ ψ 0 (W )}(u) + E ℓ {∂ α ψ 0 (W )}(v) + 1 2 E ℓ {∂ 2 γ,α ψ 0 (W )}(u, v).</formula><p>Finally appeal to Proposition D.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Residuals</head><p>Proposition D.4 (Residuals). Suppose Assumption 3.1 holds and</p><formula xml:id="formula_83">E[{Y − γ 0 (W )} 2 | W ] ≤ σ2 , α min 0 ∞ ≤ ᾱ.</formula><p>Then with probability 1 − ǫ/L,</p><formula xml:id="formula_84">|∆ 1ℓ | ≤ t 1 = 6L ǫ 1/2 ( Q + ᾱ2 ) 1/2 {R(γ ℓ )} q/2 ; |∆ 2ℓ | ≤ t 2 = 3L ǫ 1/2 σ{R(α ℓ )} 1/2 ; |∆ 3ℓ | ≤ t 3 = 3L 1/2 2ǫ {nR(γ ℓ )R(α ℓ )} 1/2 .</formula><p>Proof. We proceed in steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Markov inequality implies pr(|∆</head><formula xml:id="formula_85">1ℓ | &gt; t 1 ) ≤ E(∆ 2 1ℓ ) t 2 1 ; pr(|∆ 2ℓ | &gt; t 2 ) ≤ E(∆ 2 2ℓ ) t 2 2 ; pr(|∆ 3ℓ | &gt; t 3 ) ≤ E(|∆ 3ℓ |) t 3 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Law of iterated expectations implies</head><formula xml:id="formula_86">E(∆ 2 1ℓ ) = E{E(∆ 2 1ℓ | I c ℓ )}; E(∆ 2 2ℓ ) = E{E(∆ 2 2ℓ | I c ℓ )}; E(|∆ 3ℓ |) = E{E(|∆ 3ℓ | | I c ℓ )}.</formula><p>3. Bounding conditional moments.</p><p>Conditional on I c ℓ , (u, v) are nonrandom. Moreover, observations within fold I ℓ are independent and identically distributed. Hence by Proposition D.2 and assumption of finite</p><formula xml:id="formula_87">( Q, ᾱ) E(∆ 2 1ℓ | I c ℓ ) = E [n 1/2 ℓ E ℓ {m(W, u) − α min 0 (W )u(W )}] 2 | I c ℓ = E   n ℓ n 2 ℓ i,j∈I ℓ {m(W i , u) − α min 0 (W i )u(W i )}{m(W j , u) − α min 0 (W j )u(W j )} | I c ℓ   = n ℓ n 2 ℓ i,j∈I ℓ E {m(W i , u) − α min 0 (W i )u(W i )}{m(W j , u) − α min 0 (W j )u(W j )} | I c ℓ = n ℓ n 2 ℓ i∈I ℓ E {m(W i , u) − α min 0 (W i )u(W i )} 2 | I c ℓ = E[{m(W, u) − α min 0 (W )u(W )} 2 | I c ℓ ] ≤ 2E{m(W, u) 2 | I c ℓ } + 2E[{α min 0 (W )u(W )} 2 | I c ℓ ] ≤ 2( Q + ᾱ2 )R(γ ℓ ) q .</formula><p>Similarly by Proposition D.2 and assumption of finite σ</p><formula xml:id="formula_88">E(∆ 2 2ℓ | I c ℓ ) = E (n 1/2 ℓ E ℓ [v(W ){Y − γ 0 (W )}]) 2 | I c ℓ = E   n ℓ n 2 ℓ i,j∈I ℓ v(W i ){Y i − γ 0 (W i )}v(W j ){Y j − γ 0 (W j )} | I c ℓ   = n ℓ n 2 ℓ i,j∈I ℓ E [v(W i ){Y i − γ 0 (W i )}v(W j ){Y j − γ 0 (W j )} | I c ℓ ] = n ℓ n 2 ℓ i∈I ℓ E v(W i ) 2 {Y i − γ 0 (W i )} 2 | I c ℓ = E[v(W ) 2 {Y − γ 0 (W )} 2 | I c ℓ ] = E v(W ) 2 E[{Y − γ 0 (W )} 2 | W, I c ℓ ] | I c ℓ ≤ σ2 R(α ℓ ).</formula><p>Finally by Cauchy Schwarz inequality</p><formula xml:id="formula_89">E(|∆ 3ℓ | | I c ℓ ) = n 1/2 ℓ 2 E{| − u(W )v(W )| | I c ℓ } ≤ n 1/2 ℓ 2 [E{u(W ) 2 | I c ℓ }] 1/2 [E{v(W ) 2 | I c ℓ }] 1/2 = n 1/2 ℓ 2 {R(γ ℓ )} 1/2 {R(α ℓ )} 1/2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Collecting results gives pr(|∆</head><formula xml:id="formula_90">1ℓ | &gt; t 1 ) ≤ 2( Q + ᾱ2 )R(γ ℓ ) q t 2 1 = ǫ 3L ; pr(|∆ 2ℓ | &gt; t 2 ) ≤ σ2 R(α ℓ ) t 2 2 = ǫ 3L ; pr(|∆ 3ℓ | &gt; t 3 ) ≤ n 1/2 ℓ {R(γ ℓ )} 1/2 {R(α ℓ )} 1/2 2t 3 = ǫ 3L .</formula><p>Therefore with probability 1 − ǫ/L, the following inequalities hold:</p><formula xml:id="formula_91">|∆ 1ℓ | ≤ t 1 = 6L ǫ 1/2 ( Q + ᾱ2 ) 1/2 {R(γ ℓ )} q/2 ; |∆ 2ℓ | ≤ t 2 = 3L ǫ 1/2 σ{R(α ℓ )} 1/2 ; |∆ 3ℓ | ≤ t 3 = 3L 2ǫ n 1/2 ℓ {R(γ ℓ )} 1/2 {R(α ℓ )} 1/2 .</formula><p>Finally recall n ℓ = n/L.</p><p>Proposition D.5 (Residuals: Alternative path). Suppose Assumption 3.1 holds and</p><formula xml:id="formula_92">E[{Y − γ 0 (W )} 2 | W ] ≤ σ2 , α min 0 ∞ ≤ ᾱ, αℓ ∞ ≤ ᾱ′ .</formula><p>Then with probability 1 − ǫ/L,</p><formula xml:id="formula_93">|∆ 1ℓ | ≤ t 1 = 6L ǫ 1/2 ( Q + ᾱ2 ) 1/2 {R(γ ℓ )} q/2 ; |∆ 2ℓ | ≤ t 2 = 3L ǫ 1/2 σ{R(α ℓ )} 1/2 ; |∆ 3ℓ | ≤ t 3 = 3L 4ǫ 1/2 (ᾱ + ᾱ′ ){R(γ ℓ )} 1/2 + (4L) −1/2 [{nP(γ ℓ )R(α ℓ )} 1/2 ∧ {nR(γ ℓ )P(α ℓ )} 1/2 ].</formula><p>Proof. See Proposition D.4 for (t 1 , t 2 ). We focus on the alternative bound t 3 .</p><p>1. Decomposition.</p><formula xml:id="formula_94">Write 2∆ 3ℓ = n 1/2 ℓ E ℓ {−u(W )v(W )} = ∆ 3 ′ ℓ + ∆ 3 ′′ ℓ ,<label>where</label></formula><formula xml:id="formula_95">∆ 3 ′ ℓ = n 1/2 ℓ E ℓ [−u(W )v(W ) + E{u(W )v(W ) | I c ℓ }]; ∆ 3 ′′ ℓ = n 1/2 ℓ E{−u(W )v(W ) | I c ℓ }.</formula><p>2. Former term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>By Markov inequality pr(|∆</head><formula xml:id="formula_96">3 ′ ℓ | &gt; t) ≤ E(∆ 2 3 ′ ℓ ) t 2 .</formula><p>Law of iterated expectations implies</p><formula xml:id="formula_97">E(∆ 2 3 ′ ℓ ) = E{E(∆ 2 3 ′ ℓ | I c ℓ )}.</formula><p>We bound the conditional moment. Conditional on I c ℓ , (u, v) are nonrandom. Moreover, observations within fold I ℓ are independent and identically distributed. Since each summand in ∆ 3 ′ ℓ has conditional mean zero by construction, and since (ᾱ, ᾱ′ ) are finite by hypothesis,</p><formula xml:id="formula_98">E(∆ 2 3 ′ ℓ | I c ℓ ) = E n 1/2 ℓ E ℓ [−u(W )v(W ) + E{u(W )v(W ) | I c ℓ }] 2 | I c ℓ = E   n ℓ n 2 ℓ i,j∈I ℓ [−u(W i )v(W i ) + E{u(W i )v(W i ) | I c ℓ }][−u(W j )v(W j ) + E{u(W j )v(W j ) | I c ℓ }] | I c ℓ   = n ℓ n 2 ℓ i,j∈I ℓ E ([−u(W i )v(W i ) + E{u(W i )v(W i ) | I c ℓ }][−u(W j )v(W j ) + E{u(W j )v(W j ) | I c ℓ }] | I c ℓ ) = n ℓ n 2 ℓ i∈I ℓ E [−u(W i )v(W i ) + E{u(W i )v(W i ) | I c ℓ }] 2 | I c ℓ = E([u(W )v(W ) − E{u(W )v(W ) | I c ℓ }] 2 | I c ℓ ) ≤ E{u(W ) 2 v(W ) 2 | I c ℓ } ≤ (ᾱ + ᾱ′ ) 2 R(γ ℓ ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Collecting results gives pr(|∆</head><formula xml:id="formula_99">3 ′ ℓ | &gt; t) ≤ (ᾱ + ᾱ′ ) 2 R(γ ℓ ) t 2 = ǫ 3L .</formula><p>Therefore with probability 1 − ǫ/(3L),</p><formula xml:id="formula_100">|∆ 3 ′ ℓ | ≤ t = 3L ǫ 1/2 (ᾱ + ᾱ′ ){R(γ ℓ )} 1/2 .</formula><p>3. Latter term.</p><p>Specializing to nonparametric instrumental variable regression,</p><formula xml:id="formula_101">E{−u(W )v(W ) | I c ℓ } = E[E{−u(W 1 ) | W 2 , I c ℓ }v(W 2 ) | I c ℓ ] ≤ {E([E{u(W 1 ) | W 2 , I c ℓ }] 2 | I c ℓ )} 1/2 [E{v(W 2 ) 2 | I c ℓ }] 1/2 = {P(γ ℓ )} 1/2 {R(α ℓ )} 1/2 . Hence ∆ 3 ′′ ℓ ≤ n 1/2 ℓ {P(γ ℓ )} 1/2 {R(α ℓ )} 1/2 = L −1/2 {nP(γ ℓ )R(α ℓ )} 1/2 . Likewise E{−u(W )v(W ) | I c ℓ } = E[−u(W 1 )E{v(W 2 ) | W 1 , I c ℓ } | I c ℓ ] ≤ [E{u(W 1 ) 2 | I c ℓ }] 1/2 {E([E{v(W 2 ) | W 1 , I c ℓ }] 2 | I c ℓ )} 1/2 = {R(γ ℓ )} 1/2 {P(α ℓ )} 1/2 . Hence ∆ 3 ′′ ℓ ≤ n 1/2 ℓ {R(γ ℓ )} 1/2 {P(α ℓ )} 1/2 = L −1/2 {nR(γ ℓ )P(α ℓ )} 1/2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Combining terms.</head><p>With probability 1 − ǫ/(3L), </p><formula xml:id="formula_102">|∆ 3 | ≤ t 3 = 3L 4ǫ 1/2 (ᾱ + ᾱ′ ){R(γ ℓ )} 1/2 + (4L) −1/2 [{nP(γ ℓ )R(α ℓ )} 1/2 ∧ {nR(γ ℓ )P(α ℓ )} 1/2 ].</formula><formula xml:id="formula_103">n 1/2 σ | θ − θ| ≤ ∆ = 3L ǫσ ( Q1/2 + ᾱ){R(γ ℓ )} q/2 + σ{R(α ℓ )} 1/2 + {nR(γ ℓ )R(α ℓ )} 1/2 .</formula><p>Proof. We proceed in steps.</p><p>1. Decomposition.</p><p>By Proposition D.3, write</p><formula xml:id="formula_104">n 1/2 ( θ − θ) = n 1/2 n 1/2 ℓ 1 L L ℓ=1 n 1/2 ℓ ( θℓ − θℓ ) = L 1/2 1 L L ℓ=1 3 j=1 ∆ jℓ .</formula><p>2. Union bound.</p><p>Define the events </p><formula xml:id="formula_105">E ℓ = {for all j (j = 1, 2, 3), |∆ jℓ | ≤ t j }, E = ∩ L ℓ=1 E ℓ , E c = ∪ L ℓ=1 E c ℓ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Collecting results.</head><p>Therefore with probability 1 − ǫ,</p><formula xml:id="formula_106">n 1/2 | θ − θ| ≤ L 1/2 1 L L ℓ=1 3 j=1 |∆ jk | ≤ L 1/2 1 L L ℓ=1 3 j=1 t j = L 1/2 3 j=1 t j .</formula><p>Finally, we simplify (t j ). For a, b &gt; 0, (a + b) 1/2 ≤ a 1/2 + b 1/2 . Moreover, 3 &gt; 6 1/2 &gt; 3 1/2 &gt; 3/2. Finally, for ǫ ≤ 1, ǫ −1/2 ≤ ǫ −1 . In summary</p><formula xml:id="formula_107">t 1 = 6L ǫ 1/2 ( Q + ᾱ2 ) 1/2 {R(γ ℓ )} q/2 ≤ 3L 1/2 ǫ ( Q1/2 + ᾱ){R(γ ℓ )} q/2 ; t 2 = 3L ǫ 1/2 σ{R(α ℓ )} 1/2 ≤ 3L 1/2 ǫ σ{R(α ℓ )} 1/2 ; t 3 = 3L 1/2 2ǫ {nR(γ ℓ )R(α ℓ )} 1/2 ≤ 3L 1/2 ǫ {nR(γ ℓ )R(α ℓ )} 1/2 .</formula><p>Proposition D.7 (Oracle approximation: Alternative path). Suppose the conditions of Proposition D.5 hold. Then with probability</p><formula xml:id="formula_108">1 − ǫ n 1/2 σ | θ − θ| ≤ ∆ = 4L ǫ 1/2 σ ( Q1/2 + ᾱ + ᾱ′ ){R(γ ℓ )} q/2 + σ{R(α ℓ )} 1/2 + 1 2L 1/2 σ [{nP(γ ℓ )R(α ℓ )} 1/2 ∧ {nR(γ ℓ )P(α ℓ )} 1/2 ].</formula><p>Proof. As in Proposition D.6, Propositions D.3 and D.5 imply that with probability</p><formula xml:id="formula_109">1 − ǫ n 1/2 | θ − θ| ≤ L 1/2 3 j=1 t j .</formula><p>Finally, we simplify (t j ). For a, b &gt; 0, (a + b) 1/2 ≤ a 1/2 + b 1/2 . In summary,</p><formula xml:id="formula_110">t 1 = 6L ǫ 1/2 ( Q + ᾱ2 ) 1/2 {R(γ ℓ )} q/2 ≤ 6L ǫ 1/2 ( Q1/2 + ᾱ){R(γ ℓ )} q/2 ; t 2 = 3L ǫ 1/2 σ{R(α ℓ )} 1/2 ; t 3 = 3L 4ǫ 1/2 (ᾱ + ᾱ′ ){R(γ ℓ )} 1/2 + (4L) −1/2 [{nP(γ ℓ )R(α ℓ )} 1/2 ∧ {nR(γ ℓ )P(α ℓ )} 1/2 ].</formula><p>Finally note 6 1/2 + (3/4) 1/2 ≤ 4 when combining terms from t 1 and t 3 .</p><p>Lemma D.1 (Berry Esseen Theorem <ref type="bibr" target="#b36">[37]</ref>). Suppose (Z i ) (i = 1, ..., n) are independent and identically distributed random variables with</p><formula xml:id="formula_111">E(Z i ) = 0, E(Z 2 i ) = σ 2 , and E(|Z i | 3 ) = ξ 3 . Then sup z∈R pr n 1/2 σ E n (Z i ) ≤ z − Φ(z) ≤ c BE ξ σ 3 n − 1 2 ,</formula><p>where c BE = 0.4748 and Φ(z) is the standard Gaussian cumulative distribution function.</p><p>of Theorem 5.1. Fix z in R. First, we show that</p><formula xml:id="formula_112">pr n 1/2 σ ( θ − θ 0 ) ≤ z − Φ(z) ≤ c BE ξ σ 3 n − 1 2 + ∆ (2π) 1/2 + ǫ,</formula><p>where ∆ is defined in Propositions D.6 and D.7. We proceed in steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">High probability bound.</head><p>By Propositions D.6 and D.7, with probability 1 − ǫ,</p><formula xml:id="formula_113">n 1/2 σ ( θ − θ) ≤ n 1/2 σ | θ − θ| ≤ ∆. Observe that pr n 1/2 σ ( θ − θ 0 ) ≤ z = pr n 1/2 σ ( θ − θ 0 ) ≤ z + n 1/2 σ ( θ − θ) ≤ pr n 1/2 σ ( θ − θ 0 ) ≤ z + ∆ + ǫ.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Mean value theorem.</head><p>Let φ(z) be the standard Gaussian probability density function. There exists some z ′ such that</p><formula xml:id="formula_114">Φ(z + ∆) − Φ(z) = φ(z ′ )∆ ≤ ∆ √ 2π<label>.</label></formula><p>3. Berry Esseen theorem.</p><p>Observe that</p><formula xml:id="formula_115">θ − θ 0 = E n [m(W, γ 0 ) + α min 0 (W ){Y − γ 0 (W )}] − θ 0 = E n [ψ 0 (W )]. Therefore taking Z i = ψ 0 (W i ) in Lemma D.1, sup z ′′ pr n 1/2 σ ( θ − θ 0 ) ≤ z ′′ − Φ(z ′′ ) ≤ c BE ξ σ 3 n − 1 2 .</formula><p>Hence by the high probability bound and mean value theorem steps above, taking</p><formula xml:id="formula_116">z ′′ = z + ∆ pr n 1/2 σ ( θ − θ 0 ) ≤ z − Φ(z) ≤ pr n 1/2 σ ( θ − θ 0 ) ≤ z + ∆ − Φ(z) + ǫ = pr n 1/2 σ ( θ − θ 0 ) ≤ z + ∆ − Φ(z + ∆) + Φ(z + ∆) − Φ(z) + ǫ ≤ c BE ξ σ 3 n − 1 2 + ∆ √ 2π + ǫ.</formula><p>Next, we show that</p><formula xml:id="formula_117">Φ(z) − pr n 1/2 σ ( θ − θ 0 ) ≤ z ≤ c BE ξ σ 3 n − 1 2 + ∆ (2π) 1/2 + ǫ.</formula><p>where ∆ is defined in Propositions D.6 and D.7. We proceed in steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">High probability bound.</head><p>By Propositions D.6 and D.7, with probability 1 − ǫ,</p><formula xml:id="formula_118">n 1/2 σ ( θ − θ) ≤ n 1/2 σ | θ − θ| ≤ ∆, hence z − ∆ ≤ z − n 1/2 σ ( θ − θ). Observe that pr n 1/2 σ ( θ − θ 0 ) ≤ z − ∆ ≤ pr n 1/2 σ ( θ − θ 0 ) ≤ z − n 1/2 σ ( θ − θ) + ǫ = pr n 1/2 σ ( θ − θ 0 ) ≤ z + ǫ.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Mean value theorem.</head><p>There exists some z ′ such that</p><formula xml:id="formula_119">Φ(z) − Φ(z − ∆) = φ(z ′ )∆ ≤ ∆ √ 2π<label>.</label></formula><p>3. Berry Esseen theorem.</p><p>As argued above,</p><formula xml:id="formula_120">sup z ′′ pr n 1/2 σ ( θ − θ 0 ) ≤ z ′′ − Φ(z ′′ ) ≤ c BE ξ σ 3 n − 1 2 .</formula><p>Hence by the mean value theorem and high probability bound steps above, taking</p><formula xml:id="formula_121">z ′′ = z − ∆ Φ(z) − pr n 1/2 σ ( θ − θ 0 ) ≤ z ≤ Φ(z) − pr n 1/2 σ ( θ − θ 0 ) ≤ z − ∆ + ǫ = Φ(z) − Φ(z − ∆) + Φ(z − ∆) − pr n 1/2 σ ( θ − θ 0 ) ≤ z − ∆ + ǫ ≤ ∆ √ 2π + c BE ξ σ 3 n − 1 2 + ǫ.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Variance estimation</head><p>Recall that E ℓ (•) = n −1 ℓ i∈I ℓ (•) means the average over observations in I ℓ and E n (•) = n −1 n i=1 (•) means the average over all observations in the sample. Definition D.4 (Shorter notation). For i in I ℓ , define</p><formula xml:id="formula_122">ψ 0 (W i ) = ψ(W i , θ 0 , γ 0 , α min 0 ); ψ(W i ) = ψ(W i , θ, γℓ , αℓ ). Proposition D.8 (Foldwise second moment). E ℓ [{ ψ(W ) − ψ 0 (W )} 2 ] ≤ 4    ( θ − θ 0 ) 2 + 6 j=4 ∆ jℓ    ,<label>where</label></formula><formula xml:id="formula_123">∆ 4ℓ = E ℓ {m(W, u) 2 }; ∆ 5ℓ = E ℓ [{ αℓ (W )u(W )} 2 ]; ∆ 6ℓ = E ℓ [v(W ) 2 {Y − γ 0 (W )} 2 ]. Proof. Write ψ(W i ) − ψ 0 (W i ) = m(W i , γℓ ) + αℓ (W i ){Y i − γℓ (W i )} − θ − m(W i , γ 0 ) + α min 0 (W i ){Y i − γ 0 (W i )} − θ 0 ± αℓ {Y − γ 0 (W i )} = (θ 0 − θ) + m(W i , u) − αℓ (W i )u(W i ) + v(W i ){Y − γ 0 (W i )}. Hence { ψ(W i )−ψ 0 (W i )} 2 ≤ 4 (θ 0 − θ) 2 + m(W i , u) 2 + { αℓ (W i )u(W i )} 2 + v(W i ) 2 {Y − γ 0 (W i )} 2 .</formula><p>Finally take E ℓ (•) of both sides. Proposition D.9 (Residuals). Suppose Assumption 3.1 holds and</p><formula xml:id="formula_124">E[{Y − γ 0 (W )} | W ] 2 ≤ σ2 , αℓ ∞ ≤ ᾱ′ .</formula><p>Then with probability 1 − ǫ ′ /(2L), 2. Law of iterated expectations implies</p><formula xml:id="formula_125">∆ 4ℓ ≤ t 4 = 6L ǫ ′ QR(γ ℓ ) q ; ∆ 5ℓ ≤ t 5 = 6L ǫ ′ (ᾱ ′ ) 2 R(γ ℓ ); ∆ 6ℓ ≤ t 6 = 6L ǫ ′ σ2 R(α ℓ ).</formula><formula xml:id="formula_126">E(|∆ 4ℓ |) = E{E(|∆ 4ℓ | | I c ℓ )}; E(|∆ 5ℓ |) = E{E(|∆ 5ℓ | | I c ℓ )}; E(|∆ 6ℓ |) = E{E(|∆ 6ℓ | | I c ℓ )}.</formula><p>3. Bounding conditional moments.</p><p>Conditional on I c ℓ , (u, v) are nonrandom. Moreover, observations within fold I ℓ are independent and identically distributed. Hence by assumption of finite ( Q, σ, ᾱ′ )</p><formula xml:id="formula_127">E(|∆ 4ℓ | | I c ℓ ) = E(∆ 4ℓ | I c ℓ ) = E[{m(W, u)} 2 | I c ℓ ] ≤ QR(γ ℓ ) q . Similarly E(|∆ 5ℓ | | I c ℓ ) = E(∆ 5ℓ | I c ℓ ) = E[{ αℓ (W )u(W )} 2 | I c ℓ ] ≤ (ᾱ ′ ) 2 R(γ ℓ ). Finally E(|∆ 6ℓ | | I c ℓ ) = E(∆ 6ℓ | I c ℓ ) = E[v(W ) 2 {Y − γ 0 (W )} 2 | I c ℓ ] = E{v(W ) 2 E[{Y − γ 0 (W )} 2 | W, I c ℓ ] | I c ℓ } ≤ σ2 R(α ℓ ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Collecting results gives pr(|∆</head><formula xml:id="formula_128">4ℓ | &gt; t 4 ) ≤ QR(γ ℓ ) q t 4 = ǫ ′ 6L pr(|∆ 5ℓ | &gt; t 5 ) ≤ (ᾱ ′ ) 2 R(γ ℓ ) t 5 = ǫ ′ 6L pr(|∆ 6ℓ | &gt; t 6 ) ≤ σ2 R(α ℓ ) t 6 = ǫ ′ 6L</formula><p>Therefore with probability 1 − ǫ ′ /(2L), the following inequalities hold:</p><formula xml:id="formula_129">|∆ 4ℓ | ≤ t 4 = 6L ǫ ′ QR(γ ℓ ) q ; |∆ 5ℓ | ≤ t 5 = 6L ǫ ′ (ᾱ ′ ) 2 R(γ ℓ ); |∆ 6ℓ | ≤ t 6 = 6L ǫ ′ σ2 R(α ℓ ).</formula><p>Proposition D.10 (Oracle approximation). Suppose the conditions of Proposition D.9 hold. Then with probability 1 − ǫ ′ /2</p><p>E n [{ ψ(W ) − ψ 0 (W )} 2 ] ≤ ∆ ′ = 4( θ − θ 0 ) 2 + 24L ǫ ′ { Q + (ᾱ ′ ) 2 }R(γ ℓ ) q + σ2 R(α ℓ ) . </p><formula xml:id="formula_130">E n {ψ 0 (W ) 2 } 1/2 ≤ E n [{ ψ(W ) − ψ 0 (W )} 2 ] 1/2 |E n {ψ 0 (W ) 2 } − σ 2 | + σ 2 1/2 ≤ E n [{ ψ(W ) − ψ 0 (W )} 2 ] 1/2 |E n {ψ 0 (W ) 2 } − σ 2 |</formula><p>1/2 + σ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">High probability events.</head><p>From the previous step, we see that to control |σ pr,2 ). As in the proof of Lemma B.2, ψ 0 pr,q ≤ U 1 pr,q + [E{α min,h 0 (W ) q E(U q 2 | W )}] 1/q ≤ c( ℓ pr,q + α min,h 0 pr,q ).</p><p>Next we characterize α min,h 0 pr,q in terms of ℓ pr,q . Since α min,h 0 (w) = ℓ h (w j )α min 0 , α ℓ pr,q ≤ α min,h 0 pr,q ≤ α ℓ pr,q , α min,h 0 pr,2 = M. In summary, cα ℓ pr,2 ≤ σ ≤ c 1 + α2 ℓ pr,2 , α ℓ pr,2 ≤ M ≤ α ℓ pr,2 , ψ 0 pr,q ≤ c(1+ α) ℓ pr,q .</p><p>2. Taylor expansion.</p><p>Consider the change of variables u = (v ′ − v)/h so that du = h −1 dv ′ . Hence ℓ q pr,q ω q = ℓω q pr,q</p><formula xml:id="formula_131">= h −1 K v − v ′ h q pr,q = h −q K v ′ − v h q f V (v ′ )dv ′ = h −(q−1) |K(u)| q f V (v − uh)du.</formula><p>It follows that h −(q−1)/q f 1/q |K| q 1/q ≤ ℓ pr,q ω ≤ h −(q−1)/q f 1/q |K| q 1/q</p><p>.</p><p>Further, we have that</p><formula xml:id="formula_132">ω = h −1 K v ′ − v h f V (v ′ )dv ′ = K(u)f V (v − uh)du.</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Example 3 . 2 (</head><label>32</label><figDesc>Regression discontinuity design estimated by random forest). Let Y be an educational outcome. Let W = (D, X) concatenate test score variable D and covariates X. Let γ 0 (d, x) = E(Y | D = d, X = x) be a function estimated by a random forest. Suppose the cutoff for a scholarship is the test score D = 0. The regression discontinuity design parameter is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>where η min 0</head><label>0</label><figDesc>is the minimal Riesz representer satisfying E{m(W 1 , γ)} = E{η 0 (W 1 )γ(W 1 )} for all γ in Γ from Lemma 3.1. Uniqueness is due to the assumption of exact identification, which amounts to completeness when Γ = L 2 . In Example 3.3, w 1 = (d, x), w 2 = (z, x), and η 0 (d, x) = −∂ d log f (d | x) where f (d | x) is a conditional density. This abuse of notation allows us to state unified results. The estimating equation is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 4 . 1 (</head><label>41</label><figDesc>Debiased machine learning). Given a sample (Y i , W i ) (i = 1, ..., n), partition the sample into folds (I ℓ ) (ℓ = 1, ..., L). Denote by I c ℓ the complement of I ℓ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3. Example 3 . 3 .</head><label>33</label><figDesc>Denote the density f (d, x). If f (d | x) vanishes for each d in the boundary of the support of D given X = x almost everywhere then η 0 (d, x) = −∂ d log f (d | x). Hence the minimal representer η min 0 exists if −∂ d log f (d | x) is bounded above. Subsequently, α min 0 is the solution α to η min 0 (d, x) = E{α(X, Z) | D = d, X = x}. 4. Example B.1. Denote the density f (d, v, x). If f (d | v, x) vanishes for each d in the boundary of the support of D given (V, X) = (v, x) almost everywhere then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>3 and B.1, we prove the following technical lemma. Lemma B.3 (A weak reverse Poincare inequality). Assume that f (d | x) vanishes for each d in the boundary of the support of D given X = x almost everywhere. Next assume the following restrictions on Γ ⊂ L 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Therefore a∞</head><label></label><figDesc>Sobolev type property with respect to the first argument is a sufficient condition in Examples 3.3 and B.1. Next we examine the assumption of α min 0 ≤ ᾱ, which depends on the functional of interest. Lemma B.5 (Bounded Riesz representer for global functionals). The following conditions are sufficient for ᾱ &lt; ∞ in Examples 3.1, 3.2, 3.3, and B.1 with ℓ h ≤ C &lt; ∞ and Γ = L 2 . 1. Example 3.1. π 0 (v, x) is bounded away from zero and one. 2. Example 3.2. The bandwidth is fixed. 3. Example 3.3. α min 0 that solves −∂ d log f (d | x) = E{α(X, Z) | D = d, X = x} is bounded above. 4. Example B.1. −∂ d log f (d | v, x) is bounded above.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Corollary B. 1 (</head><label>1</label><figDesc>Confidence interval for local functionals). Suppose the conditions of Corollary 5.1 and Lemmas B.6, B.7, B.8, B.9, and B.10 hold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>D. 2</head><label>2</label><figDesc>Taylor expansion Train (γ ℓ , αℓ ) on observations in I c ℓ . Let n ℓ = |I ℓ | = n/L be the number of observations in I ℓ . Denote by E ℓ (•) = n −1 ℓ i∈I ℓ (•) the average over observations in I ℓ . Denote by E n (•) = n −1 n i=1 (•) the average over all observations in the sample. Definition D.2 (Foldwise target and oracle).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>2 ℓ</head><label>2</label><figDesc>θℓ = E ℓ [m(W, γℓ ) + αℓ (W ){Y − γℓ (W )}]; θℓ = E ℓ [m(W, γ 0 ) + α min 0 (W ){Y − γ 0 (W )}].Proposition D.3 (Taylor expansion). Let u = γℓ − γ 0 and v = αℓ − α min 0 . Then n 1/( θℓ − θℓ ) = 3 j=1 ∆ jℓ where</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>D. 4 6 (</head><label>46</label><figDesc>Main argument Definition D.3 (Overall target and oracle). Oracle approximation). Suppose the conditions of Proposition D.4 hold. Then with probability 1 − ǫ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Hence by the union bound and Proposition D.4, pr(E c ) ≤</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>8 E 2 . 2 . 3 . 11 (′ 1 /2 ζ 2 n 1 / 2 .ζ 2 n 1 / 2 .of Theorem 5 . 3 . 1 . 2 .</head><label>82231112122125312</label><figDesc>n [{ ψ(W ) − ψ 0 (W )} 2 ] = 1 L L ℓ=1 E ℓ [{ ψ(W ) − ψ 0 (W )} 2 ] ≤ 4( θ − θ 0 ) Union bound.Define the eventsE ′ ℓ = {for all j (j = 4, 5, 6), |∆ jℓ | ≤ t j }, E ′ = ∩ L ℓ=1 E ′ ℓ , (E ′ ) c = ∪ L ℓ=1 (E ′ ℓ ) c .Hence by the union bound and Proposition D.9,pr{(E ′ ) c } ≤ L ℓ=1 pr{(E ′ ℓ ) c } ≤ L ǫ ′ 2L = ǫ ′ Collecting results. Therefore with probability 1 − ǫ ′ /2, E n [{ ψ(W ) − ψ 0 (W )} 2 ] ≤ 4( θ − θ 0 ) Markov inequality). Recall σ 2 = E{ψ 0 (W ) 2 } and ζ 4 = E{ψ 0 (W ) 4 }. Suppose ζ &lt; ∞. Then with probability 1 − ǫ ′ /2 |E n {ψ 0 (W ) 2 } − σ 2 | ≤ ∆ ′′ = 2 ǫ Proof. Let A = ψ 0 (W ) 2 , Ā = E n (A).Observe thatE( Ā) = E(A) = E{ψ 0 (W ) 2 } = σ 2 , var( n {ψ 0 (W ) 2 } − σ 2 | &gt; t] = pr{| Ā − E( Ā)| &gt; t} ≤ var( We proceed in steps. Decomposition of variance estimator. Write σ2 = E n { ψ(W ) 2 } = E n [{ ψ(W ) − ψ 0 (W ) + ψ 0 (W )} 2 ] = E n [{ ψ(W ) − ψ 0 (W )} 2 ] + 2E n [{ ψ(W ) − ψ 0 (W )}ψ 0 (W )] + E n {ψ 0 (W ) 2 }. Hence σ2 − E n {ψ 0 (W ) 2 } = E n [{ ψ(W ) − ψ 0 (W )} 2 ] + 2E n [{ ψ(W ) − ψ 0 (W )}ψ 0 (W )]. Decomposition of difference. Next write σ2 − σ 2 = [σ 2 − E n {ψ 0 (W ) 2 }] + [E n {ψ 0 (W ) 2 } − σ 2 ]. Focusing on the former term σ2 − E n {ψ 0 (W ) 2 } = E n [{ ψ(W ) − ψ 0 (W )} 2 ] + 2E n [{ ψ(W ) − ψ 0 (W )}ψ 0 (W )]. Moreover E n [{ ψ(W ) − ψ 0 (W )}ψ 0 (W )] ≤ E n [{ ψ(W ) − ψ 0 (W )} 2 ] 1/2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>2 − σ 2 | 1 .E. 3 1 . 2 pr, 2 ≤(W ) 2 E(U 2 2 |pr, 2 ≤ σ 2 ≤</head><label>22131222222</label><figDesc>, it is sufficient to control two expressions: E n [{ ψ(W ) − ψ 0 (W )} 2 ] and |E n {ψ 0 (W )} 2 − σ 2 |. These are controlled in Propositions D.10 and D.11, respectively. Therefore with probability1 − ǫ ′ , |σ 2 − σ 2 | ≤ ∆ ′ + 2(∆ ′ ) 1/2 {(∆ ′′ )1/2 + σ} + ∆ ′′ . D.6 Corollary of Corollary 5.1. Immediately from ∆ = o p (1) in Theorem 5.1,θ = θ 0 + o p (1), σ −1 n 1/2 ( θ − θ 0 ) N (0, 1), pr θ 0 in θ ± σ n 1/2 → 1 − a.For the final result, it is sufficient that σ2 = σ 2 + o p (1), which follows from ∆ ′ = o p (1) and∆ ′′ = o p (1) in Theorem 5.3. As the operator norm of γ → E{m(W, γ)}, M = inf[c ≥ 0 : |E{m(W, γ)}| ≤ c for all γ in Γ such that γ pr,2 = 1].By Jensen's inequality and Assumption 3.1,|E{m(W, γ)}| ≤ [E{m(W, γ) 2 }] 1/2 ≤ Q[E{γ(W ) 2 }] q 1/2 = Q1/2 γ q pr,2 .Taking the supremum of both sides over γ in Γ such that γ pr,2 = 1, we conclude that M ≤ Q1/2 &lt; ∞. The rest of the claim is shown in[16, Lemma 2.1]. of Lemma B.1. For Examples 3.1 and 3.2, the result is immediate from standard propensity score and regression arguments. For Example 3.3. the result follows from [23, Proposition 3 and Example 5]. For Example B.1, the result follows from integration by parts. For Example 3.2, writeE[{ℓ + h (D)γ 0 (D, X) − ℓ − h (D)γ 0 (D, X)} 2 ] ≤ 2E[{ℓ + h (D)γ 0 (D, X)} 2 ] + 2E[{ℓ − h (D)γ 0 (D, X)} 2 ].Invoking the bounded weighting assumption,E[{ℓ + h (D)γ 0 (D, X)} 2 ] ≤ CE{γ 0 (D, X)} 2 ; E[{ℓ − h (D)γ 0 (D, X)} 2 ] ≤ CE{γ 0 (D, X)} 2 .ForExamples 3.3 and B.1, appeal to Lemma B.3. of Lemma B.5. The result is immediate from Lemma B.1. Local functionals of Lemma B.6. We extend [16, Lemma 3.4]. We proceed in steps. Moment bounds. As in the of Lemma B.2, σ 2 = E{E(U 2 1 | W )} + E{α min,h 0 (W ) 2 E(U 2 2 | W )}. Note that 0 ≤ E{E(U 2 1 | W )} ≤ c2 ℓ 2 pr,2 , and c2 α min,h 0 E{α min,h 0 W )} ≤ c2 α min,c2 ( ℓ 2 pr,2 + α min,h 0 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Low dimensional coverage simulation with neural network v CATE(v) Tuning Ave. Est. Ave. S.E. 80% Cov. 95% Cov.</figDesc><table><row><cell>-0.25</cell><cell>-0.10</cell><cell>0.25</cell><cell>-0.10</cell><cell>0.05</cell><cell>83%</cell><cell>94%</cell></row><row><cell>-0.25</cell><cell>-0.10</cell><cell>0.50</cell><cell>-0.10</cell><cell>0.04</cell><cell>85%</cell><cell>95%</cell></row><row><cell>-0.25</cell><cell>-0.10</cell><cell>1.00</cell><cell>-0.08</cell><cell>0.03</cell><cell>71%</cell><cell>88%</cell></row><row><cell>0.00</cell><cell>0.00</cell><cell>0.25</cell><cell>0.00</cell><cell>0.04</cell><cell>78%</cell><cell>95%</cell></row><row><cell>0.00</cell><cell>0.00</cell><cell>0.50</cell><cell>0.00</cell><cell>0.03</cell><cell>78%</cell><cell>94%</cell></row><row><cell>0.00</cell><cell>0.00</cell><cell>1.00</cell><cell>0.02</cell><cell>0.02</cell><cell>62%</cell><cell>85%</cell></row><row><cell>0.25</cell><cell>0.32</cell><cell>0.25</cell><cell>0.31</cell><cell>0.12</cell><cell>85%</cell><cell>92%</cell></row><row><cell>0.25</cell><cell>0.32</cell><cell>0.50</cell><cell>0.30</cell><cell>0.09</cell><cell>85%</cell><cell>93%</cell></row><row><cell>0.25</cell><cell>0.32</cell><cell>1.00</cell><cell>0.28</cell><cell>0.06</cell><cell>76%</cell><cell>88%</cell></row><row><cell cols="7">Ave., average; Est., estimate; S.E., standard error; Cov., coverage. The largest standard</cell></row><row><cell cols="7">error for the results in column 6 is 2%. The largest standard error for the results in</cell></row><row><cell cols="2">column 7 is 2%.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Low dimensional coverage simulation with random forest v CATE(v) Tuning Ave. Est. Ave. S.E. 80% Cov. 95% Cov.</figDesc><table><row><cell>-0.25</cell><cell>-0.10</cell><cell>0.25</cell><cell>-0.10</cell><cell>0.05</cell><cell>86%</cell><cell>93%</cell></row><row><cell>-0.25</cell><cell>-0.10</cell><cell>0.50</cell><cell>-0.09</cell><cell>0.03</cell><cell>83%</cell><cell>94%</cell></row><row><cell>-0.25</cell><cell>-0.10</cell><cell>1.00</cell><cell>-0.08</cell><cell>0.02</cell><cell>60%</cell><cell>79%</cell></row><row><cell>0.00</cell><cell>0.00</cell><cell>0.25</cell><cell>0.00</cell><cell>0.02</cell><cell>70%</cell><cell>91%</cell></row><row><cell>0.00</cell><cell>0.00</cell><cell>0.50</cell><cell>0.01</cell><cell>0.02</cell><cell>72%</cell><cell>91%</cell></row><row><cell>0.00</cell><cell>0.00</cell><cell>1.00</cell><cell>0.02</cell><cell>0.02</cell><cell>48%</cell><cell>75%</cell></row><row><cell>0.25</cell><cell>0.32</cell><cell>0.25</cell><cell>0.30</cell><cell>0.12</cell><cell>83%</cell><cell>91%</cell></row><row><cell>0.25</cell><cell>0.32</cell><cell>0.50</cell><cell>0.29</cell><cell>0.08</cell><cell>82%</cell><cell>91%</cell></row><row><cell>0.25</cell><cell>0.32</cell><cell>1.00</cell><cell>0.28</cell><cell>0.06</cell><cell>71%</cell><cell>86%</cell></row></table><note>Ave., average; Est., estimate; S.E., standard error; Cov., coverage. The largest standard error for the results in column 6 is 2%. The largest standard error for the results in column 7 is 2%.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Low dimensional coverage simulation with lasso v CATE(v) Tuning Ave. Est. Ave. S.E. 80% Cov. 95% Cov.</figDesc><table><row><cell>-0.25</cell><cell>-0.10</cell><cell>0.25</cell><cell>-0.08</cell><cell>0.08</cell><cell>81%</cell><cell>95%</cell></row><row><cell>-0.25</cell><cell>-0.10</cell><cell>0.50</cell><cell>-0.08</cell><cell>0.05</cell><cell>81%</cell><cell>95%</cell></row><row><cell>-0.25</cell><cell>-0.10</cell><cell>1.00</cell><cell>-0.06</cell><cell>0.04</cell><cell>63%</cell><cell>88%</cell></row><row><cell>0.00</cell><cell>0.00</cell><cell>0.25</cell><cell>0.00</cell><cell>0.06</cell><cell>79%</cell><cell>94%</cell></row><row><cell>0.00</cell><cell>0.00</cell><cell>0.50</cell><cell>0.01</cell><cell>0.04</cell><cell>83%</cell><cell>96%</cell></row><row><cell>0.00</cell><cell>0.00</cell><cell>1.00</cell><cell>0.02</cell><cell>0.03</cell><cell>73%</cell><cell>92%</cell></row><row><cell>0.25</cell><cell>0.32</cell><cell>0.25</cell><cell>0.30</cell><cell>0.11</cell><cell>86%</cell><cell>94%</cell></row><row><cell>0.25</cell><cell>0.32</cell><cell>0.50</cell><cell>0.29</cell><cell>0.08</cell><cell>85%</cell><cell>95%</cell></row><row><cell>0.25</cell><cell>0.32</cell><cell>1.00</cell><cell>0.28</cell><cell>0.06</cell><cell>71%</cell><cell>89%</cell></row><row><cell cols="7">Ave., average; Est., estimate; S.E., standard error; Cov., coverage. The largest standard</cell></row><row><cell cols="7">error for the results in column 6 is 2%. The largest standard error for the results in</cell></row><row><cell cols="2">column 7 is 1%.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>High dimensional coverage simulation with neural network v CATE(v) Tuning Ave. Est. Ave. S.E. 80% Cov. 95% Cov. Est., estimate; S.E., standard error; Cov., coverage. The largest standard error for the results in column 6 is 2%. The largest standard error for the results in column 7 is 2%.</figDesc><table><row><cell>-0.25</cell><cell>-0.10</cell><cell>0.25</cell><cell>-0.09</cell><cell>0.04</cell><cell>84%</cell><cell>91%</cell></row><row><cell>-0.25</cell><cell>-0.10</cell><cell>0.50</cell><cell>-0.09</cell><cell>0.03</cell><cell>78%</cell><cell>91%</cell></row><row><cell>-0.25</cell><cell>-0.10</cell><cell>1.00</cell><cell>-0.07</cell><cell>0.02</cell><cell>49%</cell><cell>74%</cell></row><row><cell>0.00</cell><cell>0.00</cell><cell>0.25</cell><cell>0.00</cell><cell>0.03</cell><cell>75%</cell><cell>95%</cell></row><row><cell>0.00</cell><cell>0.00</cell><cell>0.50</cell><cell>0.01</cell><cell>0.02</cell><cell>74%</cell><cell>91%</cell></row><row><cell>0.00</cell><cell>0.00</cell><cell>1.00</cell><cell>0.04</cell><cell>0.03</cell><cell>52%</cell><cell>75%</cell></row><row><cell>0.25</cell><cell>0.32</cell><cell>0.25</cell><cell>0.39</cell><cell>0.20</cell><cell>90%</cell><cell>97%</cell></row><row><cell>0.25</cell><cell>0.32</cell><cell>0.50</cell><cell>0.39</cell><cell>0.15</cell><cell>88%</cell><cell>97%</cell></row><row><cell>0.25</cell><cell>0.32</cell><cell>1.00</cell><cell>0.38</cell><cell>0.13</cell><cell>81%</cell><cell>95%</cell></row><row><cell cols="2">Ave., average;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>High dimensional coverage simulation with random forest v CATE(v) Tuning Ave. Est. Ave. S.E. 80% Cov. 95% Cov.</figDesc><table><row><cell>-0.25</cell><cell>-0.10</cell><cell>0.25</cell><cell>-0.09</cell><cell>0.05</cell><cell>81%</cell><cell>91%</cell></row><row><cell>-0.25</cell><cell>-0.10</cell><cell>0.50</cell><cell>-0.09</cell><cell>0.03</cell><cell>78%</cell><cell>91%</cell></row><row><cell>-0.25</cell><cell>-0.10</cell><cell>1.00</cell><cell>-0.07</cell><cell>0.02</cell><cell>53%</cell><cell>75%</cell></row><row><cell>0.00</cell><cell>0.00</cell><cell>0.25</cell><cell>0.00</cell><cell>0.02</cell><cell>76%</cell><cell>94%</cell></row><row><cell>0.00</cell><cell>0.00</cell><cell>0.50</cell><cell>0.01</cell><cell>0.02</cell><cell>74%</cell><cell>92%</cell></row><row><cell>0.00</cell><cell>0.00</cell><cell>1.00</cell><cell>0.04</cell><cell>0.03</cell><cell>44%</cell><cell>71%</cell></row><row><cell>0.25</cell><cell>0.32</cell><cell>0.25</cell><cell>0.37</cell><cell>0.18</cell><cell>91%</cell><cell>96%</cell></row><row><cell>0.25</cell><cell>0.32</cell><cell>0.50</cell><cell>0.40</cell><cell>0.16</cell><cell>88%</cell><cell>97%</cell></row><row><cell>0.25</cell><cell>0.32</cell><cell>1.00</cell><cell>0.39</cell><cell>0.14</cell><cell>81%</cell><cell>95%</cell></row></table><note>Ave., average; Est., estimate; S.E., standard error; Cov., coverage. The largest standard error for the results in column 6 is 2%. The largest standard error for the results in column 7 is 2%.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>High dimensional coverage simulation with lasso v CATE(v) Tuning Ave. Est. Ave. S.E. 80% Cov. 95% Cov.</figDesc><table><row><cell>-0.25</cell><cell>-0.10</cell><cell>0.25</cell><cell>-0.08</cell><cell>0.06</cell><cell>75%</cell><cell>90%</cell></row><row><cell>-0.25</cell><cell>-0.10</cell><cell>0.50</cell><cell>-0.07</cell><cell>0.04</cell><cell>74%</cell><cell>90%</cell></row><row><cell>-0.25</cell><cell>-0.10</cell><cell>1.00</cell><cell>-0.06</cell><cell>0.03</cell><cell>50%</cell><cell>74%</cell></row><row><cell>0.00</cell><cell>0.00</cell><cell>0.25</cell><cell>0.01</cell><cell>0.05</cell><cell>78%</cell><cell>96%</cell></row><row><cell>0.00</cell><cell>0.00</cell><cell>0.50</cell><cell>0.02</cell><cell>0.04</cell><cell>80%</cell><cell>97%</cell></row><row><cell>0.00</cell><cell>0.00</cell><cell>1.00</cell><cell>0.04</cell><cell>0.04</cell><cell>59%</cell><cell>84%</cell></row><row><cell>0.25</cell><cell>0.32</cell><cell>0.25</cell><cell>0.41</cell><cell>0.20</cell><cell>89%</cell><cell>96%</cell></row><row><cell>0.25</cell><cell>0.32</cell><cell>0.50</cell><cell>0.45</cell><cell>0.18</cell><cell>87%</cell><cell>97%</cell></row><row><cell>0.25</cell><cell>0.32</cell><cell>1.00</cell><cell>0.43</cell><cell>0.17</cell><cell>82%</cell><cell>95%</cell></row><row><cell cols="7">Ave., average; Est., estimate; S.E., standard error; Cov., coverage. The largest standard</cell></row><row><cell cols="7">error for the results in column 6 is 2%. The largest standard error for the results in</cell></row><row><cell cols="2">column 7 is 2%.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>1, random forest as in Example 3.2, kernel ridge regression as in Example 3.3, or lasso as in Example B.1, are not in terms of a critical radius.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Preprint. Under review.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and disclosure of funding</head><p>The National Science Foundation provided partial financial support via grants 1559172 and 1757140. Rahul Singh thanks the Jerry Hausman Dissertation Fellowship.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Global functionals</head><p>of Lemma B.2. We extend <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">Lemma 3.3]</ref>. Write ψ 0 (W ) = U 1 + α min 0 (W )U 2 . By law of iterated expectations,</p><p>. By triangle inequality,</p><p>of Lemma B.3. To begin, observe that</p><p>Using integration by parts and the boundary condition together with this result,</p><p>where the inequality is Cauchy Schwarz. The final results immediately follow from the definition of k γ and triangle inequality.</p><p>Invoking the bounded weighting and propensity score assumptions,</p><p>Note that</p><p>Using the Taylor expansion in h around h = 0 and the Holder inequality, there exist some</p><p>Hence there exists some h 1 in (h, h 0 ) depending only on</p><p>In summary, h −(q−1)/q f 1/q |K| q 1/q 1 2 f ≤ ℓ pr,q ≤ h −(q−1)/q f 1/q |K| q 1/q 2 f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Collecting results.</head><p>In summary, for all h &lt; h 1</p><p>, ψ 0 pr,q ℓ pr,q , ℓ pr,q ≍ h −(q−1)/q . of Lemma B.7. We prove the result for Example 3.1. The result for Example 3.2 is similar.</p><p>Without loss of generality, let Qh be the smallest finite constant for which Assumption 3.1 holds, i.e.</p><p>Since π 0 (v, x) is bounded away from zero and one,</p><p>Likewise for E{ℓ h (V ) 2 γ(0, V, X) 2 }. In summary,</p><p>Viewing the latter expression as an inner product in L 2 , it is maximized by alignment, i.e. taking γ(D, V, X)</p><p>.</p><p>Appealing to ℓ pr,q ≍ h −(q−1)/q from the proof of Lemma B.6,</p><p>Finally recall from the proof of Lemma B.8 that ℓ h ∞ h −1 . An identical argument holds for P(α h ℓ ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Approximation error</head><p>of Lemma B.10. For completeness, we quote the proof of <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">Lemma 3.6]</ref>. Define the quantities</p><p>Hence</p><p>The standard argument to control the bias of the higher order kernels employs the Taylor expansion of order v in h around h = 0; see e.g. <ref type="bibr" target="#b28">[29,</ref><ref type="bibr">Lemma B2]</ref>. Such an argument implies there exists some constant A v that depends only on v such that</p><p>Then using the relation</p><p>where C and h 1 depend only on (K, v, ḡv , fv , f ).</p><p>of Corollary B.1. By Lemma B.6, write the regularity condition on moments as</p><p>By Lemmas B.6, B.7, and B.8, write the first learning rate condition as Q1/2 + ᾱ/σ + ᾱ′ {R(γ ℓ )} 1/2 h −1 + h −1 /h −1/2 + ᾱ′ {R(γ ℓ )} 1/2 h −1 + ᾱ′ {R(γ ℓ )} 1/2 .</p><p>By Lemma B.9, write the second learning rate condition as σ{R(α h ℓ )} 1/2 σh −1 {R(α ℓ )} 1/2 . By Lemmas B.6 and B.9, write the initial term in the third learning rate condition as {nR(γ ℓ )R(α h ℓ )} 1/2 /σ {nR(γ ℓ )R(α ℓ )} 1/2 h −1 /h −1/2 = h −1/2 {nR(γ ℓ )R(α ℓ )} 1/2 . Likewise for the other terms. The approximation error condition is immediate from Lemma B.10.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Estimating conditional average treatment effects</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Abrevaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Chin</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">P</forename><surname>Lieli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Business &amp; Economic Statistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="485" to="505" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient estimation of models with conditional moment restrictions containing unknown functions</title>
		<author>
			<persName><forename type="first">Chunrong</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1795" to="1843" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Asymptotics for semiparametric econometric models via stochastic equicontinuity</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName><surname>Andrews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="page" from="43" to="72" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">High-dimensional inference for the average treatment effect under model misspecification using penalized bias-reduced double-robust estimation</title>
		<author>
			<persName><forename type="first">Vahe</forename><surname>Avagyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stijn</forename><surname>Vansteelandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics &amp; Epidemiology</title>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sparse models and methods for optimal instruments with an application to eminent domain</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Belloni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2369" to="2429" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inference for highdimensional sparse econometric models</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Belloni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Economics and Econometrics</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="245" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Uniform post-selection inference for least absolute deviation regression and other Z-estimation problems</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Belloni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kengo</forename><surname>Kato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="94" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Data-adaptive smoothing for optimal-rate estimation of possibly non-regular parameters</title>
		<author>
			<persName><forename type="first">F</forename><surname>Aurelien</surname></persName>
		</author>
		<author>
			<persName><surname>Bibaut</surname></persName>
		</author>
		<author>
			<persName><surname>Mark J Van Der Laan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.07408</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Efficient and Adaptive Estimation for Semiparametric Models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Chris</surname></persName>
		</author>
		<author>
			<persName><surname>Klaassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ya'acov Ritov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">A</forename><surname>Klaassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya'acov</forename><surname>Wellner</surname></persName>
		</author>
		<author>
			<persName><surname>Ritov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Johns Hopkins University Press</publisher>
			<biblScope unit="volume">4</biblScope>
			<pubPlace>Baltimore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-nonparametric IV estimation of shape-invariant Engel curves</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Kristensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1613" to="1669" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Nonparametric bootstrap inference for the targeted highly adaptive least absolute shrinkage and selection operator (LASSO) estimator</title>
		<author>
			<persName><forename type="first">Weixin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Van Der Laan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Biostatistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Overidentification in regular models</title>
		<author>
			<persName><forename type="first">Xiaohong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1771" to="1817" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Double/debiased machine learning for treatment and structural parameters</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Chetverikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mert</forename><surname>Demirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esther</forename><surname>Duflo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Whitney</forename><surname>Newey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Econometrics Journal</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="C1" to="C68" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<author>
			<persName><forename type="first">Victor</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Escanciano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hidehiko</forename><surname>Ichimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James M</forename><surname>Newey</surname></persName>
		</author>
		<author>
			<persName><surname>Robins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00033</idno>
	</analytic>
	<monogr>
		<title level="m">Locally robust semiparametric estimation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Demand analysis with many prices</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><forename type="middle">A</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Whitney</forename><forename type="middle">K</forename><surname>Newey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>National Bureau of Economic Research</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Debiased machine learning of global and local parameters using regularized Riesz representers</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Whitney</forename><surname>Newey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08667</idno>
	</analytic>
	<monogr>
		<title level="j">Econometrics Journal</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>to appear)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adversarial estimation of Riesz representers</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Whitney</forename><surname>Newey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasilis</forename><surname>Syrgkanis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00009</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic debiased machine learning of causal and structural effects</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Newey</surname></persName>
		</author>
		<author>
			<persName><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.05224</idno>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>to appear)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Double debiased machine learning nonparametric inference with continuous treatments</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Colangelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying-Ying</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03036</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Minimax estimation of conditional moment models</title>
		<author>
			<persName><forename type="first">Nishanth</forename><surname>Dikkala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lester</forename><surname>Mackey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasilis</forename><surname>Syrgkanis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07201</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Dylan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasilis</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><surname>Syrgkanis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09036</idno>
	</analytic>
	<monogr>
		<title level="j">Orthogonal statistical learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the nonparametric estimation of functionals</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Rafail</surname></persName>
		</author>
		<author>
			<persName><surname>Hasminskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ildar</surname></persName>
		</author>
		<author>
			<persName><surname>Ibragimov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Prague Symposium on Asymptotic Statistics</title>
				<meeting>the Second Prague Symposium on Asymptotic Statistics</meeting>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The influence function of semiparametric estimators</title>
		<author>
			<persName><forename type="first">Hidehiko</forename><surname>Ichimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><surname>Newey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01378</idno>
	</analytic>
	<monogr>
		<title level="m">Quantitative Economics</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Confidence intervals and hypothesis testing for highdimensional regression</title>
		<author>
			<persName><forename type="first">Adel</forename><surname>Javanmard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Montanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2869" to="2909" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Causal inference under unmeasured confounding with negative controls: A minimax learning approach</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Kallus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masatoshi</forename><surname>Uehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14029</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Consistent estimation of the influence function of locally asymptotically linear estimators. The Annals of Statistics</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Chris</surname></persName>
		</author>
		<author>
			<persName><surname>Klaassen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="1548" to="1562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Statistical inference for the mean outcome under a possibly non-unique optimal treatment strategy</title>
		<author>
			<persName><forename type="first">R</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><surname>Luedtke</surname></persName>
		</author>
		<author>
			<persName><surname>Mark J Van Der Laan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">713</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The asymptotic variance of semiparametric estimators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><surname>Newey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="page" from="1349" to="1382" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Kernel estimation of partial means and a general variance estimator. Econometric Theory</title>
		<author>
			<persName><forename type="first">K</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><surname>Newey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Instrumental variable estimation of nonparametric models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James L</forename><surname>Newey</surname></persName>
		</author>
		<author>
			<persName><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1565" to="1578" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lecture notes in statistics</title>
		<author>
			<persName><forename type="first">Johann</forename><surname>Pfanzagl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Contributions to a General Asymptotic Statistical Theory</title>
				<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Universal sieve-based strategies for efficient estimation using machine learning tools</title>
		<author>
			<persName><forename type="first">Hongxiang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Luedtke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Carone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2300" to="2336" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semiparametric efficiency in multivariate regression models with missing data</title>
		<author>
			<persName><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Robins</surname></persName>
		</author>
		<author>
			<persName><surname>Rotnitzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">429</biblScope>
			<biblScope unit="page" from="122" to="129" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Root-n-consistent semiparametric regression</title>
		<author>
			<persName><surname>Peter M Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="page" from="931" to="954" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Characterization of parameters with a mixed bias property</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Rotnitzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ezequiel</forename><surname>Smucler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James M</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="231" to="238" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficiency bounds for estimating linear functionals of nonparametric regression models with endogenous regressors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Severini</surname></persName>
		</author>
		<author>
			<persName><surname>Tripathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Econometrics</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="491" to="498" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">On the absolute constants in the Berry-Esseen type inequalities for identically distributed summands</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Shevtsova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1111.6554</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11076</idno>
		<title level="m">Debiased kernel methods</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Kernel instrumental variable regression</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Sahani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4595" to="4607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A unifying approach for doublyrobust ℓ 1 regularized estimation of causal contrasts</title>
		<author>
			<persName><forename type="first">Ezequiel</forename><surname>Smucler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Rotnitzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James M</forename><surname>Robins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03737</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On asymptotically optimal confidence regions and tests for high-dimensional models</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Van De Geer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruben</forename><surname>Ya'acov Ritov</surname></persName>
		</author>
		<author>
			<persName><surname>Dezeure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1166" to="1202" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Finite sample inference for targeted learning</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Van Der Laan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.09502</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Targeted Learning in Data Science</title>
		<author>
			<persName><forename type="first">Sherri</forename><surname>Mark J Van Der Laan</surname></persName>
		</author>
		<author>
			<persName><surname>Rose</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Targeted maximum likelihood learning. The International</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mark J Van Der Laan</surname></persName>
		</author>
		<author>
			<persName><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biostatistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Confidence intervals for low dimensional parameters in high dimensional linear models</title>
		<author>
			<persName><forename type="first">Cun-Hui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><forename type="middle">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="217" to="242" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cross-validated targeted minimum-loss-based estimation</title>
		<author>
			<persName><forename type="first">Wenjing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><surname>Mark J Van Der Laan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Targeted Learning</title>
				<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="459" to="474" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
