<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stability to Deformations of Manifold Filters and Manifold Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-21">21 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhiyang</forename><surname>Wang</surname></persName>
							<email>zhiyangw@seas.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Systems Engineering</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luana</forename><surname>Ruiz</surname></persName>
							<email>luanaruiz9@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Systems Engineering</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alejandro</forename><surname>Ribeiro</surname></persName>
							<email>aribeiro@seas.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Systems Engineering</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Stability to Deformations of Manifold Filters and Manifold Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-21">21 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">881110BA42D98D3ABD980FAED3B7FBC5</idno>
					<idno type="arXiv">arXiv:2106.03725v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-10-31T05:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Signal Processing</term>
					<term>Graph Neural Networks</term>
					<term>Manifolds</term>
					<term>Manifold Filters</term>
					<term>Manifold Neural Networks</term>
					<term>Manifold Deformations</term>
					<term>Operator Stability</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The paper defines and studies manifold (M) convolutional filters and neural networks (NNs). Manifold filters and MNNs are defined in terms of the Laplace-Beltrami operator exponential and are such that graph (G) filters and neural networks (NNs) are recovered as discrete approximations when the manifold is sampled. These filters admit a spectral representation which is a generalization of both the spectral representation of graph filters and the frequency response of standard convolutional filters in continuous time. The main technical contribution of the paper is to analyze the stability of manifold filters and MNNs to smooth deformations of the manifold. This analysis generalizes known stability properties of graph filters and GNNs and it is also a generalization of known stability properties of standard convolutional filters and neural networks in continuous time. The most important observation that follows from this analysis is that manifold filters, same as graph filters and standard continuous time filters, have difficulty discriminating high frequency components in the presence of deformations. This is a challenge that can be ameliorated with the use of manifold, graph, or continuous time neural networks. The most important practical consequence of this analysis is to shed light on the behavior of graph filters and GNNs in large scale graphs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Graph convolutional filters <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref> and graph neural networks (GNNs) <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref> have become the tool of choice for signal and information processing on graphs, e.g., <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b11">[12]</ref>. In several applications, graphs can be considered as samples of a manifold. This is sometimes quite explicit as in the case of, e.g., point clouds <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> and sometimes somewhat implicit as in the case of, e.g., wireless communication networks <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. In this context we can think of convolutions on graphs and GNNs as discretizations of their manifold counterparts. This is important because one can often gain valuable insights about discrete (graph) signal processing by studying continuous (manifold) signal processing.</p><p>The main technical contribution of this paper is analyzing the stability of manifold filters and manifold neural networks (MNNs) to smooth deformations of the manifold. Prior to developing stability analyses we define manifold convolutional filters and frequency responses that are consistent with graph convolutional filters and frequency responses.</p><p>Manifold Filters. Consider a manifold with Laplace-Beltrami (LB) operator L and formulate the corresponding manifold diffusion equation with respect to an auxiliary time variable t. For initial condition f (x), manifold diffusions generate the time varying manifold function u(x, t) = e −tL f (x) in which e −tL is the LB operator exponential (Section II-A). A manifold convolutional filter h(t) then acts on the function f (x) as the integral over time of the product of h(t) with the diffusion sequence u(x, t) (Section II-B),</p><formula xml:id="formula_0">g(x) = (hf )(x) = ∞ 0 h(t)e −tL f (x)dt.<label>(1)</label></formula><p>A manifold convolutional filter is such that we can recover graph filters through discretization of the manifold and discretization of the auxiliary time variable t (Section VI). The reason for this connection is that graph filters are linear combinations of the elements of the graph diffusion sequence <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> and (1) defines manifold filters as linear combinations of the elements of the manifold diffusion sequence. Manifold filters are also generalizations of standard time convolutions. This requires consideration of the wave equation on the real line so that the exponential of the derivative operator e −t∂/∂x appears in (1) (Appendix 1).</p><p>Frequency Response of Manifold Filters. We define a filter's frequency response as</p><formula xml:id="formula_1">ĥ(λ) = ∞ 0 h(t)e −tλ dt. (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>This definition is motivated by the fact that manifold convolutions can be decomposed on separate spectral components (Section II-C). Indeed, let φ i be an eigenfunction of the LB operator associated with eigenvalue</p><formula xml:id="formula_3">λ i . If [ f ] i = f, φ i L 2 (M)</formula><p>and [ĝ] i = g, φ i L 2 (M) are projections of the functions f and g in (1) on this eigenfunction, [ĝ] i depends only on [ f ] i and can be written as [ĝ] i = ĥ(λ i )[ f ] i (Proposition 2). This definition of the frequency response of a manifold filter generalizes the frequency response of a standard time filter. This is because the definition in (2) is a Laplace transform, which reduces to the Fourier transform of a filter's impulse response when restricted to the imaginary axis λ = jω. It is also a generalization of the frequency response of a graph filter, which is a z-transform <ref type="bibr" target="#b16">[17]</ref>.</p><p>Stability to Deformations. We establish stability of manifold filters and MNNs with respect to domain deformations formally defined as manifold diffeomorphisms. We consider a signal f defined on a manifold M and the signal f • τ made up of the composition of f with a diffeomorphism. The signals f and f • τ are passed through the same MNN Φ. In this paper we prove that if τ is -small and -smooth the respective MNN outputs satisfy</p><formula xml:id="formula_4">Φ(f • τ ) − Φ(f ) L 2 (M) = O( ) f L 2 (M) ,<label>(3)</label></formula><p>provided that the manifold filters in the layers of the MNN satisfy certain spectral properties (Sections III and IV). The bound in (3) is a generalization of the standard convolutional neural network (CNN) bound in <ref type="bibr" target="#b17">[18]</ref>, which studies diffeomorphisms of the real line and its effect on convolutional filter banks and CNNs. The bound is also a limit version of the GNN bounds of <ref type="bibr" target="#b18">[19]</ref> and related literature <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b23">[24]</ref>. As is the case of <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b23">[24]</ref> the bound in (3) holds when the frequency response ĥ(λ) of the filters that make up the layers of the MNN have decreasing variability with increasing λ (Section III). Thus, stability requires layers with limited ability to discriminate high frequency components and implies that one may expect multilayered MNNs to outperform filters in learning tasks in which high frequency components are important (Section V).</p><p>Related Work and Significance. We focus on the stability analysis of MNNs -the limit version of GNNs -because all of the existing GNN stability results have bounds that grow with the number of nodes <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref> in the graph. To overcome this limitation, many works have studied neural networks on graphons <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref> and more general graph models with variable sparsity <ref type="bibr" target="#b26">[27]</ref>. Results in these settings are independent of graph size -same as the results for a limit object presented here. It needs to be pointed out that graphons are limits of dense graphs in the sense that they have growing degrees. This is different from graphs sampled from manifolds that can have finite degrees as the discretization becomes finer <ref type="bibr" target="#b27">[28]</ref>. Moreover, even if there exist random graph models allowing to model moderately sparse graphs (e.g., <ref type="bibr" target="#b26">[27]</ref>), these models do not have the physical interpretation of a manifold, which is often a better descriptor of real-world domains. Of particular relevance to our paper is the work on GNN transferability for graphs that are sampled from a general topological space <ref type="bibr" target="#b28">[29]</ref>.</p><p>Organization. Section II introduces preliminary definitions (Section II-A), defines manifold convolutions (Section II-B), and introduces the spectral domain representation of manifold filters (Section II-C). Section III studies the stability of manifold filters to manifold deformations. It shows that a diffeomorphism results in a perturbation of the LB operator that involves additive and multiplicative terms (Theorem 1). It then goes on to study the effect on manifold filters of additive (Section III-A) and multiplicative (Section III-B) perturbations of the LB operator. Section IV extends the analysis of manifold filter stability to manifold neural networks and Section V discusses the implications of the results derived in Sections III and IV. Section VI explains how to recover graph filters and GNNs from the discretization of a manifold. Section VII illustrates the results of Sections III and IV with numerical examples. Section VIII concludes the paper. Proofs are deferred to appendices and supplmenetary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MANIFOLD CONVOLUTIONAL FILTERS</head><p>Consider a compact, smooth and differentiable d-dimensional submanifold M ⊂ R N . For simplicity, in this paper we use the words submanifold and manifold interchangeably, assuming the manifold M to always be embedded in R N . This embedding induces a Riemannian structure <ref type="bibr" target="#b29">[30]</ref>. In turn, the Riemannian structure allows defining a measure µ over the manifold as well as a notion of length for smooth curves on M. Given two points x, y ∈ M, the length of the shortest curve between x and y is denoted dist(x,y) and called the geodesic distance between these points.</p><p>We consider the manifold M to be the support of data that we represent as smooth real scalar functions f : M → R. We call these scalar functions manifold signals. We focus on manifold signals that have finite energy, such that f ∈ L 2 (M). Since L 2 (M) is a Hilbert space, it is equipped with an inner product given by</p><formula xml:id="formula_5">f, g L 2 (M) = M f (x)g(x)dµ(x)<label>(4)</label></formula><p>where dµ(x) is the d-dimensional volume element corresponding to measure µ. Thus, the energy of the signal f is given by</p><formula xml:id="formula_6">f 2 L 2 (M) = f, f L 2 (M) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Laplace-Beltrami Operator</head><p>On manifolds, differentiation is implemented by the intrinsic gradient -a local operator acting on a neighborhood of each point x ∈ M that is homeomorphic to a d-dimensional Euclidean space. This neighborhood contains all the vectors tangent to M at x, which is called the tangent space of x ∈ M and is denoted as T x M. The disjoint union of all tangent spaces on M is the tangent bundle T M. Formally, the intrinsic gradient is the operator ∇ : L 2 (M) → L 2 (T M) mapping scalar functions f (x) ∈ L 2 (M) to tangent vector functions ∇f (x) ∈ L 2 (T x M). The tangent vector function ∇f (x) indicates the direction of the fastest change of signal f at point x. The adjoint of the intrinsic gradient is the intrinsic divergence, defined as div : L 2 (T M) → L 2 (M). Interpreting the tangent vector field as the velocity field of a fluid, the intrinsic divergence can be seen as a measure of the net motion of the fluid at each point <ref type="bibr" target="#b12">[13]</ref>.</p><p>The Laplace-Beltrami (LB) operator of a manifold M is defined as the operator L : L 2 (M) → L 2 (M) given by the function composition of the intrinsic divergence and the intrinsic gradient. When considered in the local coordinates supported on T x M <ref type="bibr" target="#b30">[31]</ref>, the LB operator can be written as</p><formula xml:id="formula_7">Lf = −div • ∇f = −∇ • ∇f.<label>(5)</label></formula><p>Much like the Laplace operator in Euclidean domains (or the Laplacian matrix, in the case of graphs <ref type="bibr" target="#b31">[32]</ref>), the LB operator measures the total variation of function f , i.e., how much the value of f at a point deviates from local average of the values of f in its surroundings <ref type="bibr" target="#b12">[13]</ref>. Since L, like ∇, is a local operator depending on the tangent space T x M of each point x ∈ M, in the following we make this dependence explicit by writing L = L x and ∇ = ∇ x .</p><p>The LB operator plays an important role in partial differential equations (PDEs), as it governs the dynamics of the diffusion of heat over manifolds as given by the heat equation</p><formula xml:id="formula_8">∂u(x, t) ∂t + Lu(x, t) = 0.<label>(6)</label></formula><p>If u(x, t) ∈ L 2 (M) measures the temperature of point x ∈ M at time t ∈ R + , equation ( <ref type="formula" target="#formula_8">6</ref>) can be interpreted to mean that, at point x, the rate at which the manifold "cools down" is proportional to the difference between the temperature of x and the local average of the temperature of the points in its neighborhood. With initial condition u(x, 0) = f (x), the solution to this equation is given by</p><formula xml:id="formula_9">u(x, t) = e −tL f (x),<label>(7)</label></formula><p>which is the key support to implement the LB operator in the definitions proposed later.</p><p>The LB operator L is self-adjoint and positive-semidefinite. Considering that M is compact, the LB operator L has a real positive eigenvalue spectrum</p><formula xml:id="formula_10">{λ i } ∞ i=1 satisfying Lφ i = λ i φ i (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>where φ i is the eigenfunction associated with eigenvalue λ i . The indices i are such that the eigenvalues are ordered in increasing order as 0 &lt; λ 1 ≤ λ 2 ≤ λ 3 ≤ . . .. In particular, for a ddimensional manifold, we have that λ i ∝ i 2/d as a consequence of Weyl's law <ref type="bibr" target="#b32">[33]</ref>. The eigenfunctions φ i are orthonormal and form a generalized eigenbasis of L 2 (M) in the intrinsic sense. Since L is a total variation operator, the eigenvalues λ i can be interpreted as the canonical frequencies and the eigenfunctions φ i as the canonical oscillation modes of M. This further allows us to implement operator L in the spectral domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Manifold Filters</head><p>Time signals are processed by filters which compute the continuous time convolution of the input signal and the filter impulse response <ref type="bibr" target="#b16">[17]</ref>; images and high-dimensional Euclidean signals are processed by filters implementing multidimensional convolutions <ref type="bibr" target="#b33">[34]</ref>; and graph signals are filtered by computing graph convolutions <ref type="bibr" target="#b4">[5]</ref>. In this paper, we define a manifold filter as the convolution of the filter impulse response h(t) and the manifold signal f . Note that the definition of the convolution operation, denoted as M , leverages the heat diffusion dynamics described in <ref type="bibr" target="#b6">(7)</ref>.</p><p>Definition 1 (Manifold filter) Let h(t) : R + → R and let f ∈ L 2 (M) be a manifold signal. The manifold filter with impulse response h(t), denoted as h, is given by</p><formula xml:id="formula_12">g(x) = (hf )(x) := ( h M f )(x) := ∞ 0 h(t)u(x, t)dt (9)</formula><p>where h M f is the manifold convolution of h and f , and u(x, t) is the solution of the heat equation <ref type="bibr" target="#b5">(6)</ref> </p><formula xml:id="formula_13">with initial condition u(x, 0) = f (x).</formula><p>In a slight abuse of nomenclature, in the following we will use the terms manifold filter and manifold convolution interchangeably.</p><p>From Definition 1, we see that the manifold filter operates on manifold signals f (x) by (i) scaling the diffusion process <ref type="bibr" target="#b6">(7)</ref> starting at f (x) by h(t) and (ii) aggregating the outcome of the scaled diffusion process from t = 0 to t = ∞. This definition is somewhat contrived because it does not allow computing the output of the filter directly from f . To be able to do so, we need to substitute the solution of the heat equation <ref type="bibr" target="#b6">(7)</ref> for u(x, t) in <ref type="bibr" target="#b8">(9)</ref>. This leads to a closed-form expression of h that is parametric on the LB operator as shown in Proposition 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 1</head><p>The manifold filter h is a parametric map h(L) of the LB operator L, which is shown as</p><formula xml:id="formula_14">g(x) = (hf )(x) = ∞ 0 h(t)e −tL f (x)dt = h(L)f (x). (10)</formula><p>Proposition 1 tells us that manifold filters are spatial operators, since they operate directly on points x ∈ M; moreover, it tells us that they are local, because they are parametrized by L which is itself a local operator. The exponential term e −tL can be seen as a diffusion or shift operation akin to a time delay in a linear time-invariant (LTI) filter <ref type="bibr" target="#b16">[17]</ref>, or as the graph shift operator in a linear shift-invariant (LSI) graph filter <ref type="bibr" target="#b4">[5]</ref>. Indeed, if we consider the manifold M to be the real line, the manifold filter defined in <ref type="bibr" target="#b9">(10)</ref> recovers a LTI filter. If we consider it to be a set of points connected by a geometric graph, (10) recovers a LSI graph filter. We discuss these special cases in further detail in Appendix A and Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Frequency Representation of Manifold Filters</head><p>A manifold signal f ∈ L 2 (M) can be represented in the frequency domain of the manifold by projecting f onto the LB operator eigenbasis <ref type="bibr" target="#b7">(8)</ref> as</p><formula xml:id="formula_15">[ f ] i = f, φ i L 2 (M) = M f (x)φ i (x)dµ(x),<label>(11)</label></formula><p>where we claim that f is the frequency representation of the corresponding signal with f = ∞ i=1 [ f ] i φ i . Frequency representations are useful because they help understand the frequency behavior of the manifold filter h(L).</p><p>To see this, we consider the frequency representation of the manifold filter output g in <ref type="bibr" target="#b9">(10)</ref>, which is</p><formula xml:id="formula_16">[ĝ] i = M ∞ 0 h(t)e −tL f (x)dtφ i (x)dµ(x).<label>(12)</label></formula><p>Rearranging the integrals and substituting e −tL φ i = e −tλi φ i , we can get</p><formula xml:id="formula_17">[ĝ] i = ∞ 0 h(t)e −tλi dt[ f ] i . (<label>13</label></formula><formula xml:id="formula_18">)</formula><p>The expression relating ĝ and f is called the frequency response of the filter h(L).</p><p>Definition 2 (Frequency response) The frequency response of the filter h(L) is given by ĥ</p><formula xml:id="formula_19">(λ) = ∞ 0 h(t)e −tλ dt.<label>(14)</label></formula><p>An important consequence of Definition 2 is that, since ĥ(λ) is parametric on λ, the manifold filter is pointwise in the frequency domain. This can be seen by plugging <ref type="bibr" target="#b13">(14)</ref> into <ref type="bibr" target="#b12">(13)</ref>, and is stated explicitly in Proposition 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 2</head><p>The manifold filter h(L) is pointwise in the frequency domain, which is written as</p><formula xml:id="formula_20">[ĝ] i = ĥ(λ i )[ f ] i . (<label>15</label></formula><formula xml:id="formula_21">)</formula><p>Definition 2 also emphasizes that the frequency response of a manifold filter is independent of the underlying manifold. Note that, in <ref type="bibr" target="#b13">(14)</ref>, ĥ(λ) is a function of an arbitrary scalar variable λ. To obtain the frequency behavior of this filter on a given manifold M, we need to evaluate ĥ at the corresponding LB operator eigenvalues λ i [cf. <ref type="bibr" target="#b7">(8)</ref>]. If the manifold changes (or if we want to deploy the same filter on a different manifold M ), it suffices to reevaluate ĥ at λ i , i.e., at the eigenvalues of the new LB operator L .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. STABILITY OF MANIFOLD FILTERS WITH RESPECT TO MANIFOLD DEFORMATIONS</head><p>On the manifold M, we define a deformation as function τ (x) : M → M, where x ∈ M is a point on the manifold and dist(x, τ (x)) is upper bounded, i.e., τ (x) is a displaced point in the neighborhood of x. The deformation τ has a corresponding tangent map τ * : T x M → T τ (x) M and a Jacobian matrix J(τ * ). When dist(x, τ (x)) is bounded, the Frobenius norm of J(τ * ) − I can also be upper bounded, and these bounds are used to measure the size of the deformation τ (x).</p><p>Let f : M → R be a manifold signal. Because M is the codomain of τ (x), g = f • τ maps points τ (x) ∈ M to f (τ (x)) ∈ R, so that the effect of a manifold deformation on the signal f is a signal perturbation leading to a new signal g supported on the same manifold. To understand the effect of this deformation on the LB operator, let p = Lg. Since p is also a signal on M, we may define an operator L mapping f directly into p,</p><formula xml:id="formula_22">p(x) = L f (x) = Lg(x) = Lf (τ (x)). (<label>16</label></formula><formula xml:id="formula_23">)</formula><p>The operator L is the perturbed LB operator, which is effectively the new LB operator resulting from the deformation τ . Assuming that the gradient field is smooth, the difference between L and L is given by the following theorem. The proof is deferred to Appendix B.</p><p>Theorem 1 Let L be the LB operator of the manifold M. Let τ (x) : M → M be a manifold perturbation such that dist(x, τ (x)) = and J(τ * ) = I + ∆ with ∆ F = . If the gradient field is smooth, it holds that</p><formula xml:id="formula_24">L − L = EL + A,<label>(17)</label></formula><p>where E and A satisfy E = O( ) and</p><formula xml:id="formula_25">A op = O( ).</formula><p>Therefore, the perturbation of the LB operator incurred by a manifold deformation τ is a combination of an absolute perturbation A [cf. <ref type="bibr">Definition 3]</ref> and a relative perturbation EL [cf. <ref type="bibr">Definition 7]</ref>. This largely simplifies our analysis of stability. Since manifold filters are parametric on L [cf. <ref type="bibr">Proposition 2]</ref>, it is sufficient to characterize their stability to deformations of the manifold by analyzing their behavior in the presence of absolute and relative LB perturbations. This is what we do in Sections III-A and III-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Stability of Manifold Filters to Absolute Perturbations</head><p>We start by analyzing the stability of manifold filters to absolute perturbations of the LB operator, which are introduced in Definition 3.</p><p>Definition 3 (Absolute perturbations) Let L be the LB operator of manifold M. An absolute perturbation of L is defined as</p><formula xml:id="formula_26">L − L = A,<label>(18)</label></formula><p>where the absolute perturbation operator A is self-adjoint.</p><p>Like L, the operator L resulting from the absolute perturbation of L is self-adjoint due to the symmetry of A. Hence, it admits an eigendecomposition similar to <ref type="bibr" target="#b4">(5)</ref>. When the filter coefficients are fixed, the frequency response of the manifold filter <ref type="bibr" target="#b14">(15)</ref> can be obtained by evaluating ĥ(λ) at each λ i . Thus, to understand the effect of the perturbation on the filter we need to look at how the perturbation of the LB operator changes the eigenvalues λ i . The challenge in this case is that the spectrum of L is infinite-dimensional, i.e., there is an infinite (though countable) number of eigenvalue perturbations that need to be taken into account. As demonstrated by Proposition 3, however, large eigenvalues accumulate in certain parts of the real line. This suggests a strategy to partition the spectrum into finite number of partitions.</p><p>Proposition 3 Consider a d-dimensional compact manifold M ⊂ R N and let L be its LB operator with eigenvalues {λ k } ∞ k=1 . Let C 1 be an arbitrary constant and C d the volume of the d-dimensional unit ball. Let Vol(M) denote the volume of manifold M. For any α &gt; 0 and d &gt; 2, there exists N 1 ,</p><formula xml:id="formula_27">N 1 = (αd/C 1 ) d/(2−d) (C d Vol(M)) 2/(2−d)<label>(19)</label></formula><p>such that, for all k &gt; N 1 ,</p><formula xml:id="formula_28">λ k+1 − λ k ≤ α.</formula><p>Proof. This is a direct consequence of Weyl's law <ref type="bibr" target="#b34">[35,</ref><ref type="bibr">Chapter 1]</ref>. Given this asymptotic behavior, we can divide the eigenvalues into a finite number of partitions by placing eigenvalues that are less than α &gt; 0 apart from each other in groups. This spectrum separation strategy is described in Definition 4. To achieve it, we will need a specific type of manifold filter called Frequency Difference Threshold (FDT) filter as introduced in Definition 5.</p><formula xml:id="formula_29">Definition 4 (α-separated spectrum) The α-separated spec- trum of a LB operator L is defined as the partition Λ 1 (α) ∪ . . . ∪ Λ N (α) such that all λ i ∈ Λ k (α) and λ j ∈ Λ l (α), k = l, satisfy |λ i − λ j | &gt; α. (<label>20</label></formula><formula xml:id="formula_30">)</formula><p>Definition 5 (α-FDT filter) The α-frequency difference threshold (α-FDT) filter is defined as a filter h(L) whose frequency response satisfies</p><formula xml:id="formula_31">| ĥ(λ i ) − ĥ(λ j )| ≤ δ k for all λ i , λ j ∈ Λ k (α)<label>(21)</label></formula><p>with δ k ≤ δ for k = 1, . . . , N .</p><p>In the α-separated spectrum, eigenvalues λ i ∈ Λ k (α) and λ j ∈ Λ l (α) in different sets (k = l) are at least α away from each other. Conversely, eigenvalues λ i , λ j ∈ Λ k (α) are no more than α apart. This partitioning creates several eigenvalue groups spaced by at least α. Note that the sets Λ k (α) can have any size and, in particular, they can be singletons. The partitioning of the spectrum described in Definition 4 is achieved by an α-FDT filter. This filter separates the spectrum of the manifold by assigning similar frequency responsesthat deviate no more than δ k from each other-to eigenvalues</p><formula xml:id="formula_32">0 Λ 1 Λ 2 Λ 3 Λ 4 Λ 5 h(λ)</formula><formula xml:id="formula_33">λ i ∈ Λ k (α), 1 ≤ k ≤ N .</formula><p>In other words, the α-FDT filter does not discriminate between eigenvalues λ i , λ j ∈ Λ k (α). Importantly, the δ k in Definition 4 are finite, so that they can be bounded by some δ.</p><p>To obtain manifold filters that are stable to absolute perturbations of L, we also need these filters to be Lipschitz continuous as shown in Definition 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 6 (Lipschitz filter) A filter is</head><formula xml:id="formula_34">A h -Lispchitz if its frequency response is Lipschitz continuous with Lipschitz constant A h , i.e, | ĥ(a) − ĥ(b)| ≤ A h |a − b| for all a, b ∈ (0, ∞). (<label>22</label></formula><formula xml:id="formula_35">)</formula><p>Between the eigenvalue groups, the filters that we consider are assumed to be A h -Lipschitz continuous. This means that, in regions of the spectrum where the Λ k (α) are singletons, the filter can vary with slope at most A h as shown in Figure <ref type="figure" target="#fig_0">1</ref>. Note that we can always construct convolutional filters ( <ref type="formula" target="#formula_19">14</ref>) that are both Lipschitz continuous and α-FDT.</p><p>Under mild assumptions on the amplitude of the frequency response ĥ (Assumption 1), it can be shown that Lipschitz continuous α-FDT filters are stable to absolute perturbations of the LB operator. This result is stated in Theorem 2.</p><p>Assumption 1 (Non-amplifying filters) The filter frequency response ĥ : R + → R is non-amplifying. I.e., for all λ ∈ (0, ∞), ĥ satisfies | ĥ(λ)| ≤ 1.</p><p>Note that this assumption is rather reasonable, because the filter frequency response ĥ(λ) can always be normalized. </p><formula xml:id="formula_36">(L)f −h(L )f L 2 (M) ≤ πN s α − + A h + 2(N − N s )δ f L 2 (M) , (<label>23</label></formula><formula xml:id="formula_37">)</formula><p>where N is the size of the α-separated spectrum partition [cf.</p><p>Definition 4] and N s is the number of singletons.</p><p>Proof. See Appendix C. Provided that α, FDT filters are thus stable to absolute perturbations of the LB operator L. The stability bound depends on (i) the continuity of the FDT filter as measured by the Lipschitz constant A h and (ii) its frequency difference threshold α, which affects the bound directly as well as indirectly through the number of partitions N . Note that this bound consists of three terms. The first corresponds to the difference between the eigenfunctions of L and L , which affects the stability bound by changing projection directions. The second stems from the distance between the original and perturbed eigenvalues. Finally, the third reflects the bounded fluctuation of the filter frequency response within the same eigenvalue group.</p><p>The bound in Theorem 2 can be simplified by setting δ = π /(2α − 2 ) as in Corollary 1.</p><formula xml:id="formula_38">Corollary 1 Setting δ = π /(2α − 2 ), under the same assumptions of Theorem 2 it holds that h(L)f − h(L )f L 2 (M) ≤ πN α − + A h f L 2 (M) .<label>(24)</label></formula><p>A particular case of Theorem 2, the simplified stability bound in Corollary 1 is helpful to understand the effect of the filter spectrum on stability as well as of the size of the perturbation. In particular, from Corollary 1 we can tell that the filter is more stable if the Lipschitz constant A h is small and the frequency difference threshold α is large. On the other hand, small A h and large α mean that the filter is less discriminative. This reveals a stability-discriminability trade-off where discriminability should be understood as the ability to tell frequencies apart. In other words, we propose α-FDT filter to maintain the stability by trying to discriminate only eigenvalue groups instead of every single eigenvalue. Importantly, this trade-off is not related to the magnitude of the frequencies that the filters amplify (as is the case in, e.g., <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b35">[36]</ref>).We will keep observing this trade-off throughout the stability analysis of both manifold filters and MNNs. More details about this characteristic will be discussed in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Stability of Manifold Filters to Relative Perturbations</head><p>Relative perturbations of the LB operator are defined similarly as follows.</p><p>Definition 7 (Relative perturbations) Let L be the LB operator of manifold M. A relative perturbation of L is defined as</p><formula xml:id="formula_39">L − L = EL, (<label>25</label></formula><formula xml:id="formula_40">)</formula><p>where the relative perturbation term EL is self-adjoint.</p><p>Like absolute perturbations, relative perturbations also perturb the eigenvalues and eigenfunctions of L. However, in the case of relative perturbations, the perturbations to the eigenvalues are proportional to their absolute values [cf. Lemma 3 in the supplementary material]. Relative perturbations thus require a different spectrum separation strategy to guarantee stability. With relative perturbations, larger eigenvalues are impacted with larger perturbation values, which can send eigenvalues originally α-close to each other to different groups as Figure <ref type="figure" target="#fig_1">2</ref>. Therefore, we will need a different type of filter implementing a different type of spectrum separation. Our strategy is inspired by Proposition 4, which is another variation of Weyl's law.</p><p>Proposition 4 Let M be a d-dimensional compact embedded manifold in R N with LB operator L, and let {λ k } ∞ k=1 denote the eigenvalues of L. Let C 1 denote an arbitrary constant. For any γ &gt; 0, there exists N 2 given by</p><formula xml:id="formula_41">N 2 = (C 1 (γ + 1) d/2 − 1) −1 (<label>26</label></formula><formula xml:id="formula_42">)</formula><p>such that, for all k &gt; N 2 , it holds that</p><formula xml:id="formula_43">λ k+1 − λ k ≤ γλ k .</formula><p>Proof. This is a direct consequence of Weyl's law <ref type="bibr" target="#b34">[35,</ref><ref type="bibr">Chapter 1]</ref>. Hence, to enforce stability we need to separate the spectrum relatively to the ratio between neighboring eigenvalues. This partitioning is called the γ-separated spectrum and formalized in Definition 8. The γ-separated spectrum is achieved by the so-called Frequency Ratio Threshold (FRT) filters. We introduce them in Definition 9.</p><p>Definition 8 (γ-separated spectrum.) The γ-separated spectrum of a LB operator L is defined as the partition</p><formula xml:id="formula_44">Λ 1 (γ) ∪ . . . ∪ Λ M (γ) such that all λ i ∈ Λ k (γ) and λ j ∈ Λ l (γ), k = l, satisfy λ i λ j − 1 &gt; γ. (<label>27</label></formula><formula xml:id="formula_45">)</formula><p>Definition 9 (γ-FRT filter.) The γ-frequency ratio threshold (γ-FRT) filter is defined as a filter h(L) whose frequency response satisfies</p><formula xml:id="formula_46">| ĥ(λ i ) − ĥ(λ j )| ≤ δ k , for all λ i , λ j ∈ Λ k (γ) (<label>28</label></formula><formula xml:id="formula_47">)</formula><formula xml:id="formula_48">with δ k ≤ δ for k = 1, 2 . . . , M .</formula><p>In the γ-separated spectrum, the sets Λ k (γ) are built based on eigenvalue distances relative to the eigenvalues' magnitudes and weighted by the parameter γ. Eigenvalues λ j ∈ Λ k (γ) and λ i ∈ Λ l (γ) in different groups (i.e., k = l) are at least γ min(λ i , λ j ) apart from each other. This means that, for</p><formula xml:id="formula_49">λ i , λ i+1 ∈ Λ k (γ), λ i+1 − λ i ≤ γλ i .</formula><p>The γ-FRT filter achieves the spectrum separation in Definition 8 by giving eigenvalues λ i , λ j ∈ Λ k (γ) very similar frequency responses differing by at most plus or minus δ k &lt; δ. Meanwhile, eigenvalues belonging to different sets Λ k (γ) and Λ l (γ), k = l, are treated independently, and their frequency response can vary a lot.</p><p>To make a manifold filter stable to relative perturbations of the LB operator, we need a further restriction on their continuity. Lipschitz continuity [cf. <ref type="bibr">Definition 6]</ref> is not enough because in a Lipschitz filter the difference in frequency response for a perturbed eigenvalue grows with the eigenvalue magnitude, since the eigenvalue perturbation is relative. Therefore, we need our filters to be integral Lipschitz continuous as is described in Definition 10.</p><formula xml:id="formula_50">0 Λ 1 Λ 2 Λ 3 Λ 4 Λ 5 Λ 6 h(λ)</formula><p>Definition 10 (Integral Lipschitz filter) A filter is integral Lipschitz continuous with constant B h if its frequency response is given by</p><formula xml:id="formula_51">| ĥ(a) − ĥ(b)| ≤ B h |a − b| (a + b)/2 for all a, b ∈ (0, ∞).<label>(29)</label></formula><p>Integral Lipschitz filters can be seen as Lipschitz filters with variable Lipschitz constant, which decreases with λ. E.g., on the interval (a, b), the filter in Definition 10 behaves as a Lipschitz filter with Lipschitz constant 2B h /(a + b). When a and b are close, this condition can be approximated by |a ĥ (a)| ≤ B h . This implies that the filter function flattens for high-frequency eigenvalues as shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>Under Assumption 1, integral Lipschitz γ-FRT filters are stable to relative perturbations as stated in Theorem 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 3 (Manifold filter stability to relative perturbations)</head><p>Consider a manifold M with LB operator L. Let h(L) be a γ-FRT filter with δ = π /(2γ − 2 + 2γ ) [cf. <ref type="bibr">Definition 9]</ref> and B h -integral Lipschitz [cf. <ref type="bibr">Definition 10]</ref>. Consider a relative perturbation L = L + EL of the LB operator L [cf. <ref type="bibr">Definition 7]</ref> where E = &lt; γ. Then, under Assumption 1 it holds that</p><formula xml:id="formula_52">h(L)f −h(L )f L 2 (M) ≤ πM γ − + γ + 2B h 2 − f L 2 (M) (<label>30</label></formula><formula xml:id="formula_53">)</formula><p>where M is the size of the γ-separated spectrum partition [cf. <ref type="bibr">Definition 8]</ref>.</p><p>Proof. See Section B in supplementary material. When is sufficiently small ( min(γ, 2), which is typically the case with deformations such as the one in Theorem 1), the denominators on the right hand side of <ref type="bibr" target="#b29">(30)</ref> are approximately equal to γ and 2 respectively. Hence, γ-FRT integral Lipschitz filters are stable to relative perturbations of the LB operator. Besides appearing in the bound in Theorem 3), the frequency ratio threshold γ also affects stability indirectly through the partition size M . With a larger γ, fewer eigenvalues will be in singleton sets, thus decreasing M and improving stability. A smaller integral Lipschitz constant B h also increases stability. However, small B h and large γ make for smoother filters which in turn lead to a less discriminative manifold filter. Therefore, integral Lipschitz γ-FRT filters also exhibit a trade-off between discriminality and stability.</p><p>Remark 1 By comparing the illustrations of α-FDT filter (Definition 5) and γ-FRT filter (Definition 9) in Figure <ref type="figure" target="#fig_0">1</ref> and Figure <ref type="figure" target="#fig_1">2</ref>, we see that in practice these filters have a similar frequency behavior because, due to Weyl's law [cf. <ref type="bibr">Proposition 3 and 4]</ref>, high frequency components will eventually be grouped in the same group and thus share similar frequency responses. Therefore, the main difference between these filters is their effects on the low-frequency components. In the low frequency spectrum, the eigengaps λ i+1 − λ i may be smaller than the difference threshold α, but larger than the relative ratio threshold γλ i due to λ i being small. However, for appropriate values of γ a filter may be both FDT and FRT. This will be shown in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. STABILITY OF MANIFOLD NEURAL NETWORKS</head><p>Manifold neural networks (MNNs) are deep convolutional architectures comprised of L layers, where each layer consists of two components: a convolutional filterbank and a pointwise nonlinearity. At each layer l = 1, 2, . . . , L, the convolutional filters map the incoming F l−1 features from layer l − 1 into F l intermediate linear features given by</p><formula xml:id="formula_54">y p l (x) = F l−1 q=1 h pq l (L)f q l−1 (x),<label>(31)</label></formula><p>where h pq l (L) is the filter mapping the q-th feature from layer l − 1 to the p-th feature of layer l as in <ref type="bibr" target="#b14">(15)</ref>, for 1 ≤ q ≤ F l−1 and 1 ≤ p ≤ F l . The intermediate features are then processed by a pointwise nonlinearity σ : R → R as</p><formula xml:id="formula_55">f p l (x) = σ (y p l (x)) .<label>(32)</label></formula><p>The nonlinearity σ processes each feature individually and we further make an assumption on its continuity as follows. Note that this assumption is rather reasonable, since most common activation functions (e.g., the ReLU, the modulus and the sigmoid) are normalized Lipschitz by design.</p><p>At the first layer of the MNN, the input features are the input data f q for 1 ≤ q ≤ F 0 . At the output of the MNN, the output features are given by the outputs of the L-th layer, i.e., f p L for 1 ≤ p ≤ F L . To represent the MNN more succinctly, we may gather the impulse responses of the manifold convolutional filters h pq l across all layers in a function set H, and define the MNN map Φ(H, L, f ). This map emphasizes that the MNN is parameterized by both the filter functions and the LB operator L. We next will analyze the stability of Φ(H, L, f ) with respect to perturbations on the underlying manifold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Stability of MNNs to LB Operator Perturbations</head><p>MNNs inherit stability to perturbations of the LB operator from the manifold filters that compose the filterbanks in each one of their layers. This result is stated in general formencompassing both absolute and relative perturbations-in the following theorem. </p><formula xml:id="formula_56">(L)f − h(L )f L 2 (M) ≤ C per f L 2 (M) , it holds that Φ(H, L, f ) − Φ(H, L , f ) L 2 (M) ≤ LF L−1 C per f L 2 (M) .</formula><p>Proof. See Section B in supplementary material.</p><p>Theorem 4 reflects that the stability of the MNN is affected by the hyperparameters of the MNN architecture and the stability constant of the manifold filters C per . More explicitly, the stability bound grows linearly with the number of layers L and exponentially with the number of features F where the rate is determined by L. This stability result also shows that there is a linear dependence on the stability constant C per of manifold filters h(L) and the perturbation size . As we have shown in Section III-A and III-B, the stability constant is determined by the form of the perturbations (Definition 3 or Definition 7) as well as the spectrum separation achieved by the specific manifold filters (Definition 5 or Definition 9) with corresponding Lipschitz conditions (Definition 6 or Definition 10). We address the specific cases as follows. </p><formula xml:id="formula_57">C per = πN α + A h , (<label>33</label></formula><formula xml:id="formula_58">)</formula><p>where </p><formula xml:id="formula_59">C per = πM γ + B h , (<label>34</label></formula><formula xml:id="formula_60">)</formula><p>where M is the size of the γ-separated spectrum partition [cf. <ref type="bibr">Definition 8]</ref>.</p><p>Proof. The conclusions follow directly from Theorem 4 combined with Theorem 2 or Theorem 3 under the corresponding assumptions.</p><p>Combining Theorem 4 with Proposition 5, we observe that α-FDT manifold filters with Lipschitz continuity can be composed to construct MNNs which are stable to absolute perturbations; while γ-FRT manifold filters with integral Lipschitz continuity can be composed to construct MNNs which are stable relative perturbations of the LB operator. Explicitly, by inserting the stability constant C per in <ref type="bibr" target="#b22">(23)</ref>, we see that other than the perturbation size , there are three terms that determine the stability of MNNs. The first term is LF L−1 , which, as we have already discussed, is decided by the number of layers and filters in the MNN architecture. This term arises due to the propagation of the underlying operator perturbations across all the manifold filters in all layers of the MNN. The second term is πN/α or πM/γ, which results from the deviations of the eigenfunctions as well as from the frequency response variations within the same eigenvalue partition. Finally, the third term, A h or B h , is given by the Lipschitz or integral Lipschitz constants which are decided during the filter design or the training process. It is important to note that the stability constant C per brings along the trade-off between stability and discriminability. However, unlike manifold filters, MNNs can be both stable and discriminative. This arises from the effects of nonlinear activation functions, as we discuss in further detail in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Stability of MNNs to Manifold Deformations</head><p>In Theorem 4 and Proposition 5, we established the conditions under which MNNs are stable to either absolute or relative perturbations of the LB operator as defined in Definitions 3 and 7. Since a manifold deformation τ (x) : M → M, with dist(x, τ (x)) = and J(τ * ) = I + ∆, translates into both an absolute and a relative perturbation of the Laplace-Beltrami operator, in order for the MNN to be stable to this deformation we need to meet all of these conditions in items 1 and 2 of Proposition 5. I.e., the manifold filters need to be both α-FDT and γ-FRT, and both Lipschitz continuous and integral Lipschitz continuous. The spectrum can be made to be both α-separated and γ-separated by making sure the eigenvalues in different partitions satisfy both <ref type="bibr" target="#b19">(20)</ref> and <ref type="bibr" target="#b26">(27)</ref>. Assuming that all of these conditions are met, we can state our main result-that MNNs are stable to deformations of the manifold-as follows.</p><p>Theorem 5 Let M be a compact embedded manifold with LB operator L and f be manifold signal. We construct Φ(H, L, f ) as a MNN on M (32) where the filters h(L) are α-FDT [cf. <ref type="bibr">Definition 5]</ref>, α/λ 1 -FRT [cf. <ref type="bibr">Definition 9]</ref>, A h -Lipschitz [cf. <ref type="bibr">Definition 6]</ref> and B h -integral Lipschitz [cf. <ref type="bibr">Definition 10]</ref>. Consider a deformation on M as τ (x) : M → M where dist(x, τ (x)) = and J(τ * ) = I + ∆ with ∆ F = and min(α/λ 1 , α, 2). Then under Assumptions 1 and 2 it holds that</p><formula xml:id="formula_61">Φ(H, L, f ) − Φ(H, L , f ) L 2 (M) = O( ) f L 2 (M) . (35)</formula><p>Together, Theorem 1 and Theorem 4 imply that MNNs are stable to the manifold deformations υ introduced in the beginning of this section. This is because these deformations spawn a perturbation of the LB operator that consists of both an absolute and a relative perturbation. For stability to hold, the filters that make up the layers of the MNN need to be α-FDT [cf. <ref type="bibr">Definition 5]</ref>, γ-FRT [cf. <ref type="bibr">Definition 9]</ref>, Lipschitz [cf. <ref type="bibr">Definition 6]</ref> and integral Lipschitz [cf. <ref type="bibr">Definition 10]</ref>. We can propose an easier special case to relate α and γ by utilizing the spectrum property of LB operator. By setting the α-FDT filter with α = γλ 1 , eigenvalues λ i , λ i+1 ∈ Λ k (α) would lead to λ i , λ i+1 ∈ Λ l (γ) due to the fact that</p><formula xml:id="formula_62">λ i+1 − λ i ≤ α = γλ 1 ≤ γλ i ,<label>(36)</label></formula><p>with λ 1 indexed as the smallest eigenvalue in the spectrum. The requirement that the filter be α-FDT can be removed as long as λ 1 &gt; 0 and α = γλ 1 , since a γ-FRT filter is always γλ 1 -FDT, i.e. α-FDT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSIONS</head><p>Stability vs. discriminability tradeoff. In both stability theorems for manifold filters (Theorems 2, 3) and in the stability theorem for MNNs (Theorem 4), the stability bounds depend on the frequency partition threshold (α or γ), the number of total partitions (N or M ) and the Lipschitz continuity constant (A h or B h ). The frequency partition threshold and the number of partitions have a combined effect on stability. As indicated by Definitions 4 and 8, a larger frequency threshold leads to a smaller number of singletons, as eigenvalues that would otherwise be separated for small thresholds end up being grouped when the threshold is large. While a large frequency threshold results in a larger number of partitions that contain more than one eigenvalue, the total number of partitions (N or M ) either stays the same or decreases because the number of eigenvalues does not exceed the number of partitions [cf. Proposition 3 or 4]. Thus, a larger frequency threshold and a smaller number of partitions both lead to a smaller stability bound. Simultaneously, a large frequency threshold makes the spectrum separated more sparsely. Therefore, a large number of eigenvalues are amplified in a similar manner, which makes the filter function less discriminative. The Lipschitz constant (A h or B h ) affects stability and discriminability in similar ways. Smaller Lipschitz constants decrease the stability bound, but lead to smoother filter functions which give similar frequency responses to different eigenvalues. Hence, in both manifold filters and MNNs we observe a trade-off between stability and discriminability. Nevertheless, in MNNs this trade-off is alleviated due to the presence of nonlinearities as discussed below.</p><p>Pointwise nonlinearity. As demonstrated by Propositions 3 and 4, large eigenvalues of LB operator tend to be grouped together in one large group and share similar frequency responses. This is part of the reason why manifold filters have a stability-discriminability tradeoff, which implies that they cannot be stable and discriminative at the same time. However, in MNNs this problem is circumvented with the addition of nonlinearities. Nonlinearities have the effect of scattering the spectral components all over the eigenvalue spectrum. In the MNN, they mix the frequency components by spilling spectral components associated with the large eigenvalues onto the smaller eigenvalues, where they can then be discriminated by the manifold filters in the following layer. This is consistent with the role of nonlinear activation functions in graph neural networks (GNNs) n <ref type="bibr" target="#b18">[19]</ref>, which can be see as instantiations of MNNs on discrete samples of the manifold as further discussed in Section VI.</p><p>Comparison with graphons. The graphon is another infinitedimensional model that can represent the limit of convergent sequences of graphs, and a series of works have proved stability of graphon neural networks and the transferability of GNNs sampled from them <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Manifolds are however more powerful, because they can represent the limit of graphs with both bounded and unbounded degrees <ref type="bibr" target="#b36">[37]</ref>-the graphon is only the limit of sequences of dense graphs <ref type="bibr" target="#b37">[38]</ref>. Moreover, embedded manifolds in high-dimensional spaces are more realistic geometric models in a number of application scenarios, such as point clouds, 3D shape segmentation and classification. Other important differences are that (i) the stability analysis on graphon models in <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b35">[36]</ref> focuses on deformations to the adjacency matrix of the graph, which can be translated directly as perturbations of the graphon operator, and that (ii) in the case of graphons, only an absolute perturbation model makes sense since given that the graphon spectrum is bounded a relative perturbation can always be bounded by an absolute perturbation. Meanwhile, deformations to the manifold domain translate into a combination of absolute and relative perturbations of the LB operator, and the fact that the LB operator spectrum is unbounded makes the effects of absolute and relative perturbations distinct, especially in the high-frequency domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. FROM MANIFOLD NEURAL NETWORKS TO GRAPH NEURAL NETWORKS</head><p>MNNs are built from manifold convolutional filters (Definition 1) operating on a continuous manifold and over an infinite time horizon. This makes it impractical to implement directly the architecture described by <ref type="bibr" target="#b31">(32)</ref> in applications. In this section, we discuss how MNNs are implemented in practice over a set of discrete samples from the manifold in a finite and discrete time frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Discretization in the Space Domain</head><p>In practice, the explicit form of the manifold and of its LB operator are unknown. What we typically have access to is a point cloud representation of the manifold, i.e., a discrete set of sampling points. From these points' coordinates, the structure of the manifold is approximated by a geometric or a nearest neighbor graph <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b38">[39]</ref>. The LB operator is then approximated by the graph Laplacian, which can be shown to converge to the LB operator as the number of sampling points grows <ref type="bibr" target="#b38">[39]</ref>  <ref type="bibr" target="#b27">[28]</ref>.</p><p>Explicitly, suppose that X = {x 1 , x 2 , . . . , x n } is a set of n points sampled i.i.d. from measure µ of manifold M, which is embedded in R N . We can construct a complete weighted symmetric graph G n by taking the sampled points to be the vertices of the graph and setting the edge weights based on the Euclidean distance between pairs of points. Specifically, the weight w ij associated with edge (i, j) is given by</p><formula xml:id="formula_63">w ij = 1 n 1 t n (4πt n ) k/2 exp − x i − x j 2 4t n , (<label>37</label></formula><formula xml:id="formula_64">)</formula><p>where x i −x j is the Euclidean distance between points x i and x j while t n is a parameter associated with the chosen Gaussian kernel <ref type="bibr" target="#b36">[37]</ref>. The adjacency matrix A n ∈ R n×n is thus defined as [A n ] ij = w ij for 1 ≤ i, j ≤ n and the corresponding graph Laplacian matrix L n <ref type="bibr" target="#b39">[40]</ref> is given by</p><formula xml:id="formula_65">L n = diag(A n 1) − A n .<label>(38)</label></formula><p>We interpret L n the Laplacian operator of the constructed graph G n . Similarly, we define a uniform sampling operator</p><formula xml:id="formula_66">P n : L 2 (M) → L 2 (G n ) to sample manifold signals.</formula><p>Given a manifold signal f , we can use operator P n to sample graph signals x n ∈ R n as</p><formula xml:id="formula_67">x n = P n f with [x n ] i = f (x i ), x i ∈ X,<label>(39)</label></formula><p>where the i-th entry of the graph signal x n is the manifold signal f evaluated at the sample point x i .</p><p>In Proposition 1, we have shown that the manifold filter h is parametric on the LB operator. Therefore, we can also parameterize h by the discrete graph Laplacian operator L n , i.e.,</p><formula xml:id="formula_68">z n = ∞ 0 h(t)e −tLn dtx n = h(L n )x n , x n , z n ∈ R n , (<label>40</label></formula><formula xml:id="formula_69">)</formula><p>where z n , the output of the filter, is now a discrete graph signal. By cascading these discrete filters operated on graph G n and pointwise nonlinearities layer after layer, we can then approximate the MNN on G n as</p><formula xml:id="formula_70">x p l = σ   F l−1 q=1 h pq l (L n )x q l−1   ,<label>(41)</label></formula><p>where h pq l (L n ) maps the q-th feature in the l − 1-th layer to the p-th feature in the l-th layer, 1 ≤ q ≤ F l−1 and 1 ≤ p ≤ F l , and F l denotes the number of features in the l-th layer (we have dropped the subscript n in x p l and x q l−1 for simplicity). After gathering the filter functions in the set H, this neural network can be represented more succinctly as Φ(H, L n , x).</p><p>Equation ( <ref type="formula" target="#formula_70">41</ref>) is a consistent approximation of the MNN because, as n goes to infinity, the discrete graph Laplacian operator L n of the graph G n converges to the LB operator L of the manifold M, and the sampled graph signal x n converges to the manifold signal f <ref type="bibr" target="#b36">[37]</ref>. These facts combinely imply that the output of the neural network on the graph G n converges to the output of the neural network on the continuous manifold as stated in the following.</p><p>Proposition 6 Let X = {x 1 , x 2 , ...x n } be n points sampled i.i.d. from measure µ of d-dimensional manifold M ⊂ R N , with corresponding sampling operator P n <ref type="bibr" target="#b38">(39)</ref>. Let G n be a discrete graph approximation of M constructed from X as in <ref type="bibr" target="#b36">(37)</ref> with t n = n −1/(d+2+α) and α &gt; 0. Let Φ(H, •, •) be a neural network parametrized either by the LB operator L of the manifold M (32) or the graph Laplacian operator L n of G n . It holds that <ref type="bibr" target="#b41">(42)</ref> with the limit taken in probability.</p><formula xml:id="formula_71">lim n→∞ Φ(H, L n , P n f ) − P n Φ(H, L, f ) L 2 (Gn) = 0,</formula><p>Proof. See Section D in supplementary material.</p><p>This proposition provides theoretical support to state that neural networks constructed from the discrete Laplacian L n converge to MNN and thus can inherit the stability properties of the MNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discretization in the Time Domain</head><p>In order to learn an MNN <ref type="bibr" target="#b31">(32)</ref>, we need to learn the manifold convolutional filters h pq l . This means that we need to learn the impulse responses h(t) in Definition 1. However, learning continuous functions h is computationally infeasible, so we sample h over fixed intervals of duration T s and parameterize the filter with coefficients h k = h(kT s ), k = 0, 1, 2 . . . . Setting the sampling interval to T s = 1 for simplicity, the discrete-time manifold convolution can be written as</p><formula xml:id="formula_72">h(L)f (x) = ∞ k=0 h k e −kL f (x)<label>(43)</label></formula><p>where {h k } ∞ k=0 are called the filter coefficients or taps. Yet, learning ( <ref type="formula" target="#formula_73">44</ref>) is still impractical because there is an uncountable number of parameters h k . To address this, we fix a time horizon of K time steps and rewrite <ref type="bibr" target="#b43">(44)</ref> as</p><formula xml:id="formula_73">h(L)f (x) = K−1 k=0 h k e −kL f (x)<label>(44)</label></formula><p>which can be seen as a finite impulse response (FIR) filter with shift operator e −L . Indeed, the frequency response of this filter [cf. <ref type="bibr">Proposition 2]</ref> is given by ĥ</p><formula xml:id="formula_74">(λ) = K−1 k=0 h k e −kλ .<label>(45)</label></formula><p>Combining ( <ref type="formula" target="#formula_68">40</ref>) and ( <ref type="formula" target="#formula_73">44</ref>), we can bring the discretization over the spatial and time domains together to rewrite the convolution operation on the discretized manifold and in the discrete-time domain, explicitly,</p><formula xml:id="formula_75">z n = h(L n )x n = K−1 k=0 h k e −kLn x n .<label>(46)</label></formula><p>Equation ( <ref type="formula" target="#formula_75">46</ref>) recovers the definition of the graph convolution <ref type="bibr" target="#b4">[5]</ref> with graph shift operator e −Ln . This means that in practice we implement MNNs as graph neural networks (GNNs). Therefore, the stability behavior of the GNN can be seen as a proxy for the stability behavior of the MNN. We will leverage this idea in the numerical experiments of Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. NUMERICAL EXPERIMENTS</head><p>Dataset. We evaluate our MNN stability results on the Model-Net10 <ref type="bibr" target="#b40">[41]</ref> classification problem. The dataset contains 3991 meshed CAD models from 10 categories for training and 908 models for testing. For each model, 300 points are uniformly randomly sampled from all points of the model to form the point cloud. Each point is characterized by the 3D coordinates as features. We formulate the problem by modeling a dense graph neural network model to approximate MNN. Each node in the graph can be modeled as the sampling point and each edge weight is constructed based on the distance between each pair of nodes. In this work our goal is to identify the CAD model for chairs as is illustrated in Figure <ref type="figure" target="#fig_7">3</ref> with the models for chair labeled as 1 and the others as 0. We deform the underlying manifold structure by adding random perturbations to the coordinates of the sampling points. By comparing the differences of the classification error rates, we aim to show that MNNs with Lipschitz continuous and integral Lipschitz continuous manifold filters are stable via looking into the performance of the approximated GNNs.</p><p>Neural network architectures. We build dense graphs to approximate the point cloud models. We use the coordinates of each point as node features. By connecting a point with all the other points in the point cloud, the edge weight is defined based the distance between every two points and a Gaussian kernel. The Laplacian matrix is calculated for each input point cloud model. We implement different architectures, including Graph Filters (GF) and Graph Neural Networks (GNN) with 1 and 2 layers, to solve the classification problem. The architectures with a single layer contain F 0 = 3 input features which are the 3d coordinates of each point, F 1 = 64 output features and K = 5 filter taps. While the architectures with 2 layers has another layer with F 2 = 32 features and 5 filter taps. We use the ReLU as nonlinearity. The learned graph filters are not regularized in architectures with 'NoPel' while graph filters in the other architectures are both Lipschitz and integral Lipschitz.</p><p>All architectures also include a linear readout layer mapping the final output features to a binary scalar that estimates the classification.</p><p>Discriminability experiment. We train all the architectures with an ADAM optimizer <ref type="bibr" target="#b41">[42]</ref> with learning rate set as 0.005 and decaying factors as 0.9, 0.999 by minimizing the entropy loss. The training point cloud models are divided in batches of 10 over 40 epochs. We run 5 random point samplings for all the architectures and we show the average classification error rates across these realizations as well as the standard deviation in Table <ref type="table" target="#tab_1">I</ref>. We can observe that with the use of non-linearity, Graph Neural Networks perform better compared with Graph Filters. Architectures with more layers learn more accurate models which also leads to better performances.  Stability experiment. We test the same trained Graph Neural Networks and Graph Filters with 2 layers on perturbed test point cloud models with different perturbation levels. We perturb the test point clouds by adding a Gaussian random variable with mean and variance 2 to each coordinate of every sampling point, which can be seen as a deformation of the underlying manifold. We measure the stability by computing the difference between the error rates achieved based on the original test point cloud models and the perturbed ones. In Figure <ref type="figure" target="#fig_5">5</ref>, we see that this difference increases when the perturbations become larger, but overall the differences are small. We also observe that Graph    To further verify the discriminability under perturbations, we trained and tested the architectures with perturbed dataset. We can see from Table <ref type="table" target="#tab_3">II</ref> that both GNN and GF can identify the chair model with small error rates while the error rates grow slightly with the increase of perturbation levels. GNNs still outperform GFs in discriminablity with the help of nonlinearity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSIONS</head><p>In this paper, we have defined manifold convolutions and manifold neural networks. We prove that the deformations on  the embedded submanifolds can be represented as a form of perturbations to the Laplace-Beltrami operator. Considering the infinite dimensionality of LB operators, we import the definition of frequency difference threshold filters and frequency ratio threshold filters to help separate the spectrum. By assigning similar frequency responses to the eigenvalues that are close enough, these filters can be proved to be stable under absolute and relative perturbations to the LB operator respectively with Lipschitz continuous assumptions. While the manifold filters need to trade-off between the stability and discriminability. MNNs composed with layers of manifold filters and pointwise nonlinearities can be proved to be stable to absolute and relative perturbations to the LB operators. While the frequency mixing brought by pointwise nonlinearity can help with the discriminability. We conclude that the MNNs are thus both stable to deformations and discriminative. We also show the discretizations of MNNs in both spatial and time domains to make our proposed MNNs implementable. We finally verified our results numerically with a point cloud classification problem with ModelNet10 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Definition 1 and Convolutional Filters in Continuous Time</head><p>The manifold convolution in Definition 1 can also be motivated with a connection to linear time invariant filters. This requires that we consider the differential equation</p><formula xml:id="formula_76">∂u(x, t) ∂t = ∂ ∂x u(x, t).<label>(47)</label></formula><p>This is a one-sided wave equation and it is therefore not an exact analogous of the diffusion equation in (6) -this would require that the second derivative be used in the right of (47). The important observation to make here is that the exponential of the derivative operator is a time shift so that we can write u(x, t) = e t∂/∂x f (x) = f (x − t). This is true because e t∂/∂x f (x) and f (x−t) are both solutions of (47). It then follows that Definition 1 particularized to (47) yields the convolution definition</p><formula xml:id="formula_77">g(x) = ∞ 0 h(t)e t∂/∂x f (x) dt. = ∞ 0 h(t)f (x − t) dt. (<label>48</label></formula><formula xml:id="formula_78">)</formula><p>This is the standard definition of time convolutions. The frequency representation result in Proposition 2 holds for (48) and it implies that standard convolutional filters in continuous time are completely characterized by the frequency response in Definition 2. The more standard definition of a filter's frequency response as the Fourier transform of the impulse response h(t) -as opposed to the Laplace transform we use in Definition 2 -suffices because complex exponentials e jw are an orthonormal basis of eigenfunctions of the derivative operator with associated eigenvalues jω.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Theorem 1</head><p>Based on equation ( <ref type="formula" target="#formula_22">16</ref>) and the definition in <ref type="bibr" target="#b4">(5)</ref>, the operation carried out on the deformed manifold data f can be written as</p><formula xml:id="formula_79">−L f (x) = (∇ • ∇)f (τ (x)) (49) = (J(τ * ) T ∇ τ • J(τ * ) T ∇ τ )f (τ (x)).<label>(50)</label></formula><p>The equality in (50) results from the chain rule of gradient operator where ∇ τ is denoted as the intrinsic gradient around τ (x) in the tangent space T τ (x) M. By replacing J(τ * ) = I + ∆ the inner product term, (50) can be rewritten as</p><formula xml:id="formula_80">J(τ * ) T ∇ τ • J(τ * ) T ∇ τ = ∇ τ •∇ τ + 2(∆ T ∇ τ • ∇ τ ) + ∆ T ∇ τ • ∆ T ∇ τ .<label>(51)</label></formula><p>With L = −∇ τ • ∇ τ , the perturbed operator is</p><formula xml:id="formula_81">L − L = 2(∆ T ∇ τ • ∇ τ ) + ∆ T ∇ τ • ∆ T ∇ τ (52) = 2 ∆ F (∇ τ • ∇ τ ) + ∆ 2 F (∇ τ • ∇ τ ) + A. (<label>53</label></formula><formula xml:id="formula_82">)</formula><p>From ( <ref type="formula">52</ref>) to (53), we extract the relative term and use A to represent the compliment terms. This leads to E = ∆ 2 F + 2 ∆ F , as the relative perturbation term, the norm of which is bounded by the leading term as O( ). The norm of the compliment term therefore can be written as</p><formula xml:id="formula_83">A = E(∇ τ • ∇ τ ) − 2(∆ T ∇ τ • ∇ τ ) − ∆ T ∇ τ • ∆ T ∇ τ (54) ≤ 2 ∆ F (∇ τ • ∇ τ ) − 2(∆ T ∇ τ • ∇ τ ) + ∆ 2 F (∇ τ • ∇ τ ) − ∆ T ∇ τ • ∆ T ∇ τ ,<label>(55)</label></formula><p>which can be also bounded by the leading terms as O( ) combining with the boundedness of the gradient field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proof of Theorem 2</head><p>In the following, we denote •, • L 2 (M) as •, • and • L 2 (M) as • for simplicity. We start by bounding the norm difference between two outputs of filter functions on operators L and L defined in <ref type="bibr" target="#b14">(15)</ref> as</p><formula xml:id="formula_84">h(L)f − h(L )f = ∞ i=1 ĥ(λ i ) f, φ i φ i − ∞ i=1 ĥ(λ i ) f, φ i φ i . (56)</formula><p>We denote the index of partitions that contain a single eigenvalue as a set K s and the rest as a set K m . We can decompose the filter function as ĥ(λ) = h (0) (λ) + l∈Km h (l) (λ) with</p><formula xml:id="formula_85">h (0) (λ) = ĥ(λ) − l∈Km ĥ(C l ) λ ∈ [Λ k (α)] k∈Ks 0 otherwise (57) h (l) (λ) =    ĥ(C l ) λ ∈ [Λ k (α)] k∈Ks ĥ(λ) λ ∈ Λ l (α) 0 otherwise (58)</formula><p>where C l is some constant in Λ l (α). We can start by analyzing the output difference of h (0) (λ). With the triangle inequality, the norm difference can then be written as</p><formula xml:id="formula_86">∞ i=1 h (0) (λ i ) f, φ i φ i − h (0) (λ i ) f, φ i φ i = ∞ i=1 h (0) (λ i ) f, φ i φ i − h (0) (λ i ) f, φ i φ i + h (0) (λ i ) f, φ i φ i − h (0) (λ i ) f, φ i φ i (59) ≤ ∞ i=1 h (0) (λ i ) f, φ i φ i − h (0) (λ i ) f, φ i φ i + ∞ i=1 h (0) (λ i ) f, φ i φ i − h (0) (λ i ) f, φ i φ i (60) ≤ ∞ i=1 h (0) (λ i ) f, φ i φ i − f, φ i φ i + f, φ i φ i − f, φ i φ i + ∞ i=1 (h (0) (λ i ) − h (0) (λ i )) f, φ i φ i (61) ≤ ∞ i=1 h (0) (λ i ) f, φ i (φ i − φ i ) + ∞ i=1 h (0) (λ i ) f, φ i − φ i φ i + ∞ i=1 (h (0) (λ i ) − h (0) (λ i )) f, φ i φ i<label>(62)</label></formula><p>For the first term in (62), we employ Lemma 2 and therefore we have σ = λ i and ω = λ i , for λ i ∈ [Λ k (α)] k∈Ks we can have</p><formula xml:id="formula_87">φ i − φ i ≤ π 2 A α − = π 2 α − . (<label>63</label></formula><formula xml:id="formula_88">)</formula><p>Here d can be seen as d = min λi∈Λ k (α),λj ∈Λ l (α),k =l |λ i − λ j |.</p><p>Combined with the fact that |λ i − λ j | &gt; α and</p><formula xml:id="formula_89">|λ i − λ i | ≤ for all λ i ∈ Λ k (α), λ j ∈ Λ l (α), k = l, we have d ≥ α − .</formula><p>With Cauchy-Schwartz inequality, we have the first term in (62) bounded as</p><formula xml:id="formula_90">∞ i=1 h (0) (λ i ) f, φ i (φ i − φ i ) ≤ ∞ i=1 |h (0) (λ i )|| f, φ i | φ i − φ i ≤ N s π 2(α − ) f . (<label>64</label></formula><formula xml:id="formula_91">)</formula><p>The second term in (62) is bounded as</p><formula xml:id="formula_92">∞ i=1 h (0) (λ i ) f, φ i − φ i φ i ≤ ∞ i=1 |h (0) (λ i )| φ i − φ i f ≤ N s π 2(α − ) f .<label>(65)</label></formula><p>These two bounds are obtained by noting that |h (0) (λ)| &lt; 1 and</p><formula xml:id="formula_93">h (0) (λ) = 0 for λ ∈ [Λ k (α)] k∈Km .</formula><p>The number of eigenvalues within [Λ k (α)] k∈Ks is denoted as N s . The third term in (62) can be bounded by the Lipschitz continuity of h combined with Lemma 1.</p><formula xml:id="formula_94">∞ i=1 (h (0) (λ i ) − h (0) (λ i )) f, φ i φ i 2 ≤ ∞ i=1 |h (0) (λ i ) − h (0) (λ i )| 2 | f, φ i | 2 ≤ ∞ i=1 A 2 h |λ i − λ i | 2 | f, φ i | 2 ≤ A 2 h 2 f 2 . (<label>66</label></formula><formula xml:id="formula_95">)</formula><p>Then we need to analyze the output difference of h (l) (λ), we can bound this as</p><formula xml:id="formula_96">h (l) (L)f − h (l) (L )f ≤ ( ĥ(C l ) + δ)f − ( ĥ(C l ) − δ)f ≤ 2δ f ,<label>(67)</label></formula><p>where h (l) (L) and h (l) (L ) are manifold filters with filter function h (l) (λ) on the LB operators L and L respectively.</p><p>Combining the filter functions, we can write</p><formula xml:id="formula_97">h(L)f − h(L )f = h (0) (L)f + l∈Km h (l) (L)f − h (0) (L )f − l∈Km h (l) (L )f (68) ≤ h (0) (L)f − h (0) (L )f + l∈Km h (l) (L)f − h (l) (L )f (69) ≤ N s π α − f + A h f + 2(N − N s )δ f ,<label>(70)</label></formula><p>which concludes the proof.</p><p>1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Materials</head><p>A. Proof of Proposition 3</p><p>Weyl's law <ref type="bibr" target="#b34">[35,</ref><ref type="bibr">Chapter 1]</ref> states that if (M, g) is a compact Riemannian manifold of dimension d, then</p><formula xml:id="formula_98">λ k = C 1 C 2/d d k V ol(M) 2/d ,<label>(71)</label></formula><p>where C d denotes the volume of the unit ball of R d and C 1 is an arbitrary constant. Therefore, if λ k+1 − λ k ≤ α, we have</p><formula xml:id="formula_99">(k + 1) 2/d − k 2/d ≤ α C 1 (V ol(M)C d ) 2/d (72)</formula><p>while the left side can be scaled down to</p><formula xml:id="formula_100">(k + 1) 2/d − k 2/d ≥ 2 d k 2/d−1 . This implies that k 2−d d ≤ αd(V ol(M)C d ) 2/d 2C 1 d 2−d ,<label>(73)</label></formula><p>with d &gt; 2, we can claim that for</p><formula xml:id="formula_101">k &gt; (αd/C 1 ) d/(2−d) (C d Vol(M)) 2/(2−d) ,</formula><p>it holds that λ k+1 − λ k ≤ α. Proof of Proposition 4 is similar and is also based on (71).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Theorem 3</head><p>The decomposition follows the same routine as (56) shows. By decomposing the filter function as ( <ref type="formula">74</ref>) and ( <ref type="formula" target="#formula_102">75</ref>), the norm difference can also be bounded separately.</p><formula xml:id="formula_102">h (0) (λ) = ĥ(λ) − l∈Km ĥ(C l ) λ ∈ [Λ k (γ)] k∈Ks 0 otherwise (74) h (l) (λ) =    ĥ(C l ) λ ∈ [Λ k (γ)] k∈Ks ĥ(λ) λ ∈ Λ l (γ) 0 otherwise (<label>75</label></formula><formula xml:id="formula_103">)</formula><p>where now ĥ(λ) = h (0) (λ) + l∈Km h (l) (λ) with K s defined as the group index set of singletons and K m the set of partitions that contain multiple eigenvalues. For manifold filter h (0) (L) with filter function h (0) (λ), the norm difference can also be written as</p><formula xml:id="formula_104">∞ i=1 h (0) (λ i ) f, φ i φ i − h (0) (λ i ) f, φ i φ i ≤ ∞ i=1 h (0) (λ i ) f, φ i (φ i − φ i ) + ∞ i=1 h (0) (λ i ) f, φ i − φ i φ i + ∞ i=1 (h (0) (λ i ) − h (0) (λ i )) f, φ i φ i . (<label>76</label></formula><formula xml:id="formula_105">)</formula><p>The difference of the eigenvalues due to relative perturbations can be similarly addressed by Lemma 3.</p><p>The first two terms of (76) rely on the differences of eigenfunctions, which can be derived with Davis-Kahan Theorem in Lemma 2, the difference of eigenfunctions can be written as</p><formula xml:id="formula_106">ELφ i = Eλ i φ i = λ i Eφ i ≤ λ i E φ i ≤ λ i . (77)</formula><p>The first term in (76) then can be bounded as</p><formula xml:id="formula_107">∞ i=1 h (0) (λ i ) f, φ i (φ i − φ i ) ≤ ∞ i=1 |h (0) (λ i )|| f, φ i | φ i − φ i ≤ i∈Ks πλ i 2d i f . (78) Because d i = min{|λ i − λ i−1 |, |λ i − λ i−1 |, |λ i+1 − λ i |, |λ i+1 − λ i |}, with Lemma 3 implied, we have |λ i − λ i−1 | ≥ |λ i − (1 + )λ i−1 |, (<label>79</label></formula><formula xml:id="formula_108">) |λ i − λ i−1 | ≥ |(1 − )λ i − λ i−1 |, (<label>80</label></formula><formula xml:id="formula_109">) |λ i+1 − λ i | ≥ |(1 − )λ i+1 − λ i |, (81) |λ i+1 − λ i | ≥ |λ i+1 − (1 + )λ i |.<label>(82)</label></formula><p>Combine with Lemma 3 and Definition 8,</p><formula xml:id="formula_110">d i ≥ γ + γ − : |(1 − )λ i+1 − λ i | ≥ |γλ i − λ i+1 | (83) = λ i 1 − λ i+1 λ i + γ − 1 (84) ≥ λ i (γ − + γ )<label>(85)</label></formula><p>This leads to the bound as</p><formula xml:id="formula_111">∞ i=1 h (0) (λ i ) f, φ i (φ i − φ i ) ≤ M s π 2(γ − + γ ) f .<label>(86)</label></formula><p>The second term in (76) can also be bounded as</p><formula xml:id="formula_112">∞ i=1 h (0) (λ i ) f, φ i − φ i φ i ≤ ∞ i=1 |h (0) (λ i )| φ i − φ i f ≤ M s π 2(γ − + γ ) f ,<label>(87)</label></formula><p>which similarly results from the fact that |h (0) (λ)| &lt; 1 and</p><formula xml:id="formula_113">h (0) (λ) = 0 for λ ∈ [Λ k (γ)] k∈Km . The number of eigenvalues within [Λ k (γ)] k∈Ks is denoted as M s . The third term in (76) is: ∞ i=1 (h (0) (λ i ) − h (0) (λ i )) f, φ i φ i 2 ≤ ∞ i=1 B h |λ i | (λ i + λ i )/2 2 f, φ i 2 ≤ 2B h 2 − 2 f 2 , (<label>88</label></formula><formula xml:id="formula_114">)</formula><p>with the use of Lemma 3 and Definition 10.</p><p>Then we need to analyze the output difference of h (l) (λ).</p><formula xml:id="formula_115">h (l) (L)f − h (l) (L )f ≤ ( ĥ(C l ) + δ)f − ( ĥ(C l ) − δ)f ≤ 2δ f .<label>(89)</label></formula><p>Combine the filter function, we could get</p><formula xml:id="formula_116">h(L)f − h(L )f = h (0) (L)f + l∈Km h (l) (L)f − h (0) (L )f − l∈Km h (l) (L )f (90) ≤ h (0) (L)f − h (0) (L )f + l∈Km h (l) (L)f − h (l) (L )f (91) ≤ M s π γ − + γ f + 2B h 2 − f + 2(M − M s )δ f ,<label>(92)</label></formula><p>which concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Theorem 4</head><p>To bound the output difference of MNNs, we need to write in the form of features of the final layer</p><formula xml:id="formula_117">Φ(H, L, f ) − Φ(H, L , f ) = F L q=1 f q L − F L q=1 f q L . (93)</formula><p>The output signal of layer l of MNN Φ(H, L, f ) can be written as</p><formula xml:id="formula_118">f p l = σ   F l−1 q=1 h pq l (L)f q l−1   .<label>(94)</label></formula><p>Similarly, for the perturbed L the corresponding MNN is Φ(H, L , f ) the output signal can be written as</p><formula xml:id="formula_119">f p l = σ   F l−1 q=1 h pq l (L )f q l−1   .<label>(95)</label></formula><p>The difference therefore becomes</p><formula xml:id="formula_120">f p l − f p l = σ   F l−1 q=1 h pq l (L)f q l−1   − σ   F l−1 q=1 h pq l (L )f q l−1   .<label>(96)</label></formula><p>With the assumption that σ is normalized Lipschitz, we have</p><formula xml:id="formula_121">f p l − f p l ≤ F l−1 q=1 h pq l (L)f q l−1 − h pq l (L )f q l−1 (97) ≤ F l−1 q=1 h pq l (L)f q l−1 − h pq l (L )f q l−1 .<label>(98)</label></formula><p>By adding and subtracting h pq l (L )f q l−1 from each term, combined with the triangle inequality we can get</p><formula xml:id="formula_122">h pq l (L)f q l−1 − h pq l (L )f q l−1 ≤ h pq l (L)f q l−1 − h pq l (L )f q l−1 + h pq l (L )f q l−1 − h pq l (L )f q l−1<label>(99)</label></formula><p>The first term can be bounded with (70) for absolute perturbations. The second term can be decomposed by Cauchy-Schwartz inequality and non-amplifying of the filter functions as</p><formula xml:id="formula_123">f p l − f p l ≤ F l−1 q=1 C per f q l−1 + F l−1 q=1 f q l−1 − f q l−1 ,<label>(100)</label></formula><p>where C per representing the constant in the stability bound of manifold filters. To solve this recursion, we need to compute the bound for f p l . By normalized Lipschitz continuity of σ and the fact that σ(0) = 0, we can get</p><formula xml:id="formula_124">f p l ≤ F l−1 q=1 h pq l (L)f q l−1 ≤ F l−1 q=1 h pq l (L) f q l−1 ≤ F l−1 q=1 f q l−1 ≤ l−1 l =1 F l F0 q=1 f q . (<label>101</label></formula><formula xml:id="formula_125">)</formula><p>Insert this conclusion back to solve the recursion, we can get</p><formula xml:id="formula_126">f p l − f p l ≤ lC per l−1 l =1 F l F0 q=1 f q . (<label>102</label></formula><formula xml:id="formula_127">)</formula><p>Replace l with L we can obtain</p><formula xml:id="formula_128">Φ(H, L, f ) − Φ(H, L , f ) ≤ F L q=1 LC per L−1 l =1 F l F0 q=1 f q . (103) With F 0 = F L = 1 and F l = F for 1 ≤ l ≤ L − 1, then we have Φ(H, L, f ) − Φ(H, L , f ) ≤ LF L−1 C per f ,<label>(104)</label></formula><p>which concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Lemmas and Propositions</head><p>Now we need to include two important lemmas to analyze the influence on eigenvalues and eigenfunctions caused by the perturbation.</p><p>Lemma 1 [Weyl's Theorem] The eigenvalues of LB operators L and perturbed L = L + A satisfy</p><formula xml:id="formula_129">|λ i − λ i | ≤ A , for all i = 1, 2 . . .<label>(105)</label></formula><p>Proof of Lemma 1.</p><p>The minimax principle asserts that</p><formula xml:id="formula_130">λ i (L) = max codimT =i−1 λ[L, T ] = max codimT ≤i−1 min u∈T, u =1 Lu, u .<label>(106)</label></formula><p>Then for any 1 ≤ i, we have</p><formula xml:id="formula_131">λ i (L ) = max codimT ≤i−1 min u∈T, u =1 (L + A)u, u<label>(107)</label></formula><p>= max</p><formula xml:id="formula_132">codimT ≤i−1 min u∈T, u =1 ( (Lu, u + Au, u ) (108) ≥ max codimT ≤i−1 min u∈T, u =1 Lu, u + λ 1 (A)) (109) = λ 1 (A) + max codimT ≤i−1 min u∈T, u =1 Lu, u<label>(110)</label></formula><formula xml:id="formula_133">= λ i (L) + λ 1 (A).<label>(111)</label></formula><p>Similarly, we can have λ</p><formula xml:id="formula_134">i (L ) ≤ λ i (L) + max k λ k (A). This leads to λ 1 (A) ≤ λ i (L ) − λ i (L) ≤ max k λ k (A)</formula><p>. This leads to the conclusion that:</p><formula xml:id="formula_135">|λ i − λ i | ≤ A . (<label>112</label></formula><formula xml:id="formula_136">)</formula><p>To measure the difference of eigenfunctions, we introduce the Davis-Kahan sin θ theorem as follows.</p><p>Lemma 2 (Davis-Kahan sin θ Theorem) Suppose the spectra of operators L and L are partitioned as σ Σ and ω Ω respectively, with σ Σ = ∅ and ω Ω = ∅. Then we have </p><formula xml:id="formula_137">E L (σ) − E L (ω) ≤ π 2 (L − L)E L (σ) d ≤ π 2 L − L d ,<label>(</label></formula><formula xml:id="formula_138">|λ i − λ i | ≤ |λ i |, for all i = 1, 2 . . .<label>(114)</label></formula><p>Proof of Lemma 3. With the assumption that L = L + EL, we have</p><formula xml:id="formula_139">λ i (L + EL) = max codimT ≤i−1 min u∈T, u =1 (L + EL)u, u (115) = max codimT ≤i−1 min u∈T, u =1 ( Lu, u + ELu, u ) (116) = λ i (L) + max codimT ≤i−1 min u∈T, u =1 ELu, u .<label>(117)</label></formula><p>For the second term, we have</p><formula xml:id="formula_140">| ELu, u | ≤ |E||L|u, u ≤ i |λ i (L)||ξ i | 2 = |L|u, u<label>(118)</label></formula><p>Therefore, we have</p><formula xml:id="formula_141">λ i (L + EL) ≤ λ i (L) + max codimT ≤i−1 min u∈T, u =1 |L|u, u = λ i (L) + |λ i (L)|,<label>(119)</label></formula><formula xml:id="formula_142">λ i (L + EL) ≥ λ i (L) − |λ i (L)|, (<label>120</label></formula><formula xml:id="formula_143">) λ i (L) − |λ i (L)| ≤ λ i (L + EL) ≤ λ i (L) + |λ i (L)|, (121)</formula><p>which concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Proof of Proposition 6</head><p>Considering that the discrete points {x 1 , x 2 , . . . , x n } are uniformly sampled from manifold M with measure µ, the empirical measure associated with dµ can be denoted as p n = 1 n n i=1 δ xi , where δ xi is the Dirac measure supported on x i . Similar to the inner product defined in the L 2 (M) space (4), the inner product on L 2 (G n ) is denoted as</p><formula xml:id="formula_144">u, v L 2 (Gn) = u(x)v(x)dp n = 1 n n i=1 u(x i )v(x i ). (<label>122</label></formula><formula xml:id="formula_145">)</formula><p>The norm in L 2 (G n ) is therefore u 2 L 2 (Gn) = u, u L 2 (Gn) , with u, v ∈ L 2 (M). For signals u, v ∈ L 2 (G n ), the inner product is therefore u, v L 2 (Gn) = 1</p><formula xml:id="formula_146">n n i=1 [u] i [v] i .</formula><p>We first import the existing results from <ref type="bibr" target="#b43">[44]</ref> which indicates the spectral convergence of the constructed Laplacian operator based on the graph G n to the LB operator of the underlying manifold.</p><p>Theorem 6 (Theorem 2.1 <ref type="bibr" target="#b43">[44]</ref>) Let X = {x 1 , x 2 , ...x n } be a set of n points sampled i.i.d. from a d-dimensional manifold M ⊂ R N . Let G n be a graph approximation of M constructed from X with weight values set as <ref type="bibr" target="#b36">(37)</ref> with t n = n −1/(d+2+α) and α &gt; 0. Let L n be the graph Laplacian of G n and L be the Laplace-Beltrami operator of M. Let λ n i be the i-th eigenvalue of L n and φ n i be the corresponding normalized eigenfunction. Let λ i and φ i be the corresponding eigenvalue and eigenfunction of L respectively. Then, it holds that lim n→∞ λ n i = λ i , lim n→∞ |φ n i (x j ) − φ i (x j )| = 0, j = 1, 2 . . . , n (123) where the limits are taken in probability.</p><p>With the definitions of neural networks on graph G n and manifold M, the output difference can be written as</p><formula xml:id="formula_147">Φ(H, L n , P n f ) − P n Φ(H, L, f )) = F L q=1 x q L − F L q=1 P n f q L ≤ F L q=1 x q L − P n f q L .<label>(124)</label></formula><p>By inserting the definitions, we have </p><formula xml:id="formula_148">≤ F l−1 q=1 h pq l (L n )x q l−1 − P n h pq l (L)f q l−1<label>(127)</label></formula><p>The difference can be further decomposed as h pq l (L n )x q l−1 − P n h pq l (L)f q l−1 ≤ h pq l (L n )x q l−1 − h pq l (L n )P n f q l−1 + h pq l (L n )P n f q l−1 − P n h pq l (L)f q l−1 (128) ≤ h pq l (L n )x q l−1 − h pq l (L n )P n f q l−1 + h pq l (L n )P n f q l−1 − P n h pq l (L)f q l−1 (129)</p><p>The first term can be bounded as x q l−1 − P n f q l−1 with the initial condition x 0 − P n f 0 = 0. The second term can be denoted as D n l−1 . With the iteration employed, we can have</p><formula xml:id="formula_149">Φ(H, L n , P n f ) − P n Φ(H, L, f ) ≤ L l=0 L l =l F l D n l .</formula><p>Therefore, we can focus on the difference term D n l , we omit the feature and layer index to work on a general form.</p><formula xml:id="formula_150">h(L n )P n f − P n h(L)f ≤ ∞ i=1 ĥ(λ n i ) P n f, φ n i Gn φ n i − ∞ i=1 ĥ(λ i ) f, φ i M P n φ i<label>(130)</label></formula><p>We decompose the α-FDT filter function as ĥ(λ) = h (0) (λ) + l∈Km h (l) (λ) as equations ( <ref type="formula">57</ref>) and (58) show. With the triangle inequality, we start by analyzing the output difference of h (0) (λ) as</p><formula xml:id="formula_151">∞ i=1 h (0) (λ n i ) P n f, φ n i Gn φ n i − ∞ i=1 h (0) (λ i ) f, φ i M P n φ i ≤ ∞ i=1 h (0) (λ n i ) − h (0) (λ i ) P n f, φ n i Gn φ n i + ∞ i=1 h (0) (λ i ) ( P n f, φ n i Gn φ n i − f, φ i M P n φ i ) .<label>(131)</label></formula><p>The first term in (131) can be bounded by leveraging the A h -Lipschitz continuity of the frequency response. From the convergence in probability stated in (123), we can claim that for each eigenvalue λ i ≤ λ M , for all i &gt; 0 and all δ i &gt; 0, there exists some N i such that for all n &gt; N i , we have</p><formula xml:id="formula_152">P(|λ n i − λ i | ≤ i ) ≥ 1 − δ i ,<label>(132)</label></formula><p>Letting i &lt; with &gt; 0, with probability at least M i=1 (1 − δ i ) := 1 − δ, the first term is bounded as</p><formula xml:id="formula_153">∞ i=1 (h (0) (λ n i ) − h (0) (λ i )) P n f, φ n i Gn φ n i ≤ ∞ i=1 |h (0) (λ n i ) − h (0) (λ i )|| P n f, φ n i Gn | φ n i (133) ≤ Ns i=1 A h |λ n i − λ i | P n f φ n i 2 ≤ N s A h ,<label>(134)</label></formula><p>for all n &gt; max i N i := N . The second term in (131) can be bounded combined with the convergence of eigenfunctions in (136) as</p><formula xml:id="formula_154">∞ i=1 h (0) (λ i ) ( P n f, φ n i Gn φ n i − f, φ i M P n φ i ) ≤ ∞ i=1 h (0) (λ i ) ( P n f, φ n i Gn φ n i − P n f, φ n i Gn P n φ i ) + ∞ i=1 h (0) (λ i ) ( P n f, φ n i Gn P n φ i − f, φ i M P n φ i )<label>(135)</label></formula><p>From the convergence stated in (123), we can claim that for some fixed eigenfunction φ i , for all i &gt; 0 and all δ i &gt; 0, there exists some N i such that for all n &gt; N i , we have</p><formula xml:id="formula_155">P(|φ n i (x j ) − φ i (x j )| ≤ i ) ≥ 1 − δ i , for all x j ∈ X.<label>(136)</label></formula><p>Therefore, letting i &lt; with &gt; 0, with probability at least M i=1 (1 − δ i ) := 1 − δ, for all n &gt; max i N i := N , the first term in (135) can be bounded as ∞ i=1 h (0) (λ i ) ( P n f, φ n i Gn φ n i − P n f, φ n i M P n φ i )</p><formula xml:id="formula_156">≤ Ns i=1 P n f φ n i − P n φ i ≤ N s ,<label>(137)</label></formula><p>because the frequency response is non-amplifying as stated in Assumption 1. The last equation comes from the definition of norm in L 2 (G n ). The second term in (135) can be written as</p><formula xml:id="formula_157">∞ i=1 h (0) (λ n i )( P n f, φ n i Gn P n φ i − f, φ i M P n φ i ) ≤ ∞ i=1 |h (0) (λ n i )| | P n f, φ n i Gn − f, φ i M | P n φ i . (<label>138</label></formula><formula xml:id="formula_158">)</formula><p>Because {x 1 , x 2 , • • • , x n } is a set of uniform sampled points from M, based on Theorem 19 in <ref type="bibr" target="#b44">[45]</ref> we can claim that there exists some N such that for all n &gt; N</p><formula xml:id="formula_159">P (| P n f, φ n i Gn − f, φ i M | ≤ ) ≥ 1 − δ,<label>(139)</label></formula><p>for all &gt; 0 and δ &gt; 0. Taking into consider the boundedness of frequency response |h (0) (λ)| ≤ 1 and the bounded energy P n φ i . Therefore, we have for all &gt; 0 and δ &gt; 0,</p><formula xml:id="formula_160">P M i=1 ĥ(λ n i ) ( P n f, φ n i Gn − f, φ i M ) P n φ i ≤ M ≥ 1 − δ,<label>(140)</label></formula><p>for all n &gt; N .</p><p>Combining the above results, we can bound the output difference of h (0) . Then we need to analyze the output difference of h (l) (λ) and bound this as P n h (l) (L)f − h (l) (L n )P n f ≤ ( ĥ(C l ) + δ)P n f − ( ĥ(C l ) − δ)P n f ≤ 2δ P n f , (141) where h (l) (L) and h (l) (L n ) are filters with filter function h (l) (λ) on the LB operator L and graph Laplacian L n respectively. Combining the filter functions, we can write</p><formula xml:id="formula_161">P n h(L)f − h(L n )P n f = P n h (0) (L)f + P n l∈Km h (l) (L)f − h (0) (L n )P n f − l∈Km h (l) (L n )Pf (142) ≤ P n h (0) (L)f − h (0) (L n )P n f + l∈Km P n h (l) (L)f − h (l) (L n )P n f . (<label>143</label></formula><formula xml:id="formula_162">)</formula><p>Above all, we can claim that there exists some N , such that for all n &gt; N , for all &gt; 0 and δ &gt; 0, we have</p><formula xml:id="formula_163">P( h(L n )P n f − P n h(L)f ≤ ) ≥ 1 − δ. (<label>144</label></formula><formula xml:id="formula_164">)</formula><p>With lim n→∞ D n l = 0 in high probability, this concludes the proof.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of an α-FDT filter. The x-axis stands for the spectrum with each sample representing an eigenvalue. The gray shaded areas show the grouping of the eigenvalues according to Definition 4. The red lines show a set of α-FDT filters that can discriminate each eigenvalue group.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Theorem 2 (</head><label>2</label><figDesc>Manifold filter stability to absolute perturbations) Consider a manifold M with LB operator L. Let h(L) be a α-FDT manifold filter [cf. Definition 5] and A h -Lipschitz [cf. Definition 6]. Consider an absolute perturbation L = L + A of the LB operator L [cf. Definition 3] where A = &lt; α. Then, under Assumption 1 it holds that h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of a γ-FRT filter. The x-axis stands for the spectrum with each sample representing an eigenvalue. The gray shaded area shows the grouping of the eigenvalues according to Definition 8. The red lines show a set of α-FDT filters that can discriminate each eigenvalue group.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Assumption 2 (</head><label>2</label><figDesc>Normalized Lipschitz activation functions) The activation function σ is normalized Lipschitz continous, i.e., |σ(a) − σ(b)| ≤ |a − b|, with σ(0) = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Theorem 4 (</head><label>4</label><figDesc>MNN stability) Consider a compact embedded manifold M with LB operator L. Let Φ(H, L, f ) be an L-layer MNN on M (32) with F 0 = F L = 1 input and output features and F l = F, l = 1, 2, . . . , L − 1 features per layer. The filters h(L) and nonlinearity functions satisfy Assumptions 1 and 2 respectively. Let L be the perturbed LB operator [cf. Definition 3 or Definition 7] with max{α, 2, |γ/1−γ|} . If the manifold filters satisfy h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Proposition 5</head><label>5</label><figDesc>With the same conditions as Theorem 4, consider the following perturbation models. 1) If the perturbed LB operator L is an absolute perturbation, i.e., L = L + A [cf. Definition 3] with A = and the manifold filters h(L) are α-FDT [cf. Definition 5] with α and A h -Lipschitz continuous [Definition 6] with δ = π /(2α), we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>N is the size of the α-separated spectrum partition [cf. Definition 4]. 2) If the perturbed LB operator L is a relative perturbation, i.e. L = L + EL [cf. Definition 7] with E = , and the manifold filters h(L) are γ-FRT [cf. Definition 9] with γ/(1 − γ) and B h -integral Lipschitz continuous [Definition 10] with δ = π /(2γ), we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Point cloud models with 300 sampling points in each model. Our goal is to identify chair models from other models such as toilet and table.</figDesc><graphic coords="11,317.37,90.40,59.52,59.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Difference between error rates on the original test dataset and the deformed one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Difference between error rates on the original test dataset and the deformed one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Lemma 3</head><label>3</label><figDesc>113) where d satisfies min x∈σ,y∈Ω |x − y| ≥ d and min x∈Σ,y∈ω |x − y| ≥ d.Proof of Lemma 2. See<ref type="bibr" target="#b42">[43]</ref>. The eigenvalues of LB operators L and perturbed L = L + EL with E = satisfy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>≤F</head><label></label><figDesc>x 0 = P n f as the input of the first layer. With a normalized Lipschitz nonlinearity, we havex p l − P n f p l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table I :</head><label>I</label><figDesc>Classification error rates for model 'chair' in the test dataset. Average over 5 data realizations. The number of nodes is N = 300.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table II :</head><label>II</label><figDesc>Classification error rates for model 'chair' with perturbed training and test dataset. Average over 5 data realizations. The number of nodes is N = 300.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stability of neural networks on riemannian manifolds</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 29th European Signal Processing Conference (EUSIPCO)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1845" to="1849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5473" to="5477" />
		</imprint>
	</monogr>
	<note>Stability of neural networks on manifolds to relative perturbations</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph signal processing: Overview, challenges, and applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kovačević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="808" to="828" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Autoregressive moving average graph filtering</title>
		<author>
			<persName><forename type="first">E</forename><surname>Isufi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Simonetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Leus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="274" to="288" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Graphs, convolutions, and neural networks: From graph filters to graph neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Isufi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Leus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="128" to="138" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for signals supported on graphs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Leus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1034" to="1049" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph neural networks: A review of methods and applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Open</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="57" to="81" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graph neural networks for social recommendation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Session-based recommendation with graph neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="346" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unfolding wmmse using graph neural networks for efficient power allocation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Segarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Wireless Communications</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised learning for asynchronous resource allocation in ad-hoc wireless networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eisen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8143" to="8147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Geometric deep learning: Grids, groups, graphs, geodesics, and gauges</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13478</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending highdimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discrete signal processing on graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sandryhaila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on signal processing</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1644" to="1656" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Nawab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Hernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signals &amp; systems. Pearson Educación</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Group invariant scattering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1331" to="1398" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stability properties of graph neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="5680" to="5695" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stability of graph scattering transforms</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8038" to="8048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stability and generalization of graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1539" to="1548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks via scattering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1046" to="1074" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph neural networks: Architectures, stability, and transferability</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="660" to="682" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graphon and graph neural network stability</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5255" to="5259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Transferability of graph neural networks: an extended graphon approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kutyniok</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.10096</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Transferability Properties of Graph Neural Networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Chamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04629</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convergence and stability of graph convolutional networks on large random graphs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vaiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improved spectral convergence rates for graph laplacians on epsilon-graphs and k-nn graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Trillos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13476</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transferability of spectral graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kutyniok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">272</biblScope>
			<biblScope unit="page" from="1" to="59" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Differential geometry and Lie groups: a computational perspective</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gallier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quaintance</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Springer Nature</publisher>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Analysis on manifolds via the Laplacian</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Canzani</surname></persName>
		</author>
		<ptr target="http://www.math.harvard.edu/canzani/docs/Laplacian.pdf" />
	</analytic>
	<monogr>
		<title level="s">Lecture Notes</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="41" to="44" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Field theory handbook: including coordinate systems, differential equations and their solutions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Spencer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Weyl&apos;s law: Spectral properties of the laplacian in mathematics and physics</title>
		<author>
			<persName><forename type="first">W</forename><surname>Arendt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nittka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Steiner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="71" />
		</imprint>
	</monogr>
	<note>Mathematical analysis of evolution, information, and complexity</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Mathematical analysis of evolution, information, and complexity</title>
		<author>
			<persName><forename type="first">W</forename><surname>Arendt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Schleich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graphon neural networks and the transferability of graph neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1702" to="1712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards a theoretical foundation for laplacianbased manifold methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1289" to="1308" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Large networks and graph limits</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lovász</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Mathematical Soc</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spectral convergence of graph laplacian and heat kernel reconstruction in l ∞ from random samples</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Dunson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="282" to="336" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A survey of graph laplacians</title>
		<author>
			<persName><forename type="first">R</forename><surname>Merris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear and Multilinear Algebra</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="19" to="31" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Notes on the sin2θ theorem</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seelmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Integral Equations and Operator Theory</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="579" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Convergence of laplacian eigenmaps</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Consistency of spectral clustering</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">Von</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="555" to="586" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
