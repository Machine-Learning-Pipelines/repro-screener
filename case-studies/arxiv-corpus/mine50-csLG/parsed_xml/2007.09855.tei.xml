<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Wide Boosting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-25">October 25, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>PhD</roleName><forename type="first">Michael</forename><forename type="middle">T</forename><surname>Horrell</surname></persName>
						</author>
						<title level="a" type="main">Wide Boosting</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-25">October 25, 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">45179AABF26E6248F7BA19C931AE01F7</idno>
					<idno type="arXiv">arXiv:2007.09855v4[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-10-31T05:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Gradient Boosting (GB) is a popular methodology used to solve prediction problems by minimizing a differentiable loss function, L. GB performs very well on tabular machine learning (ML) problems; however, as a pure ML solver it lacks the ability to fit models with probabilistic but correlated multi-dimensional outputs, for example, multiple correlated Bernoulli outputs. GB also does not form intermediate abstract data embeddings, one property of Deep Learning that gives greater flexibility and performance on other types of problems. This paper presents a simple adjustment to GB motivated in part by artificial neural networks. Specifically, our adjustment inserts a matrix multiplication between the output of a GB model and the loss, L. This allows the output of a GB model to have increased dimension prior to being fed into the loss and is thus "wider" than standard GB implementations. We call our method Wide Boosting (WB) and show that WB outperforms GB on mult-dimesional output tasks and that the embeddings generated by WB contain are more useful in downstream prediction tasks than GB output predictions alone.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Gradient Boosting (GB), first discussed in general in <ref type="bibr" target="#b5">[6]</ref>, is a popular methodology to build prediction models. Specifically, GB finds a function, f , restricted to a class of functions, F, that attempts to minimize L(Y, f (X))</p><p>for L a differentiable loss function and Y ∈ R n×d and X ∈ R n×p and f (x) ∈ R n×q . For the most common L, q = d. Gradient Boosting accomplishes this by composing f out of additive, iterative adjustments, a i . f 0 can be initialized to be a constant function. Then</p><formula xml:id="formula_1">f i+1 = f i + η i a i<label>(2)</label></formula><p>where η i is a scalar and a i approximates or is otherwise related to − ∂L(Y,fi(X)) ∂fi(X)</p><p>; thus, GB can be viewed as a kind of gradient descent in function space.</p><p>[3] tabulated results from Kaggle, a prediction competition website, and found around 60% of winning solutions posted in 2015 used a particular software package implementing GB, XGBoost <ref type="bibr" target="#b2">[3]</ref>. In a review of the 2015 KDD Cup, <ref type="bibr" target="#b1">[2]</ref> further remarks on the effective and popular use of XGBoost in that competition. Since 2015, other software packages relying on GB have been developed and open-sourced by researchers at Microsoft (LightGBM <ref type="bibr" target="#b8">[9]</ref>) and Yandex (CatBoost <ref type="bibr" target="#b3">[4]</ref>). LightGBM has also been effectively used in several Kaggle prediction competitions <ref type="bibr" target="#b13">[14]</ref>.</p><p>In this paper, we develop a method that generalizes standard GB frameworks that we call Wide Boosting (WB). This method introduces a matrix, β, multiplied to f (X) so that the output dimension of f (X), q, can be arbitrary. The output of the usual GB model is therefore an intermediate embedding of the input data. Writing WB in terms of the overall loss, WB estimates f and β in attempt to minimize L(Y, f (X)β).</p><p>We have implemented Wide Boosting in a Python package, wideboost (available via pypi.org or at https://github.com/mthorrell/wideboost). Notably, wideboost is able to use existing GB packages as a backend; thus we can take advantage of these highly optimized packages. Currently the XGBoost and LightGBM backends are implemented for wideboost. The XGBoost backend is currently the only backend supporting multi-dimesion responses (apart from multinomial response which is supported natively by XGBoost and LightGBM).</p><p>As a broader interpretation, Wide Boosting is exactly analogous to treating f (X) as the output of the hidden layer in a dense, one-hidden-layer neural network. From this perspective, f (X) embeds the rows of X in a q-dimensional space prior to being processed for prediction. Other works combine the powerful neural network and tree fitting methodologies in different ways ( <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b14">[15]</ref>). However, these works adjust the base methodologies and introduce further complexities to merge the approaches. Wide Boosting leverages both boosting and neural network architecture methodologies without significant adjustment and hence is able to take direct advantage of improvements in worldleading implementations of boosting while gaining some properties of neural networks. As one new method combining two popular prediction paradigms, Wide Boosting potentially points to more research areas where nodes of computational networks have new structures and methods to fit them. As an additional example, <ref type="bibr" target="#b7">[8]</ref> indicates that the weights in a feed-forward neural networks can be usefully fit using boosting.</p><p>It should also be noted that the simple β multiplication isn't the only way to merge GB and neural networks. Indeed GB only requires a differentiable function relating inputs to a response (preferably twice differentiable to use XGBoost and LightGBM as a backend); thus, for example, gradient boosted decision trees can be put as the first layer in any feed-forward neural network.</p><p>The rest of this paper is structured as follows. Section 2 details Wide Boosting, its parameters and necessary calculations. Section 3 reviews numerical experiments using wideboost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Wide boosting</head><p>Let f (X) ∈ R n×q and let β ∈ R q×d . We fit f and β in attempt to minimize</p><formula xml:id="formula_2">L(Y, f (X)β).<label>(3)</label></formula><p>With this formulation, q is the output dimension of f (X), and β, using matrix multiplication, gives f (X)β a dimension that matches Y . If we think of β as part of the loss function, wide boosting is a form of gradient boosting and can make use of existing theory and methods to find f . Namely f can be additive decision trees, the usual method of estimating f in GB models. If L(Y, •) is convex in its second argument, L(Y, •β) is convex also. This property further preserves some convergence properties of note in <ref type="bibr" target="#b6">[7]</ref>. For major existing implementations of gradient boosting (XGBoost <ref type="bibr" target="#b2">[3]</ref>, LightGBM <ref type="bibr" target="#b8">[9]</ref>, CatBoost <ref type="bibr" target="#b3">[4]</ref>), wide boosting can be implemented by simply providing these frameworks with the right gradient and hessian calculations. We give general calculations of gradients and hessians in Appendix 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Constructing and estimating β</head><p>We tried a few methods of initializing β and have left these methods as hyperparameters in the wideboost package. Using an identity matrix appended to a uniform random matrix or a purely uniform random matrix make up our base β initializations. A further adjustment can be made to normalize the columns of these matrices in order to decouple the "wideness" of the network from the usual step parameter, η. Note that when β = I and q = d, WB is equivalent to GB.</p><p>Like f , β can also be estimated. β can be estimated in the same manner that one can use boosting to estimate linear regression coefficients. β is sequentially updated each round by fitting adjustment coefficients to − ∂L(Y,fi(X)) ∂fi(X)</p><p>. Many other methods could be used to estimate β, but fitting β also via boosting is convenient because the same intermediate gradients get calculated when estimating f (X) as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Computational Scaling with q</head><p>Given our main implementation of WB makes use of XGBoost or LightGBM software packages as computational backends, we inherit both the optimizations employed in those packages and their limitations. Specifically both XGBoost and LightGBM fit additional, independent trees when the output of f has additional dimensions. Thus, WB computational complexity scales linearly with the number of added dimensions. For example, if q = 2d, a single round of WB fits twice as many trees a a round fitting a GB model on the same dataset. Multi-dimension trees exist and can be used to fit WB models <ref type="bibr" target="#b4">[5]</ref>; however, initial tries at using multi-dimension-output trees for WB gave more time-efficient but worse overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Numerical experiments</head><p>We repurpose the MNIST dataset <ref type="bibr" target="#b11">[12]</ref> to investigate three properties of WB:</p><p>1. Can WB improve on multi-dimension output problems where the dimensions are not independent? We treat the usual multinomial outcome as separate bernoulli outcomes. WB, by not treating the multiple outcomes as independent, significantly outperforms GB which, in current implementations, treats outcomes as independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Can WB make useful intermediate embeddings?</head><p>To look at this we use a transfer learning setup similar to the experimental setup in <ref type="bibr" target="#b12">[13]</ref>. An "upstream" model is trained to create embeddings. Quality of those embeddings is evaluated on a "downstream" dataset. Embeddings using WB outperform using GB predictions as an embedding (a technique better known as stacking).</p><p>Embeddings from WB are shown to contain more information than GB for the modified MNIST task we consider.</p><p>3. Does WB add value as an additional hyperparameter on usual GB problems? Paired with other hyperparameter tuning, WB was able to get to lower loss values faster than GB. This does not appear to always be the case, however. Unsurprisingly, vanilla GB is sometimes best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-dimension output learning</head><p>We repeatedly (n = 10) fit WB and GB models on samples of the MNIST dataset, using hyperopt to find appropriate hyperparameter values for each WB and GB model. To shorten computation, each iteration samples new training and validation sets of size 3,000 and 2,000 respectively from the MNIST training set. The test set is a sample of size 1,000 from the usual MNIST test set. Features are a sample of size 200 from the usual 28×28 = 784 features. hyperopt is given 25 chances per trial to find good hyperparameters. XGBoost as a backend was given 20 early stopping rounds (based on validation performance) with 500 maximum boost rounds. The loss calculated is average log-loss across the 10 dimension one-hot response vector. We treat the dimensions as separate binary outcomes (as opposed to the usual multinomial outcome) in order to simulate a dataset with a multivariate response and unknown dependence across the responses.</p><p>Across the 10 trials, WB makes use of the dependence across the response dimensions to achieve a statistically significant lower average log-los (see Table <ref type="table" target="#tab_0">1</ref>).</p><p>As part of the hyperparamerter optimization, WB could have chosen plain GB (where q = d and β = I). However, the hyperparameter optimization consistently chose q &gt; d. For all ten trials q &gt; d, and the average value for q was 14.</p><p>Also of note is that WB seems to be relatively computationally efficient with a lower average estimation time. This is not necessarily an intuitive result because a single round of WB where q &gt; d will fit more trees than a single round in GB (for example, if q = 14, 40% more trees are fit in WB compared to GB). WB stopped earlier than GB for several trials. WB had 5 trials using less than 52% of the trees used in the GB trials, and one trial where WB used 10% of the trees used for GB. WB achieved a lower test loss in that trial as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Embedding Quality</head><p>Like the experiment in 3.1, we treat the MNIST response as separate binary outcomes. Following a similar experiment format to <ref type="bibr" target="#b12">[13]</ref>, we run several trials (n = 10) consisting of an "upstream" dataset used to fit an initial model on half of the binary outputs. This initial model is then used on a "downstream" dataset to generate embeddings of the downstream data for use in predictions on the other half of the binary outputs. The quality of the embedding is then judged by evaluating prediction performance on the downstream dataset using only the embeddings generated from the upstream model on the downstream data. Each trial generates new upstream and downstream datasets by sampling. Upstream datasets are of size [3,000, 2,500, 2,000] for the train, validation and test sets respectively. Downstream datasets are of size [500, 1,500, 1,000] for the train, valiation and test sets respectively. Each trial also samples 5 of the 10 response dimensions for the upstream dataset and leaves the remaining 5 as the response of the downstream dataset. We again use hyperopt to optimize the models at both the upstream and downstream stages. The hyperparamerter optimization is given 15 tries for each trial.</p><p>To ensure a fair comparison between the WB and GB embeddings, the downstream model is simply a set of GB models (one for each output dimension) using either the WB or GB embeddings as inputs. To emphasize, even though the WB trials use WB for the upstream model, the downstream model is always a GB model.</p><p>Based on downstream test set performance, the embeddings made by WB outperform using GB logits as embeddings in a downstream model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">WB as an additional Hyperparameter</head><p>If we employ WB head-to-head against GB on the classical MNIST prediction problem (this time treating the response properly as a multinomial), the advantages of WB are less clear. Specifying q &gt; d and leaving all other parameters the same gives better models for the same number of boosting rounds (Figure <ref type="figure">1(a)</ref>), but if you control for the extra number of trees, the benefit of WB disappears (Figure <ref type="figure">1(b)</ref>).</p><p>Nonetheless it is interesting that the two lines in Figure <ref type="figure">1</ref>(b) are so close. On this dataset at least, the repeated gradient corrections employed in GB doesn't seem to have any special advantages over fitting more trees simultaneously and adding them together. Figure <ref type="figure">1</ref>: Test error rates for differing q on the the MNIST dataset. Larger q shows better performance for earlier boosting rounds (subfigure (a)). However, when controlling for number of trees this difference disappears (subfigure (b)).</p><p>More generally, although on the classic task for MNIST the advantage of WB is none or negilgible, specific trials in Section 3.1 showed that WB employed with hyperparameter tuning can give better performance using less compute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Wide Boosting is a relatively straightforward generalization of Gradient Boosting and introduces greater flexibility to the GB methodology. Specifically, WB outperformed GB on a prediction problem where the response is multi-dimensional and not statistically independent across the dimensions. WB also introduces the concept of abstract embeddings to GB and outperformed GB on a transfer learning task. As in deep learning, these embeddings can be used for transfer learning or may be of direct interest themselves.</p><p>The python package wideboost makes WB publicly available. Users of the popular packages XGBoost and LightGBM in particular may find benefit in using wideboost because wideboost currently wraps both packages, supplying those packages the gradient and hessian calculations.</p><p>Given that WB can also be thought of in a computational network or Deep Learning context, there may be similar, useful network architectures that can take advantage of Gradient Boosting or other powerful prediction methodologies not traditionally related to Deep Learning. For example, future work may use gradient boosted decision trees as the first layer in deep neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Appendix: Gradient and Hessian calculations</head><p>Consider the i-th row of f (X), denoted f</p><formula xml:id="formula_3">[i] . We calculate ∂L ∂f [i] and ∂L ∂f [i] ∂f T [i]</formula><p>, where L is defined in (3). If we denote G [i] and H [[i]] as the gradient and hessian of L with respect to the i-th row of f (X) when L is defined by (1), the gradient and hessians for L with respect to f [i] when L is defined by (3) can be computed using the chain rule:</p><formula xml:id="formula_4">∂L ∂f [i] = G [i] β T ∂L ∂f [i] ∂f T [i] = βH [[i]] β T</formula><p>Applying these formulas to common loss functions we find the following example calculations for gradients and hessians for Wide Boosting. Note we will use the subscript <ref type="bibr">[i]</ref> to denote the i-th row of a matrix. Let P ∈ R n×d where</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.0.1">Regression -Squared Error</head><formula xml:id="formula_5">Y ∈ R n×1 , f (X) ∈ R n×q and β ∈ R q×1 . L(Y, f (X)β) = 1 2 Y − f (X)β 2 ∂L ∂f [i] = (f (X)β − Y ) [i] β T ∂ 2 L ∂f [i] ∂f T [i] = ββ T</formula><formula xml:id="formula_6">P [i] = exp[f (X)β] [i] exp[f (X)β] [i] 1 L(Y, f (X)β) = tr(Y log(P ) T ) ∂L ∂f [i] = (P − Y ) [i] β T ∂ 2 L ∂f [i] ∂f T [i] = β diag[P [i] ] − P T [i] P [i] β T</formula><p>where the exp[•] and log[•] functions are applied elementwise to the input matrix and 1 ∈ R d×1 is a vector of all ones. The diag[•] function takes a row vector and returns a square, diagonal matrix with diagonal elements corresponding to the elements of the vector.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>5.0. 2</head><label>2</label><figDesc>Binary Classification -Log-lossY ∈ {0, 1} n×1 , f (X) ∈ R n×q and β ∈ R q×1 Let P ∈ R n×1 where P [i] = exp[f (X)β] [i] 1 + exp[f (X)β] [i] L(Y, f (X)β) = Y T log(P ) + (1 − Y ) T log(1 − P ) ∂L ∂f [i] = (P − Y ) [i] β T ∂ 2 L ∂f [i] ∂f T [i] = P [i] − P 2 [i] ββ Twhere the exp[•] and log[•] functions are applied elementwise to the input matrix and 1 ∈ R n×1 is a vector of all ones.5.0.3 Multi-class Classification -Log-lossY ∈ {0, 1} n×d , f (X) ∈ R n×q and β ∈ R q×d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>* t-stat is significant at the 0.05 level. *** t-stat is significant at the 0.01 level.</figDesc><table><row><cell cols="2">Model Avg test log-loss</cell><cell cols="5">t-stat Avg trial time (s) t-stat Avg n trees t-stat</cell></row><row><cell>WB</cell><cell cols="2">0.0510 (0.005) -3.4***</cell><cell>1224 (544)</cell><cell>-2.1*</cell><cell>3394</cell><cell>0.1</cell></row><row><cell>GB</cell><cell>0.0582 (0.004)</cell><cell></cell><cell>1631 (279)</cell><cell></cell><cell>3294</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>**** t-stat is significant at the 0.001 level.</figDesc><table><row><cell cols="2">Model Avg downstream test log-loss</cell><cell>t-stat</cell></row><row><cell>WB</cell><cell cols="2">0.202 (0.02) -4.8****</cell></row><row><cell>GB</cell><cell>0.236 (0.01)</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Balestriero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07360</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Neural decision trees. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The present and the future of the kdd cup competition: an outsider&apos;s perspective</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bekkerman</surname></persName>
		</author>
		<ptr target="https://www.linkedin.com/pulse/present-future-kdd-cup-competition-outsiders-ron-bekkerman" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</title>
				<meeting>the 22nd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dorogush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ershov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11363</idno>
		<title level="m">Catboost: gradient boosting with categorical features support</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fast multi-class image annotation with random subwindows and multiple output randomized trees</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Marée</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Geurts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="196" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generalized boosting algorithms for convex optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grubb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on International Conference on Machine Learning</title>
				<meeting>the 28th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1209" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Gradient fitting for deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Horrell</surname></persName>
		</author>
		<ptr target="https://mthorrell.github.io/horrellblog/2019/04/28/gradient-fitting-for-deep-learning" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2020" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lightgbm: A highly efficient gradient boosting decision tree</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deepgbm: A deep learning framework distilled by gbdt for online prediction tasks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep neural decision forests</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fiterau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bulo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1467" to="1475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<title level="m">MNIST handwritten digit database</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cherepanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schwarzschild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Bruss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldblum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.15306</idno>
		<title level="m">Transfer learning with deep tabular models</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Lightgbm: Machine learning challenge winning solutions</title>
		<author>
			<persName><surname>Microsoft</surname></persName>
		</author>
		<ptr target="https://github.com/microsoft/LightGBM/tree/master/examples#machine-learning-challenge-winning-solutions" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Morozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06312</idno>
		<title level="m">Neural oblivious decision ensembles for deep learning on tabular data</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
